"""
Comprehensive Test Report Generator

Generates detailed test reports including RA compliance analysis, performance metrics,
cross-browser compatibility matrices, and production readiness assessments.

RA-Light Mode Implementation:
All report generation assumptions, data aggregation patterns, and analysis methodologies
are tagged for verification phase with comprehensive assumption tracking.
"""

import asyncio
import json
import os
import pytest
import time
import uuid
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import html

# Import project components
import sys
project_root = Path(__file__).parent.parent.parent / "src"
sys.path.insert(0, str(project_root))

from task_manager.database import TaskDatabase

# Import test modules
from test_ra_workflow import RAWorkflowTestSuite
from test_auto_switch import MultiSessionAutoSwitchTester
from test_ra_compliance import RAComplianceValidator
from test_cross_browser import CrossBrowserTestFramework

# Import test infrastructure
sys.path.insert(0, str(Path(__file__).parent.parent / "project_manager"))
from conftest import IntegrationTestDatabase


class ComprehensiveTestReportGenerator:
    """
    Comprehensive test report generator with multiple output formats.
    
    Generates detailed reports covering all aspects of the system validation
    including RA methodology compliance, performance benchmarks, cross-browser
    compatibility, and production readiness assessment.
    
    # #COMPLETION_DRIVE_IMPL: Report generation assumes comprehensive data collection
    # from all test suites and consistent result format across testing components
    """
    
    def __init__(self, database: TaskDatabase):
        """Initialize comprehensive test report generator."""
        self.database = database
        self.report_data = {}
        self.generation_start_time = time.time()
        
    async def generate_comprehensive_report(self, output_format: str = "html") -> Dict[str, Any]:
        """
        Generate comprehensive test validation report.
        
        # #COMPLETION_DRIVE_INTEGRATION: Report generation assumes all test suites
        # can be executed independently and their results can be aggregated coherently
        """
        report_start = time.time()
        
        report = {
            "report_id": f"comprehensive_report_{uuid.uuid4().hex[:8]}",
            "generated_at": datetime.utcnow().isoformat(),
            "format": output_format,
            "test_results": {},
            "analysis": {},
            "recommendations": [],
            "production_readiness": {},
            "errors": []
        }
        
        try:
            # Execute all test suites
            print("🔄 Running comprehensive test validation...")
            
            # Test Suite 1: RA Workflow Validation
            print("  📋 Running RA workflow validation...")
            ra_workflow_results = await self._run_ra_workflow_tests()
            report["test_results"]["ra_workflow"] = ra_workflow_results
            
            # Test Suite 2: Multi-Session Auto-Switch
            print("  🔄 Running multi-session auto-switch tests...")
            auto_switch_results = await self._run_auto_switch_tests()
            report["test_results"]["auto_switch"] = auto_switch_results
            
            # Test Suite 3: RA Methodology Compliance
            print("  ✅ Running RA compliance validation...")
            compliance_results = await self._run_compliance_tests()
            report["test_results"]["ra_compliance"] = compliance_results
            
            # Test Suite 4: Performance Analysis
            print("  📊 Running performance analysis...")
            performance_results = await self._run_performance_tests()
            report["test_results"]["performance"] = performance_results
            
            # Analysis Phase
            print("  🔍 Analyzing results...")
            report["analysis"] = await self._analyze_test_results(report["test_results"])
            
            # Production Readiness Assessment
            print("  🏁 Assessing production readiness...")
            report["production_readiness"] = self._assess_production_readiness(
                report["test_results"], 
                report["analysis"]
            )
            
            # Generate Recommendations
            report["recommendations"] = self._generate_recommendations(
                report["test_results"], 
                report["analysis"]
            )
            
            # Generate Output
            print(f"  📄 Generating {output_format.upper()} report...")
            if output_format == "html":
                report["html_content"] = self._generate_html_report(report)
            elif output_format == "json":
                report["json_content"] = json.dumps(report, indent=2)
            elif output_format == "markdown":
                report["markdown_content"] = self._generate_markdown_report(report)
            
            report["success"] = True
            
        except Exception as e:
            report["errors"].append({
                "phase": "report_generation",
                "error": str(e),
                "timestamp": time.time()
            })
            report["success"] = False
            
        report["generation_duration"] = (time.time() - report_start) * 1000
        return report
        
    async def _run_ra_workflow_tests(self) -> Dict[str, Any]:
        """
        Run RA workflow validation tests.
        
        # #COMPLETION_DRIVE_INTEGRATION: RA workflow testing assumes proper coordination
        # between database, WebSocket manager, and RA methodology components
        """
        try:
            from task_manager.api import ConnectionManager
            websocket_manager = ConnectionManager()
            ra_test_suite = RAWorkflowTestSuite(self.database, websocket_manager)
            
            workflow_results = await ra_test_suite.test_complete_ra_workflow()
            
            return {
                "success": workflow_results.get("success", False),
                "workflow_results": workflow_results,
                "test_summary": {
                    "phases_completed": len(workflow_results.get("phases", {})),
                    "total_duration": workflow_results.get("total_duration", 0),
                    "ra_compliance_score": workflow_results.get("ra_compliance", {}).get("compliance_score", 0),
                    "performance_targets_met": self._count_performance_targets_met(workflow_results)
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "test_summary": {
                    "phases_completed": 0,
                    "total_duration": 0,
                    "ra_compliance_score": 0,
                    "performance_targets_met": 0
                }
            }
            
    async def _run_auto_switch_tests(self) -> Dict[str, Any]:
        """
        Run multi-session auto-switch tests.
        
        # #COMPLETION_DRIVE_INTEGRATION: Auto-switch testing assumes proper session
        # management and WebSocket coordination without external dependencies
        """
        try:
            from task_manager.api import ConnectionManager
            websocket_manager = ConnectionManager()
            auto_switch_tester = MultiSessionAutoSwitchTester(self.database, websocket_manager)
            
            # Test with 3 sessions
            switch_results = await auto_switch_tester.test_multi_session_auto_switch(session_count=3)
            
            return {
                "success": switch_results.get("success", False),
                "switch_results": switch_results,
                "test_summary": {
                    "sessions_tested": switch_results.get("session_count", 0),
                    "coordination_tests": len(switch_results.get("coordination_tests", {})),
                    "total_duration": switch_results.get("total_duration", 0),
                    "performance_metrics": switch_results.get("performance_metrics", {})
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "test_summary": {
                    "sessions_tested": 0,
                    "coordination_tests": 0,
                    "total_duration": 0,
                    "performance_metrics": {}
                }
            }
            
    async def _run_compliance_tests(self) -> Dict[str, Any]:
        """
        Run RA methodology compliance validation.
        
        # #COMPLETION_DRIVE_IMPL: Compliance testing assumes measurable RA methodology
        # effectiveness criteria and consistent tag analysis across the codebase
        """
        try:
            compliance_validator = RAComplianceValidator(self.database)
            compliance_results = await compliance_validator.run_complete_compliance_validation()
            
            return {
                "success": compliance_results.get("overall_compliance", {}).get("meets_standards", False),
                "compliance_results": compliance_results,
                "test_summary": {
                    "compliance_tests": len(compliance_results.get("compliance_tests", {})),
                    "overall_score": compliance_results.get("overall_compliance", {}).get("overall_score", 0),
                    "compliance_level": compliance_results.get("overall_compliance", {}).get("compliance_level", "unknown"),
                    "total_duration": compliance_results.get("total_duration", 0)
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "test_summary": {
                    "compliance_tests": 0,
                    "overall_score": 0,
                    "compliance_level": "error",
                    "total_duration": 0
                }
            }
            
    async def _run_performance_tests(self) -> Dict[str, Any]:
        """
        Run performance analysis and benchmarking.
        
        # #COMPLETION_DRIVE_IMPL: Performance testing assumes realistic load patterns
        # and measurable performance criteria for production readiness assessment
        """
        try:
            # Create large dataset for performance testing
            print("    📊 Creating large test dataset...")
            performance_data = self.database.create_large_dataset_for_performance_testing(scale_factor=2)
            
            # Run performance measurements
            performance_metrics = {
                "dataset_creation": {
                    "total_projects": performance_data["total_projects"],
                    "total_tasks": performance_data["total_tasks"],
                    "creation_duration": performance_data["creation_duration"],
                    "tasks_per_second": performance_data["tasks_per_second"]
                },
                "query_performance": {
                    "sample_query_duration": performance_data["sample_query_duration"],
                    "performance_acceptable": performance_data["performance_acceptable"]
                }
            }
            
            # Test database operations under load
            load_test_results = await self._run_database_load_tests()
            performance_metrics["load_testing"] = load_test_results
            
            return {
                "success": performance_data["performance_acceptable"],
                "performance_data": performance_data,
                "performance_metrics": performance_metrics,
                "test_summary": {
                    "dataset_scale": performance_data["scale_factor"],
                    "total_items_created": performance_data["total_tasks"],
                    "average_query_time": performance_data["sample_query_duration"],
                    "performance_acceptable": performance_data["performance_acceptable"]
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "test_summary": {
                    "dataset_scale": 0,
                    "total_items_created": 0,
                    "average_query_time": float('inf'),
                    "performance_acceptable": False
                }
            }
            
    async def _run_database_load_tests(self) -> Dict[str, Any]:
        """
        Run database load testing with concurrent operations.
        
        # #COMPLETION_DRIVE_IMPL: Load testing assumes proper database connection handling
        # and realistic concurrent access patterns for production environment
        """
        load_start = time.time()
        
        try:
            # Test concurrent reads
            read_tasks = []
            for i in range(10):
                read_tasks.append(self._concurrent_database_read())
                
            read_results = await asyncio.gather(*read_tasks, return_exceptions=True)
            successful_reads = sum(1 for r in read_results if not isinstance(r, Exception))
            
            # Test concurrent writes
            write_tasks = []
            for i in range(5):
                write_tasks.append(self._concurrent_database_write(i))
                
            write_results = await asyncio.gather(*write_tasks, return_exceptions=True)
            successful_writes = sum(1 for r in write_results if not isinstance(r, Exception))
            
            return {
                "concurrent_reads": {
                    "attempted": len(read_tasks),
                    "successful": successful_reads,
                    "success_rate": successful_reads / len(read_tasks)
                },
                "concurrent_writes": {
                    "attempted": len(write_tasks),
                    "successful": successful_writes,
                    "success_rate": successful_writes / len(write_tasks)
                },
                "total_duration": (time.time() - load_start) * 1000
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "total_duration": (time.time() - load_start) * 1000
            }
            
    async def _concurrent_database_read(self) -> Dict[str, Any]:
        """Perform concurrent database read operation."""
        read_start = time.time()
        
        try:
            tasks = self.database.get_available_tasks(limit=50)
            return {
                "success": True,
                "tasks_retrieved": len(tasks),
                "duration": (time.time() - read_start) * 1000
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "duration": (time.time() - read_start) * 1000
            }
            
    async def _concurrent_database_write(self, index: int) -> Dict[str, Any]:
        """Perform concurrent database write operation."""
        write_start = time.time()
        
        try:
            # Create a test task
            task_id = self.database.create_task(
                epic_id=1,  # Use default epic
                title=f"Concurrent test task {index}",
                description=f"Load testing task created concurrently (index {index})",
                ra_metadata={
                    "test_generated": True,
                    "concurrent_index": index,
                    "load_test": True
                }
            )
            
            return {
                "success": True,
                "task_id": task_id,
                "duration": (time.time() - write_start) * 1000
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "duration": (time.time() - write_start) * 1000
            }
            
    def _count_performance_targets_met(self, workflow_results: Dict[str, Any]) -> int:
        """Count how many performance targets were met in workflow tests."""
        targets_met = 0
        
        phases = workflow_results.get("phases", {})
        for phase_name, phase_data in phases.items():
            if "target" in phase_data and "duration" in phase_data:
                if phase_data["duration"] <= phase_data["target"]:
                    targets_met += 1
                    
        return targets_met
        
    async def _analyze_test_results(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze aggregated test results for patterns and insights.
        
        # #COMPLETION_DRIVE_IMPL: Result analysis assumes consistent data structures
        # across all test suites and meaningful correlation patterns between metrics
        """
        analysis_start = time.time()
        
        analysis = {
            "overall_success_rate": 0,
            "performance_analysis": {},
            "ra_methodology_effectiveness": {},
            "system_stability": {},
            "scalability_assessment": {},
            "error_patterns": [],
            "strengths": [],
            "weaknesses": []
        }
        
        # Calculate overall success rate
        successful_suites = sum(
            1 for suite_results in test_results.values() 
            if suite_results.get("success", False)
        )
        total_suites = len(test_results)
        analysis["overall_success_rate"] = successful_suites / total_suites if total_suites > 0 else 0
        
        # Performance Analysis
        performance_data = test_results.get("performance", {})
        if performance_data.get("success"):
            perf_metrics = performance_data.get("performance_metrics", {})
            analysis["performance_analysis"] = {
                "database_performance": "good" if perf_metrics.get("query_performance", {}).get("performance_acceptable") else "needs_improvement",
                "load_handling": self._assess_load_handling(perf_metrics.get("load_testing", {})),
                "scalability_score": self._calculate_scalability_score(performance_data)
            }
        
        # RA Methodology Effectiveness
        ra_workflow = test_results.get("ra_workflow", {})
        ra_compliance = test_results.get("ra_compliance", {})
        
        if ra_workflow.get("success") and ra_compliance.get("success"):
            analysis["ra_methodology_effectiveness"] = {
                "workflow_integration": "excellent",
                "compliance_score": ra_compliance.get("test_summary", {}).get("overall_score", 0),
                "tag_coverage": "good",  # #SUGGEST_VALIDATION: Calculate actual tag coverage
                "system_prompt_effectiveness": "high"
            }
        
        # System Stability Assessment
        auto_switch = test_results.get("auto_switch", {})
        if auto_switch.get("success"):
            analysis["system_stability"] = {
                "multi_session_coordination": "stable",
                "websocket_reliability": "good",
                "session_isolation": "proper",
                "error_recovery": "functional"
            }
        
        # Identify Strengths and Weaknesses
        analysis["strengths"], analysis["weaknesses"] = self._identify_strengths_weaknesses(test_results)
        
        analysis["analysis_duration"] = (time.time() - analysis_start) * 1000
        return analysis
        
    def _assess_load_handling(self, load_test_data: Dict[str, Any]) -> str:
        """Assess load handling capability based on test results."""
        if not load_test_data:
            return "unknown"
            
        read_success_rate = load_test_data.get("concurrent_reads", {}).get("success_rate", 0)
        write_success_rate = load_test_data.get("concurrent_writes", {}).get("success_rate", 0)
        
        avg_success_rate = (read_success_rate + write_success_rate) / 2
        
        if avg_success_rate >= 0.95:
            return "excellent"
        elif avg_success_rate >= 0.80:
            return "good"
        elif avg_success_rate >= 0.60:
            return "acceptable"
        else:
            return "needs_improvement"
            
    def _calculate_scalability_score(self, performance_data: Dict[str, Any]) -> float:
        """Calculate scalability score based on performance metrics."""
        # #COMPLETION_DRIVE_IMPL: Scalability scoring assumes linear performance expectations
        # and proper correlation between dataset size and response times
        
        perf_data = performance_data.get("performance_data", {})
        if not perf_data:
            return 0.0
            
        tasks_per_second = perf_data.get("tasks_per_second", 0)
        query_time = perf_data.get("sample_query_duration", float('inf'))
        
        # Normalize metrics to 0-1 scale
        tasks_score = min(tasks_per_second / 100, 1.0)  # 100 tasks/sec = perfect score
        query_score = max(0, 1.0 - (query_time / 100))  # 100ms query = 0 score
        
        return (tasks_score + query_score) / 2
        
    def _identify_strengths_weaknesses(self, test_results: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """Identify system strengths and weaknesses from test results."""
        strengths = []
        weaknesses = []
        
        # Analyze each test suite
        for suite_name, suite_results in test_results.items():
            if suite_results.get("success"):
                if suite_name == "ra_workflow":
                    strengths.append("RA methodology integration is comprehensive and effective")
                elif suite_name == "auto_switch":
                    strengths.append("Multi-session coordination and auto-switch functionality works reliably")
                elif suite_name == "ra_compliance":
                    score = suite_results.get("test_summary", {}).get("overall_score", 0)
                    if score >= 0.9:
                        strengths.append(f"Exceptional RA methodology compliance ({score:.1%})")
                elif suite_name == "performance":
                    if suite_results.get("test_summary", {}).get("performance_acceptable"):
                        strengths.append("Database performance meets production requirements")
            else:
                error = suite_results.get("error", "Unknown error")
                if suite_name == "ra_workflow":
                    weaknesses.append(f"RA workflow integration issues: {error}")
                elif suite_name == "auto_switch":
                    weaknesses.append(f"Auto-switch functionality problems: {error}")
                elif suite_name == "ra_compliance":
                    weaknesses.append(f"RA methodology compliance gaps: {error}")
                elif suite_name == "performance":
                    weaknesses.append(f"Performance issues under load: {error}")
                    
        return strengths, weaknesses
        
    def _assess_production_readiness(self, test_results: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Assess overall production readiness based on test results and analysis.
        
        # #COMPLETION_DRIVE_IMPL: Production readiness assessment assumes weighted criteria
        # and measurable thresholds for different system quality aspects
        """
        readiness = {
            "overall_score": 0.0,
            "readiness_level": "not_ready",
            "critical_issues": [],
            "improvement_needed": [],
            "ready_components": [],
            "deployment_recommendation": "do_not_deploy"
        }
        
        # Calculate weighted scores
        weights = {
            "functionality": 0.30,  # Core functionality works
            "ra_methodology": 0.25,  # RA implementation quality
            "performance": 0.20,     # Performance under load
            "stability": 0.15,       # System stability
            "compliance": 0.10       # Standards compliance
        }
        
        scores = {}
        
        # Functionality Score
        overall_success_rate = analysis.get("overall_success_rate", 0)
        scores["functionality"] = overall_success_rate
        
        # RA Methodology Score
        ra_compliance = test_results.get("ra_compliance", {})
        if ra_compliance.get("success"):
            scores["ra_methodology"] = ra_compliance.get("test_summary", {}).get("overall_score", 0)
        else:
            scores["ra_methodology"] = 0.0
            
        # Performance Score
        performance = test_results.get("performance", {})
        if performance.get("success"):
            scores["performance"] = 1.0 if performance.get("test_summary", {}).get("performance_acceptable") else 0.5
        else:
            scores["performance"] = 0.0
            
        # Stability Score
        auto_switch = test_results.get("auto_switch", {})
        scores["stability"] = 1.0 if auto_switch.get("success") else 0.0
        
        # Compliance Score
        ra_workflow = test_results.get("ra_workflow", {})
        if ra_workflow.get("success"):
            compliance_score = ra_workflow.get("test_summary", {}).get("ra_compliance_score", 0)
            scores["compliance"] = compliance_score
        else:
            scores["compliance"] = 0.0
            
        # Calculate weighted overall score
        weighted_score = sum(scores[component] * weights[component] for component in weights.keys())
        readiness["overall_score"] = weighted_score
        
        # Determine readiness level and recommendations
        if weighted_score >= 0.9:
            readiness["readiness_level"] = "production_ready"
            readiness["deployment_recommendation"] = "deploy_with_monitoring"
        elif weighted_score >= 0.8:
            readiness["readiness_level"] = "mostly_ready"
            readiness["deployment_recommendation"] = "deploy_after_minor_fixes"
        elif weighted_score >= 0.7:
            readiness["readiness_level"] = "needs_improvement"
            readiness["deployment_recommendation"] = "address_issues_before_deploy"
        else:
            readiness["readiness_level"] = "not_ready"
            readiness["deployment_recommendation"] = "significant_work_needed"
            
        # Identify specific issues and ready components
        for component, score in scores.items():
            if score >= 0.8:
                readiness["ready_components"].append(component)
            elif score >= 0.6:
                readiness["improvement_needed"].append(component)
            else:
                readiness["critical_issues"].append(component)
                
        return readiness
        
    def _generate_recommendations(self, test_results: Dict[str, Any], analysis: Dict[str, Any]) -> List[str]:
        """Generate specific recommendations for improvement."""
        recommendations = []
        
        # Performance recommendations
        performance = test_results.get("performance", {})
        if not performance.get("success"):
            recommendations.append("Optimize database queries and implement connection pooling for better performance")
            
        # RA methodology recommendations
        ra_compliance = test_results.get("ra_compliance", {})
        if ra_compliance.get("success"):
            score = ra_compliance.get("test_summary", {}).get("overall_score", 0)
            if score < 0.9:
                recommendations.append("Improve RA tag coverage and system prompt effectiveness")
        else:
            recommendations.append("Address RA methodology compliance issues before production deployment")
            
        # Stability recommendations
        auto_switch = test_results.get("auto_switch", {})
        if not auto_switch.get("success"):
            recommendations.append("Fix multi-session coordination and WebSocket reliability issues")
            
        # General recommendations
        if analysis.get("overall_success_rate", 0) < 0.8:
            recommendations.append("Address failing test suites to improve overall system reliability")
            
        if not recommendations:
            recommendations.append("System shows excellent test results - consider adding more comprehensive edge case testing")
            
        return recommendations
        
    def _generate_html_report(self, report: Dict[str, Any]) -> str:
        """
        Generate HTML format comprehensive test report.
        
        # #COMPLETION_DRIVE_IMPL: HTML generation assumes proper escaping and structure
        # for safe display in browsers without security vulnerabilities
        """
        html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Comprehensive Test Validation Report</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        h3 {{ color: #7f8c8d; }}
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .summary-card {{ background: #ecf0f1; padding: 15px; border-radius: 5px; border-left: 4px solid #3498db; }}
        .success {{ border-left-color: #27ae60; background: #d5f4e6; }}
        .warning {{ border-left-color: #f39c12; background: #fef9e7; }}
        .error {{ border-left-color: #e74c3c; background: #fadbd8; }}
        .metric {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ font-size: 14px; color: #7f8c8d; margin-top: 5px; }}
        .test-result {{ margin: 15px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .test-success {{ background: #d5f4e6; border-color: #27ae60; }}
        .test-failure {{ background: #fadbd8; border-color: #e74c3c; }}
        .progress-bar {{ width: 100%; height: 20px; background: #ecf0f1; border-radius: 10px; overflow: hidden; }}
        .progress-fill {{ height: 100%; background: linear-gradient(90deg, #3498db, #2ecc71); transition: width 0.3s ease; }}
        table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #f8f9fa; font-weight: 600; }}
        .recommendations {{ background: #e8f4fd; padding: 20px; border-radius: 5px; border-left: 4px solid #3498db; }}
        .recommendations ul {{ margin: 10px 0; padding-left: 20px; }}
        .timestamp {{ color: #7f8c8d; font-size: 12px; margin-top: 20px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>🔍 Comprehensive Test Validation Report</h1>
        
        <div class="summary-grid">
            <div class="summary-card {self._get_success_class(report.get('production_readiness', {}).get('overall_score', 0))}">
                <div class="metric">{report.get('production_readiness', {}).get('overall_score', 0):.1%}</div>
                <div class="metric-label">Production Readiness</div>
            </div>
            <div class="summary-card {self._get_success_class(report.get('analysis', {}).get('overall_success_rate', 0))}">
                <div class="metric">{report.get('analysis', {}).get('overall_success_rate', 0):.1%}</div>
                <div class="metric-label">Test Success Rate</div>
            </div>
            <div class="summary-card">
                <div class="metric">{len(report.get('test_results', {}))}</div>
                <div class="metric-label">Test Suites</div>
            </div>
            <div class="summary-card">
                <div class="metric">{report.get('generation_duration', 0):.0f}ms</div>
                <div class="metric-label">Report Generation Time</div>
            </div>
        </div>
        
        <h2>📊 Test Results Summary</h2>
        {self._generate_test_results_html(report.get('test_results', {}))}
        
        <h2>🔍 Analysis</h2>
        {self._generate_analysis_html(report.get('analysis', {}))}
        
        <h2>🚀 Production Readiness Assessment</h2>
        {self._generate_production_readiness_html(report.get('production_readiness', {}))}
        
        <h2>💡 Recommendations</h2>
        <div class="recommendations">
            <ul>
                {"".join(f"<li>{html.escape(rec)}</li>" for rec in report.get('recommendations', []))}
            </ul>
        </div>
        
        <div class="timestamp">
            Generated: {report.get('generated_at', 'Unknown')} | Report ID: {report.get('report_id', 'N/A')}
        </div>
    </div>
</body>
</html>
"""
        return html_content
        
    def _get_success_class(self, score: float) -> str:
        """Get CSS class based on success score."""
        if score >= 0.8:
            return "success"
        elif score >= 0.6:
            return "warning"
        else:
            return "error"
            
    def _generate_test_results_html(self, test_results: Dict[str, Any]) -> str:
        """Generate HTML for test results section."""
        html_parts = []
        
        for suite_name, suite_results in test_results.items():
            success = suite_results.get("success", False)
            css_class = "test-success" if success else "test-failure"
            
            html_parts.append(f"""
            <div class="test-result {css_class}">
                <h3>{'✅' if success else '❌'} {suite_name.replace('_', ' ').title()}</h3>
                <p><strong>Status:</strong> {'Passed' if success else 'Failed'}</p>
                {self._format_test_summary(suite_results.get('test_summary', {}))}
            </div>
            """)
            
        return "".join(html_parts)
        
    def _format_test_summary(self, summary: Dict[str, Any]) -> str:
        """Format test summary as HTML."""
        if not summary:
            return "<p>No summary data available</p>"
            
        html_parts = ["<ul>"]
        for key, value in summary.items():
            formatted_key = key.replace('_', ' ').title()
            if isinstance(value, (int, float)):
                if key.endswith('_rate') or key.endswith('_score'):
                    html_parts.append(f"<li><strong>{formatted_key}:</strong> {value:.1%}</li>")
                elif key.endswith('_duration'):
                    html_parts.append(f"<li><strong>{formatted_key}:</strong> {value:.0f}ms</li>")
                else:
                    html_parts.append(f"<li><strong>{formatted_key}:</strong> {value}</li>")
            else:
                html_parts.append(f"<li><strong>{formatted_key}:</strong> {html.escape(str(value))}</li>")
        html_parts.append("</ul>")
        
        return "".join(html_parts)
        
    def _generate_analysis_html(self, analysis: Dict[str, Any]) -> str:
        """Generate HTML for analysis section."""
        strengths = analysis.get("strengths", [])
        weaknesses = analysis.get("weaknesses", [])
        
        html = f"""
        <div class="summary-grid">
            <div class="summary-card success">
                <h4>✅ System Strengths</h4>
                <ul>
                    {"".join(f"<li>{html.escape(strength)}</li>" for strength in strengths)}
                </ul>
            </div>
            <div class="summary-card {'warning' if weaknesses else 'success'}">
                <h4>{'⚠️' if weaknesses else '✅'} Areas for Improvement</h4>
                {"<ul>" + "".join(f"<li>{html.escape(weakness)}</li>" for weakness in weaknesses) + "</ul>" if weaknesses else "<p>No significant weaknesses identified!</p>"}
            </div>
        </div>
        """
        
        return html
        
    def _generate_production_readiness_html(self, readiness: Dict[str, Any]) -> str:
        """Generate HTML for production readiness section."""
        overall_score = readiness.get("overall_score", 0)
        readiness_level = readiness.get("readiness_level", "unknown")
        
        html = f"""
        <div class="summary-card {self._get_success_class(overall_score)}">
            <div class="metric">{overall_score:.1%}</div>
            <div class="metric-label">Overall Readiness Score</div>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {overall_score * 100}%"></div>
            </div>
            <p><strong>Status:</strong> {readiness_level.replace('_', ' ').title()}</p>
            <p><strong>Recommendation:</strong> {readiness.get('deployment_recommendation', 'Unknown').replace('_', ' ').title()}</p>
        </div>
        """
        
        return html
        
    def _generate_markdown_report(self, report: Dict[str, Any]) -> str:
        """Generate Markdown format comprehensive test report."""
        # #COMPLETION_DRIVE_IMPL: Markdown generation assumes proper formatting
        # and GitHub-compatible syntax for documentation integration
        
        md_content = f"""# Comprehensive Test Validation Report

**Report ID:** {report.get('report_id', 'N/A')}  
**Generated:** {report.get('generated_at', 'Unknown')}  
**Format:** {report.get('format', 'markdown')}

## Executive Summary

- **Production Readiness:** {report.get('production_readiness', {}).get('overall_score', 0):.1%}
- **Test Success Rate:** {report.get('analysis', {}).get('overall_success_rate', 0):.1%}
- **Test Suites Executed:** {len(report.get('test_results', {}))}
- **Generation Time:** {report.get('generation_duration', 0):.0f}ms

## Test Results

"""
        
        # Add test results
        for suite_name, suite_results in report.get('test_results', {}).items():
            success = suite_results.get("success", False)
            status_icon = "✅" if success else "❌"
            
            md_content += f"### {status_icon} {suite_name.replace('_', ' ').title()}\n\n"
            md_content += f"**Status:** {'Passed' if success else 'Failed'}\n\n"
            
            summary = suite_results.get('test_summary', {})
            if summary:
                for key, value in summary.items():
                    formatted_key = key.replace('_', ' ').title()
                    if isinstance(value, (int, float)):
                        if key.endswith('_rate') or key.endswith('_score'):
                            md_content += f"- **{formatted_key}:** {value:.1%}\n"
                        elif key.endswith('_duration'):
                            md_content += f"- **{formatted_key}:** {value:.0f}ms\n"
                        else:
                            md_content += f"- **{formatted_key}:** {value}\n"
                    else:
                        md_content += f"- **{formatted_key}:** {value}\n"
            md_content += "\n"
        
        # Add recommendations
        md_content += "## Recommendations\n\n"
        for i, rec in enumerate(report.get('recommendations', []), 1):
            md_content += f"{i}. {rec}\n"
            
        return md_content


@pytest.mark.integration
@pytest.mark.ra_validation
@pytest.mark.performance
async def test_generate_comprehensive_report_html(integration_db):
    """Test comprehensive report generation in HTML format."""
    generator = ComprehensiveTestReportGenerator(integration_db.database)
    
    report = await generator.generate_comprehensive_report(output_format="html")
    
    # Validate report structure
    assert report["success"], f"Report generation failed: {report.get('errors', [])}"
    assert report["format"] == "html"
    assert "html_content" in report
    assert len(report["html_content"]) > 1000, "HTML report should be substantial"
    
    # Validate required sections
    assert "test_results" in report
    assert "analysis" in report
    assert "production_readiness" in report
    assert "recommendations" in report
    
    # Log report summary
    print(f"\n=== Comprehensive Test Report Generated ===")
    print(f"Format: {report['format']}")
    print(f"Success: {report['success']}")
    print(f"Test Suites: {len(report.get('test_results', {}))}")
    print(f"Production Readiness: {report.get('production_readiness', {}).get('overall_score', 0):.1%}")
    print(f"Generation Time: {report.get('generation_duration', 0):.0f}ms")


@pytest.mark.integration
@pytest.mark.ra_validation
async def test_generate_comprehensive_report_markdown(integration_db):
    """Test comprehensive report generation in Markdown format."""
    generator = ComprehensiveTestReportGenerator(integration_db.database)
    
    report = await generator.generate_comprehensive_report(output_format="markdown")
    
    # Validate report structure
    assert report["success"], f"Report generation failed: {report.get('errors', [])}"
    assert report["format"] == "markdown"
    assert "markdown_content" in report
    
    # Validate markdown content
    md_content = report["markdown_content"]
    assert "# Comprehensive Test Validation Report" in md_content
    assert "## Executive Summary" in md_content
    assert "## Test Results" in md_content
    assert "## Recommendations" in md_content
    
    print(f"\n=== Markdown Report Preview ===")
    print(md_content[:500] + "...")


if __name__ == "__main__":
    """Standalone test execution for report generation."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate comprehensive test reports")
    parser.add_argument("--format", choices=["html", "markdown", "json"], default="html")
    parser.add_argument("--output", help="Output file path")
    args = parser.parse_args()
    
    async def generate_standalone_report():
        """Generate report in standalone mode."""
        # Create temporary database for testing
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.db') as temp_db:
            from task_manager.database import TaskDatabase
            db = TaskDatabase(temp_db.name)
            
            # Create some test data
            db.create_test_project_structure(project_count=2, epics_per_project=2, tasks_per_epic=5)
            
            # Generate report
            generator = ComprehensiveTestReportGenerator(db)
            report = await generator.generate_comprehensive_report(output_format=args.format)
            
            if args.output:
                content_key = f"{args.format}_content"
                with open(args.output, 'w') as f:
                    f.write(report.get(content_key, "No content generated"))
                print(f"Report saved to {args.output}")
            else:
                print("=== Report Generated ===")
                print(f"Success: {report['success']}")
                print(f"Format: {report['format']}")
                if args.format == "markdown":
                    print(report.get("markdown_content", "No content"))[:1000]
    
    asyncio.run(generate_standalone_report())