
# Autogenerated by mlir-tblgen; don't manually edit.

from ._ods_common import _cext as _ods_cext
from ._ods_common import (
    equally_sized_accessor as _ods_equally_sized_accessor,
    get_default_loc_context as _ods_get_default_loc_context,
    get_op_results_or_values as _get_op_results_or_values,
    segmented_accessor as _ods_segmented_accessor,
)
_ods_ir = _ods_cext.ir
_ods_cext.globals.register_traceback_file_exclusion(__file__)

import builtins
from typing import Sequence as _Sequence, Union as _Union, Optional as _Optional


@_ods_cext.register_dialect
class _Dialect(_ods_ir.Dialect):
  DIALECT_NAMESPACE = "amdgpu"

@_ods_cext.register_operation(_Dialect)
class DPPOp(_ods_ir.OpView):
  r"""
  This operation represents DPP functionality in a GPU program.
   DPP provides the following operations:
  - Full crossbar in a group of four (`quad_perm`)
  - Wavefront shift left by one lane (`wave_shl`)
  - Wavefront shift right by one lane (`wave_shr`)
  - Wavefront rotate right by one lane (`wave_ror`)
  - Wavefront rotate left by one lane (`wave_rol`)
  - Row shift left by 1–15 lanes (`row_shl`)
  - Row shift right by 1–15 lanes (`row_shr`)
  - Row rotate right by 1–15 lanes (`row_ror`)
  - Reverse within a row (`row_mirror`)
  - Reverse within a half-row (`row_half_mirror`)
  - Broadcast the 15th lane of each row to the next row (`row_bcast`)
  - Broadcast lane 31 to rows 2 and 3 (`row_bcast`)
  """

  OPERATION_NAME = "amdgpu.dpp"

  _ODS_REGIONS = (0, True)

  def __init__(self, old, src, kind, *, permArgument=None, row_mask=None, bank_mask=None, bound_ctrl=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(old)
    operands.append(src)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["kind"] = (kind if (
    isinstance(kind, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('AMDGPU_DPPPermAttr')) else
      _ods_ir.AttrBuilder.get('AMDGPU_DPPPermAttr')(kind, context=_ods_context))
    if permArgument is not None: attributes["permArgument"] = (permArgument if (
        isinstance(permArgument, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('anonymous_578')) else
          _ods_ir.AttrBuilder.get('anonymous_578')(permArgument, context=_ods_context))
    if row_mask is not None: attributes["row_mask"] = (row_mask if (
        isinstance(row_mask, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(row_mask, context=_ods_context))
    if bank_mask is not None: attributes["bank_mask"] = (bank_mask if (
        isinstance(bank_mask, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(bank_mask, context=_ods_context))
    if bound_ctrl is not None: attributes["bound_ctrl"] = (bound_ctrl if (
        isinstance(bound_ctrl, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(bound_ctrl, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def old(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def kind(self) -> _ods_ir.Attribute:
    return self.operation.attributes["kind"]

  @kind.setter
  def kind(self, value: _ods_ir.Attribute):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["kind"] = value

  @builtins.property
  def permArgument(self) -> _Optional[_ods_ir.Attribute]:
    if "permArgument" not in self.operation.attributes:
      return None
    return self.operation.attributes["permArgument"]

  @permArgument.setter
  def permArgument(self, value: _Optional[_ods_ir.Attribute]):
    if value is not None:
      self.operation.attributes["permArgument"] = value
    elif "permArgument" in self.operation.attributes:
      del self.operation.attributes["permArgument"]

  @permArgument.deleter
  def permArgument(self):
    del self.operation.attributes["permArgument"]

  @builtins.property
  def row_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["row_mask"]

  @row_mask.setter
  def row_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["row_mask"] = value

  @builtins.property
  def bank_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["bank_mask"]

  @bank_mask.setter
  def bank_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["bank_mask"] = value

  @builtins.property
  def bound_ctrl(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["bound_ctrl"]

  @bound_ctrl.setter
  def bound_ctrl(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["bound_ctrl"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def dpp(old, src, kind, *, perm_argument=None, row_mask=None, bank_mask=None, bound_ctrl=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return DPPOp(old=old, src=src, kind=kind, permArgument=perm_argument, row_mask=row_mask, bank_mask=bank_mask, bound_ctrl=bound_ctrl, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ExtPackedFp8Op(_ods_ir.OpView):
  r"""
  Extend one or two 8-bit floats in `source[index]` to a 32-bit float or
  two floats and return them.
  
  This rather unusual signature arises from the fact that AMD GPUs cannot
  easily work with sub 32-bit quantities, so the compiler intrinsics for
  extending 8-bit floats (which are, currently, the only way to work with
  this operation) take packed vectors of 4 such floats.
  
  If the passed-in vector has fewer than four elements, or the input is scalar,
  the remaining values in the <4 x i8> will be filled with
  undefined values as needed.
  """

  OPERATION_NAME = "amdgpu.ext_packed_fp8"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, index, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["index"] = (index if (
    isinstance(index, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(index, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def index(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["index"]

  @index.setter
  def index(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["index"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def ext_packed_fp8(res, source, index, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ExtPackedFp8Op(res=res, source=source, index=index, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class FatRawBufferCastOp(_ods_ir.OpView):
  r"""
  Wraps the memory pointed to by `source` as a raw buffer fat pointer, or,
  in LLVM terms, a `ptr addrspace(7)`, returning a memref that has the same
  sizes and layout but the `#amdgpu.address_space<fat_raw_buffer>`
  address space.
  
  This memref can be used with standard memref operations like `memref.load`,
  `memref.store`, and `memref.atomicrmw`, which will be lowered to the relevant
  buffer intrinsics. (`vector.masked_load/store` will work once there's backend
  support for lowering them, and then this document will be updated)
  
  If `validBytes` is given, it is the number of bytes that will be valid as
  an offset to `out`. If it is not provided, this will be inferred from
  the size of the memref during lowering. This size is
  max_{d = 0 upto rank(source)} (sizes[d] * strides[d]) * sizeof(element type).
  
  The flags of the buffer descriptor will be set up to enable raw usage -
  for example, stride = 0, add_tid = 0, and so on. The `boundsCheck`
  property determines if bounds checking is enabled or not (on architectures
  where this can be controlled - that is, on RDNA chips).
  
  If `cacheSwizzleStride` is provided, L1 cache swizzling will be enabled
  on architectures that support it. This swizzling, unlike the main swizzling
  mode (whose usage makes a buffer non-raw) does not affect index calculation,
  but does affect cache behavior. Mixing access between cache-swizzled raw
  buffers and other forms of memory access, like ordinary pointer loads or
  unswizzled buffer pointers can cause incorrect behavior and must be avoided.
  
  This operation preserves the sizes, strides, and offset of the input
  memref - they'll be added in by `memref.load` later. However, if
  `resetOffset` is set, that offset will be added to the base pointer.
  If the value of the memref's offset is not uniform (independent of the lane/thread ID),
  this will lead to substantially decreased performance due to the need for
  a waterfall loop on the base address of the buffer resource.
  """

  OPERATION_NAME = "amdgpu.fat_raw_buffer_cast"

  _ODS_OPERAND_SEGMENTS = [1,0,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, source, *, validBytes=None, cacheSwizzleStride=None, boundsCheck=None, resetOffset=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(validBytes)
    operands.append(cacheSwizzleStride)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if bool(resetOffset): attributes["resetOffset"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def validBytes(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def cacheSwizzleStride(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def resetOffset(self) -> bool:
    return "resetOffset" in self.operation.attributes

  @resetOffset.setter
  def resetOffset(self, value):
    if bool(value):
      self.operation.attributes["resetOffset"] = _ods_ir.UnitAttr.get()
    elif "resetOffset" in self.operation.attributes:
      del self.operation.attributes["resetOffset"]

  @resetOffset.deleter
  def resetOffset(self):
    del self.operation.attributes["resetOffset"]

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def fat_raw_buffer_cast(source, *, valid_bytes=None, cache_swizzle_stride=None, bounds_check=None, reset_offset=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return FatRawBufferCastOp(source=source, validBytes=valid_bytes, cacheSwizzleStride=cache_swizzle_stride, boundsCheck=bounds_check, resetOffset=reset_offset, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class GatherToLDSOp(_ods_ir.OpView):
  r"""
  The `amdgpu.gather_to_lds` op is a wrapper around the `global_load_lds` instructions.
  
  Operands:
  * `$src`: global memory (including fat buffer) memref to read from.
  * `$srcIndices`: indices into `$src` to read from for this thread.
  * `$dst`: LDS memory memref to write to.
  * `$dstIndices`: base indices into `$dst` to write to for the subgroup of this thread.
    The elements gathered by the subgroup will be written contiguously in order of lane ID
    starting at `$dst[$dstIndices]`. Byte-sized (ex. i8) or short-sized (ex. i16)
    types will be zero-padded/extended to 32 bits before being written. 96-bit types
    (ex. vector<3xf32>) will be zero-padded to 128 bits before being written. Only the
    offsets held by lane 0 are used.
  * `$transferType`: type of the data to be transferred by each thread. This is used to determine
    the size of the data to be transferred and the number of threads in the subgroup.
    The transfer type must be a scalar type or a vector type with a single element type.
  
  The `$dst`, along with its indices, points to the memory location the subgroup of this thread
  will write to.
  
  Note: only supported on gfx9 and gfx10.
  """

  OPERATION_NAME = "amdgpu.gather_to_lds"

  _ODS_OPERAND_SEGMENTS = [1,-1,1,-1,]

  _ODS_REGIONS = (0, True)

  def __init__(self, src, srcIndices, dst, dstIndices, transferType, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    operands.append(_get_op_results_or_values(srcIndices))
    operands.append(dst)
    operands.append(_get_op_results_or_values(dstIndices))
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["transferType"] = (transferType if (
    isinstance(transferType, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('TypeAttr')) else
      _ods_ir.AttrBuilder.get('TypeAttr')(transferType, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def srcIndices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def dst(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0]

  @builtins.property
  def dstIndices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def transferType(self) -> _ods_ir.TypeAttr:
    return self.operation.attributes["transferType"]

  @transferType.setter
  def transferType(self, value: _ods_ir.TypeAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["transferType"] = value

def gather_to_lds(src, src_indices, dst, dst_indices, transfer_type, *, loc=None, ip=None) -> GatherToLDSOp:
  return GatherToLDSOp(src=src, srcIndices=src_indices, dst=dst, dstIndices=dst_indices, transferType=transfer_type, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class LDSBarrierOp(_ods_ir.OpView):
  r"""
  `amdgpu.lds_barrier` is both a barrier (all workitems in a workgroup must reach
  the barrier before any of them may proceed past it) and a wait for all
  operations that affect the Local Data Store (LDS) issued from that wrokgroup
  to complete before the workgroup may continue. Since the LDS is per-workgroup
  memory, this barrier may be used, for example, to ensure all workitems have
  written data to LDS before any workitem attempts to read from it.
  
  Note that `lds_barrier` does **not** force reads to or from global memory
  to complete before execution continues. Therefore, it should be used when
  operations on global memory can be issued far in advance of when their results
  are used (for example, by writing them to LDS).
  
  WARNING: On architectures that do not support the BackOffBarrier feature,
  (those which will implement this barrier by emitting inline assembly),
  use of this operation will impede the usabiliity of memory watches (including
  breakpoints set on variables) when debugging.
  """

  OPERATION_NAME = "amdgpu.lds_barrier"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def lds_barrier(*, loc=None, ip=None) -> LDSBarrierOp:
  return LDSBarrierOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class MFMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.mfma` op is an MLIR wrapper around intrinsics
  for various `mfma` instructions in the CDNA architecture, which perform
  multiple outer products in order to allow fast matrix multiplication.
  
  The wrapper will select an appropriate `mfma` instruction, if one is available,
  based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the
  types of the source and destination arguments.
  
  For information on the layouts of the input and output matrces (which are stored
  in `sourceA`, `sourceB`, `destC`, and `destD`), see the CDNA ISA documentation.
  
  The `cbsz`, `abid`, and `blgp` parameters control how the lanes of the wave
  are permuted when matrix data is being loaded: `blgp` can be any number of
  fixed permutations, `cbsz` specifies the log_2 of the number of chunks the lanes
  holding sourceA are split into, and `abid` selects one of those chunks.
  
  Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA
  intrinsics that take an integer type of width `4K`. For example,
  one can provide a vector<4xi8> as an argument to an MFMA instruction that
  logically takes 4 i8s but whose intrinsics are specified to take an i32.
  In these cases, the bytes in the vector will be concatenated in little-endian
  order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).
  
  The negateA, negateB, and negateC flags are only supported for double-precision
  operations on gfx94x.
  """

  OPERATION_NAME = "amdgpu.mfma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, blocks, sourceA, sourceB, destC, *, cbsz=None, abid=None, blgp=None, reducePrecision=None, negateA=None, negateB=None, negateC=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    attributes["blocks"] = (blocks if (
    isinstance(blocks, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(blocks, context=_ods_context))
    if cbsz is not None: attributes["cbsz"] = (cbsz if (
        isinstance(cbsz, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(cbsz, context=_ods_context))
    if abid is not None: attributes["abid"] = (abid if (
        isinstance(abid, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(abid, context=_ods_context))
    if blgp is not None: attributes["blgp"] = (blgp if (
        isinstance(blgp, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('AMDGPU_MFMAPermBAttr')) else
          _ods_ir.AttrBuilder.get('AMDGPU_MFMAPermBAttr')(blgp, context=_ods_context))
    if bool(reducePrecision): attributes["reducePrecision"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(negateA): attributes["negateA"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(negateB): attributes["negateB"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(negateC): attributes["negateC"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def blocks(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["blocks"]

  @blocks.setter
  def blocks(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["blocks"] = value

  @builtins.property
  def cbsz(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["cbsz"]

  @cbsz.setter
  def cbsz(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["cbsz"] = value

  @builtins.property
  def abid(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["abid"]

  @abid.setter
  def abid(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["abid"] = value

  @builtins.property
  def blgp(self) -> _ods_ir.Attribute:
    return self.operation.attributes["blgp"]

  @blgp.setter
  def blgp(self, value: _ods_ir.Attribute):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["blgp"] = value

  @builtins.property
  def reducePrecision(self) -> bool:
    return "reducePrecision" in self.operation.attributes

  @reducePrecision.setter
  def reducePrecision(self, value):
    if bool(value):
      self.operation.attributes["reducePrecision"] = _ods_ir.UnitAttr.get()
    elif "reducePrecision" in self.operation.attributes:
      del self.operation.attributes["reducePrecision"]

  @reducePrecision.deleter
  def reducePrecision(self):
    del self.operation.attributes["reducePrecision"]

  @builtins.property
  def negateA(self) -> bool:
    return "negateA" in self.operation.attributes

  @negateA.setter
  def negateA(self, value):
    if bool(value):
      self.operation.attributes["negateA"] = _ods_ir.UnitAttr.get()
    elif "negateA" in self.operation.attributes:
      del self.operation.attributes["negateA"]

  @negateA.deleter
  def negateA(self):
    del self.operation.attributes["negateA"]

  @builtins.property
  def negateB(self) -> bool:
    return "negateB" in self.operation.attributes

  @negateB.setter
  def negateB(self, value):
    if bool(value):
      self.operation.attributes["negateB"] = _ods_ir.UnitAttr.get()
    elif "negateB" in self.operation.attributes:
      del self.operation.attributes["negateB"]

  @negateB.deleter
  def negateB(self):
    del self.operation.attributes["negateB"]

  @builtins.property
  def negateC(self) -> bool:
    return "negateC" in self.operation.attributes

  @negateC.setter
  def negateC(self, value):
    if bool(value):
      self.operation.attributes["negateC"] = _ods_ir.UnitAttr.get()
    elif "negateC" in self.operation.attributes:
      del self.operation.attributes["negateC"]

  @negateC.deleter
  def negateC(self):
    del self.operation.attributes["negateC"]

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def mfma(m, n, k, blocks, source_a, source_b, dest_c, *, cbsz=None, abid=None, blgp=None, reduce_precision=None, negate_a=None, negate_b=None, negate_c=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return MFMAOp(m=m, n=n, k=k, blocks=blocks, sourceA=source_a, sourceB=source_b, destC=dest_c, cbsz=cbsz, abid=abid, blgp=blgp, reducePrecision=reduce_precision, negateA=negate_a, negateB=negate_b, negateC=negate_c, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class MemoryCounterWaitOp(_ods_ir.OpView):
  r"""
  Wait for the specified counters to be less-than or equal-to the provided
  values before continuing.
  
  Counters can lower to different instructions on different architectires,
  including clamping to the some HW supported max value or combining multiple
  counters into one.
  """

  OPERATION_NAME = "amdgpu.memory_counter_wait"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, load=None, store=None, ds=None, exp=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    if load is not None: attributes["load"] = (load if (
        isinstance(load, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(load, context=_ods_context))
    if store is not None: attributes["store"] = (store if (
        isinstance(store, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(store, context=_ods_context))
    if ds is not None: attributes["ds"] = (ds if (
        isinstance(ds, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(ds, context=_ods_context))
    if exp is not None: attributes["exp"] = (exp if (
        isinstance(exp, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(exp, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def load(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "load" not in self.operation.attributes:
      return None
    return self.operation.attributes["load"]

  @load.setter
  def load(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["load"] = value
    elif "load" in self.operation.attributes:
      del self.operation.attributes["load"]

  @load.deleter
  def load(self):
    del self.operation.attributes["load"]

  @builtins.property
  def store(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "store" not in self.operation.attributes:
      return None
    return self.operation.attributes["store"]

  @store.setter
  def store(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["store"] = value
    elif "store" in self.operation.attributes:
      del self.operation.attributes["store"]

  @store.deleter
  def store(self):
    del self.operation.attributes["store"]

  @builtins.property
  def ds(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "ds" not in self.operation.attributes:
      return None
    return self.operation.attributes["ds"]

  @ds.setter
  def ds(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["ds"] = value
    elif "ds" in self.operation.attributes:
      del self.operation.attributes["ds"]

  @ds.deleter
  def ds(self):
    del self.operation.attributes["ds"]

  @builtins.property
  def exp(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "exp" not in self.operation.attributes:
      return None
    return self.operation.attributes["exp"]

  @exp.setter
  def exp(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["exp"] = value
    elif "exp" in self.operation.attributes:
      del self.operation.attributes["exp"]

  @exp.deleter
  def exp(self):
    del self.operation.attributes["exp"]

def memory_counter_wait(*, load=None, store=None, ds=None, exp=None, loc=None, ip=None) -> MemoryCounterWaitOp:
  return MemoryCounterWaitOp(load=load, store=store, ds=ds, exp=exp, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class PackedScaledTruncOp(_ods_ir.OpView):
  r"""
  Scale and round the inputs `source` (which is undefined if not
  specified) into the low or high word (bottom two or top two) elements
  of the returned vector, keeping the other two elements of `existing`
  unchanged if present (or undefined if it was not passed in).
  
  The reason for this odd signature is that AMD GPUs cannot easily work with
  sub-registers, and so the conversion intrinsics take 32-bit wide
  packed vectors of float values.
  """

  OPERATION_NAME = "amdgpu.packed_scaled_trunc"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, scale, index, *, existing=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(scale)
    if existing is not None: operands.append(existing)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["index"] = (index if (
    isinstance(index, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(index, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def scale(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def existing(self) -> _Optional[_ods_ir.Value]:
    return None if len(self.operation.operands) < 3 else self.operation.operands[2]

  @builtins.property
  def index(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["index"]

  @index.setter
  def index(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["index"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def packed_scaled_trunc(res, source, scale, index, *, existing=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PackedScaledTruncOp(res=res, source=source, scale=scale, index=index, existing=existing, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class PackedStochRoundFp8Op(_ods_ir.OpView):
  r"""
  Round the input `source`, adding in `stochiasticParam`, and place it into
  the `storeIndex`th element of `res`.
  
  If `existing` is passed in, elements of `res` other than the one at `storeIndex`
  are copied from `existing`.
  
  The reason for this odd signature is that AMD GPUs cannot easily work with
  sub-registers, and so the conversion intrinsics (which are currently the
  only way to work with 8-bit float types) take packed vectors of 4 8-bit
  values.
  """

  OPERATION_NAME = "amdgpu.packed_stoch_round_fp8"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, stochiasticParam, storeIndex, *, existing=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(stochiasticParam)
    if existing is not None: operands.append(existing)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["storeIndex"] = (storeIndex if (
    isinstance(storeIndex, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(storeIndex, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def stochiasticParam(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def existing(self) -> _Optional[_ods_ir.Value]:
    return None if len(self.operation.operands) < 3 else self.operation.operands[2]

  @builtins.property
  def storeIndex(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["storeIndex"]

  @storeIndex.setter
  def storeIndex(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["storeIndex"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def packed_stoch_round_fp8(res, source, stochiastic_param, store_index, *, existing=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PackedStochRoundFp8Op(res=res, source=source, stochiasticParam=stochiastic_param, storeIndex=store_index, existing=existing, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class PackedTrunc2xFp8Op(_ods_ir.OpView):
  r"""
  Round the inputs `sourceA` and `sourceB` (which is undefined if not
  specified) into the low or high word (bottom two or top two) elements
  of the returned vector, keeping the other two elements of `existing`
  unchanged if present (or undefined if it was not passed in).
  
  The reason for this odd signature is that AMD GPUs cannot easily work with
  sub-registers, and so the conversion intrinsics (which are currently the
  only way to work with 8-bit float types) take packed vectors of 4 8-bit
  values.
  """

  OPERATION_NAME = "amdgpu.packed_trunc_2xfp8"

  _ODS_OPERAND_SEGMENTS = [1,0,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, res, sourceA, wordIndex, *, sourceB=None, existing=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(existing)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["wordIndex"] = (wordIndex if (
    isinstance(wordIndex, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(wordIndex, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def sourceB(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def existing(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def wordIndex(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["wordIndex"]

  @wordIndex.setter
  def wordIndex(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["wordIndex"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def packed_trunc_2xfp8(res, source_a, word_index, *, source_b=None, existing=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PackedTrunc2xFp8Op(res=res, sourceA=source_a, wordIndex=word_index, sourceB=source_b, existing=existing, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class PermlaneSwapOp(_ods_ir.OpView):
  r"""
  High-level wrapper on `rocdl.permlane{16,32}.swap` variants for permutations
  on rows of lanes in a subgroup.
  
  Supports arbitrary int/float/vector types, which will be repacked to i32 and
  one or more `rocdl.permlane_swap` ops during lowering.
  Supported lane permutations:
  - Swap the data between odd and even rows of 16 lanes
  - Swap the data between the first 32 lanes and the last 32 lanes
  
  Example:
  ```mlir
  %0 = amdgpu.permlane_swap %src 16 : f16
  %1 = amdgpu.permlane_swap %src 32 { fetch_inactive = true, bound_ctrl = true } : f16
  ```
  
  Operands:
  * `$src`: Vector register to permute across lanes of the subgroup.
  * `$row_length`: The length of a row to permute in number of lanes (valid values are 16 and 32).
  * `$fetch_inactive`: Optional. Used to dertermine behavior of a fetch from a disabled lane.
    `fetch_inactive = false`: If the source lane is disabled, use `bound_ctrl` to determine the source value.
    `fetch_inactive = true`: If the source lane is disabled, fetch the source value anyway (ignoring `bound_ctrl`).
  * `$bound_ctrl`: Optional. Used to determine what a thread should do if its source operand is from
    a disabled lane: use the value zero, or disable the write.
    `bound_ctrl = false`: Do not write when source is from a disabled lane
    `bound_ctrl = true`: Use zero as input if source is from a disabled lane
  
  Note: Lowering is only supported on gfx950 and up.
  """

  OPERATION_NAME = "amdgpu.permlane_swap"

  _ODS_REGIONS = (0, True)

  def __init__(self, src, row_length, *, fetch_inactive=None, bound_ctrl=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["row_length"] = (row_length if (
    isinstance(row_length, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(row_length, context=_ods_context))
    if fetch_inactive is not None: attributes["fetch_inactive"] = (fetch_inactive if (
        isinstance(fetch_inactive, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(fetch_inactive, context=_ods_context))
    if bound_ctrl is not None: attributes["bound_ctrl"] = (bound_ctrl if (
        isinstance(bound_ctrl, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(bound_ctrl, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def row_length(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["row_length"]

  @row_length.setter
  def row_length(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["row_length"] = value

  @builtins.property
  def fetch_inactive(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["fetch_inactive"]

  @fetch_inactive.setter
  def fetch_inactive(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["fetch_inactive"] = value

  @builtins.property
  def bound_ctrl(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["bound_ctrl"]

  @bound_ctrl.setter
  def bound_ctrl(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["bound_ctrl"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def permlane_swap(src, row_length, *, fetch_inactive=None, bound_ctrl=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return PermlaneSwapOp(src=src, row_length=row_length, fetch_inactive=fetch_inactive, bound_ctrl=bound_ctrl, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicCmpswapOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_cmpswap` op is a wrapper around the
  buffer-based atomic compare-and-swap min available on AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_cmpswap"

  _ODS_OPERAND_SEGMENTS = [1,1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, src, cmp, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    operands.append(cmp)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def cmp(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 4)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

  @builtins.property
  def value(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def raw_buffer_atomic_cmpswap(src, cmp, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return RawBufferAtomicCmpswapOp(src=src, cmp=cmp, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicFaddOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_fadd` op is a wrapper around the
  buffer-based atomic floating point addition available on the MI-* series
  of AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_fadd"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_fadd(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicFaddOp:
  return RawBufferAtomicFaddOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicFmaxOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_fmax` op is a wrapper around the
  buffer-based atomic floating point max available on AMD GPUs (except GFX9).
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_fmax"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_fmax(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicFmaxOp:
  return RawBufferAtomicFmaxOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicSmaxOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_smax` op is a wrapper around the
  buffer-based atomic signed integer max available on AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_smax"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_smax(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicSmaxOp:
  return RawBufferAtomicSmaxOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferAtomicUminOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_atomic_umin` op is a wrapper around the
  buffer-based atomic signed integer min available on AMD GPUs.
  
  The index into the buffer is computed as for `memref.store` with the addition
  of `indexOffset` (which is used to aid in emitting vectorized code) and,
  if present `sgprOffset` (which is added after bounds checks and includes
  any non-zero offset on the memref type).
  
  All indexing components are given in terms of the memref's element size, not
  the byte lengths required by the intrinsic.
  
  Out of bounds atomic operations are ignored in hardware.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_atomic_umin"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_atomic_umin(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferAtomicUminOp:
  return RawBufferAtomicUminOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class RawBufferLoadOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_load` op is a wrapper around the buffer load intrinsics
  available on AMD GPUs, including extensions in newer GPUs.
  
  The index into the buffer is computed as for `memref.load` with the additon
  of `indexOffset` and `sgprOffset` (which **may or may not** be considered
  in bounds checks and includes any offset present on the memref type if it's
  non-zero).
  
  All indices and offsets are in units of the memref's data type and are
  converted to bytes during lowering.
  
  When a load is out of bounds, the instruction returns zero.
  Partially-out of bounds have chipset-dependent behavior: whether reading
  2 elements starting at index 7 of a `memref<8xf32>` returns the last element
  in the first vector component depends on the architecture.
  
  The memref struct is converted into a buffer resource (a V#) and the arguments
  are translated to intrinsic arguments as follows:
  - The base address of the buffer is the base address of the memref
  - The stride is 0 to enable raw mode
  - The number of records is the size of the memref, in bytes
    In the case of dynamically-shaped memrefs, this is computed at runtime
    as max_d (size(d) * stride(d)) * sizeof(elementType(memref))
  - The offset enable bit is 1, the index enable bit is 0.
  - The thread ID addition bit is off
  - If `boundsCheck` is false and the target chipset is RDNA, OOB_SELECT is set
    to 2 to disable bounds checks, otherwise it is 3
  - The cache coherency bits are off
  """

  OPERATION_NAME = "amdgpu.raw_buffer_load"

  _ODS_OPERAND_SEGMENTS = [1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    results.append(value)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

  @builtins.property
  def value(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def raw_buffer_load(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return RawBufferLoadOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class RawBufferStoreOp(_ods_ir.OpView):
  r"""
  The `amdgpu.raw_buffer_store` op is a wrapper around the buffer store
  intrinsics available on AMD GPUs, including extensions in newer GPUs.
  
  The store index is computed as in `memref.store` with the addition of
  `indexOffset` (which is included for uniformity with atomics and may be useful
  when writing vectorized code) and `sgprOffset` (which is added after bounds
  checks and implicitly includes the offset of the memref type if non-zero).
  All index components are in terms of the elements of the memref, not bytes,
  and are scaled up appropriately.
  
  Out of bounds stores are ignored in hardware.
  Wthether a vector write that includes some in-bounds and soeme out-of-bounds
  components is partically completed is chipset-dependent.
  
  See `amdgpu.raw_buffer_load` for a description of how the underlying
  instruction is constructed.
  """

  OPERATION_NAME = "amdgpu.raw_buffer_store"

  _ODS_OPERAND_SEGMENTS = [1,1,-1,0,]

  _ODS_REGIONS = (0, True)

  def __init__(self, value, memref, indices, *, boundsCheck=None, indexOffset=None, sgprOffset=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(value)
    operands.append(memref)
    operands.append(_get_op_results_or_values(indices))
    operands.append(sgprOffset)
    _ods_context = _ods_get_default_loc_context(loc)
    if boundsCheck is not None: attributes["boundsCheck"] = (boundsCheck if (
        isinstance(boundsCheck, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(boundsCheck, context=_ods_context))
    if indexOffset is not None: attributes["indexOffset"] = (indexOffset if (
        isinstance(indexOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(indexOffset, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def value(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 0)
    return operand_range[0]

  @builtins.property
  def memref(self) -> _ods_ir.Value:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 1)
    return operand_range[0]

  @builtins.property
  def indices(self) -> _ods_ir.OpOperandList:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 2)
    return operand_range

  @builtins.property
  def sgprOffset(self) -> _Optional[_ods_ir.Value]:
    operand_range = _ods_segmented_accessor(
         self.operation.operands,
         self.operation.attributes["operandSegmentSizes"], 3)
    return operand_range[0] if len(operand_range) > 0 else None

  @builtins.property
  def boundsCheck(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["boundsCheck"]

  @boundsCheck.setter
  def boundsCheck(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["boundsCheck"] = value

  @builtins.property
  def indexOffset(self) -> _Optional[_ods_ir.IntegerAttr]:
    if "indexOffset" not in self.operation.attributes:
      return None
    return self.operation.attributes["indexOffset"]

  @indexOffset.setter
  def indexOffset(self, value: _Optional[_ods_ir.IntegerAttr]):
    if value is not None:
      self.operation.attributes["indexOffset"] = value
    elif "indexOffset" in self.operation.attributes:
      del self.operation.attributes["indexOffset"]

  @indexOffset.deleter
  def indexOffset(self):
    del self.operation.attributes["indexOffset"]

def raw_buffer_store(value, memref, indices, *, bounds_check=None, index_offset=None, sgpr_offset=None, loc=None, ip=None) -> RawBufferStoreOp:
  return RawBufferStoreOp(value=value, memref=memref, indices=indices, boundsCheck=bounds_check, indexOffset=index_offset, sgprOffset=sgpr_offset, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ScaledExtPackedOp(_ods_ir.OpView):
  r"""
  Extend and scale two packed floats in `source[index]` to two floats and
  return them.
  
  This rather unusual signature arises from the fact that AMD GPUs cannot
  easily work with sub 32-bit quantities, so the compiler intrinsics for
  extending 8-bit floats (which are, currently, the only way to work with
  this operation) take packed vectors of 2 such floats.
  
  If the passed-in vector has fewer than two elements, or the input is scalar,
  the remaining values in the <2 x i8> will be filled with
  undefined values as needed.
  """

  OPERATION_NAME = "amdgpu.scaled_ext_packed"

  _ODS_REGIONS = (0, True)

  def __init__(self, res, source, scale, index, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.append(scale)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["index"] = (index if (
    isinstance(index, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(index, context=_ods_context))
    results = []
    results.append(res)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def scale(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def index(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["index"]

  @index.setter
  def index(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["index"] = value

  @builtins.property
  def res(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def scaled_ext_packed(res, source, scale, index, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return ScaledExtPackedOp(res=res, source=source, scale=scale, index=index, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class ScaledMFMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.scaled_mfma` op is an MLIR wrapper around intrinsics
  for various scaled versions of `mfma` instructions in the CDNA architecture, which perform
  multiple outer products in order to allow fast matrix multiplication.
  
  The wrapper will select an appropriate `mfma` instruction, if one is available,
  based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the
  types of the source and destination arguments.
  
  Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA
  intrinsics that take an integer type of width `4K`. For example,
  one can provide a `vector<4xi8>` as an argument to an MFMA instruction that
  logically takes 4 i8s but whose intrinsics are specified to take an i32.
  In these cases, the bytes in the vector will be concatenated in little-endian
  order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).
  
  This wrapper takes inspiration from `amdgpu.mfma`, but has some key differences:
  - `amdgpu.scaled_mfma` operates on fp4 (f4E2M1FN), fp6 (f6E2M3FN and f6E3M2FN) and
  fp8 (f8E4M3FN and f8E5M2) types using either M=N=16, K=128 or M=N=32, K=64 as their tile
  size.
  - `amdgpu.scaled_mfma` does not support broadcasting. So, `cbsz`, `abid`, and `blgp`
  are omitted from this wrapper.
  - The `negateA`, `negateB`, and `negateC` flags in `amdgpu.mfma` are only supported for
  double-precision operations on gfx94x and so are not included here.
  """

  OPERATION_NAME = "amdgpu.scaled_mfma"

  _ODS_REGIONS = (0, True)

  def __init__(self, m, n, k, sourceA, sourceB, destC, scalesA, scalesB, scalesIdxA, scalesIdxB, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    operands.append(scalesA)
    operands.append(scalesB)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["m"] = (m if (
    isinstance(m, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(m, context=_ods_context))
    attributes["n"] = (n if (
    isinstance(n, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(n, context=_ods_context))
    attributes["k"] = (k if (
    isinstance(k, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(k, context=_ods_context))
    attributes["scalesIdxA"] = (scalesIdxA if (
    isinstance(scalesIdxA, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(scalesIdxA, context=_ods_context))
    attributes["scalesIdxB"] = (scalesIdxB if (
    isinstance(scalesIdxB, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(scalesIdxB, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def scalesA(self) -> _ods_ir.Value:
    return self.operation.operands[3]

  @builtins.property
  def scalesB(self) -> _ods_ir.Value:
    return self.operation.operands[4]

  @builtins.property
  def m(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["m"]

  @m.setter
  def m(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["m"] = value

  @builtins.property
  def n(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["n"]

  @n.setter
  def n(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["n"] = value

  @builtins.property
  def k(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["k"]

  @k.setter
  def k(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["k"] = value

  @builtins.property
  def scalesIdxA(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["scalesIdxA"]

  @scalesIdxA.setter
  def scalesIdxA(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["scalesIdxA"] = value

  @builtins.property
  def scalesIdxB(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["scalesIdxB"]

  @scalesIdxB.setter
  def scalesIdxB(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["scalesIdxB"] = value

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def scaled_mfma(m, n, k, source_a, source_b, dest_c, scales_a, scales_b, scales_idx_a, scales_idx_b, *, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return ScaledMFMAOp(m=m, n=n, k=k, sourceA=source_a, sourceB=source_b, destC=dest_c, scalesA=scales_a, scalesB=scales_b, scalesIdxA=scales_idx_a, scalesIdxB=scales_idx_b, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class SchedBarrierOp(_ods_ir.OpView):
  r"""
  `amdgpu.sched_barrier` serves as a barrier that could be
  configured to restrict movements of instructions through it as
  defined by sched_barrier_opts.
  """

  OPERATION_NAME = "amdgpu.sched_barrier"

  _ODS_REGIONS = (0, True)

  def __init__(self, opts, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["opts"] = (opts if (
    isinstance(opts, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('AMDGPU_SchedBarrierOpOptAttr')) else
      _ods_ir.AttrBuilder.get('AMDGPU_SchedBarrierOpOptAttr')(opts, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def opts(self) -> _ods_ir.Attribute:
    return self.operation.attributes["opts"]

  @opts.setter
  def opts(self, value: _ods_ir.Attribute):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["opts"] = value

def sched_barrier(opts, *, loc=None, ip=None) -> SchedBarrierOp:
  return SchedBarrierOp(opts=opts, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class SwizzleBitModeOp(_ods_ir.OpView):
  r"""
  High-level wrapper on bitmode `rocdl.ds_swizzle` op, masks are represented
  as separate fields so user won't need to do manual bitpacking.
  
  Supports arbitrary int/float/vector types, which will be repacked to i32 and
  one or more `rocdl.ds_swizzle` ops during lowering.
  """

  OPERATION_NAME = "amdgpu.swizzle_bitmode"

  _ODS_REGIONS = (0, True)

  def __init__(self, src, and_mask, or_mask, xor_mask, *, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["and_mask"] = (and_mask if (
    isinstance(and_mask, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(and_mask, context=_ods_context))
    attributes["or_mask"] = (or_mask if (
    isinstance(or_mask, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(or_mask, context=_ods_context))
    attributes["xor_mask"] = (xor_mask if (
    isinstance(xor_mask, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I32Attr')) else
      _ods_ir.AttrBuilder.get('I32Attr')(xor_mask, context=_ods_context))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def and_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["and_mask"]

  @and_mask.setter
  def and_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["and_mask"] = value

  @builtins.property
  def or_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["or_mask"]

  @or_mask.setter
  def or_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["or_mask"] = value

  @builtins.property
  def xor_mask(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["xor_mask"]

  @xor_mask.setter
  def xor_mask(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["xor_mask"] = value

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def swizzle_bitmode(src, and_mask, or_mask, xor_mask, *, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return SwizzleBitModeOp(src=src, and_mask=and_mask, or_mask=or_mask, xor_mask=xor_mask, results=results, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class TransposeLoadOp(_ods_ir.OpView):
  r"""
  The `amdgpu.transpose_load` op is a wrapper around the `ds_read_tr` instructions.
  The transpose load op represents a subgroup load from LDS memory,
  where the subgroup of threads collectively reads a matrix from the source
  memref, with each thread reading a vector of the matrix, and gets a transposed matrix
  in as the result. That is, each thread reads a vector of the col-major matrix at different
  indices, and the thread's read result is a vector of the corresponding row of the transposed
  matrix.
  
  This op is a direct wrapper around the ROCDL `ds_read_tr` family intrinsics. Please refer
  to the CDNA4 ISA documentation for more details about its exact semantics.
  
  Format example:
  ```
  %0 = amdgpu.transpose_load %src[%srcIndices] : memref<128x256xf16> -> vector<4xf16>
  ```
  Operands:
  * `$src`: LDS memref to read from.
  * `$srcIndices`: indices into `$src` to read from for this thread.
  * `$result`: target register this transpose load instruction will write to.
  
  Note: Lowering is only supported on gfx950 and up.
  """

  OPERATION_NAME = "amdgpu.transpose_load"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, src, srcIndices, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(src)
    operands.extend(_get_op_results_or_values(srcIndices))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def src(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def srcIndices(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 2 + 1
    return self.operation.operands[1:1 + _ods_variadic_group_length]

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def transpose_load(result, src, src_indices, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return TransposeLoadOp(result=result, src=src, srcIndices=src_indices, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class WMMAOp(_ods_ir.OpView):
  r"""
  The `amdgpu.wmma` op is an MLIR wrapper around intrinsics
  for various `wmma` instructions in the RDNA3 or RDNA4 architecture, which
  perform a 16x16 * 16x16 matrix multiplication for different data types.
  Note that in gfx12/RDNA4, there is also a 16x32 * 32x16 instruction for 4-bit
  integer inputs.
  
  On gfx11/RDNA3, emitting f16->f16 (or bf16->bf16) wmma the output is a 16xf16
  (or 16xbf16) vector containing only 8 valid values:
    - If `subwordOffset` is 0, then the output is stored at indices 0, 2, 4, ..., 14.
    - If `subwordOffset` is 1, then the output is stored at indices 1, 3, 5, ..., 15.
  On gfx12/RDNA4, the result is instead returned as a vector<8 x f16/bf16> where
  all values are valid and the `subwordOffset` must be `0`, as it cannot be used.
  
  `unsignedA` and `unsignedB` flag that the `int8` LLVM inputs are unsigned.
  
  The `clamp` flag is used to saturate the output of type T to numeric_limits<T>::max()
  in case of overflow.
  """

  OPERATION_NAME = "amdgpu.wmma"

  _ODS_REGIONS = (0, True)

  def __init__(self, sourceA, sourceB, destC, *, subwordOffset=None, unsignedA=None, unsignedB=None, clamp=None, results=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(sourceA)
    operands.append(sourceB)
    operands.append(destC)
    _ods_context = _ods_get_default_loc_context(loc)
    if subwordOffset is not None: attributes["subwordOffset"] = (subwordOffset if (
        isinstance(subwordOffset, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('I32Attr')) else
          _ods_ir.AttrBuilder.get('I32Attr')(subwordOffset, context=_ods_context))
    if bool(unsignedA): attributes["unsignedA"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(unsignedB): attributes["unsignedB"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    if bool(clamp): attributes["clamp"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def sourceA(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def sourceB(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def destC(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def subwordOffset(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["subwordOffset"]

  @subwordOffset.setter
  def subwordOffset(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["subwordOffset"] = value

  @builtins.property
  def unsignedA(self) -> bool:
    return "unsignedA" in self.operation.attributes

  @unsignedA.setter
  def unsignedA(self, value):
    if bool(value):
      self.operation.attributes["unsignedA"] = _ods_ir.UnitAttr.get()
    elif "unsignedA" in self.operation.attributes:
      del self.operation.attributes["unsignedA"]

  @unsignedA.deleter
  def unsignedA(self):
    del self.operation.attributes["unsignedA"]

  @builtins.property
  def unsignedB(self) -> bool:
    return "unsignedB" in self.operation.attributes

  @unsignedB.setter
  def unsignedB(self, value):
    if bool(value):
      self.operation.attributes["unsignedB"] = _ods_ir.UnitAttr.get()
    elif "unsignedB" in self.operation.attributes:
      del self.operation.attributes["unsignedB"]

  @unsignedB.deleter
  def unsignedB(self):
    del self.operation.attributes["unsignedB"]

  @builtins.property
  def clamp(self) -> bool:
    return "clamp" in self.operation.attributes

  @clamp.setter
  def clamp(self, value):
    if bool(value):
      self.operation.attributes["clamp"] = _ods_ir.UnitAttr.get()
    elif "clamp" in self.operation.attributes:
      del self.operation.attributes["clamp"]

  @clamp.deleter
  def clamp(self):
    del self.operation.attributes["clamp"]

  @builtins.property
  def destD(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def wmma(source_a, source_b, dest_c, *, subword_offset=None, unsigned_a=None, unsigned_b=None, clamp=None, results=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return WMMAOp(sourceA=source_a, sourceB=source_b, destC=dest_c, subwordOffset=subword_offset, unsignedA=unsigned_a, unsignedB=unsigned_b, clamp=clamp, results=results, loc=loc, ip=ip).result
