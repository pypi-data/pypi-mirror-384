
# Autogenerated by mlir-tblgen; don't manually edit.

from ._ods_common import _cext as _ods_cext
from ._ods_common import (
    equally_sized_accessor as _ods_equally_sized_accessor,
    get_default_loc_context as _ods_get_default_loc_context,
    get_op_results_or_values as _get_op_results_or_values,
    segmented_accessor as _ods_segmented_accessor,
)
_ods_ir = _ods_cext.ir
_ods_cext.globals.register_traceback_file_exclusion(__file__)

import builtins
from typing import Sequence as _Sequence, Union as _Union, Optional as _Optional


@_ods_cext.register_dialect
class _Dialect(_ods_ir.Dialect):
  DIALECT_NAMESPACE = "iree_gpu"

@_ods_cext.register_operation(_Dialect)
class BarrierRegionOp(_ods_ir.OpView):
  r"""
  This op is designed to represent synchronization of workers on the operands
  and results of the given region. This operation naturally arises when combining
  the regions of producer-consumer `scf.forall` operations that share a
  mapping type.
  
  For example, consider the following pair of parallel loops.
  ```mlir
    %0 = scf.forall (%idy, %idx) in (2, 32) shared_outs(%init = %empty) -> (tensor<4x128xf32>) {
      %in = ...
      %2 = affine.apply #affine_map<(d0) -> (d0 * 2)> (%idy)
      %3 = affine.apply #affine_map<(d0) -> (d0 * 4)> (%idx)
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %in into %init[%2, %3] [2, 4] [1, 1]
          : tensor<2x4xf32> into tensor<4x128xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %1 = scf.forall (%idy, %idx) in (8, 8) -> (tensor<128x128xf32>) {
      %4 = affine.apply #affine_map<(d0) -> (d0 * 16)> (%idx)
      %extracted_slice = tensor.extract_slice %0[0, %4] [4, 16] [1, 1]
        : tensor<4x128xf32> to tensor<4x16xf32>
      ...
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  ```
  
  Because these loops share the same worker type and total count, the bodies
  of these two loops can be merged with a barrier an insert_slice and a
  shuffle where the boundary of the loops currently is.
  
  ```mlir
    %0 = scf.forall (%idy, %idx) in (8, 8) -> (tensor<4x128xf32>) {
      %alloc = bufferization.alloc_tensor {memory_space = #gpu.address_space<workgroup>}
        : tensor<4x128xf32>
      %barrier = iree_gpu.barrier_region %alloc {
      ^bb0(%shared: tensor<4x128xf32>):
        %ids = affine.delinearize_index %idy * 8 + %idx to (2, 32) : index
        %in = ...
        %2 = affine.apply #affine_map<(d0) -> (d0 * 2)> (%ids#0)
        %3 = affine.apply #affine_map<(d0) -> (d0 * 4)> (%ids#1)
        %inserted_slice = tensor.insert_slice %in into %shared[%2, %3] [2, 4] [1, 1]
          : tensor<2x4xf32> to tensor<4x128xf32>
        iree_gpu.yield %slice : tensor<4x16xf32>
      } : tensor<4x128xf32> -> tensor<4x16xf32>
      %4 = affine.apply #affine_map<(d0) -> (d0 * 16)> (%idx)
      %slice = tensor.extract_slice %barrier[0, %4] [4, 16] [1, 1] : tensor<4x128xf32> to tensor<4x16xf32>
      ...
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  ```
  
  A barrier_region can be lowered to two barriers, one on the input operands
  and a second one on the results.
  
  Movtivation and Intended Use Cases:
  
  The primary way this op is generated is when fusing parallel loops with
  tensor results. This operation helps to make lowerings more progressive
  and flexible.
    - Lowering directly to an alloc + reads and writes breaks the dependency
      chain making transformations like barrier placement and pipelining
      potentially more difficult.
    - Allows the option of non-vector based lowering paths.
  """

  OPERATION_NAME = "iree_gpu.barrier_region"

  _ODS_REGIONS = (1, True)

  def __init__(self, results_, inputs, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(inputs))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.extend(results_)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def inputs(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

  @builtins.property
  def results_(self) -> _ods_ir.OpResultList:
    _ods_variadic_group_length = len(self.operation.results) - 1 + 1
    return self.operation.results[0:0 + _ods_variadic_group_length]

  @builtins.property
  def region(self) -> _ods_ir.Region:
    return self.regions[0]

def barrier_region(results_, inputs, *, loc=None, ip=None) -> _Union[_ods_ir.OpResult, _ods_ir.OpResultList, BarrierRegionOp]:
  op = BarrierRegionOp(results_=results_, inputs=inputs, loc=loc, ip=ip); results = op.results
  return results if len(results) > 1 else (results[0] if len(results) == 1 else op)

@_ods_cext.register_operation(_Dialect)
class BufferResourceCastOp(_ods_ir.OpView):
  r"""
  Nominal cast of a tensor to AMDGPU buffer resource memory space before
  bufferization. This op takes the parameters with which to perform the cast
  if |input| bufferizes to `storage_buffer` memory space. If |input| resolves
  to any other memory space this op is silently dropped and has no effect.
  
  If |cache_swizzle_stride| is present, there is verification before
  bufferization that all producers of |input| are view-like and single source
  and user (i.e. trivially no alias). In all other cases this op is best
  effort and has no verification or failure modes.
  
  // TODO: Add other parameters for casting as needed.
  """

  OPERATION_NAME = "iree_gpu.buffer_resource_cast"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, input, *, cache_swizzle_stride=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(input)
    if cache_swizzle_stride is not None: operands.append(cache_swizzle_stride)
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def input(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def cache_swizzle_stride(self) -> _Optional[_ods_ir.Value]:
    return None if len(self.operation.operands) < 2 else self.operation.operands[1]

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def buffer_resource_cast(result, input, *, cache_swizzle_stride=None, loc=None, ip=None) -> _ods_ir.OpResult:
  return BufferResourceCastOp(result=result, input=input, cache_swizzle_stride=cache_swizzle_stride, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class CoalescedGatherDMAOp(_ods_ir.OpView):
  r"""
  Performs a coalesced gather operation.
  This operation can exist in two forms: a tensor-based (value-semantic) form
  and a buffer-based (memref-semantic) form.
  
  In both forms, it reads elements from a source operand based on indices.
  * When the source is a tensor, the operation produces a new result tensor
    containing the gathered data.
  * When the source is a memref, the operation writes the gathered data into
    a destination out memref operand.
  
  The operation is specifically designed for subgroup-level parallelism, where
  threads within a subgroup cooperatively gather data with coalesced memory
  accesses. It implements ParallelCombiningOpInterface and must live inside an
  op implementing `InParallelOpInterface`, such as `scf.forall.in_parallel`.
  
  ## Lowering Paths
  
  Two lowering strategies are supported:
  1. Lowers to `amdgpu.gather_to_lds` operations when lowering requirements
     are met.
  2. Default lowering using `vector.gather` operations.
  
  ## Operands and Results
  
  * `$indices`: Tensor/memref of index type specifying gather locations in
    the source tensor
  * `$source`: Source tensor/memref containing the data to be gathered
  * `$init`: Destination tensor/memref receiving the gathered data
    (destination-passing style)
  * `$result`: Output tensor/memref with gathered data (same type as `$init`)
  
  ## Example
  
  The following example shows how this op is designed to be used in a tiled
  scenario, which sets up lowering path for efficient gathering.
  
  1. Outer `scf.forall` represents workgroup-level parallelism.
  2. Inner `scf.forall` represents subgroup-level parallelism.
  3. Each thread in the subgroup coalesces its memory accesses when
     gathering data and writes to its lane-offset in the destination tensor.
  
  ```mlir
  %result = scf.forall (%wg_i, %wg_j) in (16, 1) shared_outs(%wg_out = %dest) -> (tensor<128x16xf32>) {
    %indices_wg_slice = tensor.extract_slice ...
    %dest_wg_slice = tensor.extract_slice ...
  
    %inner_result = scf.forall (%sg_i, %sg_j) in (32, 1) shared_outs(%sg_out = %dest_wg_slice) -> (tensor<8x16xf32>) {
      scf.forall.in_parallel {
        iree_gpu.coalesced_gather_dma %indices_wg_slice, %source into %sg_out : ...
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
  
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %inner_result into %wg_out[...] : ...
    }
  } {mapping = [#gpu.warp<linear_dim_1>, #gpu.warp<linear_dim_0>]}
  ```
  """

  OPERATION_NAME = "iree_gpu.coalesced_gather_dma"

  _ODS_REGIONS = (0, True)

  def __init__(self, result, indices, source, init, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(indices)
    operands.append(source)
    operands.append(init)
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.append(result)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def indices(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def source(self) -> _ods_ir.Value:
    return self.operation.operands[1]

  @builtins.property
  def init(self) -> _ods_ir.Value:
    return self.operation.operands[2]

  @builtins.property
  def result(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def coalesced_gather_dma(result, indices, source, init, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return CoalescedGatherDMAOp(result=result, indices=indices, source=source, init=init, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class GlobalLoadDMAOp(_ods_ir.OpView):
  r"""
  This operation represents a subgroup-level global load DMA operation.
  It is used to represent a direct gathering operation from global memory to workgroup.
  To be specific, the thread gathers data from the global memoryspace at the designated
  indices, and stores it to the thread's lane-offset of the workgroup memref at the
  designated indices.
  
  Specifically, if the thread's subgroup lane id is `lane_id`, the thread will load the data
  from `$source[sourceIndices]` and store it to `$target[targetIndices] + lane_id`.
  Collectively, all threads in the subgroup orchestrate the load DMA operation.
  
  Note: each gather has a load width is 32bit.
  """

  OPERATION_NAME = "iree_gpu.global_load_dma"

  _ODS_REGIONS = (0, True)

  def __init__(self, source, sourceIndices, target, targetIndices, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(source)
    operands.extend(_get_op_results_or_values(sourceIndices))
    operands.append(target)
    operands.extend(_get_op_results_or_values(targetIndices))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def source(self) -> _ods_ir.Value:
    start, elements_per_group = _ods_equally_sized_accessor(self.operation.operands, 2, 2, 0, 0)
    return self.operation.operands[start]

  @builtins.property
  def sourceIndices(self) -> _ods_ir.OpOperandList:
    start, elements_per_group = _ods_equally_sized_accessor(self.operation.operands, 2, 2, 1, 0)
    return self.operation.operands[start:start + elements_per_group]

  @builtins.property
  def target(self) -> _ods_ir.Value:
    start, elements_per_group = _ods_equally_sized_accessor(self.operation.operands, 2, 2, 1, 1)
    return self.operation.operands[start]

  @builtins.property
  def targetIndices(self) -> _ods_ir.OpOperandList:
    start, elements_per_group = _ods_equally_sized_accessor(self.operation.operands, 2, 2, 2, 1)
    return self.operation.operands[start:start + elements_per_group]

def global_load_dma(source, source_indices, target, target_indices, *, loc=None, ip=None) -> GlobalLoadDMAOp:
  return GlobalLoadDMAOp(source=source, sourceIndices=source_indices, target=target, targetIndices=target_indices, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ValueBarrierOp(_ods_ir.OpView):
  r"""
  This operation acts as a barrier on a value semantic SSA values (tensor or
  vector). It takes multiple operands and produces a value equivalent to each
  input. This does not have copy and/or data movement semantics and simply
  represents a barrier on all writes in the tensor case, and a barrier until
  all threads acquire the input vector in the vector case.
  
  The inputs must be either all tensors, or all vectors.
  
  This operation is a no-op when not present in a parallel context. This
  operation is pure as it only requires synchronization for the value it
  produces.
  """

  OPERATION_NAME = "iree_gpu.value_barrier"

  _ODS_REGIONS = (0, True)

  def __init__(self, results_, inputs, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(inputs))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    results.extend(results_)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def inputs(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

  @builtins.property
  def results_(self) -> _ods_ir.OpResultList:
    _ods_variadic_group_length = len(self.operation.results) - 1 + 1
    return self.operation.results[0:0 + _ods_variadic_group_length]

def value_barrier(results_, inputs, *, loc=None, ip=None) -> _Union[_ods_ir.OpResult, _ods_ir.OpResultList, ValueBarrierOp]:
  op = ValueBarrierOp(results_=results_, inputs=inputs, loc=loc, ip=ip); results = op.results
  return results if len(results) > 1 else (results[0] if len(results) == 1 else op)

@_ods_cext.register_operation(_Dialect)
class YieldOp(_ods_ir.OpView):
  r"""
  This operation is used to yield values from a within a region.
  """

  OPERATION_NAME = "iree_gpu.yield"

  _ODS_REGIONS = (0, True)

  def __init__(self, values, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.extend(_get_op_results_or_values(values))
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def values(self) -> _ods_ir.OpOperandList:
    _ods_variadic_group_length = len(self.operation.operands) - 1 + 1
    return self.operation.operands[0:0 + _ods_variadic_group_length]

def yield_(values, *, loc=None, ip=None) -> YieldOp:
  return YieldOp(values=values, loc=loc, ip=ip)
