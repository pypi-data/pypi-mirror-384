"""
Advanced Kusto query sampling and validation tools.

These tools help build reliable KQL queries by sampling table data and validating
queries before execution, preventing common AI hallucination issues.
"""

import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

from ..core.exceptions import ValidationError, QueryError
from ..core.logging_config import get_logger
from ..core.performance import measure_operation
from ..utils.kusto_client import get_connection_manager
from ..utils.anti_hallucination import create_anti_hallucination_guard
from ..utils.helpers import truncate_string
from ..services.schema_cache_service import get_schema_cache_service

logger = get_logger("sampling")


class QuerySampler:
    """Samples table data to help build accurate queries"""
    
    def __init__(self):
        self.connection_manager = get_connection_manager()
        self.schema_cache = get_schema_cache_service()
        self.hallucination_guard = create_anti_hallucination_guard(self.schema_cache)
    
    async def sample_table_for_query_building(
        self,
        cluster: str,
        database: str,
        table: str,
        sample_size: int = 10,
        include_schema: bool = True
    ) -> Dict[str, Any]:
        """
        Sample a table specifically for query building purposes
        
        Returns comprehensive information needed to build reliable queries
        """
        with measure_operation("sample_table_for_query_building", {
            "table": f"{cluster}.{database}.{table}",
            "sample_size": sample_size
        }):\n            # Get schema information\n            schema = None\n            if include_schema:\n                schema = await self.schema_cache.get_schema(cluster, database, table)\n            \n            # Get sample data\n            sample_data = await self.connection_manager.sample_table(\n                cluster, database, table, sample_size\n            )\n            \n            if not sample_data:\n                raise ValidationError(f\"Unable to sample table {cluster}.{database}.{table}\")\n            \n            # Analyze sample data for query building insights\n            analysis = self._analyze_sample_data(sample_data[\"sample_rows\"], schema)\n            \n            result = {\n                \"table_path\": f\"{cluster}.{database}.{table}\",\n                \"cluster\": cluster,\n                \"database\": database,\n                \"table\": table,\n                \"sample_rows\": sample_data[\"sample_rows\"],\n                \"sample_size\": len(sample_data[\"sample_rows\"]),\n                \"sampled_at\": sample_data[\"sampled_at\"],\n                \"analysis\": analysis\n            }\n            \n            if schema:\n                result[\"schema\"] = {\n                    \"columns\": schema.columns,\n                    \"time_columns\": schema.time_columns,\n                    \"numeric_columns\": schema.numeric_columns,\n                    \"string_columns\": schema.string_columns,\n                    \"primary_time_column\": schema.primary_time_column\n                }\n            \n            logger.info(f\"Successfully sampled {cluster}.{database}.{table}: {len(sample_data['sample_rows'])} rows\")\n            return result\n    \n    def _analyze_sample_data(\n        self, \n        sample_rows: List[Dict[str, Any]], \n        schema: Optional[Any] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze sample data to provide query building insights\"\"\"\n        if not sample_rows:\n            return {\"insights\": [], \"recommendations\": []}\n        \n        insights = []\n        recommendations = []\n        \n        # Analyze column characteristics\n        column_analysis = {}\n        first_row = sample_rows[0]\n        \n        for column_name in first_row.keys():\n            values = [row.get(column_name) for row in sample_rows if column_name in row]\n            non_null_values = [v for v in values if v is not None]\n            \n            if not non_null_values:\n                continue\n            \n            # Calculate uniqueness\n            unique_values = set(str(v) for v in non_null_values)\n            uniqueness_ratio = len(unique_values) / len(non_null_values)\n            \n            column_analysis[column_name] = {\n                \"non_null_count\": len(non_null_values),\n                \"unique_count\": len(unique_values),\n                \"uniqueness_ratio\": uniqueness_ratio,\n                \"sample_values\": list(unique_values)[:5]  # First 5 unique values\n            }\n            \n            # Generate insights\n            if uniqueness_ratio < 0.1:\n                insights.append(f\"Column '{column_name}' has low cardinality (good for grouping)\")\n                recommendations.append(f\"Consider using '{column_name}' in GROUP BY clauses\")\n            elif uniqueness_ratio > 0.9:\n                insights.append(f\"Column '{column_name}' has high cardinality (likely identifier)\")\n            \n            # Check for time-like patterns\n            if any(self._looks_like_timestamp(str(v)) for v in non_null_values[:3]):\n                insights.append(f\"Column '{column_name}' appears to contain timestamps\")\n                recommendations.append(f\"Use '{column_name}' for time-based filtering with ago() function\")\n        \n        # Overall recommendations\n        if schema and schema.time_columns:\n            recommendations.append(f\"Primary time column: {schema.primary_time_column or schema.time_columns[0]}\")\n        \n        if schema and len(schema.string_columns) > 0:\n            low_cardinality_strings = [\n                col for col in schema.string_columns \n                if col in column_analysis and column_analysis[col][\"uniqueness_ratio\"] < 0.3\n            ]\n            if low_cardinality_strings:\n                recommendations.append(f\"Good grouping columns: {', '.join(low_cardinality_strings[:3])}\")\n        \n        return {\n            \"column_analysis\": column_analysis,\n            \"insights\": insights,\n            \"recommendations\": recommendations,\n            \"total_columns\": len(first_row),\n            \"sample_row_count\": len(sample_rows)\n        }\n    \n    def _looks_like_timestamp(self, value: str) -> bool:\n        \"\"\"Check if a value looks like a timestamp\"\"\"\n        timestamp_patterns = [\n            r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}',  # ISO format\n            r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',  # SQL format\n            r'\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}',  # US format\n        ]\n        \n        import re\n        return any(re.search(pattern, value) for pattern in timestamp_patterns)\n    \n    async def validate_query_with_sampling(\n        self,\n        cluster: str,\n        database: str,\n        table: str,\n        query: str,\n        sample_execution: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Validate a query by sampling and optional test execution\n        \n        This helps prevent hallucination by verifying the query against actual schema\n        and optionally testing it with a small limit.\n        \"\"\"\n        with measure_operation(\"validate_query_with_sampling\", {\n            \"table\": f\"{cluster}.{database}.{table}\",\n            \"query_length\": len(query)\n        }):\n            validation_result = {\n                \"valid\": True,\n                \"errors\": [],\n                \"warnings\": [],\n                \"suggestions\": []\n            }\n            \n            # Validate against schema\n            is_valid, errors, warnings = self.hallucination_guard.validate_query_against_schema(\n                query, cluster, database, table\n            )\n            \n            validation_result[\"valid\"] = is_valid\n            validation_result[\"errors\"].extend(errors)\n            validation_result[\"warnings\"].extend(warnings)\n            \n            # Sample execution if requested and query is valid\n            if sample_execution and is_valid:\n                try:\n                    # Add limit to query if not present\n                    test_query = query\n                    if \"take\" not in query.lower() and \"limit\" not in query.lower():\n                        test_query = f\"{query}\\n| take 5\"\n                    \n                    results, columns = await self.connection_manager.execute_query(\n                        cluster, database, test_query\n                    )\n                    \n                    validation_result[\"sample_execution\"] = {\n                        \"success\": True,\n                        \"row_count\": len(results),\n                        \"columns\": [col[\"name\"] for col in columns],\n                        \"sample_rows\": results[:3]  # Only first 3 rows\n                    }\n                    \n                    if not results:\n                        validation_result[\"warnings\"].append(\"Query returned no results\")\n                    \n                except Exception as e:\n                    validation_result[\"sample_execution\"] = {\n                        \"success\": False,\n                        \"error\": str(e)\n                    }\n                    validation_result[\"errors\"].append(f\"Sample execution failed: {str(e)}\")\n                    validation_result[\"valid\"] = False\n            \n            logger.info(f\"Query validation completed for {cluster}.{database}.{table}: {'VALID' if validation_result['valid'] else 'INVALID'}\")\n            return validation_result\n    \n    async def suggest_query_improvements(\n        self,\n        cluster: str,\n        database: str,\n        table: str,\n        query: str,\n        user_intent: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Analyze a query and suggest improvements based on table characteristics\n        \"\"\"\n        with measure_operation(\"suggest_query_improvements\"):\n            # Get table sample and schema\n            sample_info = await self.sample_table_for_query_building(\n                cluster, database, table, sample_size=20\n            )\n            \n            suggestions = {\n                \"performance_improvements\": [],\n                \"correctness_improvements\": [],\n                \"best_practices\": [],\n                \"column_suggestions\": {}\n            }\n            \n            # Analyze query for common issues\n            query_lower = query.lower()\n            \n            # Performance suggestions\n            if \"take\" not in query_lower and \"limit\" not in query_lower:\n                suggestions[\"performance_improvements\"].append(\n                    \"Consider adding '| take N' to limit results and improve performance\"\n                )\n            \n            if \"where\" not in query_lower and sample_info.get(\"schema\", {}).get(\"time_columns\"):\n                time_col = sample_info[\"schema\"][\"time_columns\"][0]\n                suggestions[\"performance_improvements\"].append(\n                    f\"Consider adding time filter: '| where {time_col} > ago(1h)' for better performance\"\n                )\n            \n            # Column suggestions based on analysis\n            if \"schema\" in sample_info:\n                schema = sample_info[\"schema\"]\n                analysis = sample_info[\"analysis\"]\n                \n                # Suggest good grouping columns\n                good_grouping_cols = [\n                    col for col in schema[\"string_columns\"]\n                    if col in analysis[\"column_analysis\"] \n                    and analysis[\"column_analysis\"][col][\"uniqueness_ratio\"] < 0.3\n                ]\n                \n                if good_grouping_cols:\n                    suggestions[\"column_suggestions\"][\"grouping\"] = good_grouping_cols[:3]\n                \n                # Suggest time columns\n                if schema[\"time_columns\"]:\n                    suggestions[\"column_suggestions\"][\"time_filtering\"] = schema[\"time_columns\"]\n                \n                # Suggest numeric columns for aggregation\n                if schema[\"numeric_columns\"]:\n                    suggestions[\"column_suggestions\"][\"aggregation\"] = schema[\"numeric_columns\"][:5]\n            \n            # Best practices\n            suggestions[\"best_practices\"].extend([\n                \"Use specific column names instead of '*' when possible\",\n                \"Add time-based filters to improve query performance\",\n                \"Use 'summarize' for aggregations instead of multiple queries\"\n            ])\n            \n            return suggestions


def create_query_sampler() -> QuerySampler:\n    \"\"\"Create and configure query sampler\"\"\"\n    return QuerySampler()"