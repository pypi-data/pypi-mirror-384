"""Polyadic tensor format and implementations of core operations.

A `PolyadicTensor` ``T`` represents a tensor in the polyadic decomposition (PD) format.
This format stores the factor matrices ``T.factors`` of the decomposition. The number of
factor matrices corresponds to the number of dimensions ``T.ndim`` of the tensor, i.e.,
its order. The polyadic decomposition expresses a tensor as a sum of ``T.nterm`` rank-1
tensors, which are the outer product of ``T.ndim`` nonzero vectors. For a third-order
tensor the first rank-1 tensor equals ``numpy.einsum("i,j,k", *[f[:,0] for f in
T.factors])``. The sum of all rank-1 tensors equals ``numpy.einsum("ir,jr,kr->ijk",
*T.factors)``.

Notes
-----
Refer to the canonical polyadic decomposition (CPD) routines in the
:mod:`pytensorlab.algorithms` module for the computation of a CPD.

See Also
--------
.cpd

Examples
--------
A `PolyadicTensor` can be created by providing the factor matrices:

>>> import pytensorlab as tl
>>> shape = (3, 4, 5)
>>> nterm = 2
>>> factors = tuple(tl.random.randn((s, nterm)) for s in shape)
>>> T = tl.PolyadicTensor(factors)

Refer to the documentation of :class:`PolyadicTensor` for more examples.
"""

import math
import sys
import warnings
from collections import abc
from collections.abc import Iterator, Sequence
from functools import singledispatch, singledispatchmethod
from typing import (
    Any,
    Literal,
    TypeVar,
    cast,
    overload,
)

import numba as nb  # type:ignore
import numpy as np
import scipy.optimize as opt
from numpy.typing import ArrayLike, DTypeLike

if sys.version_info >= (3, 11):
    from typing import Self
else:
    from typing_extensions import Self
from typing_extensions import Self

import pytensorlab.backends.numpy as tlb
from pytensorlab.datatypes.tensor import DenseTensor, Tensor
from pytensorlab.typing import (
    ArrayType,
    Axis,
    AxisLike,
    MatrixType,
    NormalizedBasicIndexType,
    NormalizedIndexType,
    NumberType,
    PositiveInt,
    Shape,
    SimpleIndex,
    SupportsArrayFromShape,
    VectorType,
    isbool,
    isint,
)
from pytensorlab.typing.core import ShapeLike
from pytensorlab.util.indextricks import (
    _reshape_axes,
    findfirst,
    normalize_axes,
)

from ..random._utils import _random, _random_like
from ..random.rng import get_rng
from ..util.utils import cumsum, hadamard, inprod_kr, is_numba_enabled, kr, noisy
from .core import (
    _mtkronprod_all,
    _mtkronprod_single,
    _mtkrprod_all,
    _mtkrprod_single,
    _tmprod,
    _tvprod,
    frob,
    tens2mat,
)


class PolyadicTensor(DenseTensor):
    """Represent a tensor in the polyadic decomposition format.

    Refer to the module documentation for more information on the polyadic
    decomposition. The array representation of a third-order `PolyadicTensor` can be
    constructed as ``np.einsum("ir,jr,kr->ijk", *T.factors)``.

    Parameters
    ----------
    factors : Sequence[ArrayLike]
        The factor matrices of the polyadic decomposition.

    Raises
    ------
    ValueError
        If factors is not a sequence of matrices.
    ValueError
        If not all factors have the same number of columns.

    Examples
    --------
    A `PolyadicTensor` can be created by providing the factor matrices:

    >>> import pytensorlab as tl
    >>> shape = (3, 4, 5)
    >>> nterm = 2
    >>> factors = tuple(tl.random.randn((s, nterm)) for s in shape)
    >>> T = tl.PolyadicTensor(factors)

    Alternatively, a `PolyadicTensor` can be created by proving a 1-D array which
    consists of the vectorized versions of the factor matrices stacked on top of each
    other:

    >>> data = np.concatenate([f.ravel() for f in factors])
    >>> T = tl.PolyadicTensor.from_vector(data, shape, nterm)

    A `PolyadicTensor` with random factor matrices can be created by providing the shape
    and the number of terms, which refers to the number of rank-1 tensors:

    >>> T = tl.PolyadicTensor.randn(shape, nterm)

    Here the standard normal distribution is used to generate the factor matrices. Other
    distributions are also available, and :func:`PolyadicTensor.random` can be used to
    specify any distribution. Complex-valued factors can be generated by using the
    optional `imag` argument.

    See Also
    --------
    from_vector, random, randn, rand, random_like, randn_like, rand_like, empty,
    empty_like, zeros, zeros_like
    """

    def __init__(self, factors: Sequence[ArrayLike]):
        try:
            # Test in case a third-order tensor is supplied as the user might expect
            # that CPD is computed, instead of constructing a PolyadicTensor.
            if factors.ndim == 3:  # type: ignore
                warnings.warn(
                    f"the supplied third-order ndarray is interpreted as a "
                    f"list of {factors.shape[0]} factor matrices; "  # type:ignore
                    f"If you mean to compute a CPD, use the cpd function. "
                    f"To disable this warning, use a list or tuple of factors "
                    f"instead.",
                    UserWarning,
                )
        except AttributeError:
            pass
        if any(not isinstance(f, np.ndarray) for f in factors):
            factors = tuple(np.asarray(f) for f in factors)
        factors = cast(tuple[MatrixType, ...], tuple(factors))

        if any(f.ndim != 2 for f in factors):
            idx = findfirst(f.ndim != 2 for f in factors)
            raise ValueError(
                f"factors is not a sequence of matrices: factor {idx} has "
                f"{factors[idx].ndim} dimensions while expected 2"
            )
        if any(0 in f.shape for f in factors):
            idx = findfirst(0 in f.shape for f in factors)
            raise ValueError(
                f"factor {idx} with shape ({factors[idx].shape[0]},"
                f"{factors[idx].shape[1]}) contains zero dimension"
            )
        if any(f.shape[1] != factors[0].shape[1] for f in factors):
            idx = findfirst(f.shape[1] != factors[0].shape[1] for f in factors)
            raise ValueError(
                f"not all factors have the same number of columns: factor {idx} has "
                f"{factors[idx].shape[1]} columns while expected {factors[0].shape[1]}"
            )

        self._data: VectorType = np.concatenate([f.ravel() for f in factors])
        shape = tuple(f.shape[0] for f in factors)
        super().__init__(shape)
        self.nterm: int = factors[0].shape[1]
        self._compute_factor_views()

    @classmethod
    def from_vector(
        cls, data: ArrayType, shape: Shape, nterm: PositiveInt, copy: bool = True
    ) -> Self:
        """Create a `PolyadicTensor` using the factors in vectorized format.

        Parameters
        ----------
        data : ArrayLike
            The vectorized factor matrices, concatenated in a vector.
        shape : Shape
            The dimensions of the tensor.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        copy : bool, default  = True
            If True, the `PolyadicTensor` stores an array copy of `data`. Otherwise, it
            stores a view to `data`.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with the factors provided in `data`.

        Raises
        ------
        ValueError
            If ``numpy.size(data)`` does not equal ``sum(shape) * nterm``.
        """
        obj = cls.__new__(cls)
        super(PolyadicTensor, obj).__init__(tuple(shape))
        obj._data = np.asarray(data, copy=copy if copy else None).ravel()
        if isinstance(data, np.ndarray):
            obj.setflags(write=copy or data.flags["WRITEABLE"])
        if obj._data.size != sum(shape) * nterm:
            raise ValueError(
                f"size of data does not match the expected number of parameters "
                f"(sum(shape) * nterm): size is {obj._data.size} while expected "
                f"{sum(shape) * nterm}"
            )

        obj.nterm = nterm
        obj._compute_factor_views()
        return obj

    @classmethod
    def empty(cls, shape: Shape, nterm: PositiveInt) -> Self:
        """Create a `PolyadicTensor` with empty factors.

        Parameters
        ----------
        shape : Shape
            The dimensions of the tensor.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with empty factors.

        See Also
        --------
        numpy.empty
        """
        return cls.random(shape, nterm, real=np.empty)

    @classmethod
    def zeros(cls, shape: Shape, nterm: PositiveInt) -> Self:
        """Create a `PolyadicTensor` with factors containing all zeros.

        Parameters
        ----------
        shape : Shape
            The dimensions of the tensor.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with factors containing all zeros.

        See Also
        --------
        numpy.zeros
        """
        return cls.random(shape, nterm, real=np.zeros)

    @classmethod
    def empty_like(cls, array: ArrayLike, nterm: PositiveInt) -> Self:
        """Create a `PolyadicTensor` with empty factors.

        Parameters
        ----------
        array : ArrayLike
            The shape of `array` determines the shape of the returned tensor. If `array`
            is complex, then the returned tensor is complex too.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with empty factors.

        See Also
        --------
        numpy.empty_like
        """
        return cls.random_like(array, nterm, real=np.empty)

    @classmethod
    def zeros_like(cls, array: ArrayLike, nterm: PositiveInt) -> Self:
        """Create a `PolyadicTensor` with factors containing all zeros.

        Parameters
        ----------
        array : ArrayLike
            The shape of `array` determines the shape of the returned tensor. If `array`
            is complex, then the returned tensor is complex too.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with factors containing all zeros.

        See Also
        --------
        numpy.zeros_like
        """
        return cls.random_like(array, nterm, real=np.zeros)

    @classmethod
    def random(
        cls,
        shape: Shape,
        nterm: PositiveInt,
        real: SupportsArrayFromShape | None = None,
        imag: SupportsArrayFromShape | bool | None = None,
    ) -> Self:
        """Generate a `PolyadicTensor` with random factors.

        By default, real factors are generated, drawn from a uniform distribution.
        The distribution can be changed by providing an alternative random number
        generating function to `real`. Complex factors can be generated by
        providing a random number generating function to `imag`.

        Parameters
        ----------
        shape : Shape
            The shape of the tensor.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        real : SupportsArrayFromShape, default = tl.get_rng().random
            Random number generating function for generating the real part of the
            factors.
        imag : SupportsArrayFromShape | bool, optional
            If this argument is provided, complex factors are generated. If `imag` is
            True, the function provided to `real` is also used to generate the imaginary
            part of the factors. If a random number generating function is provided,
            this function is used for generating the imaginary part of the factors.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with random factors, in which each element is drawn
            from `real` (with an optional imaginary part drawn from `imag`).

        See Also
        --------
        randn, rand, random_like, randn_like, rand_like
        """
        if nterm <= 0:
            raise ValueError(f"nterm is {nterm} but should be greater than zero")
        size = sum(shape) * nterm
        data = _random(size, real, imag)
        return cls.from_vector(data, shape, nterm)

    @classmethod
    def randn(
        cls,
        shape: Shape,
        nterm: PositiveInt,
        imag: bool | None = None,
        _rng: np.random.Generator | int | None = None,
    ) -> Self:
        """Generate random factors, drawn from a standard normal distribution.

        Parameters
        ----------
        shape : Shape
            The shape of the tensor.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        imag : bool, optional
            If True, the factors are complex, with the imaginary part also drawn
            from a standard normal distribution.
        _rng : numpy.random.Generator | int, optional
            Seed or random number generator used for all random operations in this
            function. If an integer is given, a new generator is created with that seed.
            If a generator is provided, it is used directly. If None, the global
            generator (set via `set_rng`) is used.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with random factors, in which each element is drawn
            from a standard normal distribution.
        """
        _rng = get_rng(_rng)

        return cls.random(shape, nterm, real=_rng.standard_normal, imag=imag)

    @classmethod
    def rand(
        cls,
        shape: Shape,
        nterm: PositiveInt,
        imag: bool | None = None,
        _rng: np.random.Generator | int | None = None,
    ) -> Self:
        """Generate random factors, drawn from a uniform distribution.

        Parameters
        ----------
        shape : Shape
            The shape of the tensor.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        imag : bool, optional
             If True, the factors are complex, with the imaginary part also drawn
             from a uniform distribution.
        _rng : numpy.random.Generator | int, optional
            Seed or random number generator used for all random operations in this
            function. If an integer is given, a new generator is created with that seed.
            If a generator is provided, it is used directly. If None, the global
            generator (set via `set_rng`) is used.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with random factors, in which each element is drawn
            from a uniform distribution.
        """
        _rng = get_rng(_rng)

        return cls.random(shape, nterm, real=_rng.random, imag=imag)

    @classmethod
    def random_like(
        cls,
        array: ArrayLike,
        nterm: PositiveInt,
        real: SupportsArrayFromShape | None = None,
        imag: SupportsArrayFromShape | bool | None = None,
    ) -> Self:
        """Generate a `PolyadicTensor` with random factors.

        By default, real factors are generated, drawn from a uniform distribution.
        The distribution can be changed by providing an alternative random number
        generating function to `real`. Complex factors can be generated by
        providing a random number generating function to `imag`.

        Parameters
        ----------
        array : ArrayLike
            The shape of `array` determines the shape of the returned tensor. If `array`
            is complex, then the returned tensor is complex too.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        real : SupportsArrayFromShape, default = tl.get_rng().random
            Random number generating function for generating the real part of the
            factors.
        imag : SupportsArrayFromShape | bool, optional
            Random number generating function for generating the imaginary part of the
            factors.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with random factors, in which each element is drawn
            from `real` (with an optional imaginary part drawn from `imag`).
        """
        shape = np.shape(array)
        size = sum(shape) * nterm
        data = _random_like(array, size, real, imag)
        return cls.from_vector(data, shape, nterm)

    @classmethod
    def randn_like(
        cls,
        array: ArrayLike,
        nterm: PositiveInt,
        imag: bool | None = None,
        _rng: np.random.Generator | int | None = None,
    ) -> Self:
        """Generate random factors, drawn from a standard normal distribution.

        Parameters
        ----------
        array : ArrayLike
            The shape of `array` determines the shape of the returned tensor. If `array`
            is complex, then the returned tensor is complex too.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        imag : bool, optional
            If True, also generate the imaginary part of the factors from the standard
            normal distribution.
        _rng : numpy.random.Generator | int, optional
            Seed or random number generator used for all random operations in this
            function. If an integer is given, a new generator is created with that seed.
            If a generator is provided, it is used directly. If None, the global
            generator (set via `set_rng`) is used.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with random factors, in which each element is drawn
            from a standard normal distribution.
        """
        _rng = get_rng(_rng)

        return cls.random_like(array, nterm, real=_rng.standard_normal, imag=imag)

    @classmethod
    def rand_like(
        cls,
        array: ArrayLike,
        nterm: PositiveInt,
        imag: bool | None = None,
        _rng: np.random.Generator | int | None = None,
    ) -> Self:
        """Generate random factors, drawn from a uniform distribution.

        Parameters
        ----------
        array : ArrayLike
            The shape of `array` determines the shape of the returned tensor. If `array`
            is complex, then the returned tensor is complex too.
        nterm : PositiveInt
            The number of terms or rank-1 tensors in the polyadic decomposition.
        imag : bool, optional
             If True, also generate the imaginary part of the factors from the uniform
             distribution.
        _rng : numpy.random.Generator | int, optional
            Seed or random number generator used for all random operations in this
            function. If an integer is given, a new generator is created with that seed.
            If a generator is provided, it is used directly. If None, the global
            generator (set via `set_rng`) is used.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with random factors, in which each element is drawn
            from a uniform distribution.
        """
        _rng = get_rng(_rng)

        return cls.random_like(array, nterm, real=_rng.random, imag=imag)

    def _compute_factor_views(self) -> None:
        """Compute views of the factors.

        Views of the factors are computed and stored.
        """
        bounds = [0] + list(cumsum(s * self.nterm for s in self.shape))
        self._factors = tuple(
            self._data[i:j].reshape(s, self.nterm)
            for (i, j, s) in zip(bounds[:-1], bounds[1:], self.shape)
        )

    @property
    def dtype(self) -> np.dtype[Any]:
        """Datatype of the tensor elements.

        Returns
        -------
        numpy.dtype[Any]
            The datatype of the tensor elements.
        """
        return self._data.dtype

    @property
    def factors(self) -> tuple[MatrixType, ...]:
        """Factor matrices of the polyadic tensor.

        The factor matrices are returned as a tuple of views of the underlying data.

        Returns
        -------
        tuple[MatrixType, ...]
            The factors of the `PolyadicTensor`.

        Examples
        --------
        The following can be used to view the first factor:

        >>> import pytensorlab as tl
        >>> T = tl.PolyadicTensor.randn((3, 3, 3), 2)
        >>> T.factors[0]
        array([[ 1.10572788,  0.5304342 ],
        ...    [ 0.22575759, -0.20260285],
        ...    [-0.03839152, -0.18930443]]) #random

        As `factors` provides a tuple of views of the underlying data, changing all
        elements in a factor matrix should be done as follows:

        >>> import numpy as np
        >>> T.factors[0][:] = np.ones((3, 2))

        Changing elements as follows is not allowed:

        >>> T.factors[0] = np.ones((3, 2))
        Traceback (most recent call last):
            ...
        TypeError: 'tuple' object does not support item assignment

        To change specific elements, use:

        >>> T.factors[2][0, 1] = 0
        """
        return self._factors

    def term(self, key: SimpleIndex, weights: ArrayLike | None = None) -> Self:
        """Select one or several rank-1 terms from the polyadic format.

        Parameters
        ----------
        key : SimpleIndex
            Indices that indicate which rank-1 terms are selected.
        weights : ArrayLike, optional
            The selected terms are scaled using `weights` in the resulting
            `PolyadicTensor`.

        Returns
        -------
        PolyadicTensor
            A `PolyadicTensor` with the terms selected by `key`, which optionally have
            been scaled by `weights`.

        Raises
        ------
        ValueError
            If the number of weights differs from the number of requested terms.
        """
        key = key if isinstance(key, Sequence) else (key,)
        if len(key) == 0:
            return type(self).zeros(self.shape, 1)
        if any(not isint(k) and not isbool(k) for k in key):
            raise IndexError(
                "key is not an integer or an array of integers or booleans"
            )
        if all(isbool(k) for k in key):
            key = tuple(i for i, b in enumerate(key) if b)
            if max(key) >= self.nterm:
                raise IndexError(
                    f"boolean index out of bounds; index {max(key)} is True but "
                    f"number of terms is {self.nterm}"
                )
        oobidx = next((i for i in key if not -self.nterm <= i < self.nterm), None)
        if oobidx:
            raise IndexError(f"term {oobidx} is out of bounds for {self.nterm} terms")

        factors = [
            f if f.ndim == 2 else np.reshape(f, (f.shape[0], 1))
            for f in (factor[:, key] for factor in self.factors)
        ]
        if weights is not None:
            weights = np.asarray(weights)
            if weights.size != factors[0].shape[1]:
                raise ValueError(
                    f"the number of weights ({weights.size}) does not match"
                    "the requested number of terms ({factors[0].shape})"
                )
            factors[0] *= np.reshape(weights, (1, factors[0].shape[1]))

        return type(self)(factors)

    def term_iter(self) -> Iterator[Self]:
        """Return an iterator over every rank-1 term.

        Returns
        -------
        Iterator[PolyadicTensor]
            Iterator over all ``self.nterm`` rank-1 terms as a rank-1 `PolyadicTensor`.

        See Also
        --------
        term
        """
        return iter(self.term(r) for r in range(self.nterm))

    def __str__(self) -> str:
        return f"Polyadic tensor of shape {self.shape} with {self.nterm} term(s)"

    def __repr__(self) -> str:
        if Tensor._REPR_MODE != "repr":
            return str(self)
        with np.printoptions(threshold=50, linewidth=88, precision=3, edgeitems=2):
            factors = ",\n".join(map(repr, self.factors))
            return f"{self.__class__.__name__}(\n({factors}),\n)"

    def transpose(self, axes: AxisLike | None = None) -> Self:
        """Return a tensor with permuted axes.

        Parameters
        ----------
        axes : AxisLike, optional
            If specified, it must be a permutation of ``range(self.ndim)``. The i-th
            axis of the returned tensor will correspond to the axis ``axes[i]`` of the
            input tensor. Negative indexing of these axes is allowed. If `axes` does not
            include all values in ``range(self.ndim)``, the remaining axes are appended
            in decreasing order.

        Returns
        -------
        PolyadicTensor
            A new `PolyadicTensor` with permuted axes.
        """
        axes = normalize_axes(axes, self.ndim, fill=True, fillreversed=True)
        return type(self)([self.factors[i] for i in axes])

    def __array__(self, dtype: DTypeLike = None, copy: bool | None = None) -> ArrayType:
        return _cpdgen(self.factors, (slice(None),) * self.ndim)

    def _entry(self, indices: Sequence[NormalizedIndexType]) -> ArrayType:
        return _cpdgen(self.factors, indices)

    def _getitem_sliced(self, key: Sequence[NormalizedBasicIndexType]) -> Self | float:
        def _factors(factors, key):
            factoriter = iter(factors)
            for k in key:
                if k is None:
                    yield np.ones((1, self.nterm), self._data.dtype)
                else:
                    yield np.reshape(next(factoriter)[k, :], (-1, self.nterm))

        factors = list(_factors(self.factors, key))

        # Return a scalar if all indices are integers
        if all(isint(k) for k in key):
            return np.sum(np.prod(factors, 0))

        # Remove modes with dimension one if selected using integer
        intidx = tuple(i for i, k in enumerate(key) if isint(k))
        if intidx:
            rmfactors = np.prod([factors[i] for i in intidx], 0)
            factors = [f for f, k in zip(factors, key) if not isint(k)]
            factors[-1] = factors[-1].copy() * rmfactors

        return type(self)(factors)

    def normalize(self) -> Self:
        """Normalize factors.

        Normalize the factors by ensuring that corresponding columns in each factor have
        the same norm.

        In the special case where a rank-1 term has a nonfinite norm due to an
        infinite-valued element, that term is not normalized.

        Examples
        --------
        >>> import pytensorlab as tl
        >>> T = tl.PolyadicTensor.randn((3, 4, 5), 3)
        >>> T.normalize()
        PolyadicTensor(
        (array([[-0.452, -1.22 , -0.43 ],
               [-1.076, -0.464, -0.304],
               [-0.291, -1.141, -1.83 ]]),
        array([[-0.603, -0.917,  1.466],
               [-0.367,  1.122, -0.627],
               [-0.794,  0.654,  0.26 ],
               [-0.564, -0.692,  1.008]]),
        array([[ 0.717, -0.967,  0.093],
               [-0.132, -0.985, -0.493],
               [ 0.193,  0.812, -0.738],
               [-0.558, -0.565,  1.348],
               [ 0.752, -0.35 , -1.007]])),
        )  #random

        In every rank-1 term all factor vectors have the same norm, i.e., for any ``r``,
        these are all equal:

        >>> import numpy as np
        >>> for r in range(T.nterm):
        >>>     [np.linalg.norm(T.term(r).factors[n]) for n in range(T.ndim)]
        """
        factor_norms = [tlb.norm(i, axis=0) for i in self.factors]
        rank1_norms = cast(
            VectorType, math.prod(factor_norms) ** (1 / float(self.ndim))
        )
        mask = [math.isfinite(rank1_norm) for rank1_norm in rank1_norms]
        for factor_norm in factor_norms:
            np.place(factor_norm, factor_norm == 0, 1)
        for factor, factor_norm in zip(self.factors, factor_norms):
            factor[:, mask] *= rank1_norms[mask] / factor_norm[mask]
        return self

    def termnorm(self, squared: bool = False) -> Sequence[float]:
        """Compute the Frobenius norms of all rank-1 terms.

        Parameters
        ----------
        squared : bool, default = False
            If True, return the squared Frobenius norms.

        Returns
        -------
        Sequence[float]
            The Frobenius norms of the rank-1 terms.
        """
        nrm = np.multiply.reduce([tlb.norm(f, 2, 0) for f in self.factors])
        return nrm**2 if squared else nrm

    def error(self, T: Self) -> Sequence[float]:
        """Compute the relative error per factor.

        Compute the relative Frobenius error for each factor, after optimally permuting
        and scaling the factor vectors in `T`. In particular, let ``U`` equal
        ``self.factors``, and ``V`` equal ``T.factors``, then, for ``n`` in
        ``range(self.ndim)``::

            err[n] = frob(U[n] - V[n] @ P @ D[n]) / frob(U[n])

        in which ``P`` is the permutation matrix and ``D`` is a tuple of diagonal
        scaling matrices ``D[n]``, each corresponding to the factor ``V[n]``.

        Parameters
        ----------
        T : PolyadicTensor
            The tensor to compare to.

        Returns
        -------
        err : Sequence[float]
            Relative difference in Frobenius norm for each factor.

        See Also
        --------
        term_error, match_congruence_kr

        Notes
        -----
        The relative errors for all factor matrices are computed independently, hence
        the product of the scaling matrices ``D[n]`` may not be equal to the identity
        matrix.
        """
        return [
            tlb.norm(u - v) / tlb.norm(u)
            for u, v in zip(self.factors, self.match(T, preserve_scale=False).factors)
        ]

    @overload
    def match(
        self,
        T: Self,
        preserve_scale: bool = True,
        permutation_vector: Literal[False] = False,
    ) -> Self: ...

    @overload
    def match(
        self, T: Self, preserve_scale: bool, permutation_vector: Literal[True]
    ) -> tuple[Self, Sequence[int]]: ...

    @overload
    def match(
        self, T: Self, *, permutation_vector: Literal[True]
    ) -> tuple[Self, Sequence[int]]: ...

    def match(
        self,
        T: Self,
        preserve_scale: bool = True,
        permutation_vector: bool = False,
    ) -> Self | tuple[Self, Sequence[int]]:
        """Match the rank-1 terms of a given tensor to the rank-1 terms of this tensor.

        Scale and permute the factor matrices of `T` to best match those of this tensor.

        Parameters
        ----------
        T : PolyadicTensor
            Tensor to be matched.
        preserve_scale : bool, default = True
            If True, the norms of the factor vectors of the rank-1 terms of `T` as well
            as the signs of their entries remain unchanged. Otherwise, the norms and
            signs are updated to match this tensor's rank-1 terms.
        permutation_vector : bool, default = False
            If True, returns the permutation vector to match the rank-1 terms as a
            second output argument.

        Returns
        -------
        PolyadicTensor
            `T` but with its rank-1 terms reordered to match the rank-1 terms in this
            tensor. If `preserve_scale` is False, the norms of the factor vectors of the
            rank-1 terms as well as the signs of their entries match those of this
            tensor's rank-1 terms.
        Sequence[int]
            The permutation vector used to match the rank-1 terms of `T`. Only returned
            if `permutation_vector` is True.

        See Also
        --------
        match_by_congruence

        Notes
        -----
        If `preserve_scale` is False, the norms and signs are updated to match this
        the rank-1 terms of this tensor. If there is a scale mismatch, each factor
        vector is scaled by the ``T.ndim``-th root of the scale ratio. If there are sign
        mismatches between the entries of factcor vectors, the appropriate sign changes
        are applied to the columns of the first factor matrix.
        """
        U = T.factors
        index = match_congruence_kr(self.factors, U)
        scale = np.empty((T.ndim, T.nterm), dtype=T._data.dtype)
        for n, (factor, u) in enumerate(zip(self.factors, U)):
            u[:] = u[:, index]
            scale[n, :] = np.diag(
                np.dot(factor.T, u.conj()) / np.diag(np.dot(u.conj().T, u))
            )
            if not preserve_scale:
                u[:] = u * scale[n, :]

        if preserve_scale:
            term_scale = np.multiply.reduce(scale, 0)
            if not np.iscomplexobj(term_scale):
                U[0][:, :] *= np.sign(term_scale)
                term_scale = abs(term_scale)
            for u, s in zip(U, scale):
                u[:] = u * s / term_scale ** (1 / T.ndim)

        return (type(self)(U), index) if permutation_vector else type(self)(U)

    def term_error(self, T: Self, high_precision: bool = False) -> Sequence[float]:
        """Compute relative errors between rank-1 terms of given tensor and this tensor.

        The relative error for each rank-1 term is computed after matching the rank-1
        terms in `T` to those of this tensor::

            term[r] = np.array(self.term(r))
            other[s] = np.array(T.term(s))
            error[r] = tl.frob(term[r] - other[s]) / tl.frob(term[r])

        where term ``s`` is matched to term ``r``.

        Parameters
        ----------
        T : PolyadicTensor
            Tensor to compare to.
        high_precision : bool, default = False
            To increase the speed, the difference between two rank-1 terms is computed
            indirectly, which may reduce the accuracy if the rank-1 terms are close. If
            the error between rank-1 terms is smaller than the square root of the
            precision and a high precision is needed, set to True.

        Returns
        -------
        Sequence[float]
            Relative error for each rank-1 term.
        """
        index = match_congruence_kr(self.factors, T.factors)
        residuals = (self.term(i) + T.term(j, -1) for i, j in enumerate(index))
        if high_precision:
            nrm = [frob(np.asarray(r)) for r in residuals]
        else:
            nrm = [frob(r) for r in residuals]
        return [n / nt for n, nt in zip(nrm, self.termnorm())]

    def reshape(self, shape: ShapeLike) -> Self:
        axes = _reshape_axes(self.shape, shape)

        def _new_factors():
            for ax in axes:
                if len(ax) == 1:
                    yield self.factors[ax[0]]
                else:
                    factors = tuple(self.factors[n] for n in ax)
                    yield kr(*factors)

        return type(self)(tuple(_new_factors()))

    @singledispatchmethod
    def __add__(self, T: Any) -> Any:
        return NotImplemented

    def copy(self) -> Self:
        """Create copy of tensor.

        Returns
        -------
        PolyadicTensor
            Copy of the tensor.
        """
        return type(self).from_vector(self._data, self.shape, self.nterm, copy=True)

    def __neg__(self) -> Self:
        T = self.copy()
        T.factors[0][:] *= -1
        return T

    def __sub__(self, other: Any) -> Any:
        return self + (-other)

    def iscomplex(self) -> bool:
        """Test if the tensor elements are complex.

        Returns
        -------
        bool
            If the tensor elements are complex.
        """
        return tlb.iscomplexobj(self._data)

    def conj(self) -> Self:
        """Compute complex conjugate tensor.

        Returns
        -------
        PolyadicTensor
            Complex conjugate `PolyadicTensor`.
        """
        if self.iscomplex():
            return cast(
                Self, type(self).from_vector(self._data.conj(), self.shape, self.nterm)
            )
        return self


def match_congruence_kr(
    A: PolyadicTensor | Sequence[MatrixType],
    B: PolyadicTensor | Sequence[MatrixType],
) -> Sequence[int]:
    """Match columns of Khatri--Rao matrices by congruence.

    Match each column in the Khatri--Rao product of the matrices in `A` with
    exactly one column in the Khatri--Rao product of the matrices in `B`, and
    return the permutation vector containing the optimal match for each column
    in the Khatri--Rao product of the matrices in `A`.

    Parameters
    ----------
    A : Sequence[MatrixType] | PolyadicTensor
        List of matrices with the same number of columns. If a PolyadicTensor is given,
        its factors are used instead.
    B : Sequence[MatrixType] | PolyadicTensor
        List of matrices with the same shape as the corresponding matrix in `A`. If a
        PolyadicTensor is given, its factors are used instead.

    Returns
    -------
    Sequence[int]
        Index vector that contains for each column in `A` the optimally matched column
        in `B`.

    Notes
    -----
    The matching is performed by formulating a linear sum assignment problem; see
    :func:`scipy.optimize.linear_sum_assignment`. The cost of the assignment problem
    is given by one minus the absolute value of the congruence (inner product of vectors
    normalized with respect to 2-norm) between the columns of the input matrices. The
    problem is solved using the Hungarian algorithm, which greedily chooses the lowest
    costs to match the columns without repetition.

    Examples
    --------
    >>> import pytensorlab as tl
    >>> from pytensorlab.datatypes.polyadic import match_congruence_kr
    >>> A = [tl.random.randn((3, 3)), tl.random.randn((4, 3))]
    >>> B = [a[:, [2, 0, 1]] + 1e-8 * tl.random.randn(a.shape) for a in A]
    >>> match_congruence_kr(A, B)
    [1, 2, 0]  #random
    """
    if isinstance(A, PolyadicTensor):
        A = A.factors
    if isinstance(B, PolyadicTensor):
        B = B.factors
    if len(A) != len(B):
        raise ValueError(f"len(A) = {len(A)} does not match len(B) = {len(B)}")
    if any(a.ndim != 2 for a in A):
        i = findfirst(a.ndim != 2 for a in A)
        raise ValueError(f"Element {i} in A is not a matrix")
    if any(a.shape != b.shape for a, b in zip(A, B)):
        i = findfirst(a.shape != b.shape for a, b in zip(A, B))
        raise ValueError(
            f"shape of A {A[i].shape} does not match shape of B {B[i].shape} for "
            f"index {i}"
        )

    An = [a / tlb.norm(a, 2, 0) for a in A]
    Bn = [b / tlb.norm(b, 2, 0) for b in B]

    C = np.abs(np.multiply.reduce([np.dot(a.conj().T, b) for a, b in zip(An, Bn)]))
    i, j = opt.linear_sum_assignment(1 - C)
    j[i] = j

    return j


T = TypeVar("T", MatrixType, Sequence[MatrixType])


@overload
def match_by_congruence(
    A: T,
    B: T,
    permutation_vector: Literal[False] = False,
) -> T: ...


@overload
def match_by_congruence(
    A: T, B: T, permutation_vector: Literal[True]
) -> tuple[T, Sequence[int]]: ...


def match_by_congruence(
    A: Sequence[MatrixType] | MatrixType,
    B: Sequence[MatrixType] | MatrixType,
    permutation_vector: bool = False,
) -> (
    tuple[MatrixType | Sequence[MatrixType], Sequence[int]]
    | MatrixType
    | Sequence[MatrixType]
):
    """Match columns by congruence.

    Match each column in `A` with exactly one column in `B`. If `A` and `B` are
    sequences of matrices, match each column in the Khatri--Rao product of the
    matrices in `A` with exactly one column in the Khatri--Rao product of the matrices
    in `B`.

    Parameters
    ----------
    A : Sequence[MatrixType] | MatrixType
        Matrix or list of matrices with the same number of columns.
    B : Sequence[MatrixType] | MatrixType
        Matrix or list of matrices with corresponding shape(s).
    permutation_vector : bool, default = False
        If True, returns the permutation vector to match the rank-1 terms as a second
        output argument.

    Returns
    -------
    Sequence[MatrixType]
        The permuted matrix `B` (or list of matrices `B`) such that all its columns
        match best with the columns of matrix `A` (or list of matrices `A`).
    Sequence[int]
        Index vector that contains for each column in `A` the optimally matched
        column in `B`. Only returned `when permutation_vector` is set to True.

    Notes
    -----
    The matching is performed by formulating a linear sum assignment problem; see
    :func:`scipy.optimize.linear_sum_assignment`. The cost of the assignment problem
    is given by one minus the absolute value of the congruence (inner product of vectors
    normalized with respect to 2-norm) between the columns of the input matrices. The
    problem is solved using the Hungarian algorithm, which greedily chooses the lowest
    costs to match the columns without repetition.

    Examples
    --------
    >>> from pytensorlab.datatypes.polyadic import match_by_congruence
    >>> import numpy as np
    >>> A = np.array(((1, -1), (0, 2)))
    >>> B = np.array(((-1.1, 1.1), (1.9, 0.1)))
    >>> match_by_congruence(A, B)
    array([[ 1.1, -1.1],
           [ 0.1,  1.9]])
    """
    return _match_by_congruence(A, B, permutation_vector)


@singledispatch
def _match_by_congruence(
    A: Sequence[MatrixType] | MatrixType,
    B: Sequence[MatrixType] | MatrixType,
    permutation_vector: bool = False,
) -> (
    tuple[MatrixType | Sequence[MatrixType], Sequence[int]]
    | MatrixType
    | Sequence[MatrixType]
):
    raise NotImplementedError(
        f"A and B have incompatible types: expected {type(A)}, but got {type(B)}"
    )


@_match_by_congruence.register
def _match_by_congruence_seq(
    A: abc.Sequence,
    B: Sequence[MatrixType],
    permutation_vector: bool = False,
) -> tuple[Sequence[MatrixType], Sequence[int]] | Sequence[MatrixType]:
    if isinstance(B, Sequence):
        perm = match_congruence_kr(A, B)
        Bmatched = [b[:, perm] for b in B]
    else:
        raise NotImplementedError(
            f"A and B have incompatible types: expected {type(A)}, but got {type(B)}"
        )

    if permutation_vector:
        return Bmatched, perm
    else:
        return Bmatched


@_match_by_congruence.register
def _match_by_congruence_mat(
    A: np.ndarray,
    B: MatrixType,
    permutation_vector: bool = False,
) -> tuple[MatrixType, Sequence[int]] | MatrixType:

    if isinstance(B, Sequence):
        raise NotImplementedError(
            f"A and B have incompatible types: expected {type(A)}, got {type(B)}"
        )
    else:
        perm = match_congruence_kr((A,), (B,))
        Bmatched = B[:, perm]

    if permutation_vector:
        return Bmatched, perm
    else:
        return Bmatched


@nb.njit(parallel=True, cache=True)
def _cpdgen_individual(factors, indices):
    M = len(indices[0])
    N = len(factors)
    R = factors[0].shape[1]
    dtype = factors[0].dtype
    out = np.empty(M, dtype=dtype)
    for i in nb.prange(M):
        tmp = np.ones(R, dtype=dtype)
        for j in nb.prange(N):
            tmp *= factors[j][indices[j][i], :]
        out[i] = sum(tmp)
    return out


def _cpdgen(
    factors: Sequence[MatrixType], indices: Sequence[NormalizedIndexType]
) -> ArrayType:
    # special case no slices, only individual indices
    if is_numba_enabled() and not any(isinstance(ind, slice) for ind in indices):
        return _cpdgen_individual(factors, indices)

    # determine output size
    def _operands():
        i = 0
        f_accumulate = None
        for f, ind in zip(factors, indices):
            if isinstance(ind, slice):
                if f_accumulate is not None:
                    yield f_accumulate.reshape((-1, f_accumulate.shape[1]))
                    yield (i, len(factors))
                    i += 1
                yield f[ind, :].reshape((-1, f.shape[1]))
                yield (i, len(factors))
                i += 1
                f_accumulate = None
            else:
                if f_accumulate is None:
                    f_accumulate = f[ind, :]
                else:
                    f_accumulate *= f[ind, :]
        if f_accumulate is not None:
            yield f_accumulate.reshape((-1, f_accumulate.shape[1]))
            yield (i, len(factors))
            i += 1
        yield np.arange(i)

    return tlb.einsum(*tuple(_operands()), optimize="greedy")


@frob.register
def _frob_polyadic(
    T: PolyadicTensor, squared: bool = False, _large_scale: bool = False
):
    if T.nterm**T.ndim < 1e4 and not _large_scale:
        U = [
            tlb.qr(u, mode="r")[: u.shape[0], :] if u.shape[0] > u.shape[1] else u
            for u in T.factors
        ]
        return frob(_cpdgen(U, (slice(None),) * T.ndim), squared)
    else:
        nrm2 = abs(np.sum(np.prod([np.dot(f.conj().T, f) for f in T.factors], 0)))
        return nrm2 if squared else math.sqrt(nrm2)


@noisy.register
def _noisy_polyadic(array: PolyadicTensor, snr: float, _rng=None):
    _rng = get_rng(_rng)

    return PolyadicTensor(noisy(array.factors, snr, _rng=_rng))


@_mtkrprod_single.register
def _mtkrprod_single_polyadic(
    T: PolyadicTensor, U: Sequence[MatrixType], axis: int, conjugate: bool = True
) -> MatrixType:
    return T.factors[axis] @ inprod_kr(T.factors, U, axis).conj()


@_mtkrprod_all.register
def _mtkrprod_all_polyadic(
    T: PolyadicTensor, U: Sequence[MatrixType], conjugate: bool = True
) -> Sequence[MatrixType]:
    V = T.factors
    VU = [v.T @ u.conj() for (v, u) in zip(V, U)]
    return [v @ hadamard(VU, exclude=n) for n, v in enumerate(V)]


@_mtkronprod_single.register
def _mtkronprod_single_polyadic(
    T: PolyadicTensor,
    U: Sequence[MatrixType],
    axis: int,
    transpose: Literal["T", "H"] | None = None,
) -> MatrixType:
    if transpose == "T":
        U = [u.T for u in U]
    elif transpose == "H":
        U = [u.conj().T for u in U]

    Umod = [u.conj().T @ f for n, (u, f) in enumerate(zip(U, T.factors)) if n != axis]
    Umod.insert(0, T.factors[axis])

    return tens2mat(PolyadicTensor(Umod), 0)


@_mtkronprod_all.register
def _mtkronprod_all_polyadic(
    T: PolyadicTensor,
    U: Sequence[MatrixType],
    transpose: Literal["T", "H"] | None = None,
) -> Sequence[MatrixType]:
    if transpose == "T":
        U = [u.T for u in U]
    elif transpose == "H":
        U = [u.conj().T for u in U]

    XHU = [u.conj().T @ f for u, f in zip(U, T.factors)]

    mtkronprods = []
    for n in range(T.ndim):
        Umod = [f for k, f in enumerate(XHU) if k != n]
        Umod.insert(0, T.factors[n])
        mtkronprods.append(tens2mat(PolyadicTensor(Umod), 0))

    return mtkronprods


@_tmprod.register
def _tmprod_polyadic(
    T: PolyadicTensor,
    matrices: Sequence[MatrixType],
    axes: Axis,
) -> PolyadicTensor:
    matiter = iter(matrices)
    factors = [next(matiter) @ f if n in axes else f for n, f in enumerate(T.factors)]
    return PolyadicTensor(factors)


@_tvprod.register
def _tvprod_polyadic(
    T: PolyadicTensor, vectors: Sequence[VectorType], axes: Axis
) -> PolyadicTensor | VectorType | NumberType:
    row_factors = [v[np.newaxis, :] @ T.factors[a] for a, v in zip(axes, vectors)]
    rows_prod = np.multiply.reduce(np.stack(row_factors))
    if len(axes) == T.ndim:
        return np.sum(rows_prod)

    factors = [f for a, f in enumerate(T.factors) if a not in axes]
    factors[0] = factors[0] * rows_prod
    if len(axes) == T.ndim - 1:
        return np.sum(factors[0], axis=1)
    return PolyadicTensor(factors)
