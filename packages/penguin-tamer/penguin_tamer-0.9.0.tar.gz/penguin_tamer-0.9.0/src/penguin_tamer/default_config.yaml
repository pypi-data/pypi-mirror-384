
language: en

global:
  user_content: You are a professional in Linux and Windows. Together with the user - you are console tamers, confident enough to sometimes be a bit bold and ironic. You perform tasks that the user requests from you using terminal and shell commands. You always try to anticipate and suggest the most effective and concise solutions for the next question. You ALWAYS warn about potentially dangerous commands.
  current_LLM: "llm_1" # Текущий выбранный LLM из списка supported_LLMs (ID)
  # === LLM Generation Parameters ===
  temperature: 0.8 # Температура (0.0-2.0): выше = креативнее, ниже = детерминированнее
  max_tokens: null # Максимум токенов в ответе (null = без ограничения, рекомендуется: 2000-4000)
  top_p: 0.95 # Nucleus sampling (0.0-1.0): альтернатива temperature, обычно 0.9-1.0
  frequency_penalty: 0.0 # Штраф за повторы (-2.0 до 2.0): положительные значения уменьшают повторение токенов
  presence_penalty: 0.0 # Штраф за упоминание (-2.0 до 2.0): стимулирует новые темы
  stop: null # Стоп-последовательности (null или список строк, например: ["\n\n\n", "Human:"])
  seed: null # Seed для детерминизма (null = случайно, число = воспроизводимо)

  # === Interface Settings ===
  sleep_time: 0.01 # Задержка между обновлениями в потоковом режиме (секунды)
  refresh_per_second: 10 # Частота обновления интерфейса в потоковом режиме (обновлений в секунду)
  markdown_theme: "default" # Тема для Markdown: default, monokai, dracula, nord, solarized_dark, github, matrix, minimal
  debug_mode: false # Режим отладки: показывает структуру всех сообщений, отправляемых в LLM

  # === Context Management ===
  add_execution_to_context: true # Добавлять результаты выполнения команд в контекст беседы (true/false). При false экономит токены.

  # === Demo System Settings ===
  demo_mode: "off" # Режим демонстрации: off, record, play
  demo_file: null # Файл для воспроизведения в режиме play (если null, используется последний записанный)
  demo_play_first_input: false # В режиме воспроизведения, воспроизводить ли первый ввод пользователя (true/false)

supported_LLMs:
  llm_1:
    provider: Pollinations
    model: mistral
  llm_2:
    provider: "OpenRouter (free)"
    model: "deepseek/deepseek-chat-v3.1:free"
  llm_3:
    provider: "OpenAI"
    model: "gpt-4.1-mini"


supported_Providers:
  Pollinations:
    client_name: "pollinations" # Тип клиента: openrouter, openai, pollinations (НЕ РЕДАКТИРОВАТЬ в UI)
    api_url: "https://text.pollinations.ai"
    api_list: "https://text.pollinations.ai/models"
    api_key: "Not used" # API ключ не требуется для Pollinations (бесплатные модели tier=anonymous)
    filter: null # Опционально: фильтр моделей по подстроке
  OpenRouter (free):
    client_name: "openrouter" # Тип клиента: openrouter, openai, pollinations (НЕ РЕДАКТИРОВАТЬ в UI)
    api_url: "https://openrouter.ai/api/v1"
    api_list: "https://openrouter.ai/api/v1/models"
    api_key: "" # Your OpenRouter API key here
    filter: "free"
  OpenRouter:
    client_name: "openrouter" # Тип клиента: openrouter, openai, pollinations (НЕ РЕДАКТИРОВАТЬ в UI)
    api_url: "https://openrouter.ai/api/v1"
    api_list: "https://openrouter.ai/api/v1/models"
    api_key: "" # Your OpenRouter API key here
    filter: null # Optional: filter models by substring (e.g., "free" to show only free models)
  OpenAI:
    client_name: "openai" # Тип клиента: openrouter, openai, pollinations (НЕ РЕДАКТИРОВАТЬ в UI)
    api_url: "https://api.openai.com/v1"
    api_list: "https://api.openai.com/v1/models"
    api_key: "" # Your OpenAI API key here
    filter: null # Optional: filter models by substring

