Metadata-Version: 2.4
Name: fmtr.tools
Version: 1.3.77
Summary: Collection of high-level tools to simplify everyday development tasks, with a focus on AI/ML
Home-page: https://github.com/fmtr/fmtr.tools
Author: Frontmatter
Author-email: innovative.fowler@mask.pro.fmtr.dev
License: Copyright Â© 2025 Frontmatter. All rights reserved.
Description-Content-Type: text/markdown
License-File: LICENSE
Provides-Extra: dev
Requires-Dist: logfire; extra == "dev"
Requires-Dist: semver; extra == "dev"
Requires-Dist: pydevd-pycharm~=251.25410.159; extra == "dev"
Requires-Dist: pydantic-settings; extra == "dev"
Requires-Dist: pydantic; extra == "dev"
Requires-Dist: pydantic-extra-types; extra == "dev"
Requires-Dist: pycountry; extra == "dev"
Requires-Dist: yamlscript; extra == "dev"
Requires-Dist: pyyaml; extra == "dev"
Requires-Dist: yamlscript; extra == "dev"
Requires-Dist: pyyaml; extra == "dev"
Requires-Dist: beanie[odm]; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest-cov; extra == "test"
Provides-Extra: yaml
Requires-Dist: yamlscript; extra == "yaml"
Requires-Dist: pyyaml; extra == "yaml"
Provides-Extra: logging
Requires-Dist: logfire; extra == "logging"
Provides-Extra: parallel
Requires-Dist: dask[bag]; extra == "parallel"
Requires-Dist: distributed; extra == "parallel"
Requires-Dist: bokeh; extra == "parallel"
Provides-Extra: tokenization
Requires-Dist: tokenizers; extra == "tokenization"
Provides-Extra: augmentation
Requires-Dist: faker; extra == "augmentation"
Requires-Dist: sre_yield; extra == "augmentation"
Provides-Extra: process
Requires-Dist: logfire; extra == "process"
Provides-Extra: profiling
Requires-Dist: contexttimer; extra == "profiling"
Provides-Extra: docker-client
Requires-Dist: python-on-whales; extra == "docker-client"
Provides-Extra: unicode
Requires-Dist: Unidecode; extra == "unicode"
Provides-Extra: version
Provides-Extra: version-dev
Requires-Dist: semver; extra == "version-dev"
Provides-Extra: spaces
Requires-Dist: tinynetrc; extra == "spaces"
Provides-Extra: netrc
Requires-Dist: tinynetrc; extra == "netrc"
Provides-Extra: hfh
Requires-Dist: huggingface_hub; extra == "hfh"
Provides-Extra: merging
Requires-Dist: deepmerge; extra == "merging"
Provides-Extra: api
Requires-Dist: fastapi; extra == "api"
Requires-Dist: uvicorn[standard]; extra == "api"
Requires-Dist: logfire; extra == "api"
Requires-Dist: pydantic; extra == "api"
Requires-Dist: pydantic-extra-types; extra == "api"
Requires-Dist: pycountry; extra == "api"
Requires-Dist: logfire[fastapi]; extra == "api"
Provides-Extra: ai
Requires-Dist: peft; extra == "ai"
Requires-Dist: transformers[sentencepiece]; extra == "ai"
Requires-Dist: torchvision; extra == "ai"
Requires-Dist: torchaudio; extra == "ai"
Requires-Dist: pydantic; extra == "ai"
Requires-Dist: pydantic-extra-types; extra == "ai"
Requires-Dist: pycountry; extra == "ai"
Provides-Extra: dm
Requires-Dist: pydantic; extra == "dm"
Requires-Dist: pydantic-extra-types; extra == "dm"
Requires-Dist: pycountry; extra == "dm"
Provides-Extra: openai-api
Requires-Dist: openai; extra == "openai-api"
Provides-Extra: ai-client
Requires-Dist: logfire; extra == "ai-client"
Requires-Dist: pydantic; extra == "ai-client"
Requires-Dist: pydantic-extra-types; extra == "ai-client"
Requires-Dist: pycountry; extra == "ai-client"
Requires-Dist: openai; extra == "ai-client"
Requires-Dist: pydantic-ai[logfire,openai]; extra == "ai-client"
Requires-Dist: ollama; extra == "ai-client"
Provides-Extra: json-fix
Requires-Dist: json_repair; extra == "json-fix"
Provides-Extra: semantic
Requires-Dist: sentence_transformers; extra == "semantic"
Requires-Dist: pandas; extra == "semantic"
Requires-Dist: tabulate; extra == "semantic"
Requires-Dist: openpyxl; extra == "semantic"
Requires-Dist: odfpy; extra == "semantic"
Requires-Dist: deepdiff; extra == "semantic"
Provides-Extra: metric
Requires-Dist: pandas; extra == "metric"
Requires-Dist: tabulate; extra == "metric"
Requires-Dist: openpyxl; extra == "metric"
Requires-Dist: odfpy; extra == "metric"
Requires-Dist: deepdiff; extra == "metric"
Provides-Extra: tabular
Requires-Dist: pandas; extra == "tabular"
Requires-Dist: tabulate; extra == "tabular"
Requires-Dist: openpyxl; extra == "tabular"
Requires-Dist: odfpy; extra == "tabular"
Requires-Dist: deepdiff; extra == "tabular"
Provides-Extra: html
Requires-Dist: html2text; extra == "html"
Provides-Extra: interface
Requires-Dist: flet[all]; extra == "interface"
Requires-Dist: flet-video; extra == "interface"
Requires-Dist: flet-webview; extra == "interface"
Provides-Extra: google-api
Requires-Dist: google-auth; extra == "google-api"
Requires-Dist: google-auth-oauthlib; extra == "google-api"
Requires-Dist: google-auth-httplib2; extra == "google-api"
Requires-Dist: google-api-python-client; extra == "google-api"
Provides-Extra: caching
Requires-Dist: diskcache; extra == "caching"
Requires-Dist: cachetools; extra == "caching"
Provides-Extra: pdf
Requires-Dist: pymupdf; extra == "pdf"
Requires-Dist: pydantic; extra == "pdf"
Requires-Dist: pydantic-extra-types; extra == "pdf"
Requires-Dist: pycountry; extra == "pdf"
Requires-Dist: pymupdf4llm; extra == "pdf"
Provides-Extra: debug
Requires-Dist: pydevd-pycharm~=251.25410.159; extra == "debug"
Provides-Extra: sets
Requires-Dist: pydantic-settings; extra == "sets"
Requires-Dist: pydantic; extra == "sets"
Requires-Dist: pydantic-extra-types; extra == "sets"
Requires-Dist: pycountry; extra == "sets"
Requires-Dist: yamlscript; extra == "sets"
Requires-Dist: pyyaml; extra == "sets"
Provides-Extra: path-app
Requires-Dist: appdirs; extra == "path-app"
Provides-Extra: path-type
Requires-Dist: filetype; extra == "path-type"
Provides-Extra: dns
Requires-Dist: dnspython[doh]; extra == "dns"
Requires-Dist: httpx; extra == "dns"
Requires-Dist: httpx_retries; extra == "dns"
Requires-Dist: logfire; extra == "dns"
Requires-Dist: logfire[httpx]; extra == "dns"
Provides-Extra: patterns
Requires-Dist: regex; extra == "patterns"
Provides-Extra: http
Requires-Dist: httpx; extra == "http"
Requires-Dist: httpx_retries; extra == "http"
Requires-Dist: logfire; extra == "http"
Requires-Dist: logfire[httpx]; extra == "http"
Provides-Extra: setup
Requires-Dist: setuptools; extra == "setup"
Provides-Extra: webhook
Requires-Dist: httpx; extra == "webhook"
Requires-Dist: httpx_retries; extra == "webhook"
Requires-Dist: logfire; extra == "webhook"
Requires-Dist: logfire[httpx]; extra == "webhook"
Provides-Extra: browsers
Requires-Dist: playwright; extra == "browsers"
Provides-Extra: db
Provides-Extra: db-document
Requires-Dist: beanie[odm]; extra == "db-document"
Provides-Extra: all
Requires-Dist: tabulate; extra == "all"
Requires-Dist: Unidecode; extra == "all"
Requires-Dist: pycountry; extra == "all"
Requires-Dist: flet-webview; extra == "all"
Requires-Dist: appdirs; extra == "all"
Requires-Dist: html2text; extra == "all"
Requires-Dist: logfire; extra == "all"
Requires-Dist: bokeh; extra == "all"
Requires-Dist: google-auth-oauthlib; extra == "all"
Requires-Dist: pytest-cov; extra == "all"
Requires-Dist: pymupdf; extra == "all"
Requires-Dist: httpx; extra == "all"
Requires-Dist: logfire[httpx]; extra == "all"
Requires-Dist: setuptools; extra == "all"
Requires-Dist: regex; extra == "all"
Requires-Dist: python-on-whales; extra == "all"
Requires-Dist: semver; extra == "all"
Requires-Dist: pandas; extra == "all"
Requires-Dist: playwright; extra == "all"
Requires-Dist: faker; extra == "all"
Requires-Dist: transformers[sentencepiece]; extra == "all"
Requires-Dist: httpx_retries; extra == "all"
Requires-Dist: odfpy; extra == "all"
Requires-Dist: logfire[fastapi]; extra == "all"
Requires-Dist: pydantic-settings; extra == "all"
Requires-Dist: pymupdf4llm; extra == "all"
Requires-Dist: ollama; extra == "all"
Requires-Dist: pyyaml; extra == "all"
Requires-Dist: pydevd-pycharm~=251.25410.159; extra == "all"
Requires-Dist: dask[bag]; extra == "all"
Requires-Dist: sre_yield; extra == "all"
Requires-Dist: deepdiff; extra == "all"
Requires-Dist: flet[all]; extra == "all"
Requires-Dist: openai; extra == "all"
Requires-Dist: dnspython[doh]; extra == "all"
Requires-Dist: json_repair; extra == "all"
Requires-Dist: pydantic-extra-types; extra == "all"
Requires-Dist: pydantic; extra == "all"
Requires-Dist: flet-video; extra == "all"
Requires-Dist: sentence_transformers; extra == "all"
Requires-Dist: google-auth; extra == "all"
Requires-Dist: filetype; extra == "all"
Requires-Dist: torchaudio; extra == "all"
Requires-Dist: deepmerge; extra == "all"
Requires-Dist: google-auth-httplib2; extra == "all"
Requires-Dist: diskcache; extra == "all"
Requires-Dist: peft; extra == "all"
Requires-Dist: fastapi; extra == "all"
Requires-Dist: yamlscript; extra == "all"
Requires-Dist: contexttimer; extra == "all"
Requires-Dist: uvicorn[standard]; extra == "all"
Requires-Dist: pydantic-ai[logfire,openai]; extra == "all"
Requires-Dist: google-api-python-client; extra == "all"
Requires-Dist: cachetools; extra == "all"
Requires-Dist: huggingface_hub; extra == "all"
Requires-Dist: tinynetrc; extra == "all"
Requires-Dist: openpyxl; extra == "all"
Requires-Dist: beanie[odm]; extra == "all"
Requires-Dist: distributed; extra == "all"
Requires-Dist: torchvision; extra == "all"
Requires-Dist: tokenizers; extra == "all"
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: summary

# `fmtr.tools`

A collection of high-level tools to simplify everyday development tasks, with a slight focus on full-stack AI/ML.

This repository is an attempt to provide a one-stop source for a wide range of utilities and tools designed to streamline a typical, modern development workflow. There is an emphasis on a lean and nimble approach to dependencies, which tries to strike a balance between powerful functionality while avoiding unnecessary bloat.

## Why?

Personally, I'm grossly impatient, and simply resent writing the same code, however simple, in multiple projects.

This could be trivial stuff like reading an integer from an environment variable (while handling errors gracefully) - or more complex ones (like just wanting a simple parallel-processing function without writing Queues, or remembering which libraries you need to do it for you).

At the same time, I find that traditional tools collections inevitably become bloated and unwieldy over time, so wanted something with a somewhat sophisticated approach to dependencies.

## Key Features

- Wide-Ranging Utilities: The collection includes tools for configuration, data types, environment management, functions, hashing, importing, iterating, JSON handling, path manipulation, platform-specific operations, randomness, and string operations.
- Lean Dependencies: Dependencies are managed via extras, allowing you to install only what you need. Missing dependencies are handled in a clear way, telling you what's missing and how to install it.

## Installing

The base library can be installed like this:

```bash
pip install fmtr.tools
```

## Usage

Some simple import and usage examples

### Read an integer from an environment variable and write it to a (human-readable) JSON file

```python
from fmtr import tools
from fmtr.tools import Path

value=tools.env.get_int('MY_VALUE',default=None)
data=dict(value=value)
Path('data.json').write_json(data)
```

### Zero-faff parallel multi-processing

Install the extra:

```bash
pip install fmtr.tools[parallel] --upgrade
```

```python
from fmtr.tools import parallel

def expensive_computation(n):
    import math
    result = 0
    for i in range(1, n + 1):
        result += math.sqrt(i) * math.sin(i) * math.log(i)
    return result

if __name__ == '__main__':
    results=parallel.apply(expensive_computation, [10_000] * 1_000)
```

## Extras

Most tools require no additional dependencies, but for any that do, you can add them like this:

```bash
pip install fmtr.tools[<extra>] --upgrade
```

If you try to use a module without the required extras, you'll get a message telling you which one is needed:

```
MissingExtraError: The current module is missing dependencies. To install them, run: `pip install fmtr.tools[logging] --upgrade`
```

## Modules

The included modules, plus any extra requirements, are as follows:

- `tools.ai`: Manages bulk inference for LLMs using dynamic batching. Includes classes for managing prompt encoding, generating outputs, and handling tool calls, with support for both local and remote models. Uses Pytorch and Transformers for model operations, and provides functionality for encoding prompts, generating responses, and applying tool functions.
  - Extras: `ai`
- `tools.config`: Base config class with overridable field processors.
    - Extras: None
-
`tools.settings`: A base configuration system built on Pydantic Settings that provides a flexible way to manage application settings from multiple sources, based on a standard
  - Extras: `sets`
`path.PackagePaths` project layout.
  - Extras: `sets`
- `tools.dataclass`: Utilities for extracting and filtering fields and metadata from dataclasses, with support for applying filters and retrieving enabled fields based on metadata attributes.
    - Extras: None
- `tools.datatype`
    - Extras: None
- `tools.dm`: Defines custom data modelling base classes for creating Pydantic models with error-tolerant deserialization from JSON (e.g. when output from an LLM).
  - Extras: `dm`
- `tools.environment`: Tools for managing environment variables, including functions to retrieve variables with type conversions and default values. Features include environment variable fetching, handling missing variables, and creating type-specific getters for integers, floats, booleans, dates, and paths.
    - Extras: None
- `tools.env`: Alias of `tools.environment`.
    - Extras: None
- `tools.function`: Utilities for combining and splitting arguments and keyword arguments.
    - Extras: None
- `tools.hash`: String hashing
    - Extras: None
- `tools.hfh`: Utilities for caching and managing Hugging Face model repositories: setting tokens, downloading snapshots, tagging repositories, and retrieving local cache paths.
  - Extras: `hfh`
- `tools.html`: Utilities for converting HTML documents to plain text.
  - Extras: `html`
- `tools.interface`: Provides a base class for building Flutter/Flet apps.
  - Extras: `interface`
- `tools.iterator`: Pivoting/unpivoting data structures
    - Extras: None
- `tools.json`: Serialisation/deserialisation to human-readable, unicode JSON.
    - Extras: None
- `tools.merge`: Utility for recursively merging multiple dictionaries or objects using the DeepMerge library.
  - Extras: `merge`
- `tools.name`: Generates random memorable names (similar to Docker Container names) by combining an adjective with a surname.
  - Extras: None
- `tools.openai`: Utilities for interacting with the OpenAI API, simple text-to-text output, etc.
  - Extras: `openai.api`
- `tools.Path`: Enhanced
  `pathlib.Path` object with additional functionality for Windows-to-Unix path conversion in WSL environments, reading/writing JSON and YAML files with proper encoding.
    - Extras: None
-
`tools.PackagePaths` class for managing canonical package paths, like settings files, artifact directories, version files.
  - Extras: None
- `tools.AppPaths` Wrapper around `appdirs` for application paths.
  - Extras: `paths.app`
- `tools.platform`: Detecting if host is WSL, Docker etc.
    - Extras: None
- `tools.ContextProcess`: Manages a function running in a separate process using a context manager. Provides methods to start, stop, and restart the process, with configurable restart delays. Useful for ensuring clean process management and automatic stopping when the context manager exits.
    - Extras: None
- `tools.random`: Provides additional functions for random number generation and selection, useful for data augmentation.
    - Extras: None
- `tools.semantic`: Manages semantic similarity operations using Sentence Transformers: loading a pre-trained model, vectorizing a text corpus, and retrieving the top matches based on similarity scores for a given query string.
  - Extras: `semantic`
- `tools.string`: Provides utilities for handling string formatting.
    - Extras: None
- `tools.logging`: Configures and initializes a logger using the Logfire library to log to an OpenTelemetry consumer.
    - Extras: `logging`
- `tools.logger`: Prefabricated
  `logger` object, suitable for most projects: service name, colour-coded, timestamped, etc.
    - Extras: `logging`
- `tools.augmentation`: Data augmentation stub.
    - Extras: `augmentation`
- `tools.Container`: Runs a Docker container within a context manager, ensuring the container is stopped and removed when the context is exited.
    - Extras: `docker.api`
- `tools.parallel`: Provides utilities for parallel computation using Dask. Supports executing functions across multiple workers or processes, handles different data formats, and options for progress display and parallelism configuration.
    - Extras: `parallel`
- `tools.profiling`: Context-based code timing.
    - Extras: `profiling`
- `tools.tokenization`: Provides utilities for creating and configuring tokenizers using the Tokenizers library. Iincludes functions for training both word-level and byte-pair encoding (BPE) tokenizers, applying special formatting and templates, and managing tokenizer configurations such as padding, truncation, and special tokens.
    - Extras: `tokenization`
- `tools.unicode`: Simple unicode decoding (via `Unidecode`).
  - Extras: `unicode`

## Contribution

Any contributions would be most welcome! If you have a utility that fits well within this collection, or improvements to existing tools, feel free to open a pull request.

## License

This project is licensed under the Apache License Version 2.0. See the [LICENSE](LICENSE) file for more details.
