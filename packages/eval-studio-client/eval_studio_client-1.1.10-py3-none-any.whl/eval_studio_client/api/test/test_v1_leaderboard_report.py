# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_leaderboard_report import V1LeaderboardReport

class TestV1LeaderboardReport(unittest.TestCase):
    """V1LeaderboardReport unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1LeaderboardReport:
        """Test V1LeaderboardReport
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1LeaderboardReport`
        """
        model = V1LeaderboardReport()
        if include_optional:
            return V1LeaderboardReport(
                results = [
                    eval_studio_client.api.models.v1_leaderboard_report_result.v1LeaderboardReportResult(
                        key = '', 
                        input = '', 
                        corpus = [
                            ''
                            ], 
                        context = [
                            ''
                            ], 
                        categories = [
                            ''
                            ], 
                        relationships = [
                            eval_studio_client.api.models.v1_leaderboard_report_result_relationship.v1LeaderboardReportResultRelationship(
                                type = '', 
                                target = '', 
                                target_type = '', )
                            ], 
                        expected_output = '', 
                        output_constraints = [
                            ''
                            ], 
                        output_condition = '', 
                        actual_output = '', 
                        actual_duration = 1.337, 
                        cost = 1.337, 
                        model_key = '', 
                        test_case_key = '', 
                        metrics = [
                            eval_studio_client.api.models.v1_metric_score.v1MetricScore(
                                key = '', 
                                value = 1.337, )
                            ], 
                        result_error_message = '', 
                        actual_output_meta = [
                            eval_studio_client.api.models.v1_leaderboard_report_actual_output_meta.v1LeaderboardReportActualOutputMeta(
                                tokenization = '', 
                                data = [
                                    eval_studio_client.api.models.v1_leaderboard_report_actual_output_data.v1LeaderboardReportActualOutputData(
                                        text = '', )
                                    ], )
                            ], )
                    ],
                models = [
                    eval_studio_client.api.models.v1_leaderboard_report_model.v1LeaderboardReportModel(
                        connection = '', 
                        model_type = '', 
                        name = '', 
                        collection_id = '', 
                        collection_name = '', 
                        llm_model_name = '', 
                        documents = [
                            ''
                            ], 
                        key = '', )
                    ],
                evaluator = eval_studio_client.api.models.v1_leaderboard_report_evaluator.v1LeaderboardReportEvaluator(
                    id = '', 
                    name = '', 
                    display_name = '', 
                    tagline = '', 
                    description = '', 
                    brief_description = '', 
                    model_types = [
                        ''
                        ], 
                    can_explain = [
                        ''
                        ], 
                    explanation_scopes = [
                        ''
                        ], 
                    explanations = [
                        eval_studio_client.api.models.v1_leaderboard_report_explanation.v1LeaderboardReportExplanation(
                            explanation_type = '', 
                            name = '', 
                            category = '', 
                            scope = '', 
                            has_local = '', 
                            formats = [
                                ''
                                ], )
                        ], 
                    parameters = [
                        eval_studio_client.api.models.v1_leaderboard_report_evaluator_parameter.v1LeaderboardReportEvaluatorParameter(
                            name = '', 
                            description = '', 
                            comment = '', 
                            type = '', 
                            predefined = [
                                None
                                ], 
                            tags = [
                                ''
                                ], 
                            min = 1.337, 
                            max = 1.337, 
                            category = '', )
                        ], 
                    keywords = [
                        ''
                        ], 
                    metrics_meta = [
                        eval_studio_client.api.models.v1_leaderboard_report_metrics_meta_entry.v1LeaderboardReportMetricsMetaEntry(
                            key = '', 
                            display_name = '', 
                            data_type = '', 
                            display_value = '', 
                            description = '', 
                            value_range = [
                                1.337
                                ], 
                            value_enum = [
                                ''
                                ], 
                            higher_is_better = True, 
                            threshold = 1.337, 
                            is_primary_metric = True, 
                            parent_metric = '', 
                            exclude = True, )
                        ], )
            )
        else:
            return V1LeaderboardReport(
        )
        """

    def testV1LeaderboardReport(self):
        """Test V1LeaderboardReport"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
