# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_create_evaluator_response import V1CreateEvaluatorResponse

class TestV1CreateEvaluatorResponse(unittest.TestCase):
    """V1CreateEvaluatorResponse unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1CreateEvaluatorResponse:
        """Test V1CreateEvaluatorResponse
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1CreateEvaluatorResponse`
        """
        model = V1CreateEvaluatorResponse()
        if include_optional:
            return V1CreateEvaluatorResponse(
                evaluator = eval_studio_client.api.models.v1_evaluator.v1Evaluator(
                    name = '', 
                    create_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                    creator = '', 
                    update_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                    updater = '', 
                    delete_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                    deleter = '', 
                    display_name = '', 
                    description = '', 
                    content = 'YQ==', 
                    mime_type = '', 
                    filename = '', 
                    identifier = '', 
                    tags = [
                        ''
                        ], 
                    parameters = [
                        eval_studio_client.api.models.based_on_the_h2_o/ai_eval_studio_evaluator_resource/
https://github/com/h2oai/h2o_sonar/blob/0492b2f2651fb2fde08d981b72c9fda7dc6b9697/h2o_sonar/lib/api/evaluators/py#l53_l81.Based on the H2O.ai Eval Studio Evaluator resource.
https://github.com/h2oai/h2o-sonar/blob/0492b2f2651fb2fde08d981b72c9fda7dc6b9697/h2o_sonar/lib/api/evaluators.py#L53-L81(
                            name = '', 
                            type = 'EVALUATOR_PARAM_TYPE_UNSPECIFIED', 
                            description = '', 
                            comment = '', 
                            string_val = '', 
                            float_val = 1.337, 
                            bool_val = True, 
                            min = 1.337, 
                            max = 1.337, 
                            predefined = [
                                ''
                                ], 
                            category = [
                                ''
                                ], )
                        ], 
                    brief_description = '', 
                    enabled = True, 
                    tagline = '', 
                    primary_metric = '', 
                    primary_metric_default_threshold = 1.337, )
            )
        else:
            return V1CreateEvaluatorResponse(
        )
        """

    def testV1CreateEvaluatorResponse(self):
        """Test V1CreateEvaluatorResponse"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
