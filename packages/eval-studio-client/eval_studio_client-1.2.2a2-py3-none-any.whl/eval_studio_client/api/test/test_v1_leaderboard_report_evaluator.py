# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_leaderboard_report_evaluator import V1LeaderboardReportEvaluator

class TestV1LeaderboardReportEvaluator(unittest.TestCase):
    """V1LeaderboardReportEvaluator unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1LeaderboardReportEvaluator:
        """Test V1LeaderboardReportEvaluator
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1LeaderboardReportEvaluator`
        """
        model = V1LeaderboardReportEvaluator()
        if include_optional:
            return V1LeaderboardReportEvaluator(
                id = '',
                name = '',
                display_name = '',
                tagline = '',
                description = '',
                brief_description = '',
                model_types = [
                    ''
                    ],
                can_explain = [
                    ''
                    ],
                explanation_scopes = [
                    ''
                    ],
                explanations = [
                    eval_studio_client.api.models.v1_leaderboard_report_explanation.v1LeaderboardReportExplanation(
                        explanation_type = '', 
                        name = '', 
                        category = '', 
                        scope = '', 
                        has_local = '', 
                        formats = [
                            ''
                            ], )
                    ],
                parameters = [
                    eval_studio_client.api.models.v1_leaderboard_report_evaluator_parameter.v1LeaderboardReportEvaluatorParameter(
                        name = '', 
                        description = '', 
                        comment = '', 
                        type = '', 
                        predefined = [
                            None
                            ], 
                        tags = [
                            ''
                            ], 
                        min = 1.337, 
                        max = 1.337, 
                        category = '', )
                    ],
                keywords = [
                    ''
                    ],
                metrics_meta = [
                    eval_studio_client.api.models.v1_leaderboard_report_metrics_meta_entry.v1LeaderboardReportMetricsMetaEntry(
                        key = '', 
                        display_name = '', 
                        data_type = '', 
                        display_value = '', 
                        description = '', 
                        value_range = [
                            1.337
                            ], 
                        value_enum = [
                            ''
                            ], 
                        higher_is_better = True, 
                        threshold = 1.337, 
                        is_primary_metric = True, 
                        parent_metric = '', 
                        exclude = True, )
                    ]
            )
        else:
            return V1LeaderboardReportEvaluator(
        )
        """

    def testV1LeaderboardReportEvaluator(self):
        """Test V1LeaderboardReportEvaluator"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
