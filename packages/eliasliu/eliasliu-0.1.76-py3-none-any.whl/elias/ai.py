# -*- coding: utf-8 -*-
"""
Created on Mon May 20 14:23:23 2024

@author: Elias liu
"""

def ollama_chat(system_prompt = '',user_prompt='Why is the sky blue?',model='test1',host='http://192.168.31.92:11434',chinese=0):
    '''
    
    pip install ollama

    Parameters
    ----------
    system_prompt : TYPE, optional
        ç³»ç»Ÿæç¤ºè¯. The default is ''.
    user_prompt : TYPE, optional
        ç”¨æˆ·æç¤ºè¯. The default is 'Why is the sky blue?'.
    model : TYPE, optional
        æ¨¡å‹. The default is 'test1'.
    host : TYPE, optional
        æœåŠ¡ip. The default is 'http://192.168.31.92:11434'.
    chinese : TYPE, optional
        å½“chinese=1æ—¶ï¼Œ{
          'role': 'system',
          'content': f'ä½ æ˜¯ä¸€ä¸ªä¸­å›½äººï¼Œåªä¼šè¯´æ±‰è¯­ï¼Œæ‰€ä»¥æ¥ä¸‹æ¥ä¸ç®¡ä»»ä½•äººç”¨ä»»æ„è¯­è¨€ï¼Œè¯·éƒ½ç”¨ä¸­æ–‡ä¸ä»–äº¤æµã€‚{system_prompt}',
        },. The default is 0.

    Returns
    -------
    result : TYPE
        DESCRIPTION.

    '''
    from ollama import Client
    client = Client(host=host)
    
    if chinese == 1:
        if system_prompt == None:
            system_prompt = ''
        response = client.chat(model=model, messages=[
          {
            'role': 'system',
            'content': f'ä½ æ˜¯ä¸€ä¸ªä¸­å›½äººï¼Œåªä¼šè¯´æ±‰è¯­ï¼Œæ‰€ä»¥æ¥ä¸‹æ¥ä¸ç®¡ä»»ä½•äººç”¨ä»»æ„è¯­è¨€ï¼Œè¯·éƒ½ç”¨ä¸­æ–‡ä¸ä»–äº¤æµã€‚{system_prompt}',
          },
          {
            'role': 'user',
            'content': f'{user_prompt}',
          },
        ])
    
    else:
    
        if system_prompt == '' or system_prompt == None:
            response = client.chat(model=model, messages=[
              {
                'role': 'user',
                'content': f'{user_prompt}',
              },
            ])
            
            
        else:
            response = client.chat(model=model, messages=[
              {
                'role': 'system',
                'content': f'{system_prompt}',
              },
              {
                'role': 'user',
                'content': f'{user_prompt}',
              },
            ])
        

    
    
    result = response['message']['content']
    print(result)
    return result

from elias_ollama import OllamaClient
import os

import time
from loguru import logger

def ollama_read_pic(image_path=None, model=None, url="http://kkteam.online:10434", content=None):
    """ä¸»ç¨‹åºå…¥å£"""
    # ç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
    # sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


    def print_separator(title=None):
        """æ‰“å°åˆ†éš”çº¿ï¼Œä½¿è¾“å‡ºæ›´æ¸…æ™°"""
        if title:
            print(f"\n{'='*50}\n{title}\n{'='*50}")
        else:
            print("\n" + "="*50 + "\n")
    
   
    # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    print("æ­£åœ¨è¿æ¥åˆ°OllamaæœåŠ¡å™¨...\n")
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹ï¼Œå¦‚æœæä¾›äº†modelå‚æ•°åˆ™ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„qwen3æ¨¡å‹
        default_model = "qwen2.5vl:latest" if model is None else model
        client = OllamaClient(base_url="http://kkteam.online:10434", default_model=default_model)
        print("è¿æ¥æˆåŠŸï¼\n")
    except Exception as e:
        print(f"è¿æ¥OllamaæœåŠ¡å™¨å¤±è´¥: {str(e)}")
        print("è¯·ç¡®ä¿OllamaæœåŠ¡å™¨å·²å¯åŠ¨ï¼Œå¹¶ä¸”åœ°å€æ­£ç¡®")
        return
    
        
    try:
        print_separator("4. å›¾ç‰‡åˆ†æç¤ºä¾‹")
        
        if not image_path:
            # é»˜è®¤å›¾ç‰‡è·¯å¾„ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å®é™…æƒ…å†µä¿®æ”¹
            default_image = r"C:\Users\Administrator\Desktop\test\638948447558127941.png"
            image_path = default_image
            print(f"ä½¿ç”¨é»˜è®¤å›¾ç‰‡è·¯å¾„: {image_path}\n")
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(image_path):
            # ç¡®ä¿ä½¿ç”¨æ”¯æŒå›¾ç‰‡çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¾‹å¦‚qwen2.5vl:latest
            # æŸ¥æ‰¾æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹
            # å¦‚æœæä¾›äº†modelå‚æ•°ä¸”æ”¯æŒå¤šæ¨¡æ€ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
            if model is not None:
                multimodal_models = [model]
            
            if multimodal_models:
                # ä½¿ç”¨æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹
                multimodal_model = multimodal_models[0]
                print(f"ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹: {multimodal_model}\n")
                
                # åˆ›å»ºæ¶ˆæ¯å¹¶è°ƒç”¨chatæ–¹æ³•è¿›è¡Œå›¾ç‰‡åˆ†æ
                image_messages = [{"role": "user", "content": content}]
                response = client.chat(
                    model=multimodal_model,
                    messages=image_messages,
                    images=[image_path],
                    stream=True  # å¯¹äºé•¿æ–‡æœ¬å›å¤ï¼Œç¦ç”¨æµå¼è¾“å‡ºå¯èƒ½æ›´æ˜“äºé˜…è¯»
                )
                # print(f"å›¾ç‰‡åˆ†æç»“æœï¼š{response}")
                return response
            else:
                print("æœªæ‰¾åˆ°æ”¯æŒå›¾ç‰‡åˆ†æçš„å¤šæ¨¡æ€æ¨¡å‹")
                print("è¯·ä½¿ç”¨å‘½ä»¤'ollama pull llava:latest'æˆ–'ollama pull qwen2-vl:latest'ä¸‹è½½æ”¯æŒå›¾ç‰‡çš„æ¨¡å‹")
        else:
            print(f"é”™è¯¯ï¼šå›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            print("è¯·æ£€æŸ¥å›¾ç‰‡è·¯å¾„æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        print(model)
        print(f"å›¾ç‰‡åˆ†æå‡ºé”™ï¼š{str(e)}")



import requests
import base64
from PIL import Image
import io
import json

def download_image_from_url(url):
    """ä»URLä¸‹è½½å›¾ç‰‡å¹¶è¿”å›PIL Imageå¯¹è±¡"""
    response = requests.get(url, stream=True)
    response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    return Image.open(io.BytesIO(response.content))

def image_to_base64(image):
    """å°†PIL Imageå¯¹è±¡è½¬æ¢ä¸ºbase64ç¼–ç """
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def ollama_chat_with_images(image_urls,  model="qwen2.5vl:latest", url="http://kkteam.online:10434/api/chat", prompt="è¯·ç®€å•æè¿°è¿™äº›å›¾ç‰‡"):
    """
    é€šè¿‡HTTP APIä¸qwen2.5vlæ¨¡å‹å¯¹è¯ï¼Œå¤„ç†æµå¼è¾“å‡ºå¹¶è¿”å›å®Œæ•´ç»“æœ
    è¿”å›: å®Œæ•´çš„AIå›å¤å†…å®¹
    """
    # Ollama APIç«¯ç‚¹
    # url = "http://kkteam.online:10434/api/chat"
    
    # ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
    images = []
    for image_url in image_urls:
        try:
            img = download_image_from_url(image_url)
            images.append(image_to_base64(img))
        except Exception as e:
            print(f"æ— æ³•å¤„ç†å›¾ç‰‡ {image_url}: {str(e)}")
            continue
    
    if not images:
        raise Exception("æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡å¯ä¾›å¤„ç†")
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    data = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": prompt,
                "images": images
            }
        ],
        "stream": True
    }
    
    # å‘é€è¯·æ±‚å¹¶å¤„ç†æµå¼å“åº”
    full_response = ""
    print("AIå›å¤:")
    with requests.post(url, json=data, stream=True) as response:
        if response.status_code != 200:
            raise Exception(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, é”™è¯¯: {response.text}")
        
        # å¤„ç†æµå¼å“åº”
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line.decode('utf-8'))
                    content = chunk.get('message', {}).get('content', '')
                    if content:
                        print(content, end='', flush=True)  # å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡º
                        full_response += content
                except json.JSONDecodeError:
                    print("æ— æ³•è§£æJSONå“åº”:", line)
    
    print()  # æœ€åæ¢è¡Œ
    return full_response


# =============================================================================
# dify api


# -----------------------------------------------------------------------------
# å·¥ä½œæµå‹åº”ç”¨
def dify_workflow(query,api_key,api_url,user='elias.liu',response_mode='streaming',inputs={}):
    import requests
    
    # æ›¿æ¢ä¸ºæ‚¨çš„ API ç«¯ç‚¹
    api_url = f'{api_url}/workflows/run'
    
    # è®¾ç½®è¯·æ±‚å¤´ï¼ŒåŒ…æ‹¬è®¤è¯å’Œå†…å®¹ç±»å‹
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    # è®¾ç½®è¯·æ±‚æ•°æ®ï¼ŒåŒ…æ‹¬å¯¹è¯çš„ query å’Œå…¶ä»–å‚æ•°
    if inputs == {}:
        data = {
            "inputs": {
                        "content": f"{query}",  # ç”¨æˆ·çš„æŸ¥è¯¢æˆ–å¯¹è¯å†…å®¹
                       },  # æ ¹æ®éœ€è¦æä¾›è¾“å…¥å‚æ•°
            "response_mode": f"{response_mode}",  # å“åº”æ¨¡å¼ï¼Œæµå¼è¿”å›
            # "conversation_id": "",  # å¯¹è¯IDï¼Œç”¨äºç»´æŒå¯¹è¯
            "user": f"{user}"  # ç”¨æˆ·æ ‡è¯†
        }
    else:
        data = {
            "inputs": inputs,  # æ ¹æ®éœ€è¦æä¾›è¾“å…¥å‚æ•°
            "response_mode": f"{response_mode}",  # å“åº”æ¨¡å¼ï¼Œæµå¼è¿”å›
            # "conversation_id": "",  # å¯¹è¯IDï¼Œç”¨äºç»´æŒå¯¹è¯
            "user": f"{user}"  # ç”¨æˆ·æ ‡è¯†
        }
        
    
    # å‘é€ POST è¯·æ±‚
    response = requests.post(api_url, headers=headers, json=data)
    
    import json
    
    response_text = response.text
    # print(response_text)
    
    # å°†å“åº”æ–‡æœ¬åˆ†å‰²æˆå¤šæ¡æ¶ˆæ¯
    messages = response_text.strip().split('data: ')[1:][-1]
    
    data = json.loads(messages)
    msg = data['data']['outputs']['text']
    # print(msg)
    return msg


# # æ›¿æ¢ä¸ºæ‚¨çš„ API å¯†é’¥
# api_key = 'app-w3UJGtZP9z1ImtTZ1GXfLpoS'
# # æ›¿æ¢ä¸ºæ‚¨çš„ API ç«¯ç‚¹
# api_url = 'http://192.168.31.92/v1'

# query = '''
# 20+ğŸ’°é€è¿è´¹é™©ï¼Œè½¯ç³¯çŒ«çªç‹—çªï¼Œå¿«æ¡æ¼â—
# è½¯è½¯ç³¯ç³¯ï¼Œå…¬ä¸»é£çš„çŒ«çªç‹—çªåªè¦äºŒåå‡ ğŸ’°å°±èƒ½å¤Ÿæ‹¥æœ‰ï¼Œå¿«æ¥æ¡æ¼â—
# ç°åœ¨ä¸‹å•ï¼Œ48hå†…å‘è´§ï¼Œæ˜¥èŠ‚å‰å´½å´½å°±èƒ½ç”¨ä¸Šæš–æš–å’Œå’Œçš„çªçªäº†â—â—
# çªçªé¢œå€¼è¶…çº§é«˜ï¼Œ3ä»¶å¥—ï¼Œçªåº•æ‰˜+å«å­+è•¾ä¸èŠ±è¾¹ï¼Œé£æ ¼è¶…çº§ä¹–ï¼Œ4ç§é¢œè‰²å¯ä»¥é€‰ï¼å¡«å……åšå®æ¾è½¯ï¼Œâ„å†¬å¤©ç”¨å¾ˆæš–å’Œï¼Œé™æ¸©äº†å´½å´½ä¹Ÿèƒ½èˆ’èˆ’æœæœç¡ä¸ªå¥½è§‰ğŸ’¤
# 20æ–¤ä»¥ä¸‹çš„å´½å´½éƒ½å¯ä»¥ç”¨å“¦
# ä½ æ”¹å†™çš„æ–‡ç« æ˜¯ï¼š
# '''

# dify_workflow(query,api_key,api_url)


# -----------------------------------------------------------------------------
# å¯¹è¯å‹åº”ç”¨

def dify_chat(query,api_key,api_url,user='elias.liu',conversation_id='',response_mode='streaming',inputs = {}):
    import requests
    
    # æ›¿æ¢ä¸ºæ‚¨çš„ API ç«¯ç‚¹
    api_url = f'{api_url}/chat-messages'
    
    # è®¾ç½®è¯·æ±‚å¤´ï¼ŒåŒ…æ‹¬è®¤è¯å’Œå†…å®¹ç±»å‹
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    # è®¾ç½®è¯·æ±‚æ•°æ®ï¼ŒåŒ…æ‹¬å¯¹è¯çš„ query å’Œå…¶ä»–å‚æ•°
    data = {
        "inputs": inputs,  # æ ¹æ®éœ€è¦æä¾›è¾“å…¥å‚æ•°
        "query": f"{query}",  # ç”¨æˆ·çš„æŸ¥è¯¢æˆ–å¯¹è¯å†…å®¹
        "response_mode": f"{response_mode}",  # å“åº”æ¨¡å¼ï¼Œæµå¼è¿”å›
        "conversation_id": f"{conversation_id}",  # å¯¹è¯IDï¼Œç”¨äºç»´æŒå¯¹è¯
        "user": f"{user}"  # ç”¨æˆ·æ ‡è¯†
    }
    
    # å‘é€ POST è¯·æ±‚
    response = requests.post(api_url, headers=headers, json=data)
    
    
    import json
    
    response_text = response.text
    
    # å°†å“åº”æ–‡æœ¬åˆ†å‰²æˆå¤šæ¡æ¶ˆæ¯
    messages = response_text.strip().split('data: ')[1:][:-1]
    
    # éå†æ¯æ¡æ¶ˆæ¯å¹¶è§£ç 
    decoded_messages = []
    for message in messages:
        # è§£æ JSON æ•°æ®
        data = json.loads(message)
        # print(data['answer'])
        # è·å– 'answer' å­—æ®µå¹¶è§£ç  Unicode è½¬ä¹‰åºåˆ—
        answer = data['answer']
        # å°†è§£ç åçš„ç­”æ¡ˆæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        decoded_messages.append(answer)
    
    msg = ''.join(decoded_messages)
    # print(msg)
    return msg



# # æ›¿æ¢ä¸ºæ‚¨çš„ API å¯†é’¥
# api_key = 'app-xvaJqDQsGS5m2JIKEeD3NBDh'


# # æ›¿æ¢ä¸ºæ‚¨çš„ API ç«¯ç‚¹
# api_url = 'http://192.168.31.92/v1'


# query = '''
# 20+ğŸ’°é€è¿è´¹é™©ï¼Œè½¯ç³¯çŒ«çªç‹—çªï¼Œå¿«æ¡æ¼â—
# è½¯è½¯ç³¯ç³¯ï¼Œå…¬ä¸»é£çš„çŒ«çªç‹—çªåªè¦äºŒåå‡ ğŸ’°å°±èƒ½å¤Ÿæ‹¥æœ‰ï¼Œå¿«æ¥æ¡æ¼â—
# ç°åœ¨ä¸‹å•ï¼Œ48hå†…å‘è´§ï¼Œæ˜¥èŠ‚å‰å´½å´½å°±èƒ½ç”¨ä¸Šæš–æš–å’Œå’Œçš„çªçªäº†â—â—
# çªçªé¢œå€¼è¶…çº§é«˜ï¼Œ3ä»¶å¥—ï¼Œçªåº•æ‰˜+å«å­+è•¾ä¸èŠ±è¾¹ï¼Œé£æ ¼è¶…çº§ä¹–ï¼Œ4ç§é¢œè‰²å¯ä»¥é€‰ï¼å¡«å……åšå®æ¾è½¯ï¼Œâ„å†¬å¤©ç”¨å¾ˆæš–å’Œï¼Œé™æ¸©äº†å´½å´½ä¹Ÿèƒ½èˆ’èˆ’æœæœç¡ä¸ªå¥½è§‰ğŸ’¤
# 20æ–¤ä»¥ä¸‹çš„å´½å´½éƒ½å¯ä»¥ç”¨å“¦
# ä½ æ”¹å†™çš„æ–‡ç« æ˜¯ï¼š
# '''

# dify_chat(query,api_key,api_url)


# -----------------------------------------------------------------------------
# å¯¹è¯å‹åº”ç”¨

def dify_text(query,api_key,api_url,user='elias.liu',response_mode='streaming',inputs = {},text=''):
    import requests
    
    # æ›¿æ¢ä¸ºæ‚¨çš„ API ç«¯ç‚¹
    api_url = f'{api_url}/completion-messages'
    
    # è®¾ç½®è¯·æ±‚å¤´ï¼ŒåŒ…æ‹¬è®¤è¯å’Œå†…å®¹ç±»å‹
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    # è®¾ç½®è¯·æ±‚æ•°æ®ï¼ŒåŒ…æ‹¬ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬å’Œå¿…éœ€çš„ query å‚æ•°
    if inputs == {}:
        data = {
            "inputs": {
                "text": f"{text}",  # ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
                "query": f"{query}"  # æ·»åŠ  query å‚æ•°ï¼Œå…¶å€¼æ ¹æ® API çš„å…·ä½“è¦æ±‚æ¥è®¾ç½®
            },
            "response_mode": f"{response_mode}",  # å“åº”æ¨¡å¼ï¼Œæµå¼è¿”å›
            "user": f"{user}"  # ç”¨æˆ·æ ‡è¯†
        }
    else:
        data = {
            "inputs": inputs,
            "response_mode": f"{response_mode}",  # å“åº”æ¨¡å¼ï¼Œæµå¼è¿”å›
            "user": f"{user}"  # ç”¨æˆ·æ ‡è¯†
        }
        
    
    # å‘é€ POST è¯·æ±‚
    response = requests.post(api_url, headers=headers, json=data)
    
    # æ‰“å°å“åº”å†…å®¹
    # print(response.text)
    
    import json
    
    response_text = response.text
    
    # å°†å“åº”æ–‡æœ¬åˆ†å‰²æˆå¤šæ¡æ¶ˆæ¯
    messages = response_text.strip().split('data: ')[1:][:-1]
    
    # éå†æ¯æ¡æ¶ˆæ¯å¹¶è§£ç 
    decoded_messages = []
    for message in messages:
        # è§£æ JSON æ•°æ®
        data = json.loads(message)
        # print(data['answer'])
        # è·å– 'answer' å­—æ®µå¹¶è§£ç  Unicode è½¬ä¹‰åºåˆ—
        answer = data['answer']
        # å°†è§£ç åçš„ç­”æ¡ˆæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        decoded_messages.append(answer)
    
    msg = ''.join(decoded_messages)
    # print(msg)
    return msg


# # æ›¿æ¢ä¸ºæ‚¨çš„ API å¯†é’¥
# api_key = 'app-h1s2porhLFnhLRD5eom1nhi3'

# # æ›¿æ¢ä¸ºæ‚¨çš„ API ç«¯ç‚¹
# api_url = 'http://192.168.31.92/v1'


# query = '''
# 20+ğŸ’°é€è¿è´¹é™©ï¼Œè½¯ç³¯çŒ«çªç‹—çªï¼Œå¿«æ¡æ¼â—
# è½¯è½¯ç³¯ç³¯ï¼Œå…¬ä¸»é£çš„çŒ«çªç‹—çªåªè¦äºŒåå‡ ğŸ’°å°±èƒ½å¤Ÿæ‹¥æœ‰ï¼Œå¿«æ¥æ¡æ¼â—
# ç°åœ¨ä¸‹å•ï¼Œ48hå†…å‘è´§ï¼Œæ˜¥èŠ‚å‰å´½å´½å°±èƒ½ç”¨ä¸Šæš–æš–å’Œå’Œçš„çªçªäº†â—â—
# çªçªé¢œå€¼è¶…çº§é«˜ï¼Œ3ä»¶å¥—ï¼Œçªåº•æ‰˜+å«å­+è•¾ä¸èŠ±è¾¹ï¼Œé£æ ¼è¶…çº§ä¹–ï¼Œ4ç§é¢œè‰²å¯ä»¥é€‰ï¼å¡«å……åšå®æ¾è½¯ï¼Œâ„å†¬å¤©ç”¨å¾ˆæš–å’Œï¼Œé™æ¸©äº†å´½å´½ä¹Ÿèƒ½èˆ’èˆ’æœæœç¡ä¸ªå¥½è§‰ğŸ’¤
# 20æ–¤ä»¥ä¸‹çš„å´½å´½éƒ½å¯ä»¥ç”¨å“¦
# ä½ æ”¹å†™çš„æ–‡ç« æ˜¯ï¼š
# '''

# dify_text(query,api_key,api_url)


# =============================================================================
# openai api (å¯è°ƒç”¨ç¬¬ä¸‰æ–¹api)

def openai_chat(system_prompt = '',user_prompt="è®²ä¸ªç¬‘è¯", api_key = "sk-LqP8IxSTpjTq6c9qvhqFO74qQqMTl7YDzdQ1KcDjIE9djYtK",base_url = "https://api.fe8.cn/v1",model="gpt-3.5-turbo",info_txt='chat_completion.txt'):
    from openai import OpenAI
    from loguru import logger
    
    import tools
    client = OpenAI(
        api_key = api_key,
        base_url = base_url
    )
    
    if system_prompt == '' or system_prompt == None:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": f"{user_prompt}",
                }
            ],
            model=f"{model}", #æ­¤å¤„æ›´æ¢å…¶å®ƒæ¨¡å‹,è¯·å‚è€ƒæ¨¡å‹åˆ—è¡¨ eg: google/gemma-7b-it
        )
    else:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": f"{system_prompt}",
                },
                {
                    "role": "user",
                    "content": f"{user_prompt}",
                }
            ],
            model=f"{model}", #æ­¤å¤„æ›´æ¢å…¶å®ƒæ¨¡å‹,è¯·å‚è€ƒæ¨¡å‹åˆ—è¡¨ eg: google/gemma-7b-it
        )
    try:
        tools.write_text_to_file(info_txt, str(chat_completion))
    except:
        logger.info(f'{info_txt}ä¸å­˜åœ¨ï¼Œinfo_txt é‡ç½®ä¸º chat_completion.txt ')
        info_txt='chat_completion.txt'
        tools.write_text_to_file(info_txt, str(chat_completion))
    msg = chat_completion.choices[0].message.content
    # print(msg)
    return msg

# api_key = "sk-LqP8IxSTpjTq6c9qvhqFO74qQqMTl7YDzdQ1KcDjIE9djYtK"
# base_url = "https://api.fe8.cn/v1"
# model="gpt-3.5-turbo"
# user_prompt="è®²ä¸ªç¬‘è¯"
# openai_chat(system_prompt = '',user_prompt=user_prompt, api_key = api_key,base_url = base_url,model=model)





# =============================================================================
# openai api (å¯è°ƒç”¨ç¬¬ä¸‰æ–¹api)



def openai_chat_time_limit(system_prompt = '',user_prompt="è®²ä¸ªç¬‘è¯", api_key = "sk-LqP8IxSTpjTq6c9qvhqFO74qQqMTl7YDzdQ1KcDjIE9djYtK",base_url = "https://api.fe8.cn/v1",model="gpt-3.5-turbo",info_txt='chat_completion.txt',time_limit = 15):
    
    import threading
    import queue
    import time
    from loguru import logger
    
    class TimeoutException(Exception):
        pass
    
    def chat_function(output_queue):
        try:
            start = time.time()
            # å‡è®¾client.chat.completions.create æ˜¯ä½ è°ƒç”¨APIçš„å‡½æ•°
            msg = openai_chat(system_prompt,user_prompt,api_key,base_url,model,info_txt)
            end = time.time()
            output_queue.put((msg, round(end - start, 2)))
        except Exception as e:
            output_queue.put(e)
    
    def chat_with_timeout(timeout):
        output_queue = queue.Queue()
        chat_thread = threading.Thread(target=chat_function, args=(output_queue,))
        chat_thread.start()
        
        start_time = time.time()
        elapsed_time = 0
        
        logger.info(f'LLM ({model}) start !')
        while elapsed_time < timeout:
            if not chat_thread.is_alive():
                break
            time.sleep(1)
            elapsed_time = time.time() - start_time
            # print(f"Elapsed time: {int(elapsed_time)} seconds")
            logger.info(f"LLM ({model}) is running: {int(elapsed_time)} seconds")
        
        if chat_thread.is_alive():
            raise TimeoutException(f"The chat function timed out after {timeout} seconds")
        
        result = output_queue.get()
        if isinstance(result, Exception):
            raise result
        
        content, usetime = result
        logger.info(f'chat_completion.choices[0].message.content:\n\n"""\n{content}\n"""\n')
        logger.info(f'usetime: {usetime} seconds')
        
        return content
    
    try:
        content = chat_with_timeout(time_limit)
        return content
    except TimeoutException as e:
        logger.error(e)
    except Exception as e:
        logger.error("An error occurred:", e)
        
        
# api_key = "sk-LqP8IxSTpjTq6c9qvhqFO74qQqMTl7YDzdQ1KcDjIE9djYtK"
# base_url = "https://api.fe8.cn/v1"
# model="gpt-4o-mini"
# user_prompt="è®²ä¸ªç¬‘è¯"
# time_limit = 15
# content = openai_chat_time_limit(system_prompt = '',user_prompt=user_prompt, api_key = api_key,base_url = base_url,model=model,time_limit=time_limit)
