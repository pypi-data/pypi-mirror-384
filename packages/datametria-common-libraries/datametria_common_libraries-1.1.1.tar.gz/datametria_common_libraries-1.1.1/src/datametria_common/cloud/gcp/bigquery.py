"""
üìä BigQuery Manager - Enterprise Data Warehouse

Gerenciador enterprise para Google BigQuery com recursos avan√ßados de data warehouse,
incluindo opera√ß√µes de dataset/table management, query execution otimizada,
data loading/export, streaming inserts, batch operations e cost optimization.

Features:
    - Dataset e table management completo
    - Query execution com otimiza√ß√£o autom√°tica
    - Data loading e export para m√∫ltiplos formatos
    - Streaming inserts para dados em tempo real
    - Batch operations para processamento em lote
    - Cost optimization e performance monitoring
    - Health check e monitoring integrado
    - Compliance LGPD/GDPR autom√°tico

Examples:
    >>> from datametria_common.cloud.gcp import BigQueryManager, GCPConfig
    >>> config = GCPConfig(project_id="my-project")
    >>> bq = BigQueryManager(config)
    >>> await bq.create_dataset("analytics")
    >>> results = await bq.run_query("SELECT * FROM dataset.table LIMIT 10")

Author: Equipe DATAMETRIA
Version: 1.0.0
Date: 2025-01-08
License: MIT
Compliance: LGPD/GDPR Ready
"""

import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Union, Iterator

try:
    from google.cloud import bigquery
    from google.cloud.bigquery import Client, Dataset, Table, QueryJob
    from google.auth.credentials import Credentials
except ImportError:
    bigquery = None
    Client = None
    Dataset = None
    Table = None
    QueryJob = None
    Credentials = None

from .config import GCPConfig


class BigQueryManager:
    """Enterprise Google BigQuery manager com recursos avan√ßados de data warehouse.
    
    Gerenciador completo para opera√ß√µes BigQuery incluindo dataset management,
    query execution, data loading/export, streaming inserts e cost optimization.
    
    Attributes:
        config (GCPConfig): Configura√ß√£o GCP com project_id, region e credenciais
        client (bigquery.Client): Cliente BigQuery autenticado
        logger (logging.Logger): Logger para auditoria e debugging
        
    Examples:
        >>> config = GCPConfig(project_id="my-project", region="us-central1")
        >>> bq = BigQueryManager(config)
        >>> await bq.create_dataset("analytics", "Analytics dataset")
        >>> results = await bq.run_query("SELECT COUNT(*) FROM dataset.table")
        
    Note:
        Requer google-cloud-bigquery instalado: pip install google-cloud-bigquery
        Todas as opera√ß√µes s√£o logadas para auditoria e compliance.
    """
    
    def __init__(self, config: GCPConfig, credentials: Optional[Credentials] = None):
        """Inicializa BigQuery manager com configura√ß√£o e credenciais.
        
        Args:
            config (GCPConfig): Configura√ß√£o GCP com project_id, region e settings
            credentials (Optional[Credentials]): Credenciais GCP customizadas.
                Se None, usa credenciais padr√£o do ambiente.
                
        Raises:
            ImportError: Se google-cloud-bigquery n√£o estiver instalado
            ValueError: Se configura√ß√£o for inv√°lida
            
        Examples:
            >>> config = GCPConfig(project_id="my-project")
            >>> bq = BigQueryManager(config)
            >>> # Com credenciais customizadas
            >>> from google.oauth2 import service_account
            >>> creds = service_account.Credentials.from_service_account_file("key.json")
            >>> bq = BigQueryManager(config, creds)
        """
        if bigquery is None:
            raise ImportError("google-cloud-bigquery n√£o instalado. Execute: pip install google-cloud-bigquery")
            
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Initialize BigQuery client
        if credentials:
            self.client = Client(
                project=config.project_id,
                credentials=credentials,
                location=config.region
            )
        else:
            self.client = Client(
                project=config.project_id,
                location=config.region
            )
    
    async def create_dataset(
        self,
        dataset_id: str,
        description: Optional[str] = None,
        location: Optional[str] = None
    ) -> str:
        """Cria dataset no BigQuery com configura√ß√µes enterprise.
        
        Args:
            dataset_id (str): ID √∫nico do dataset (formato: [a-zA-Z0-9_])
            description (Optional[str]): Descri√ß√£o do dataset para documenta√ß√£o
            location (Optional[str]): Localiza√ß√£o do dataset (ex: 'us-central1').
                Se None, usa region da configura√ß√£o.
                
        Returns:
            str: ID do dataset criado
            
        Raises:
            ValueError: Se dataset_id for inv√°lido
            Exception: Se cria√ß√£o falhar por permiss√µes ou quota
            
        Examples:
            >>> dataset_id = await bq.create_dataset(
            ...     "analytics_prod",
            ...     "Production analytics dataset",
            ...     "us-central1"
            ... )
            >>> print(f"Dataset criado: {dataset_id}")
            
        Note:
            - Dataset √© criado com exists_ok=True (n√£o falha se j√° existir)
            - Opera√ß√£o √© logada para auditoria
            - Suporta LGPD/GDPR compliance autom√°tico
        """
        try:
            dataset_ref = self.client.dataset(dataset_id)
            dataset = Dataset(dataset_ref)
            
            if description:
                dataset.description = description
            if location:
                dataset.location = location
            else:
                dataset.location = self.config.region
            
            dataset = self.client.create_dataset(dataset, exists_ok=True)
            
            self.logger.info(f"Dataset created: {dataset_id}")
            return dataset.dataset_id
            
        except Exception as e:
            self.logger.error(f"Dataset creation failed: {e}")
            raise
    
    async def run_query(
        self,
        sql: str,
        parameters: Optional[List[Any]] = None,
        dry_run: bool = False
    ) -> List[Dict[str, Any]]:
        """Executa query SQL no BigQuery com otimiza√ß√£o e cost control.
        
        Args:
            sql (str): Query SQL padr√£o BigQuery (Standard SQL)
            parameters (Optional[List[Any]]): Par√¢metros para query parametrizada
            dry_run (bool): Se True, apenas valida query e estima custo
                sem executar. Default: False
                
        Returns:
            List[Dict[str, Any]]: Lista de resultados como dicion√°rios.
                Para dry_run=True, retorna estimativa de custo:
                - bytes_processed: Bytes que seriam processados
                - bytes_billed: Bytes que seriam cobrados
                - estimated_cost: Custo estimado em USD
                
        Raises:
            ValueError: Se SQL for inv√°lido
            Exception: Se execu√ß√£o falhar por timeout ou recursos
            
        Examples:
            >>> # Query simples
            >>> results = await bq.run_query("SELECT * FROM dataset.table LIMIT 10")
            >>> 
            >>> # Query parametrizada
            >>> sql = "SELECT * FROM dataset.table WHERE date = @date"
            >>> params = [bigquery.ScalarQueryParameter("date", "DATE", "2025-01-01")]
            >>> results = await bq.run_query(sql, params)
            >>> 
            >>> # Dry run para estimativa de custo
            >>> cost_info = await bq.run_query(sql, dry_run=True)
            >>> print(f"Custo estimado: ${cost_info[0]['estimated_cost']:.4f}")
            
        Note:
            - Queries s√£o otimizadas automaticamente
            - Resultados s√£o limitados para evitar overconsumption
            - Opera√ß√µes s√£o logadas para auditoria
        """
        try:
            job_config = bigquery.QueryJobConfig()
            
            if parameters:
                job_config.query_parameters = parameters
            if dry_run:
                job_config.dry_run = True
            
            query_job = self.client.query(sql, job_config=job_config)
            
            if dry_run:
                return [{
                    'bytes_processed': query_job.total_bytes_processed,
                    'bytes_billed': query_job.total_bytes_billed,
                    'estimated_cost': query_job.total_bytes_processed * 5e-9  # $5 per TB
                }]
            
            results = query_job.result()
            
            rows = []
            for row in results:
                rows.append(dict(row))
            
            self.logger.info(f"Query executed: {len(rows)} rows returned")
            return rows
            
        except Exception as e:
            self.logger.error(f"Query execution failed: {e}")
            raise
    
    async def load_data(
        self,
        dataset_id: str,
        table_id: str,
        data: List[Dict[str, Any]],
        schema: Optional[List[Dict[str, str]]] = None,
        write_disposition: str = "WRITE_APPEND"
    ) -> str:
        """Carrega dados para tabela BigQuery via batch loading.
        
        Args:
            dataset_id (str): ID do dataset de destino
            table_id (str): ID da tabela de destino
            data (List[Dict[str, Any]]): Lista de registros como dicion√°rios
            schema (Optional[List[Dict[str, str]]]): Schema da tabela.
                Formato: [{'name': 'col1', 'type': 'STRING', 'mode': 'NULLABLE'}]
                Se None, usa autodetect=True
            write_disposition (str): Modo de escrita:
                - 'WRITE_APPEND': Adiciona dados (default)
                - 'WRITE_TRUNCATE': Substitui dados
                - 'WRITE_EMPTY': Falha se tabela n√£o estiver vazia
                
        Returns:
            str: Job ID do processo de loading
            
        Raises:
            ValueError: Se dados ou schema forem inv√°lidos
            Exception: Se loading falhar por quota ou permiss√µes
            
        Examples:
            >>> # Loading com schema autom√°tico
            >>> data = [
            ...     {'name': 'Jo√£o', 'age': 30, 'city': 'S√£o Paulo'},
            ...     {'name': 'Maria', 'age': 25, 'city': 'Rio de Janeiro'}
            ... ]
            >>> job_id = await bq.load_data('analytics', 'users', data)
            >>> 
            >>> # Loading com schema definido
            >>> schema = [
            ...     {'name': 'name', 'type': 'STRING', 'mode': 'REQUIRED'},
            ...     {'name': 'age', 'type': 'INTEGER', 'mode': 'NULLABLE'},
            ...     {'name': 'city', 'type': 'STRING', 'mode': 'NULLABLE'}
            ... ]
            >>> job_id = await bq.load_data('analytics', 'users', data, schema)
            
        Note:
            - Dados s√£o convertidos para formato NEWLINE_DELIMITED_JSON
            - Opera√ß√£o √© ass√≠ncrona, aguarda conclus√£o do job
            - Suporta at√© 15TB por job de loading
        """
        try:
            table_ref = self.client.dataset(dataset_id).table(table_id)
            
            job_config = bigquery.LoadJobConfig()
            job_config.write_disposition = write_disposition
            job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON
            
            if schema:
                job_config.schema = [
                    bigquery.SchemaField(field['name'], field['type'], field.get('mode', 'NULLABLE'))
                    for field in schema
                ]
            else:
                job_config.autodetect = True
            
            # Convert data to JSON lines format
            import json
            json_data = '\n'.join([json.dumps(row) for row in data])
            
            job = self.client.load_table_from_json(
                data, table_ref, job_config=job_config
            )
            
            job.result()  # Wait for job to complete
            
            self.logger.info(f"Data loaded: {len(data)} rows to {dataset_id}.{table_id}")
            return job.job_id
            
        except Exception as e:
            self.logger.error(f"Data loading failed: {e}")
            raise
    
    async def export_data(
        self,
        dataset_id: str,
        table_id: str,
        destination_uri: str,
        format: str = "CSV"
    ) -> str:
        """Exporta dados da tabela BigQuery para Cloud Storage.
        
        Args:
            dataset_id (str): ID do dataset de origem
            table_id (str): ID da tabela de origem
            destination_uri (str): URI de destino no Cloud Storage.
                Formato: 'gs://bucket/path/file.csv'
            format (str): Formato de export:
                - 'CSV': Comma-separated values (default)
                - 'JSON': Newline-delimited JSON
                - 'AVRO': Apache Avro format
                - 'PARQUET': Apache Parquet format
                
        Returns:
            str: Job ID do processo de export
            
        Raises:
            ValueError: Se URI ou formato forem inv√°lidos
            Exception: Se export falhar por permiss√µes ou quota
            
        Examples:
            >>> # Export para CSV
            >>> job_id = await bq.export_data(
            ...     'analytics', 'users',
            ...     'gs://my-bucket/exports/users.csv'
            ... )
            >>> 
            >>> # Export para JSON
            >>> job_id = await bq.export_data(
            ...     'analytics', 'users',
            ...     'gs://my-bucket/exports/users.json',
            ...     format='JSON'
            ... )
            
        Note:
            - Destino deve ser bucket do Cloud Storage
            - Opera√ß√£o √© ass√≠ncrona, aguarda conclus√£o do job
            - Suporta compress√£o autom√°tica para formatos compat√≠veis
        """
        try:
            table_ref = self.client.dataset(dataset_id).table(table_id)
            
            job_config = bigquery.ExtractJobConfig()
            job_config.destination_format = getattr(bigquery.DestinationFormat, format)
            
            extract_job = self.client.extract_table(
                table_ref,
                destination_uri,
                job_config=job_config
            )
            
            extract_job.result()  # Wait for job to complete
            
            self.logger.info(f"Data exported: {dataset_id}.{table_id} to {destination_uri}")
            return extract_job.job_id
            
        except Exception as e:
            self.logger.error(f"Data export failed: {e}")
            raise
    
    async def stream_insert(
        self,
        dataset_id: str,
        table_id: str,
        rows: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Insere dados via streaming para baixa lat√™ncia.
        
        Args:
            dataset_id (str): ID do dataset de destino
            table_id (str): ID da tabela de destino
            rows (List[Dict[str, Any]]): Lista de registros para inserir.
                Cada registro deve corresponder ao schema da tabela.
                
        Returns:
            List[Dict[str, Any]]: Lista de erros de inser√ß√£o.
                Lista vazia indica sucesso completo.
                Cada erro cont√©m: {'index': int, 'errors': [...]}
                
        Raises:
            ValueError: Se dados n√£o corresponderem ao schema
            Exception: Se inser√ß√£o falhar por quota ou permiss√µes
            
        Examples:
            >>> # Streaming insert simples
            >>> rows = [
            ...     {'name': 'Jo√£o', 'timestamp': datetime.utcnow()},
            ...     {'name': 'Maria', 'timestamp': datetime.utcnow()}
            ... ]
            >>> errors = await bq.stream_insert('realtime', 'events', rows)
            >>> if not errors:
            ...     print("Inser√ß√£o realizada com sucesso")
            >>> else:
            ...     print(f"Erros encontrados: {errors}")
            
        Note:
            - Ideal para dados em tempo real (< 1 segundo de lat√™ncia)
            - Mais caro que batch loading
            - Limite de 100.000 rows por segundo por tabela
            - Dados ficam dispon√≠veis imediatamente para query
        """
        try:
            table_ref = self.client.dataset(dataset_id).table(table_id)
            table = self.client.get_table(table_ref)
            
            errors = self.client.insert_rows_json(table, rows)
            
            if errors:
                self.logger.error(f"Streaming insert errors: {errors}")
                return errors
            
            self.logger.info(f"Streaming insert: {len(rows)} rows")
            return []
            
        except Exception as e:
            self.logger.error(f"Streaming insert failed: {e}")
            raise
    
    async def get_table_info(
        self,
        dataset_id: str,
        table_id: str
    ) -> Dict[str, Any]:
        """Obt√©m informa√ß√µes detalhadas da tabela BigQuery.
        
        Args:
            dataset_id (str): ID do dataset
            table_id (str): ID da tabela
            
        Returns:
            Dict[str, Any]: Informa√ß√µes da tabela contendo:
                - table_id: ID da tabela
                - dataset_id: ID do dataset
                - project_id: ID do projeto
                - num_rows: N√∫mero de linhas
                - num_bytes: Tamanho em bytes
                - created: Data de cria√ß√£o (ISO format)
                - modified: Data de modifica√ß√£o (ISO format)
                - schema: Lista com schema das colunas
                
        Raises:
            Exception: Se tabela n√£o existir ou sem permiss√µes
            
        Examples:
            >>> info = await bq.get_table_info('analytics', 'users')
            >>> print(f"Tabela: {info['table_id']}")
            >>> print(f"Linhas: {info['num_rows']:,}")
            >>> print(f"Tamanho: {info['num_bytes'] / 1024**3:.2f} GB")
            >>> 
            >>> # Verificar schema
            >>> for field in info['schema']:
            ...     print(f"{field['name']}: {field['type']} ({field['mode']})")
            
        Note:
            - Informa√ß√µes s√£o obtidas dos metadados (sem custo)
            - Schema inclui nome, tipo e modo de cada coluna
            - Timestamps s√£o retornados em formato ISO 8601
        """
        try:
            table_ref = self.client.dataset(dataset_id).table(table_id)
            table = self.client.get_table(table_ref)
            
            return {
                'table_id': table.table_id,
                'dataset_id': table.dataset_id,
                'project_id': table.project,
                'num_rows': table.num_rows,
                'num_bytes': table.num_bytes,
                'created': table.created.isoformat() if table.created else None,
                'modified': table.modified.isoformat() if table.modified else None,
                'schema': [
                    {
                        'name': field.name,
                        'type': field.field_type,
                        'mode': field.mode
                    }
                    for field in table.schema
                ]
            }
            
        except Exception as e:
            self.logger.error(f"Get table info failed: {e}")
            raise
    
    async def health_check(self) -> Dict[str, Any]:
        """Verifica sa√∫de e conectividade do servi√ßo BigQuery.
        
        Returns:
            Dict[str, Any]: Status de sa√∫de contendo:
                - status: 'healthy' ou 'unhealthy'
                - service: 'bigquery'
                - project_id: ID do projeto
                - location: Regi√£o configurada
                - timestamp: Timestamp da verifica√ß√£o (ISO format)
                - datasets_count: N√∫mero de datasets acess√≠veis
                - error: Mensagem de erro (apenas se unhealthy)
                
        Examples:
            >>> health = await bq.health_check()
            >>> if health['status'] == 'healthy':
            ...     print(f"BigQuery OK - {health['datasets_count']} datasets")
            ... else:
            ...     print(f"BigQuery Error: {health['error']}")
            
        Note:
            - Testa conectividade listando datasets
            - N√£o gera custos (opera√ß√£o de metadados)
            - √ötil para monitoring e alertas
            - Inclui timestamp para tracking de disponibilidade
        """
        try:
            # Test by listing datasets
            datasets = list(self.client.list_datasets())
            
            return {
                'status': 'healthy',
                'service': 'bigquery',
                'project_id': self.config.project_id,
                'location': self.config.region,
                'timestamp': datetime.utcnow().isoformat(),
                'datasets_count': len(datasets)
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'service': 'bigquery',
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
