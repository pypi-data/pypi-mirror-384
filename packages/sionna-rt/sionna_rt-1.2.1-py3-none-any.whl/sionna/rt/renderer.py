#
# SPDX-FileCopyrightText: Copyright (c) 2021-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
"""Rendering module of Sionna RT"""

from __future__ import annotations

import drjit as dr
import mitsuba as mi
import numpy as np

from  sionna import rt
from .utils import make_render_sensor, paths_to_segments, unmultiply_alpha, \
                   twosided_diffuse, radio_map_to_emissive_shape, \
                   scoped_set_log_level, scene_scale, clone_mesh, \
                   rotation_matrix


def render(scene: rt.Scene,
           camera: str | rt.Camera | mi.ScalarTransform4f | mi.Sensor,
           paths: rt.Paths | None,
           show_devices: bool,
           show_orientations: bool,
           num_samples: int,
           resolution: tuple[int, int],
           fov: float | None = None,
           clip_at: float | None = None,
           clip_plane_orientation: tuple[float, float, float] = (0, 0, -1),
           max_depth: int = 8,
           radio_map: rt.RadioMap | None = None,
           rm_tx: int | str | None = None,
           rm_db_scale: bool = True,
           rm_cmap: str | callable | None = None,
           rm_vmin: float | None = None,
           rm_vmax: float | None = None,
           rm_metric: str = "path_gain",
           envmap: str | None = None,
           lighting_scale: float = 1.0) -> mi.Bitmap:
    r"""
    Renders two images with path tracing:
    1. Base scene with the meshes
    2. Paths, radio devices and radio map,
    then composites them together.

    We adopt this approach because as of the time of writing, Mitsuba
    does not support adding or removing objects from a scene after
    it has been loaded.

    Input
    ------
    camera: str | :class:`mitsuba.Sensor`
        The name or instance of a :class:`mitsuba.Sensor`

    paths: :class:`~sionna.rt.Paths` | `None`
        Simulated paths generated by
        :meth:`~sionna.rt.Scene.compute_paths()` or `None`.
        If `None`, only the scene is rendered.

    show_devices: bool
        If `paths` is not `None`, shows the radio devices.

    show_orientations: bool
        If `paths` is not `None`, shows the orientation of the radio devices.

    radio_map: :class:`~sionna.rt.RadioMap` | `None`
        An optional coverage map to overlay in the scene for visualization.
        Defaults to `None`.

    rm_tx: int | str | None
        When `radio_map` is specified, controls which of the transmitters
        to display the coverage map for. Either the transmitter's name
        or index can be given. If `None`, the maximum metric over all
        transmitters is shown.
        Defaults to `None`.

    rm_db_scale: bool
        Use logarithmic scale for coverage map visualization, i.e. the
        coverage values are mapped with:
        :math:`y = 10 \cdot \log_{10}(x)`.
        Defaults to `True`.

    rm_cmap: str | callable | None
        For coverage map visualization, defines the colormap to use.
        If set to None, then the default colormap is used.
        If a string is given, it is interpreted as a Matplotlib colormap name.
        If a callable is given, it is used as a custom colormap function with
        the same interface as a Matplotlib colormap.
        Defaults to `None`.

    rm_vmin, rm_vmax: floot | None
        For coverage map visualization, defines the range of path gains that
        the colormap covers.
        If set to None, then covers the complete range.
        Defaults to `None`.

    rm_metric: str, one of ["path_gain", "rss", "sinr"]
        Metric of the coverage map to be displayed.
        Defaults to `path_gain`.

    num_samples: int
        Number of rays thrown per pixel.

    resolution: [2], int
        Size of the rendered figure.

    fov: float
        Field of view [deg]. If `None`, the field of view will
        default to 45 degrees, unless `camera` is set to `"preview"`, in
        which case the field of view of the preview camera is used.

    clip_at: float | None
        If not ``None``, the scene geometry will be clipped (cut) by a plane
        with normal orientation ``clip_plane_orientation`` and offset
        ``clip_at``. That means that everything *behind* the plane becomes
        invisible. This allows visualizing the interior of meshes, such as
        buildings.

    clip_plane_orientation: tuple[float, float, float]
        Normal vector of the clipping plane.
        Defaults to (0,0,-1).

    max_depth: int
        Maximum number of light interactions with the scene to render.

    envmap: str | None
        Path to an environment map image file (e.g. in EXR format) to use for
        scene lighting.

    lighting_scale: float
        Scale to apply to the lighting in the scene (whether from a constant
        uniform emitter or a given environment map).

    Output
    -------
    : :class:`~mitsuba.Bitmap`
        Rendered image
    """
    # Use an RGB variant matching the current backend.
    rendering_variant = ("cuda_ad_rgb"
                         if dr.backend_v(mi.Float) == dr.JitBackend.CUDA
                         else "llvm_ad_rgb")

    with mi.util.scoped_set_variant(rendering_variant,
                                    "cuda_ad_rgb", "llvm_ad_rgb"):
        # 1. Prepare the scene for rendering in the visual domain.
        # For now, we perform this conversion from scratch at every call because
        # it would be difficult to track all possible changes to keep
        # `visual_scene` up to date.
        sensor = make_render_sensor(scene, camera=camera, resolution=resolution,
                                    fov=fov)
        exclude_mesh_ids = set()

        rm_is_part_of_scene = False
        if isinstance(radio_map, rt.MeshRadioMap) and not rm_is_part_of_scene:
            # Note: we assume that even though the radio map's measurement
            # surface is mesh-based, it's not a real object that should be
            # part of the scene. We only render it as part of the overlay.
            exclude_mesh_ids.add(radio_map.measurement_surface.id())

        visual_scene = visual_scene_from_wireless_scene(
            scene, sensor=sensor, max_depth=max_depth,
            clip_at=clip_at, clip_plane_orientation=clip_plane_orientation,
            envmap=envmap, lighting_scale=lighting_scale,
            exclude_mesh_ids=exclude_mesh_ids
        )
        visual_scene = mi.load_dict(visual_scene)

        # 2. Render the scene geometry.
        main_image =  mi.render(visual_scene, spp=num_samples).numpy()

        # 3. If needed, create a separate scene with paths, coverage map, etc.
        overlay_scene = get_overlay_scene(
            scene, sensor, paths=paths,
            show_sources=show_devices, show_targets=show_devices,
            show_orientations=show_orientations,
            radio_map=radio_map,
            rm_tx=rm_tx, rm_db_scale=rm_db_scale,
            rm_cmap=rm_cmap, rm_vmin=rm_vmin, rm_vmax=rm_vmax,
            rm_metric=rm_metric
        )
        if not overlay_scene:
            # Won't need to composite anything, we can just return right away.
            return main_image

        # TODO: avoid the shearing warning when creating cylinders
        with scoped_set_log_level(mi.LogLevel.Error):
            overlay_scene = mi.load_dict(overlay_scene)

        # Note: we use the clipping planes for the base scene, but not for the
        # overlaid elements such as paths, radio devices, etc.
        depth_integrator = mi.load_dict({"type": "depth"})
        clipped_depth_integrator = depth_integrator
        if clip_at is not None:
            clipped_depth_integrator = visual_scene.integrator() \
                                                   .as_depth_integrator()
        # The overlay scene is made up of emissive object, we make sure
        # to disable `hide_emitters`.
        unclipped_integrator = mi.load_dict({
            "type": "path",
            "max_depth": max_depth,
            "hide_emitters": False,
        })

        depth1 = mi.render(visual_scene, sensor=sensor,
                           integrator=clipped_depth_integrator, spp=4)
        depth1 = unmultiply_alpha(depth1.numpy())

        overlay_image = mi.render(overlay_scene, sensor=sensor,
                                  integrator=unclipped_integrator,
                                  spp=num_samples)
        overlay_image = overlay_image.numpy()
        depth2 = mi.render(overlay_scene, sensor=sensor,
                           integrator=depth_integrator,
                           spp=num_samples)
        depth2 = unmultiply_alpha(depth2.numpy())


        # Alpha compositing using the renderings (stored as pre-multiplied
        # alpha)
        alpha1 = main_image[:, :, 3]
        alpha2 = overlay_image[:, :, 3]
        composite = overlay_image + main_image * (1 - alpha2[:, :, None])

        # Use the composite only in unoccluded regions (based on depth info)
        # TODO: can probably do a nicer transition based on depth values
        # In low-alpha regions, the depth values become imprecise, so we let
        # the overlay image take over.
        prefer_overlay = (alpha1[:, :, None] < 0.1) & (depth1 < 2 * depth2)
        if rm_is_part_of_scene:
            # Since the measurement surface is being rendered as part of both
            # the base and overlay scenes, we add a small threshold to make
            # sure the radio map from the overly is preferred.
            prefer_overlay |= np.abs(depth1 - depth2) < 0.01 * np.abs(depth1)

        result = np.where(
            (alpha1[:, :, None] > 0) & (depth1 < depth2) & (~prefer_overlay),
            main_image,
            composite
        )
        result[:, :, 3] = np.maximum(main_image[:, :, 3], composite[:, :, 3])

        return mi.Bitmap(result)

# pylint: disable=line-too-long
def visual_scene_from_wireless_scene(scene: rt.Scene,
                                     sensor: mi.Sensor,
                                     max_depth: int = 8,
                                     clip_at: float | None = None,
                                     clip_plane_orientation: tuple[float, float, float] = (0, 0, -1),
                                     envmap: str | None = None,
                                     lighting_scale: float = 1.0,
                                     exclude_mesh_ids: set[str] = None) -> dict:
    if dr.size_v(mi.Spectrum) != 3:
        raise ValueError("This function is expected to be run using a" +
                         " rendering-focused Mitsuba variant such as" +
                        f" 'cuda_ad_rgb', but found '{mi.variant()}'.")

    bbox: mi.ScalarBoundingBox3f = scene.mi_scene.bbox()

    result = {
        "type": "scene",
        "sensor": sensor,
    }

    # --- Integrator
    integrator = {
        "type": "sliced_path",
        'hide_emitters': True,
        "max_depth": max_depth,
    }
    if clip_at is not None:
        integrator["type"] = "sliced_path"
        integrator["slice1"] = {
            "type": "rectangle",
            "to_world": mi.ScalarTransform4f() \
                .look_at(origin=(0, 0, 0),
                         target=-mi.ScalarPoint3f(clip_plane_orientation),
                         up=(0, 1, 0)) \
                .translate([0, 0, clip_at])
                .scale(1.2 * bbox.extents())
        }
    else:
        integrator["type"] = "path"
    result["integrator"] = integrator

    # --- Environment emitter
    if envmap:
        emitter = {
            "type": "envmap",
            "filename": envmap,
            "scale": lighting_scale,
            "to_world": mi.ScalarTransform4f() \
                .rotate(axis=[1, 0, 0], angle=90) \
                .rotate(axis=[0, 1, 0], angle=45)
        }
    else:
        emitter = {
            "type": "constant",
            "radiance": {
                "type": "rgb",
                "value": [lighting_scale, lighting_scale, lighting_scale]
            }
        }
    result["emitter"] = emitter

    # --- Visual BSDFs
    bsdfs = {}
    for name, mat in scene.radio_materials.items():
        bsdfs[name] = twosided_diffuse(mat.color)

    # Default BSDF
    default_bsdf = twosided_diffuse((0.7,0.7,0.7))

    # --- Shapes (copied from the original scene)
    for i, sh in enumerate(scene.mi_scene.shapes()):
        assert sh.is_mesh()
        if exclude_mesh_ids and sh.id() in exclude_mesh_ids:
            continue

        # Try and respect the shape's RadioMaterial.color property
        original_id = sh.bsdf().name
        new_id = f"shape-{i}-{sh.id()}"
        props = mi.Properties()
        props["bsdf"] = bsdfs.get(original_id, default_bsdf)
        result[new_id] = clone_mesh(sh, name=new_id, props=props)

    return result

# pylint: disable=line-too-long
def get_overlay_scene(scene: rt.Scene, sensor: mi.Sensor, paths: any | None = None,
                      show_sources: bool = True, show_targets: bool = True,
                      show_orientations: bool = False,
                      radio_map: rt.RadioMap | None = None,
                      rm_tx: int | str | None = None, rm_db_scale: bool = True,
                      rm_cmap: str | callable | None = None,
                      rm_vmin: float | None = None, rm_vmax: float | None = None,
                      rm_metric: str = "path_gain") -> dict:
    result = {
        "type": "scene",
        "sensor": sensor,
    }

    # TODO: also account for positions of receivers, etc in the scene scale?
    sc = scene_scale(scene)
    if sc == 0.:
        sc = 1.
    radius = max(0.005 * sc, 0.5)

    # TODO: should use the source and target positions from the `paths` object
    # if given?
    for prefix, devices, enabled in (
        ('source', scene.transmitters, show_sources),
        ('target', scene.receivers, show_targets)):
        if not enabled:
            continue

        for name, rd in devices.items():
            key = f'rd-{prefix}-{name}'
            if rd.display_radius is not None:
                display_radius = rd.display_radius
            else:
                display_radius = radius

            assert key not in result
            rd_pos = rd.position.numpy().squeeze()
            result[key] = {
                'type': 'sphere',
                'center': rd_pos,
                'radius': display_radius,
                'light': {
                    'type': 'area',
                    'radiance': {'type': 'rgb', 'value': rd.color},
                },
            }

            if show_orientations:
                line_length = 3 * display_radius
                n = rotation_matrix(rd.orientation) @ mi.ScalarNormal3f(1, 0, 0)
                n = n.numpy().squeeze()
                n_norm = np.linalg.norm(n)
                if n_norm > 0:
                    result[key + '-orientation'] = {
                        'type': 'cylinder',
                        'p0': rd_pos,
                        'p1': rd_pos + line_length * (n / n_norm),
                        'radius': 0.1 * display_radius,
                        'light': {
                            'type': 'area',
                            'radiance': {'type': 'rgb', 'value': rd.color},
                        },
                    }

    # --- Paths, shown as cylinders (the closest we have to lines)
    if paths is not None:
        starts, ends, colors = paths_to_segments(paths)

        radius = min(0.20, 0.005 * sc)
        for i, (s, e, c) in enumerate(zip(starts, ends, colors)):
            result[f'path-{i:06d}'] = {
                'type': 'cylinder',
                'p0': s,
                'p1': e,
                'radius': radius,
                'light': {
                    'type': 'area',
                    'radiance': {'type': 'rgb', 'value': c},
                },
            }

    # --- Coverage map
    if radio_map is not None:
        result["radio-map"] = radio_map_to_emissive_shape(
            radio_map, tx=rm_tx, db_scale=rm_db_scale,
            rm_cmap=rm_cmap, vmin=rm_vmin, vmax=rm_vmax, rm_metric=rm_metric,
            viewpoint=sensor.world_transform().translation())

    return result
