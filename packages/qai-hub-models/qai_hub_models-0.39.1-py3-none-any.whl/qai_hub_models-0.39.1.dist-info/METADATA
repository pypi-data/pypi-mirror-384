Metadata-Version: 2.4
Name: qai_hub_models
Version: 0.39.1
Summary: Popular Machine Learning models optimized for Qualcomm chipsets.
Author-email: "QualcommÂ® Technologies, Inc" <ai-hub-support@qti.qualcomm.com>
License-Expression: BSD-3-Clause
Project-URL: Homepage, https://aihub.qualcomm.com/models
Project-URL: GitHub, https://github.com/quic/ai-hub-models
Project-URL: Issues, https://github.com/quic/ai-hub-models/issues
Project-URL: Slack, https://aihub.qualcomm.com/community/slack
Project-URL: HuggingFace, https://huggingface.co/qualcomm
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Operating System :: OS Independent
Requires-Python: <3.13,>=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: Pillow<12,>10
Requires-Dist: numpydoc==1.9.0
Requires-Dist: gdown==4.7.1
Requires-Dist: gitpython==3.1.42
Requires-Dist: huggingface_hub<1.0,>=0.34.0
Requires-Dist: ipython==8.12.3
Requires-Dist: numpy<2
Requires-Dist: onnx<1.20,>=1.17
Requires-Dist: onnxruntime<1.23,>=1.19
Requires-Dist: opencv-python<5,>4
Requires-Dist: pandas<2.3,>2
Requires-Dist: prettytable==3.11.0
Requires-Dist: requests_toolbelt==1.0.0
Requires-Dist: schema==0.7.5
Requires-Dist: torch<2.9.0,>=2.1
Requires-Dist: tabulate==0.9.0
Requires-Dist: torchaudio<2.9.0,>=2.1.2
Requires-Dist: torchvision<0.24.0,>=0.16
Requires-Dist: typing-extensions>=4.12.2
Requires-Dist: tqdm>=4.66
Requires-Dist: pyarrow==19.0.1
Requires-Dist: qai_hub>=0.34.0
Requires-Dist: datasets==2.17.0
Requires-Dist: ruamel-yaml==0.18.10
Requires-Dist: filelock>=3.16.1
Requires-Dist: pydantic<3,>=2
Requires-Dist: pydantic_yaml==1.4.0
Requires-Dist: scipy<2,>=1.8.1
Requires-Dist: pyquaternion==0.9.9
Requires-Dist: numba==0.60.0
Provides-Extra: dev
Requires-Dist: boto3<1.36,>=1.34; extra == "dev"
Requires-Dist: botocore<1.36,>=1.34; extra == "dev"
Requires-Dist: jinja2<3.2; extra == "dev"
Requires-Dist: mypy==1.13.0; extra == "dev"
Requires-Dist: pre-commit==4.0.1; extra == "dev"
Requires-Dist: pytest<9,>7; extra == "dev"
Requires-Dist: pytest-cov<5.2,>=5; extra == "dev"
Requires-Dist: pytest-xdist<4,>3; extra == "dev"
Requires-Dist: tflite==2.10.0; extra == "dev"
Requires-Dist: types-pillow==10.2.0.20240213; extra == "dev"
Requires-Dist: types-tabulate==0.9.0.20240106; extra == "dev"
Requires-Dist: types-requests==2.31.0.6; extra == "dev"
Requires-Dist: wheel==0.44.0; extra == "dev"
Requires-Dist: packaging<24,>23; extra == "dev"
Requires-Dist: adbutils>=2.8.0; extra == "dev"
Requires-Dist: boto3-stubs[s3]==1.36.9; extra == "dev"
Requires-Dist: setuptools>=75.8.0; extra == "dev"
Requires-Dist: build>=1.3.0; extra == "dev"
Provides-Extra: rtmdet
Requires-Dist: chumpy==0.71; extra == "rtmdet"
Requires-Dist: mmdet==3.3.0+mmcv220; extra == "rtmdet"
Requires-Dist: mmcv==2.2.0; extra == "rtmdet"
Requires-Dist: object-detection-metrics==0.4.post1; extra == "rtmdet"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "rtmdet"
Requires-Dist: torchmetrics==1.4.0.post0; extra == "rtmdet"
Provides-Extra: lama-dilated
Requires-Dist: albumentations==0.5.2; extra == "lama-dilated"
Requires-Dist: pytorch-lightning<3,>2; extra == "lama-dilated"
Requires-Dist: webdataset==0.2.86; extra == "lama-dilated"
Requires-Dist: easydict==1.13; extra == "lama-dilated"
Requires-Dist: kornia==0.5.0; extra == "lama-dilated"
Requires-Dist: hydra-core==1.3.0; extra == "lama-dilated"
Requires-Dist: scikit-learn<1.6,>1.1; extra == "lama-dilated"
Requires-Dist: tensorboard==2.13.0; extra == "lama-dilated"
Provides-Extra: lama-dilated
Requires-Dist: albumentations==0.5.2; extra == "lama-dilated"
Requires-Dist: pytorch-lightning<3,>2; extra == "lama-dilated"
Requires-Dist: webdataset==0.2.86; extra == "lama-dilated"
Requires-Dist: easydict==1.13; extra == "lama-dilated"
Requires-Dist: kornia==0.5.0; extra == "lama-dilated"
Requires-Dist: hydra-core==1.3.0; extra == "lama-dilated"
Requires-Dist: scikit-learn<1.6,>1.1; extra == "lama-dilated"
Requires-Dist: tensorboard==2.13.0; extra == "lama-dilated"
Provides-Extra: rf-detr
Requires-Dist: object-detection-metrics==0.4.post1; extra == "rf-detr"
Requires-Dist: shapely==2.0.3; extra == "rf-detr"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "rf-detr"
Requires-Dist: supervision==0.25.1; extra == "rf-detr"
Requires-Dist: peft~=0.17.1; extra == "rf-detr"
Provides-Extra: rf-detr
Requires-Dist: object-detection-metrics==0.4.post1; extra == "rf-detr"
Requires-Dist: shapely==2.0.3; extra == "rf-detr"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "rf-detr"
Requires-Dist: supervision==0.25.1; extra == "rf-detr"
Requires-Dist: peft~=0.17.1; extra == "rf-detr"
Provides-Extra: dla102x
Requires-Dist: timm==1.0.15; extra == "dla102x"
Provides-Extra: fastsam-s
Requires-Dist: ultralytics~=8.3.0; extra == "fastsam-s"
Provides-Extra: fastsam-s
Requires-Dist: ultralytics~=8.3.0; extra == "fastsam-s"
Provides-Extra: whisper-small
Requires-Dist: jiwer==3.0.3; extra == "whisper-small"
Requires-Dist: transformers~=4.56.2; extra == "whisper-small"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-small"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-small"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-small"
Provides-Extra: whisper-small
Requires-Dist: jiwer==3.0.3; extra == "whisper-small"
Requires-Dist: transformers~=4.56.2; extra == "whisper-small"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-small"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-small"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-small"
Provides-Extra: rtmpose-body2d
Requires-Dist: chumpy==0.71; extra == "rtmpose-body2d"
Requires-Dist: mmdet==3.3.0+mmcv220; extra == "rtmpose-body2d"
Requires-Dist: mmpose==1.2.0; extra == "rtmpose-body2d"
Requires-Dist: mmcv==2.2.0; extra == "rtmpose-body2d"
Requires-Dist: matplotlib==3.7.3; extra == "rtmpose-body2d"
Provides-Extra: rtmpose-body2d
Requires-Dist: chumpy==0.71; extra == "rtmpose-body2d"
Requires-Dist: mmdet==3.3.0+mmcv220; extra == "rtmpose-body2d"
Requires-Dist: mmpose==1.2.0; extra == "rtmpose-body2d"
Requires-Dist: mmcv==2.2.0; extra == "rtmpose-body2d"
Requires-Dist: matplotlib==3.7.3; extra == "rtmpose-body2d"
Provides-Extra: stable-diffusion-v1-5
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "stable-diffusion-v1-5"
Requires-Dist: transformers~=4.56.2; extra == "stable-diffusion-v1-5"
Requires-Dist: diffusers[torch]~=0.35.1; extra == "stable-diffusion-v1-5"
Requires-Dist: onnx<1.20,>=1.17; extra == "stable-diffusion-v1-5"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "stable-diffusion-v1-5"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "stable-diffusion-v1-5"
Provides-Extra: stable-diffusion-v1-5
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "stable-diffusion-v1-5"
Requires-Dist: transformers~=4.56.2; extra == "stable-diffusion-v1-5"
Requires-Dist: diffusers[torch]~=0.35.1; extra == "stable-diffusion-v1-5"
Requires-Dist: onnx<1.20,>=1.17; extra == "stable-diffusion-v1-5"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "stable-diffusion-v1-5"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "stable-diffusion-v1-5"
Provides-Extra: detr-resnet101-dc5
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet101-dc5"
Provides-Extra: detr-resnet101-dc5
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet101-dc5"
Provides-Extra: fomm
Requires-Dist: imageio[ffmpeg]==2.31.5; extra == "fomm"
Requires-Dist: ffmpeg==1.4; extra == "fomm"
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "fomm"
Requires-Dist: scikit-learn<1.6,>1.1; extra == "fomm"
Requires-Dist: matplotlib==3.7.5; extra == "fomm"
Requires-Dist: scipy<2,>=1.8.1; extra == "fomm"
Provides-Extra: conditional-detr-resnet50
Requires-Dist: transformers~=4.56.2; extra == "conditional-detr-resnet50"
Provides-Extra: conditional-detr-resnet50
Requires-Dist: transformers~=4.56.2; extra == "conditional-detr-resnet50"
Provides-Extra: sequencer2d
Requires-Dist: timm==1.0.15; extra == "sequencer2d"
Provides-Extra: whisper-large-v3-turbo
Requires-Dist: jiwer==3.0.3; extra == "whisper-large-v3-turbo"
Requires-Dist: transformers~=4.56.2; extra == "whisper-large-v3-turbo"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-large-v3-turbo"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-large-v3-turbo"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-large-v3-turbo"
Provides-Extra: whisper-large-v3-turbo
Requires-Dist: jiwer==3.0.3; extra == "whisper-large-v3-turbo"
Requires-Dist: transformers~=4.56.2; extra == "whisper-large-v3-turbo"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-large-v3-turbo"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-large-v3-turbo"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-large-v3-turbo"
Provides-Extra: mobilesam
Requires-Dist: pycocotools==2.0.7; extra == "mobilesam"
Requires-Dist: matplotlib==3.7.5; extra == "mobilesam"
Requires-Dist: timm==1.0.15; extra == "mobilesam"
Provides-Extra: face-det-lite
Requires-Dist: xtcocotools==1.14.3; extra == "face-det-lite"
Requires-Dist: object-detection-metrics==0.4.post1; extra == "face-det-lite"
Provides-Extra: face-det-lite
Requires-Dist: xtcocotools==1.14.3; extra == "face-det-lite"
Requires-Dist: object-detection-metrics==0.4.post1; extra == "face-det-lite"
Provides-Extra: yolov7
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov7"
Requires-Dist: shapely==2.0.3; extra == "yolov7"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov7"
Provides-Extra: foot-track-net
Requires-Dist: object-detection-metrics==0.4.post1; extra == "foot-track-net"
Requires-Dist: shapely==2.0.3; extra == "foot-track-net"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "foot-track-net"
Requires-Dist: xtcocotools==1.14.3; extra == "foot-track-net"
Provides-Extra: foot-track-net
Requires-Dist: object-detection-metrics==0.4.post1; extra == "foot-track-net"
Requires-Dist: shapely==2.0.3; extra == "foot-track-net"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "foot-track-net"
Requires-Dist: xtcocotools==1.14.3; extra == "foot-track-net"
Provides-Extra: yolox
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolox"
Requires-Dist: loguru==0.7.3; extra == "yolox"
Requires-Dist: shapely==2.0.3; extra == "yolox"
Provides-Extra: yolov8-det
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov8-det"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov8-det"
Requires-Dist: shapely==2.0.3; extra == "yolov8-det"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov8-det"
Provides-Extra: yolov8-det
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov8-det"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov8-det"
Requires-Dist: shapely==2.0.3; extra == "yolov8-det"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov8-det"
Provides-Extra: llama-v3-1-8b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-1-8b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-1-8b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-1-8b-instruct"
Requires-Dist: psutil; extra == "llama-v3-1-8b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-1-8b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-1-8b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-1-8b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-1-8b-instruct"
Provides-Extra: llama-v3-1-8b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-1-8b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-1-8b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-1-8b-instruct"
Requires-Dist: psutil; extra == "llama-v3-1-8b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-1-8b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-1-8b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-1-8b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-1-8b-instruct"
Provides-Extra: hrnet-w48-ocr
Requires-Dist: yacs==0.1.8; extra == "hrnet-w48-ocr"
Provides-Extra: hrnet-w48-ocr
Requires-Dist: yacs==0.1.8; extra == "hrnet-w48-ocr"
Provides-Extra: falcon-v3-7b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "falcon-v3-7b-instruct"
Requires-Dist: transformers==4.45.0; extra == "falcon-v3-7b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "falcon-v3-7b-instruct"
Requires-Dist: psutil; extra == "falcon-v3-7b-instruct"
Requires-Dist: onnx==1.17.0; extra == "falcon-v3-7b-instruct"
Requires-Dist: torch==2.4.1; extra == "falcon-v3-7b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "falcon-v3-7b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "falcon-v3-7b-instruct"
Provides-Extra: falcon-v3-7b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "falcon-v3-7b-instruct"
Requires-Dist: transformers==4.45.0; extra == "falcon-v3-7b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "falcon-v3-7b-instruct"
Requires-Dist: psutil; extra == "falcon-v3-7b-instruct"
Requires-Dist: onnx==1.17.0; extra == "falcon-v3-7b-instruct"
Requires-Dist: torch==2.4.1; extra == "falcon-v3-7b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "falcon-v3-7b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "falcon-v3-7b-instruct"
Provides-Extra: llama-v3-taide-8b-chat
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-taide-8b-chat"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-taide-8b-chat"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-taide-8b-chat"
Requires-Dist: psutil; extra == "llama-v3-taide-8b-chat"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-taide-8b-chat"
Requires-Dist: torch==2.4.1; extra == "llama-v3-taide-8b-chat"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-taide-8b-chat"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-taide-8b-chat"
Provides-Extra: llama-v3-taide-8b-chat
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-taide-8b-chat"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-taide-8b-chat"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-taide-8b-chat"
Requires-Dist: psutil; extra == "llama-v3-taide-8b-chat"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-taide-8b-chat"
Requires-Dist: torch==2.4.1; extra == "llama-v3-taide-8b-chat"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-taide-8b-chat"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-taide-8b-chat"
Provides-Extra: efficientformer
Requires-Dist: timm==1.0.15; extra == "efficientformer"
Provides-Extra: video-mae
Requires-Dist: av==14.0.1; extra == "video-mae"
Requires-Dist: transformers~=4.56.2; extra == "video-mae"
Provides-Extra: video-mae
Requires-Dist: av==14.0.1; extra == "video-mae"
Requires-Dist: transformers~=4.56.2; extra == "video-mae"
Provides-Extra: whisper-small-quantized
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "whisper-small-quantized"
Requires-Dist: transformers~=4.56.2; extra == "whisper-small-quantized"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-small-quantized"
Requires-Dist: onnx<1.20,>=1.17; extra == "whisper-small-quantized"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "whisper-small-quantized"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "whisper-small-quantized"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-small-quantized"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-small-quantized"
Provides-Extra: whisper-small-quantized
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "whisper-small-quantized"
Requires-Dist: transformers~=4.56.2; extra == "whisper-small-quantized"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-small-quantized"
Requires-Dist: onnx<1.20,>=1.17; extra == "whisper-small-quantized"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "whisper-small-quantized"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "whisper-small-quantized"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-small-quantized"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-small-quantized"
Provides-Extra: litehrnet
Requires-Dist: chumpy==0.71; extra == "litehrnet"
Requires-Dist: mmdet==3.3.0+mmcv220; extra == "litehrnet"
Requires-Dist: mmpose==1.2.0; extra == "litehrnet"
Requires-Dist: mmcv==2.2.0; extra == "litehrnet"
Provides-Extra: deepbox
Requires-Dist: object-detection-metrics==0.4.post1; extra == "deepbox"
Requires-Dist: shapely==2.0.3; extra == "deepbox"
Requires-Dist: ultralytics~=8.3.0; extra == "deepbox"
Provides-Extra: detr-resnet50
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet50"
Requires-Dist: object-detection-metrics==0.4.post1; extra == "detr-resnet50"
Requires-Dist: shapely==2.0.3; extra == "detr-resnet50"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "detr-resnet50"
Provides-Extra: detr-resnet50
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet50"
Requires-Dist: object-detection-metrics==0.4.post1; extra == "detr-resnet50"
Requires-Dist: shapely==2.0.3; extra == "detr-resnet50"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "detr-resnet50"
Provides-Extra: hrnet-pose
Requires-Dist: yacs==0.1.8; extra == "hrnet-pose"
Requires-Dist: chumpy==0.71; extra == "hrnet-pose"
Requires-Dist: mmdet==3.3.0+mmcv220; extra == "hrnet-pose"
Requires-Dist: mmpose==1.2.0; extra == "hrnet-pose"
Requires-Dist: mmcv==2.2.0; extra == "hrnet-pose"
Requires-Dist: xtcocotools==1.14.3; extra == "hrnet-pose"
Provides-Extra: hrnet-pose
Requires-Dist: yacs==0.1.8; extra == "hrnet-pose"
Requires-Dist: chumpy==0.71; extra == "hrnet-pose"
Requires-Dist: mmdet==3.3.0+mmcv220; extra == "hrnet-pose"
Requires-Dist: mmpose==1.2.0; extra == "hrnet-pose"
Requires-Dist: mmcv==2.2.0; extra == "hrnet-pose"
Requires-Dist: xtcocotools==1.14.3; extra == "hrnet-pose"
Provides-Extra: depth-anything
Requires-Dist: matplotlib==3.7.5; extra == "depth-anything"
Requires-Dist: transformers~=4.56.2; extra == "depth-anything"
Provides-Extra: depth-anything
Requires-Dist: matplotlib==3.7.5; extra == "depth-anything"
Requires-Dist: transformers~=4.56.2; extra == "depth-anything"
Provides-Extra: nasnet
Requires-Dist: timm==1.0.15; extra == "nasnet"
Provides-Extra: yolov11-seg
Requires-Dist: ultralytics~=8.3.0; extra == "yolov11-seg"
Requires-Dist: shapely==2.0.3; extra == "yolov11-seg"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov11-seg"
Requires-Dist: torchmetrics==1.4.0.post0; extra == "yolov11-seg"
Provides-Extra: yolov11-seg
Requires-Dist: ultralytics~=8.3.0; extra == "yolov11-seg"
Requires-Dist: shapely==2.0.3; extra == "yolov11-seg"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov11-seg"
Requires-Dist: torchmetrics==1.4.0.post0; extra == "yolov11-seg"
Provides-Extra: ffnet-54s
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-54s"
Provides-Extra: ffnet-54s
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-54s"
Provides-Extra: whisper-base
Requires-Dist: jiwer==3.0.3; extra == "whisper-base"
Requires-Dist: transformers~=4.56.2; extra == "whisper-base"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-base"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-base"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-base"
Provides-Extra: whisper-base
Requires-Dist: jiwer==3.0.3; extra == "whisper-base"
Requires-Dist: transformers~=4.56.2; extra == "whisper-base"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-base"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-base"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-base"
Provides-Extra: llama-v3-1-sea-lion-3-5-8b-r
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: psutil; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: torch==2.4.1; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Provides-Extra: llama-v3-1-sea-lion-3-5-8b-r
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: psutil; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: torch==2.4.1; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-1-sea-lion-3-5-8b-r"
Provides-Extra: llama-v3-8b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-8b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-8b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-8b-instruct"
Requires-Dist: psutil; extra == "llama-v3-8b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-8b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-8b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-8b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-8b-instruct"
Provides-Extra: llama-v3-8b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-8b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-8b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-8b-instruct"
Requires-Dist: psutil; extra == "llama-v3-8b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-8b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-8b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-8b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-8b-instruct"
Provides-Extra: resnet-mixed
Requires-Dist: av==14.0.1; extra == "resnet-mixed"
Provides-Extra: resnet-mixed
Requires-Dist: av==14.0.1; extra == "resnet-mixed"
Provides-Extra: track-anything
Requires-Dist: psutil<7,>6; extra == "track-anything"
Provides-Extra: track-anything
Requires-Dist: psutil<7,>6; extra == "track-anything"
Provides-Extra: yolov11-det
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov11-det"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov11-det"
Requires-Dist: shapely==2.0.3; extra == "yolov11-det"
Provides-Extra: yolov11-det
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov11-det"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov11-det"
Requires-Dist: shapely==2.0.3; extra == "yolov11-det"
Provides-Extra: detr-resnet50-dc5
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet50-dc5"
Requires-Dist: timm==1.0.15; extra == "detr-resnet50-dc5"
Provides-Extra: detr-resnet50-dc5
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet50-dc5"
Requires-Dist: timm==1.0.15; extra == "detr-resnet50-dc5"
Provides-Extra: fastsam-x
Requires-Dist: ultralytics~=8.3.0; extra == "fastsam-x"
Provides-Extra: fastsam-x
Requires-Dist: ultralytics~=8.3.0; extra == "fastsam-x"
Provides-Extra: bgnet
Requires-Dist: pysodmetrics==1.3.0; extra == "bgnet"
Provides-Extra: nomic-embed-text
Requires-Dist: transformers~=4.56.2; extra == "nomic-embed-text"
Requires-Dist: einops==0.3.2; extra == "nomic-embed-text"
Provides-Extra: nomic-embed-text
Requires-Dist: transformers~=4.56.2; extra == "nomic-embed-text"
Requires-Dist: einops==0.3.2; extra == "nomic-embed-text"
Provides-Extra: posenet-mobilenet
Requires-Dist: xtcocotools==1.14.3; extra == "posenet-mobilenet"
Provides-Extra: posenet-mobilenet
Requires-Dist: xtcocotools==1.14.3; extra == "posenet-mobilenet"
Provides-Extra: depth-anything-v2
Requires-Dist: matplotlib==3.7.5; extra == "depth-anything-v2"
Requires-Dist: transformers~=4.56.2; extra == "depth-anything-v2"
Provides-Extra: depth-anything-v2
Requires-Dist: matplotlib==3.7.5; extra == "depth-anything-v2"
Requires-Dist: transformers~=4.56.2; extra == "depth-anything-v2"
Provides-Extra: real-esrgan-general-x4v3
Requires-Dist: seaborn==0.11.0; extra == "real-esrgan-general-x4v3"
Requires-Dist: basicsr==1.4.2; extra == "real-esrgan-general-x4v3"
Provides-Extra: real-esrgan-general-x4v3
Requires-Dist: seaborn==0.11.0; extra == "real-esrgan-general-x4v3"
Requires-Dist: basicsr==1.4.2; extra == "real-esrgan-general-x4v3"
Provides-Extra: controlnet-canny
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "controlnet-canny"
Requires-Dist: transformers~=4.56.2; extra == "controlnet-canny"
Requires-Dist: diffusers[torch]~=0.35.1; extra == "controlnet-canny"
Requires-Dist: onnx<1.20,>=1.17; extra == "controlnet-canny"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "controlnet-canny"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "controlnet-canny"
Provides-Extra: controlnet-canny
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "controlnet-canny"
Requires-Dist: transformers~=4.56.2; extra == "controlnet-canny"
Requires-Dist: diffusers[torch]~=0.35.1; extra == "controlnet-canny"
Requires-Dist: onnx<1.20,>=1.17; extra == "controlnet-canny"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "controlnet-canny"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "controlnet-canny"
Provides-Extra: efficientvit-l2-cls
Requires-Dist: segment-anything==1.0; extra == "efficientvit-l2-cls"
Provides-Extra: efficientvit-l2-cls
Requires-Dist: segment-anything==1.0; extra == "efficientvit-l2-cls"
Provides-Extra: huggingface-wavlm-base-plus
Requires-Dist: transformers~=4.56.2; extra == "huggingface-wavlm-base-plus"
Requires-Dist: soundfile==0.13.1; extra == "huggingface-wavlm-base-plus"
Requires-Dist: librosa==0.10.1; extra == "huggingface-wavlm-base-plus"
Requires-Dist: jiwer==3.0.3; extra == "huggingface-wavlm-base-plus"
Requires-Dist: torch>=2.6; extra == "huggingface-wavlm-base-plus"
Provides-Extra: huggingface-wavlm-base-plus
Requires-Dist: transformers~=4.56.2; extra == "huggingface-wavlm-base-plus"
Requires-Dist: soundfile==0.13.1; extra == "huggingface-wavlm-base-plus"
Requires-Dist: librosa==0.10.1; extra == "huggingface-wavlm-base-plus"
Requires-Dist: jiwer==3.0.3; extra == "huggingface-wavlm-base-plus"
Requires-Dist: torch>=2.6; extra == "huggingface-wavlm-base-plus"
Provides-Extra: ffnet-78s-lowres
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-78s-lowres"
Provides-Extra: ffnet-78s-lowres
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-78s-lowres"
Provides-Extra: mask2former
Requires-Dist: transformers==4.51.3; extra == "mask2former"
Provides-Extra: easyocr
Requires-Dist: easyocr==1.7.2; extra == "easyocr"
Provides-Extra: resnet-3d
Requires-Dist: av==14.0.1; extra == "resnet-3d"
Provides-Extra: resnet-3d
Requires-Dist: av==14.0.1; extra == "resnet-3d"
Provides-Extra: ddcolor
Requires-Dist: timm==1.0.15; extra == "ddcolor"
Provides-Extra: yolov8-seg
Requires-Dist: ultralytics~=8.3.0; extra == "yolov8-seg"
Requires-Dist: shapely==2.0.3; extra == "yolov8-seg"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov8-seg"
Requires-Dist: torchmetrics==1.4.0.post0; extra == "yolov8-seg"
Provides-Extra: yolov8-seg
Requires-Dist: ultralytics~=8.3.0; extra == "yolov8-seg"
Requires-Dist: shapely==2.0.3; extra == "yolov8-seg"
Requires-Dist: fiftyone<1.9,>=1.0.1; extra == "yolov8-seg"
Requires-Dist: torchmetrics==1.4.0.post0; extra == "yolov8-seg"
Provides-Extra: detr-resnet101
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet101"
Provides-Extra: detr-resnet101
Requires-Dist: transformers~=4.56.2; extra == "detr-resnet101"
Provides-Extra: midas
Requires-Dist: timm==1.0.15; extra == "midas"
Requires-Dist: matplotlib==3.7.5; extra == "midas"
Requires-Dist: scipy<2,>=1.8.1; extra == "midas"
Provides-Extra: yamnet
Requires-Dist: resampy==0.4.3; extra == "yamnet"
Requires-Dist: torchaudio<2.9.0,>=2.1.2; extra == "yamnet"
Requires-Dist: soundfile==0.13.1; extra == "yamnet"
Requires-Dist: torchmetrics==1.4.0.post0; extra == "yamnet"
Provides-Extra: ffnet-40s
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-40s"
Provides-Extra: ffnet-40s
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-40s"
Provides-Extra: openai-clip
Requires-Dist: ftfy==6.1.1; extra == "openai-clip"
Requires-Dist: regex==2023.10.3; extra == "openai-clip"
Provides-Extra: openai-clip
Requires-Dist: ftfy==6.1.1; extra == "openai-clip"
Requires-Dist: regex==2023.10.3; extra == "openai-clip"
Provides-Extra: levit
Requires-Dist: transformers~=4.56.2; extra == "levit"
Provides-Extra: llama-v3-2-1b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-2-1b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-2-1b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-2-1b-instruct"
Requires-Dist: psutil; extra == "llama-v3-2-1b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-2-1b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-2-1b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-2-1b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-2-1b-instruct"
Provides-Extra: llama-v3-2-1b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-2-1b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-2-1b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-2-1b-instruct"
Requires-Dist: psutil; extra == "llama-v3-2-1b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-2-1b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-2-1b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-2-1b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-2-1b-instruct"
Provides-Extra: yolov10-det
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov10-det"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov10-det"
Requires-Dist: shapely==2.0.3; extra == "yolov10-det"
Provides-Extra: yolov10-det
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov10-det"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov10-det"
Requires-Dist: shapely==2.0.3; extra == "yolov10-det"
Provides-Extra: ffnet-78s
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-78s"
Provides-Extra: ffnet-78s
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-78s"
Provides-Extra: resnet-2plus1d
Requires-Dist: av==14.0.1; extra == "resnet-2plus1d"
Provides-Extra: resnet-2plus1d
Requires-Dist: av==14.0.1; extra == "resnet-2plus1d"
Provides-Extra: llama-v2-7b-chat
Requires-Dist: transformers==4.41.1; extra == "llama-v2-7b-chat"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v2-7b-chat"
Requires-Dist: psutil; extra == "llama-v2-7b-chat"
Provides-Extra: llama-v2-7b-chat
Requires-Dist: transformers==4.41.1; extra == "llama-v2-7b-chat"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v2-7b-chat"
Requires-Dist: psutil; extra == "llama-v2-7b-chat"
Provides-Extra: yolov5
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov5"
Requires-Dist: shapely==2.0.3; extra == "yolov5"
Requires-Dist: ultralytics~=8.3.0; extra == "yolov5"
Provides-Extra: ffnet-122ns-lowres
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-122ns-lowres"
Provides-Extra: ffnet-122ns-lowres
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "ffnet-122ns-lowres"
Provides-Extra: efficientvit-b2-cls
Requires-Dist: segment-anything==1.0; extra == "efficientvit-b2-cls"
Provides-Extra: efficientvit-b2-cls
Requires-Dist: segment-anything==1.0; extra == "efficientvit-b2-cls"
Provides-Extra: yolov6
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov6"
Requires-Dist: shapely==2.0.3; extra == "yolov6"
Provides-Extra: efficientvit-l2-seg
Requires-Dist: segment-anything==1.0; extra == "efficientvit-l2-seg"
Provides-Extra: efficientvit-l2-seg
Requires-Dist: segment-anything==1.0; extra == "efficientvit-l2-seg"
Provides-Extra: llama-v3-2-3b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-2-3b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-2-3b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-2-3b-instruct"
Requires-Dist: psutil; extra == "llama-v3-2-3b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-2-3b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-2-3b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-2-3b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-2-3b-instruct"
Provides-Extra: llama-v3-2-3b-instruct
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "llama-v3-2-3b-instruct"
Requires-Dist: transformers==4.45.0; extra == "llama-v3-2-3b-instruct"
Requires-Dist: sentencepiece==0.2.0; extra == "llama-v3-2-3b-instruct"
Requires-Dist: psutil; extra == "llama-v3-2-3b-instruct"
Requires-Dist: onnx==1.17.0; extra == "llama-v3-2-3b-instruct"
Requires-Dist: torch==2.4.1; extra == "llama-v3-2-3b-instruct"
Requires-Dist: torchvision==0.19.1; extra == "llama-v3-2-3b-instruct"
Requires-Dist: onnxruntime==1.22; extra == "llama-v3-2-3b-instruct"
Provides-Extra: yolov3
Requires-Dist: ultralytics~=8.3.0; extra == "yolov3"
Requires-Dist: object-detection-metrics==0.4.post1; extra == "yolov3"
Requires-Dist: shapely==2.0.3; extra == "yolov3"
Provides-Extra: facemap-3dmm
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "facemap-3dmm"
Requires-Dist: xtcocotools==1.14.3; extra == "facemap-3dmm"
Provides-Extra: facemap-3dmm
Requires-Dist: scikit-image<0.25,>0.21.0; extra == "facemap-3dmm"
Requires-Dist: xtcocotools==1.14.3; extra == "facemap-3dmm"
Provides-Extra: trocr
Requires-Dist: transformers~=4.56.2; extra == "trocr"
Requires-Dist: sentencepiece==0.2.0; extra == "trocr"
Requires-Dist: torch>=2.6; extra == "trocr"
Provides-Extra: mediapipe-selfie
Requires-Dist: tflite==2.10.0; extra == "mediapipe-selfie"
Provides-Extra: mediapipe-selfie
Requires-Dist: tflite==2.10.0; extra == "mediapipe-selfie"
Provides-Extra: stable-diffusion-v2-1
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "stable-diffusion-v2-1"
Requires-Dist: transformers~=4.56.2; extra == "stable-diffusion-v2-1"
Requires-Dist: diffusers[torch]~=0.35.1; extra == "stable-diffusion-v2-1"
Requires-Dist: onnx<1.20,>=1.17; extra == "stable-diffusion-v2-1"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "stable-diffusion-v2-1"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "stable-diffusion-v2-1"
Provides-Extra: stable-diffusion-v2-1
Requires-Dist: aimet-onnx==2.14.0; (sys_platform == "linux" and python_version == "3.10") and extra == "stable-diffusion-v2-1"
Requires-Dist: transformers~=4.56.2; extra == "stable-diffusion-v2-1"
Requires-Dist: diffusers[torch]~=0.35.1; extra == "stable-diffusion-v2-1"
Requires-Dist: onnx<1.20,>=1.17; extra == "stable-diffusion-v2-1"
Requires-Dist: onnxsim<=0.4.36; python_version < "3.12" and extra == "stable-diffusion-v2-1"
Requires-Dist: onnxsim-prebuilt==0.4.36.post1; python_version >= "3.12" and extra == "stable-diffusion-v2-1"
Provides-Extra: real-esrgan-x4plus
Requires-Dist: seaborn==0.11.0; extra == "real-esrgan-x4plus"
Requires-Dist: basicsr==1.4.2; extra == "real-esrgan-x4plus"
Provides-Extra: real-esrgan-x4plus
Requires-Dist: seaborn==0.11.0; extra == "real-esrgan-x4plus"
Requires-Dist: basicsr==1.4.2; extra == "real-esrgan-x4plus"
Provides-Extra: qwen2-5-7b-instruct
Requires-Dist: onnx==1.17.0; extra == "qwen2-5-7b-instruct"
Requires-Dist: transformers==4.45.0; extra == "qwen2-5-7b-instruct"
Requires-Dist: psutil; extra == "qwen2-5-7b-instruct"
Provides-Extra: qwen2-5-7b-instruct
Requires-Dist: onnx==1.17.0; extra == "qwen2-5-7b-instruct"
Requires-Dist: transformers==4.45.0; extra == "qwen2-5-7b-instruct"
Requires-Dist: psutil; extra == "qwen2-5-7b-instruct"
Provides-Extra: sam2
Requires-Dist: pycocotools==2.0.7; extra == "sam2"
Requires-Dist: matplotlib==3.7.5; extra == "sam2"
Requires-Dist: hydra-core==1.3.0; extra == "sam2"
Requires-Dist: iopath==0.1.10; extra == "sam2"
Provides-Extra: whisper-tiny
Requires-Dist: jiwer==3.0.3; extra == "whisper-tiny"
Requires-Dist: transformers~=4.56.2; extra == "whisper-tiny"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-tiny"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-tiny"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-tiny"
Provides-Extra: whisper-tiny
Requires-Dist: jiwer==3.0.3; extra == "whisper-tiny"
Requires-Dist: transformers~=4.56.2; extra == "whisper-tiny"
Requires-Dist: audio2numpy==0.1.2; extra == "whisper-tiny"
Requires-Dist: scipy<2,>=1.8.1; extra == "whisper-tiny"
Requires-Dist: sounddevice==0.5.2; extra == "whisper-tiny"
Provides-Extra: mobile-vit
Requires-Dist: transformers~=4.56.2; extra == "mobile-vit"
Requires-Dist: torch>=2.6; extra == "mobile-vit"
Provides-Extra: mobile-vit
Requires-Dist: transformers~=4.56.2; extra == "mobile-vit"
Requires-Dist: torch>=2.6; extra == "mobile-vit"
Provides-Extra: beit
Requires-Dist: transformers~=4.56.2; extra == "beit"
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist

# [QualcommÂ® AI Hub Models](https://aihub.qualcomm.com/)

[![Release](https://img.shields.io/github/v/release/quic/ai-hub-models)](https://github.com/quic/ai-hub-models/releases/latest)
[![Tag](https://img.shields.io/github/v/tag/quic/ai-hub-models)](https://github.com/quic/ai-hub-models/releases/latest)
[![PyPi](https://img.shields.io/pypi/v/qai-hub-models)](https://pypi.org/project/qai-hub-models/)
![Python 3.9, 3.10, 3.11, 3.12](https://img.shields.io/badge/python-3.9%2C%203.10%20(Recommended)%2C%203.11%2C%203.12-yellow)

The QualcommÂ® AI Hub Models are a collection of
state-of-the-art machine learning models optimized for deployment on QualcommÂ® devices.

* [List of Models by Category](#model-directory)
* [On-Device Performance Data](https://aihub.qualcomm.com/models)
* [Device-Native Sample Apps](https://github.com/quic/ai-hub-apps)

See supported: [On-Device Runtimes](#on-device-runtimes), [Hardware Targets & Precision](#device-hardware--precision), [Chipsets](#chipsets), [Devices](#devices)

&nbsp;

## Setup

### 1. Install Python Package

The package is available via pip:

```shell
# NOTE for Snapdragon X Elite users:
# Only AMDx64 (64-bit) Python in supported on Windows.
# Installation will fail when using Windows ARM64 Python.

pip install qai_hub_models
```

Some models (e.g. [YOLOv7](https://github.com/quic/ai-hub-models/tree/main/qai_hub_models/models/yolov7)) require
additional dependencies that can be installed as follows:

```shell
pip install "qai_hub_models[yolov7]"
```

&nbsp;

### 2. Configure AI Hub Access

Many features of AI Hub Models _(such as model compilation, on-device profiling, etc.)_ require access to QualcommÂ® AI Hub:

-  [Create a QualcommÂ® ID](https://myaccount.qualcomm.com/signup), and use it to [login to QualcommÂ® AI Hub](https://app.aihub.qualcomm.com/).
-  Configure your [API token](https://app.aihub.qualcomm.com/account/): `qai-hub configure --api_token API_TOKEN`

&nbsp;

## Getting Started

### Export and Run A Model on a Physical Device

All [models in our directory](#model-directory) can be compiled and profiled on a hosted
QualcommÂ® device:

```shell
pip install "qai_hub_models[yolov7]"

python -m qai_hub_models.models.yolov7.export [--target-runtime ...] [--device ...] [--help]
```

_Using QualcommÂ® AI Hub_, the export script will:

1. **Compile** the model for the chosen device and target runtime (see: [Compiling Models on AI Hub](https://app.aihub.qualcomm.com/docs/hub/compile_examples.html)).
2. If applicable, **Quantize** the model (see: [Quantization on AI Hub](https://app.aihub.qualcomm.com/docs/hub/quantize_examples.html))
3. **Profile** the compiled model on a real device in the cloud (see: [Profiling Models on AI Hub](https://app.aihub.qualcomm.com/docs/hub/profile_examples.html)).
4. **Run inference** with a sample input data on a real device in the cloud, and compare on-device model output with PyTorch output (see: [Running Inference on AI Hub](https://app.aihub.qualcomm.com/docs/hub/inference_examples.html))
5. **Download** the compiled model to disk.

&nbsp;

### End-To-End Model Demos

Most [models in our directory](#model-directory) contain CLI demos that run the model _end-to-end_:

```shell
pip install "qai_hub_models[yolov7]"
# Predict and draw bounding boxes on the provided image
python -m qai_hub_models.models.yolov7.demo [--image ...] [--eval-mode {fp,on-device}] [--help]
```

_End-to-end_ demos:
1. **Preprocess** human-readable input into model input
2. Run **model inference**
3. **Postprocess** model output to a human-readable format

**Many end-to-end demos use AI Hub to run inference on a real cloud-hosted device** _(with `--eval-mode on-device`)_. All end-to-end demos can also run locally via PyTorch (with `--eval-mode fp`).

&nbsp;

### Sample Applications

**Native** applications that can run our models (with pre- and post-processing) on physical devices are published in the [AI Hub Apps repository](https://github.com/quic/ai-hub-apps/).

**Python** applications are defined for all models [(from qai_hub_models.models.\<model_name> import App)](https://github.com/quic/ai-hub-models/blob/main/qai_hub_models/models/yolov7/app.py). These apps wrap model inference with pre- and post-processing steps written using torch & numpy. **These apps are optimized to be an easy-to-follow example, rather than to minimize prediction time.**

&nbsp;

## Model Support Data

### On-Device Runtimes

| Runtime | Supported OS |
| -- | -- |
| [Qualcomm AI Engine Direct](https://www.qualcomm.com/developer/artificial-intelligence#overview) | Android, Linux, Windows
| [LiteRT (TensorFlow Lite)](https://www.tensorflow.org/lite) | Android, Linux
| [ONNX](https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html) | Android, Linux, Windows

### Device Hardware & Precision

| Device Compute Unit | Supported Precision |
| -- | -- |
| CPU | FP32, INT16, INT8
| GPU | FP32, FP16
| NPU (includes [Hexagon DSP](https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor), [HTP](https://developer.qualcomm.com/hardware/qualcomm-innovators-development-kit/ai-resources-overview/ai-hardware-cores-accelerators)) | FP16*, INT16, INT8

*Some older chipsets do not support fp16 inference on their NPU.

### Chipsets
* Snapdragon [8 Elite](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-elite-mobile-platform), [8 Gen 3](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform), [8 Gen 2](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-2-mobile-platform), and [8 Gen 1](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-1-mobile-platform) Mobile Platforms
* [Snapdragon X Elite](https://www.qualcomm.com/products/mobile/snapdragon/pcs-and-tablets/snapdragon-x-elite) Compute Platform
* SA8255P, SA8295P, SA8650P, and SA8775P Automotive Platforms
* [QCS 6490](https://www.qualcomm.com/products/internet-of-things/industrial/building-enterprise/qcs6490),  [QCS 8250](https://www.qualcomm.com/products/internet-of-things/consumer/cameras/qcs8250), and [QCS 8550](https://www.qualcomm.com/products/technology/processors/qcs8550) IoT Platforms
* QCS8450 XR Platform

and many more.

### Devices
* Samsung Galaxy S21, S22, S23, and S24 Series
* Xiaomi 12 and 13
* Snapdragon X Elite CRD (Compute Reference Device)
* Qualcomm RB3 Gen 2, RB5

and many more.

&nbsp;

## Model Directory

### Computer Vision

| Model | README |
| -- | -- |
| | |
| **Image Classification**
| [Beit](https://aihub.qualcomm.com/models/beit) | [qai_hub_models.models.beit](qai_hub_models/models/beit/README.md) |
| [ConvNext-Base](https://aihub.qualcomm.com/models/convnext_base) | [qai_hub_models.models.convnext_base](qai_hub_models/models/convnext_base/README.md) |
| [ConvNext-Tiny](https://aihub.qualcomm.com/models/convnext_tiny) | [qai_hub_models.models.convnext_tiny](qai_hub_models/models/convnext_tiny/README.md) |
| [DLA-102-X](https://aihub.qualcomm.com/models/dla102x) | [qai_hub_models.models.dla102x](qai_hub_models/models/dla102x/README.md) |
| [DenseNet-121](https://aihub.qualcomm.com/models/densenet121) | [qai_hub_models.models.densenet121](qai_hub_models/models/densenet121/README.md) |
| [EfficientFormer](https://aihub.qualcomm.com/models/efficientformer) | [qai_hub_models.models.efficientformer](qai_hub_models/models/efficientformer/README.md) |
| [EfficientNet-B0](https://aihub.qualcomm.com/models/efficientnet_b0) | [qai_hub_models.models.efficientnet_b0](qai_hub_models/models/efficientnet_b0/README.md) |
| [EfficientNet-B4](https://aihub.qualcomm.com/models/efficientnet_b4) | [qai_hub_models.models.efficientnet_b4](qai_hub_models/models/efficientnet_b4/README.md) |
| [EfficientNet-V2-s](https://aihub.qualcomm.com/models/efficientnet_v2_s) | [qai_hub_models.models.efficientnet_v2_s](qai_hub_models/models/efficientnet_v2_s/README.md) |
| [EfficientViT-b2-cls](https://aihub.qualcomm.com/models/efficientvit_b2_cls) | [qai_hub_models.models.efficientvit_b2_cls](qai_hub_models/models/efficientvit_b2_cls/README.md) |
| [EfficientViT-l2-cls](https://aihub.qualcomm.com/models/efficientvit_l2_cls) | [qai_hub_models.models.efficientvit_l2_cls](qai_hub_models/models/efficientvit_l2_cls/README.md) |
| [GPUNet](https://aihub.qualcomm.com/models/gpunet) | [qai_hub_models.models.gpunet](qai_hub_models/models/gpunet/README.md) |
| [GoogLeNet](https://aihub.qualcomm.com/models/googlenet) | [qai_hub_models.models.googlenet](qai_hub_models/models/googlenet/README.md) |
| [Inception-v3](https://aihub.qualcomm.com/models/inception_v3) | [qai_hub_models.models.inception_v3](qai_hub_models/models/inception_v3/README.md) |
| [LeViT](https://aihub.qualcomm.com/models/levit) | [qai_hub_models.models.levit](qai_hub_models/models/levit/README.md) |
| [MNASNet05](https://aihub.qualcomm.com/models/mnasnet05) | [qai_hub_models.models.mnasnet05](qai_hub_models/models/mnasnet05/README.md) |
| [Mobile-VIT](https://aihub.qualcomm.com/models/mobile_vit) | [qai_hub_models.models.mobile_vit](qai_hub_models/models/mobile_vit/README.md) |
| [MobileNet-v2](https://aihub.qualcomm.com/models/mobilenet_v2) | [qai_hub_models.models.mobilenet_v2](qai_hub_models/models/mobilenet_v2/README.md) |
| [MobileNet-v3-Large](https://aihub.qualcomm.com/models/mobilenet_v3_large) | [qai_hub_models.models.mobilenet_v3_large](qai_hub_models/models/mobilenet_v3_large/README.md) |
| [MobileNet-v3-Small](https://aihub.qualcomm.com/models/mobilenet_v3_small) | [qai_hub_models.models.mobilenet_v3_small](qai_hub_models/models/mobilenet_v3_small/README.md) |
| [NASNet](https://aihub.qualcomm.com/models/nasnet) | [qai_hub_models.models.nasnet](qai_hub_models/models/nasnet/README.md) |
| [RegNet](https://aihub.qualcomm.com/models/regnet) | [qai_hub_models.models.regnet](qai_hub_models/models/regnet/README.md) |
| [RegNet-Y-800MF](https://aihub.qualcomm.com/models/regnet_y_800mf) | [qai_hub_models.models.regnet_y_800mf](qai_hub_models/models/regnet_y_800mf/README.md) |
| [ResNeXt101](https://aihub.qualcomm.com/models/resnext101) | [qai_hub_models.models.resnext101](qai_hub_models/models/resnext101/README.md) |
| [ResNeXt50](https://aihub.qualcomm.com/models/resnext50) | [qai_hub_models.models.resnext50](qai_hub_models/models/resnext50/README.md) |
| [ResNet101](https://aihub.qualcomm.com/models/resnet101) | [qai_hub_models.models.resnet101](qai_hub_models/models/resnet101/README.md) |
| [ResNet18](https://aihub.qualcomm.com/models/resnet18) | [qai_hub_models.models.resnet18](qai_hub_models/models/resnet18/README.md) |
| [ResNet50](https://aihub.qualcomm.com/models/resnet50) | [qai_hub_models.models.resnet50](qai_hub_models/models/resnet50/README.md) |
| [Sequencer2D](https://aihub.qualcomm.com/models/sequencer2d) | [qai_hub_models.models.sequencer2d](qai_hub_models/models/sequencer2d/README.md) |
| [Shufflenet-v2](https://aihub.qualcomm.com/models/shufflenet_v2) | [qai_hub_models.models.shufflenet_v2](qai_hub_models/models/shufflenet_v2/README.md) |
| [SqueezeNet-1.1](https://aihub.qualcomm.com/models/squeezenet1_1) | [qai_hub_models.models.squeezenet1_1](qai_hub_models/models/squeezenet1_1/README.md) |
| [Swin-Base](https://aihub.qualcomm.com/models/swin_base) | [qai_hub_models.models.swin_base](qai_hub_models/models/swin_base/README.md) |
| [Swin-Small](https://aihub.qualcomm.com/models/swin_small) | [qai_hub_models.models.swin_small](qai_hub_models/models/swin_small/README.md) |
| [Swin-Tiny](https://aihub.qualcomm.com/models/swin_tiny) | [qai_hub_models.models.swin_tiny](qai_hub_models/models/swin_tiny/README.md) |
| [VIT](https://aihub.qualcomm.com/models/vit) | [qai_hub_models.models.vit](qai_hub_models/models/vit/README.md) |
| [WideResNet50](https://aihub.qualcomm.com/models/wideresnet50) | [qai_hub_models.models.wideresnet50](qai_hub_models/models/wideresnet50/README.md) |
| | |
| **Image Editing**
| [AOT-GAN](https://aihub.qualcomm.com/models/aotgan) | [qai_hub_models.models.aotgan](qai_hub_models/models/aotgan/README.md) |
| [DDColor](https://aihub.qualcomm.com/models/ddcolor) | [qai_hub_models.models.ddcolor](qai_hub_models/models/ddcolor/README.md) |
| [LaMa-Dilated](https://aihub.qualcomm.com/models/lama_dilated) | [qai_hub_models.models.lama_dilated](qai_hub_models/models/lama_dilated/README.md) |
| | |
| **Super Resolution**
| [ESRGAN](https://aihub.qualcomm.com/models/esrgan) | [qai_hub_models.models.esrgan](qai_hub_models/models/esrgan/README.md) |
| [QuickSRNetLarge](https://aihub.qualcomm.com/models/quicksrnetlarge) | [qai_hub_models.models.quicksrnetlarge](qai_hub_models/models/quicksrnetlarge/README.md) |
| [QuickSRNetMedium](https://aihub.qualcomm.com/models/quicksrnetmedium) | [qai_hub_models.models.quicksrnetmedium](qai_hub_models/models/quicksrnetmedium/README.md) |
| [QuickSRNetSmall](https://aihub.qualcomm.com/models/quicksrnetsmall) | [qai_hub_models.models.quicksrnetsmall](qai_hub_models/models/quicksrnetsmall/README.md) |
| [Real-ESRGAN-General-x4v3](https://aihub.qualcomm.com/models/real_esrgan_general_x4v3) | [qai_hub_models.models.real_esrgan_general_x4v3](qai_hub_models/models/real_esrgan_general_x4v3/README.md) |
| [Real-ESRGAN-x4plus](https://aihub.qualcomm.com/models/real_esrgan_x4plus) | [qai_hub_models.models.real_esrgan_x4plus](qai_hub_models/models/real_esrgan_x4plus/README.md) |
| [SESR-M5](https://aihub.qualcomm.com/models/sesr_m5) | [qai_hub_models.models.sesr_m5](qai_hub_models/models/sesr_m5/README.md) |
| [XLSR](https://aihub.qualcomm.com/models/xlsr) | [qai_hub_models.models.xlsr](qai_hub_models/models/xlsr/README.md) |
| | |
| **Semantic Segmentation**
| [BGNet](https://aihub.qualcomm.com/models/bgnet) | [qai_hub_models.models.bgnet](qai_hub_models/models/bgnet/README.md) |
| [BiseNet](https://aihub.qualcomm.com/models/bisenet) | [qai_hub_models.models.bisenet](qai_hub_models/models/bisenet/README.md) |
| [DDRNet23-Slim](https://aihub.qualcomm.com/models/ddrnet23_slim) | [qai_hub_models.models.ddrnet23_slim](qai_hub_models/models/ddrnet23_slim/README.md) |
| [DeepLabV3-Plus-MobileNet](https://aihub.qualcomm.com/models/deeplabv3_plus_mobilenet) | [qai_hub_models.models.deeplabv3_plus_mobilenet](qai_hub_models/models/deeplabv3_plus_mobilenet/README.md) |
| [DeepLabV3-ResNet50](https://aihub.qualcomm.com/models/deeplabv3_resnet50) | [qai_hub_models.models.deeplabv3_resnet50](qai_hub_models/models/deeplabv3_resnet50/README.md) |
| [DeepLabXception](https://aihub.qualcomm.com/models/deeplab_xception) | [qai_hub_models.models.deeplab_xception](qai_hub_models/models/deeplab_xception/README.md) |
| [EfficientViT-l2-seg](https://aihub.qualcomm.com/models/efficientvit_l2_seg) | [qai_hub_models.models.efficientvit_l2_seg](qai_hub_models/models/efficientvit_l2_seg/README.md) |
| [FCN-ResNet50](https://aihub.qualcomm.com/models/fcn_resnet50) | [qai_hub_models.models.fcn_resnet50](qai_hub_models/models/fcn_resnet50/README.md) |
| [FFNet-122NS-LowRes](https://aihub.qualcomm.com/models/ffnet_122ns_lowres) | [qai_hub_models.models.ffnet_122ns_lowres](qai_hub_models/models/ffnet_122ns_lowres/README.md) |
| [FFNet-40S](https://aihub.qualcomm.com/models/ffnet_40s) | [qai_hub_models.models.ffnet_40s](qai_hub_models/models/ffnet_40s/README.md) |
| [FFNet-54S](https://aihub.qualcomm.com/models/ffnet_54s) | [qai_hub_models.models.ffnet_54s](qai_hub_models/models/ffnet_54s/README.md) |
| [FFNet-78S](https://aihub.qualcomm.com/models/ffnet_78s) | [qai_hub_models.models.ffnet_78s](qai_hub_models/models/ffnet_78s/README.md) |
| [FFNet-78S-LowRes](https://aihub.qualcomm.com/models/ffnet_78s_lowres) | [qai_hub_models.models.ffnet_78s_lowres](qai_hub_models/models/ffnet_78s_lowres/README.md) |
| [FastSam-S](https://aihub.qualcomm.com/models/fastsam_s) | [qai_hub_models.models.fastsam_s](qai_hub_models/models/fastsam_s/README.md) |
| [FastSam-X](https://aihub.qualcomm.com/models/fastsam_x) | [qai_hub_models.models.fastsam_x](qai_hub_models/models/fastsam_x/README.md) |
| [HRNet-W48-OCR](https://aihub.qualcomm.com/models/hrnet_w48_ocr) | [qai_hub_models.models.hrnet_w48_ocr](qai_hub_models/models/hrnet_w48_ocr/README.md) |
| [Mask2Former](https://aihub.qualcomm.com/models/mask2former) | [qai_hub_models.models.mask2former](qai_hub_models/models/mask2former/README.md) |
| [MediaPipe-Selfie-Segmentation](https://aihub.qualcomm.com/models/mediapipe_selfie) | [qai_hub_models.models.mediapipe_selfie](qai_hub_models/models/mediapipe_selfie/README.md) |
| [MobileSam](https://aihub.qualcomm.com/models/mobilesam) | [qai_hub_models.models.mobilesam](qai_hub_models/models/mobilesam/README.md) |
| [PidNet](https://aihub.qualcomm.com/models/pidnet) | [qai_hub_models.models.pidnet](qai_hub_models/models/pidnet/README.md) |
| [SINet](https://aihub.qualcomm.com/models/sinet) | [qai_hub_models.models.sinet](qai_hub_models/models/sinet/README.md) |
| [SalsaNext](https://aihub.qualcomm.com/models/salsanext) | [qai_hub_models.models.salsanext](qai_hub_models/models/salsanext/README.md) |
| [Segformer-Base](https://aihub.qualcomm.com/models/segformer_base) | [qai_hub_models.models.segformer_base](qai_hub_models/models/segformer_base/README.md) |
| [Segment-Anything-Model-2](https://aihub.qualcomm.com/models/sam2) | [qai_hub_models.models.sam2](qai_hub_models/models/sam2/README.md) |
| [Unet-Segmentation](https://aihub.qualcomm.com/models/unet_segmentation) | [qai_hub_models.models.unet_segmentation](qai_hub_models/models/unet_segmentation/README.md) |
| [YOLOv11-Segmentation](https://aihub.qualcomm.com/models/yolov11_seg) | [qai_hub_models.models.yolov11_seg](qai_hub_models/models/yolov11_seg/README.md) |
| [YOLOv8-Segmentation](https://aihub.qualcomm.com/models/yolov8_seg) | [qai_hub_models.models.yolov8_seg](qai_hub_models/models/yolov8_seg/README.md) |
| | |
| **Video Classification**
| [ResNet-2Plus1D](https://aihub.qualcomm.com/models/resnet_2plus1d) | [qai_hub_models.models.resnet_2plus1d](qai_hub_models/models/resnet_2plus1d/README.md) |
| [ResNet-3D](https://aihub.qualcomm.com/models/resnet_3d) | [qai_hub_models.models.resnet_3d](qai_hub_models/models/resnet_3d/README.md) |
| [ResNet-Mixed-Convolution](https://aihub.qualcomm.com/models/resnet_mixed) | [qai_hub_models.models.resnet_mixed](qai_hub_models/models/resnet_mixed/README.md) |
| [Video-MAE](https://aihub.qualcomm.com/models/video_mae) | [qai_hub_models.models.video_mae](qai_hub_models/models/video_mae/README.md) |
| | |
| **Video Generation**
| [First-Order-Motion-Model](https://aihub.qualcomm.com/models/fomm) | [qai_hub_models.models.fomm](qai_hub_models/models/fomm/README.md) |
| | |
| **Video Object Tracking**
| [Track-Anything](https://aihub.qualcomm.com/models/track_anything) | [qai_hub_models.models.track_anything](qai_hub_models/models/track_anything/README.md) |
| | |
| **Object Detection**
| [3D-Deep-BOX](https://aihub.qualcomm.com/models/deepbox) | [qai_hub_models.models.deepbox](qai_hub_models/models/deepbox/README.md) |
| [Conditional-DETR-ResNet50](https://aihub.qualcomm.com/models/conditional_detr_resnet50) | [qai_hub_models.models.conditional_detr_resnet50](qai_hub_models/models/conditional_detr_resnet50/README.md) |
| [DETR-ResNet101](https://aihub.qualcomm.com/models/detr_resnet101) | [qai_hub_models.models.detr_resnet101](qai_hub_models/models/detr_resnet101/README.md) |
| [DETR-ResNet101-DC5](https://aihub.qualcomm.com/models/detr_resnet101_dc5) | [qai_hub_models.models.detr_resnet101_dc5](qai_hub_models/models/detr_resnet101_dc5/README.md) |
| [DETR-ResNet50](https://aihub.qualcomm.com/models/detr_resnet50) | [qai_hub_models.models.detr_resnet50](qai_hub_models/models/detr_resnet50/README.md) |
| [DETR-ResNet50-DC5](https://aihub.qualcomm.com/models/detr_resnet50_dc5) | [qai_hub_models.models.detr_resnet50_dc5](qai_hub_models/models/detr_resnet50_dc5/README.md) |
| [Facial-Attribute-Detection](https://aihub.qualcomm.com/models/face_attrib_net) | [qai_hub_models.models.face_attrib_net](qai_hub_models/models/face_attrib_net/README.md) |
| [Lightweight-Face-Detection](https://aihub.qualcomm.com/models/face_det_lite) | [qai_hub_models.models.face_det_lite](qai_hub_models/models/face_det_lite/README.md) |
| [MediaPipe-Face-Detection](https://aihub.qualcomm.com/models/mediapipe_face) | [qai_hub_models.models.mediapipe_face](qai_hub_models/models/mediapipe_face/README.md) |
| [MediaPipe-Hand-Detection](https://aihub.qualcomm.com/models/mediapipe_hand) | [qai_hub_models.models.mediapipe_hand](qai_hub_models/models/mediapipe_hand/README.md) |
| [PPE-Detection](https://aihub.qualcomm.com/models/gear_guard_net) | [qai_hub_models.models.gear_guard_net](qai_hub_models/models/gear_guard_net/README.md) |
| [Person-Foot-Detection](https://aihub.qualcomm.com/models/foot_track_net) | [qai_hub_models.models.foot_track_net](qai_hub_models/models/foot_track_net/README.md) |
| [RF-DETR](https://aihub.qualcomm.com/models/rf_detr) | [qai_hub_models.models.rf_detr](qai_hub_models/models/rf_detr/README.md) |
| [RTMDet](https://aihub.qualcomm.com/models/rtmdet) | [qai_hub_models.models.rtmdet](qai_hub_models/models/rtmdet/README.md) |
| [YOLOv10-Detection](https://aihub.qualcomm.com/models/yolov10_det) | [qai_hub_models.models.yolov10_det](qai_hub_models/models/yolov10_det/README.md) |
| [YOLOv11-Detection](https://aihub.qualcomm.com/models/yolov11_det) | [qai_hub_models.models.yolov11_det](qai_hub_models/models/yolov11_det/README.md) |
| [YOLOv8-Detection](https://aihub.qualcomm.com/models/yolov8_det) | [qai_hub_models.models.yolov8_det](qai_hub_models/models/yolov8_det/README.md) |
| [Yolo-X](https://aihub.qualcomm.com/models/yolox) | [qai_hub_models.models.yolox](qai_hub_models/models/yolox/README.md) |
| [Yolo-v3](https://aihub.qualcomm.com/models/yolov3) | [qai_hub_models.models.yolov3](qai_hub_models/models/yolov3/README.md) |
| [Yolo-v5](https://aihub.qualcomm.com/models/yolov5) | [qai_hub_models.models.yolov5](qai_hub_models/models/yolov5/README.md) |
| [Yolo-v6](https://aihub.qualcomm.com/models/yolov6) | [qai_hub_models.models.yolov6](qai_hub_models/models/yolov6/README.md) |
| [Yolo-v7](https://aihub.qualcomm.com/models/yolov7) | [qai_hub_models.models.yolov7](qai_hub_models/models/yolov7/README.md) |
| | |
| **Pose Estimation**
| [Facial-Landmark-Detection](https://aihub.qualcomm.com/models/facemap_3dmm) | [qai_hub_models.models.facemap_3dmm](qai_hub_models/models/facemap_3dmm/README.md) |
| [HRNetPose](https://aihub.qualcomm.com/models/hrnet_pose) | [qai_hub_models.models.hrnet_pose](qai_hub_models/models/hrnet_pose/README.md) |
| [LiteHRNet](https://aihub.qualcomm.com/models/litehrnet) | [qai_hub_models.models.litehrnet](qai_hub_models/models/litehrnet/README.md) |
| [MediaPipe-Pose-Estimation](https://aihub.qualcomm.com/models/mediapipe_pose) | [qai_hub_models.models.mediapipe_pose](qai_hub_models/models/mediapipe_pose/README.md) |
| [Movenet](https://aihub.qualcomm.com/models/movenet) | [qai_hub_models.models.movenet](qai_hub_models/models/movenet/README.md) |
| [Posenet-Mobilenet](https://aihub.qualcomm.com/models/posenet_mobilenet) | [qai_hub_models.models.posenet_mobilenet](qai_hub_models/models/posenet_mobilenet/README.md) |
| [RTMPose-Body2d](https://aihub.qualcomm.com/models/rtmpose_body2d) | [qai_hub_models.models.rtmpose_body2d](qai_hub_models/models/rtmpose_body2d/README.md) |
| | |
| **Depth Estimation**
| [Depth-Anything](https://aihub.qualcomm.com/models/depth_anything) | [qai_hub_models.models.depth_anything](qai_hub_models/models/depth_anything/README.md) |
| [Depth-Anything-V2](https://aihub.qualcomm.com/models/depth_anything_v2) | [qai_hub_models.models.depth_anything_v2](qai_hub_models/models/depth_anything_v2/README.md) |
| [Midas-V2](https://aihub.qualcomm.com/models/midas) | [qai_hub_models.models.midas](qai_hub_models/models/midas/README.md) |

### Multimodal

| Model | README |
| -- | -- |
| | |
| [EasyOCR](https://aihub.qualcomm.com/models/easyocr) | [qai_hub_models.models.easyocr](qai_hub_models/models/easyocr/README.md) |
| [Nomic-Embed-Text](https://aihub.qualcomm.com/models/nomic_embed_text) | [qai_hub_models.models.nomic_embed_text](qai_hub_models/models/nomic_embed_text/README.md) |
| [OpenAI-Clip](https://aihub.qualcomm.com/models/openai_clip) | [qai_hub_models.models.openai_clip](qai_hub_models/models/openai_clip/README.md) |
| [TrOCR](https://aihub.qualcomm.com/models/trocr) | [qai_hub_models.models.trocr](qai_hub_models/models/trocr/README.md) |

### Audio

| Model | README |
| -- | -- |
| | |
| **Speech Recognition**
| [HuggingFace-WavLM-Base-Plus](https://aihub.qualcomm.com/models/huggingface_wavlm_base_plus) | [qai_hub_models.models.huggingface_wavlm_base_plus](qai_hub_models/models/huggingface_wavlm_base_plus/README.md) |
| [Whisper-Base](https://aihub.qualcomm.com/models/whisper_base) | [qai_hub_models.models.whisper_base](qai_hub_models/models/whisper_base/README.md) |
| [Whisper-Large-V3-Turbo](https://aihub.qualcomm.com/models/whisper_large_v3_turbo) | [qai_hub_models.models.whisper_large_v3_turbo](qai_hub_models/models/whisper_large_v3_turbo/README.md) |
| [Whisper-Small](https://aihub.qualcomm.com/models/whisper_small) | [qai_hub_models.models.whisper_small](qai_hub_models/models/whisper_small/README.md) |
| [Whisper-Small-Quantized](https://aihub.qualcomm.com/models/whisper_small_quantized) | [qai_hub_models.models.whisper_small_quantized](qai_hub_models/models/whisper_small_quantized/README.md) |
| [Whisper-Tiny](https://aihub.qualcomm.com/models/whisper_tiny) | [qai_hub_models.models.whisper_tiny](qai_hub_models/models/whisper_tiny/README.md) |
| | |
| **Audio Classification**
| [YamNet](https://aihub.qualcomm.com/models/yamnet) | [qai_hub_models.models.yamnet](qai_hub_models/models/yamnet/README.md) |

### Generative AI

| Model | README |
| -- | -- |
| | |
| **Image Generation**
| [ControlNet-Canny](https://aihub.qualcomm.com/models/controlnet_canny) | [qai_hub_models.models.controlnet_canny](qai_hub_models/models/controlnet_canny/README.md) |
| [Stable-Diffusion-v1.5](https://aihub.qualcomm.com/models/stable_diffusion_v1_5) | [qai_hub_models.models.stable_diffusion_v1_5](qai_hub_models/models/stable_diffusion_v1_5/README.md) |
| [Stable-Diffusion-v2.1](https://aihub.qualcomm.com/models/stable_diffusion_v2_1) | [qai_hub_models.models.stable_diffusion_v2_1](qai_hub_models/models/stable_diffusion_v2_1/README.md) |
| | |
| **Text Generation**
| [Baichuan2-7B](https://aihub.qualcomm.com/models/baichuan2_7b) | [qai_hub_models.models.baichuan2_7b](qai_hub_models/models/baichuan2_7b/README.md) |
| [Falcon3-7B-Instruct](https://aihub.qualcomm.com/models/falcon_v3_7b_instruct) | [qai_hub_models.models.falcon_v3_7b_instruct](qai_hub_models/models/falcon_v3_7b_instruct/README.md) |
| [IBM-Granite-v3.1-8B-Instruct](https://aihub.qualcomm.com/models/ibm_granite_v3_1_8b_instruct) | [qai_hub_models.models.ibm_granite_v3_1_8b_instruct](qai_hub_models/models/ibm_granite_v3_1_8b_instruct/README.md) |
| [IndusQ-1.1B](https://aihub.qualcomm.com/models/indus_1b) | [qai_hub_models.models.indus_1b](qai_hub_models/models/indus_1b/README.md) |
| [JAIS-6p7b-Chat](https://aihub.qualcomm.com/models/jais_6p7b_chat) | [qai_hub_models.models.jais_6p7b_chat](qai_hub_models/models/jais_6p7b_chat/README.md) |
| [Llama-SEA-LION-v3.5-8B-R](https://aihub.qualcomm.com/models/llama_v3_1_sea_lion_3_5_8b_r) | [qai_hub_models.models.llama_v3_1_sea_lion_3_5_8b_r](qai_hub_models/models/llama_v3_1_sea_lion_3_5_8b_r/README.md) |
| [Llama-v2-7B-Chat](https://aihub.qualcomm.com/models/llama_v2_7b_chat) | [qai_hub_models.models.llama_v2_7b_chat](qai_hub_models/models/llama_v2_7b_chat/README.md) |
| [Llama-v3-8B-Instruct](https://aihub.qualcomm.com/models/llama_v3_8b_instruct) | [qai_hub_models.models.llama_v3_8b_instruct](qai_hub_models/models/llama_v3_8b_instruct/README.md) |
| [Llama-v3.1-8B-Instruct](https://aihub.qualcomm.com/models/llama_v3_1_8b_instruct) | [qai_hub_models.models.llama_v3_1_8b_instruct](qai_hub_models/models/llama_v3_1_8b_instruct/README.md) |
| [Llama-v3.2-1B-Instruct](https://aihub.qualcomm.com/models/llama_v3_2_1b_instruct) | [qai_hub_models.models.llama_v3_2_1b_instruct](qai_hub_models/models/llama_v3_2_1b_instruct/README.md) |
| [Llama-v3.2-3B-Instruct](https://aihub.qualcomm.com/models/llama_v3_2_3b_instruct) | [qai_hub_models.models.llama_v3_2_3b_instruct](qai_hub_models/models/llama_v3_2_3b_instruct/README.md) |
| [Llama3-TAIDE-LX-8B-Chat-Alpha1](https://aihub.qualcomm.com/models/llama_v3_taide_8b_chat) | [qai_hub_models.models.llama_v3_taide_8b_chat](qai_hub_models/models/llama_v3_taide_8b_chat/README.md) |
| [Ministral-3B](https://aihub.qualcomm.com/models/ministral_3b) | [qai_hub_models.models.ministral_3b](qai_hub_models/models/ministral_3b/README.md) |
| [Mistral-3B](https://aihub.qualcomm.com/models/mistral_3b) | [qai_hub_models.models.mistral_3b](qai_hub_models/models/mistral_3b/README.md) |
| [Mistral-7B-Instruct-v0.3](https://aihub.qualcomm.com/models/mistral_7b_instruct_v0_3) | [qai_hub_models.models.mistral_7b_instruct_v0_3](qai_hub_models/models/mistral_7b_instruct_v0_3/README.md) |
| [PLaMo-1B](https://aihub.qualcomm.com/models/plamo_1b) | [qai_hub_models.models.plamo_1b](qai_hub_models/models/plamo_1b/README.md) |
| [Phi-3.5-Mini-Instruct](https://aihub.qualcomm.com/models/phi_3_5_mini_instruct) | [qai_hub_models.models.phi_3_5_mini_instruct](qai_hub_models/models/phi_3_5_mini_instruct/README.md) |
| [Qwen2-7B-Instruct](https://aihub.qualcomm.com/models/qwen2_7b_instruct) | [qai_hub_models.models.qwen2_7b_instruct](qai_hub_models/models/qwen2_7b_instruct/README.md) |
| [Qwen2.5-7B-Instruct](https://aihub.qualcomm.com/models/qwen2_5_7b_instruct) | [qai_hub_models.models.qwen2_5_7b_instruct](qai_hub_models/models/qwen2_5_7b_instruct/README.md) |


## Need help?
Slack: https://aihub.qualcomm.com/community/slack

GitHub Issues: https://github.com/quic/ai-hub-models/issues

Email: ai-hub-support@qti.qualcomm.com.

## LICENSE

QualcommÂ® AI Hub Models is licensed under BSD-3. See the [LICENSE file](LICENSE).
