---
title: "Interactive Chat Patterns"
description: "Create interactive chat interfaces with persistent conversation memory"
icon: "message-circle"
---

## Building a chat loop

With mcp-use you can build interactive interface where users can have conversations with
your `MCPAgent`, maintaining context and memory across multiple queries.

## Basic chat loop
Here's a basic chat-loop with conversation memory enabled:

<CodeGroup>
```python Python
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def basic_chat_loop():
    """Simple console chat loop with MCPAgent"""
    # Load environment variables
    load_dotenv()

    # MCP server configuration
    config = {
        "mcpServers": {
            "playwright": {
                "command": "npx",
                "args": ["@playwright/mcp@latest"],
                "env": {"DISPLAY": ":1"}
            },
            "filesystem": {
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
            }
        }
    }

    # Create client and agent
    client = MCPClient(config)
    llm = ChatOpenAI(model="gpt-4o")

    agent = MCPAgent(llm=llm,
        client=client,
        memory_enabled=True, # Enable memory to track conversation history
        max_steps=20)

    # Some initial messages
    print("ü§ñ MCP Agent Chat")
    print("Type 'quit/exit' to exit the chat.")
    print("Type 'clear' to clear conversation history")

    try:
        while True:
            user_input = input("\nYou: ")

            if user_input.lower() in ['quit', 'exit']:
                print("üëã Goodbye!")
                break

            if user_input.lower() == 'clear':
                agent.clear_conversation_history()
                print("üßπ Conversation history cleared.")
                continue

            # Skip empty messages
            if not user_input:
                continue

            try:
                print("\nü§ñ Assistant: ", end="", flush=True)
                response = await agent.run(user_input)
                print(response)
            except KeyboardInterrupt: # Handle keyboard interrupt
                print("\n\n‚è∏Ô∏è Interrupted by user")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                print("Please try again or type 'exit' to quit.")
    finally:
        await client.close_all_sessions()

if __name__ == "__main__":
    asyncio.run(basic_chat_loop())
```

```typescript TypeScript
import readline from 'node:readline'
import { config } from 'dotenv'
import { ChatOpenAI } from '@langchain/openai'
import { MCPAgent, MCPClient } from 'mcp-use'

config() // Load environment variables

async function basicChatLoop() {
    // MCP server configuration
    const configuration = {
        mcpServers: {
            playwright: {
                command: 'npx',
                args: ['@playwright/mcp@latest'],
                env: { DISPLAY: ':1' }
            },
            filesystem: {
                command: 'npx',
                args: ['-y', '@modelcontextprotocol/server-filesystem', '/tmp']
            }
        }
    }

    // Create client and agent
    const client = new MCPClient(configuration)
    const llm = new ChatOpenAI({ model: 'gpt-4o' })

    const agent = new MCPAgent({
        llm,
        client,
        memoryEnabled: true, // Enable memory to track conversation history
        maxSteps: 20
    })

    // Some initial messages
    console.log('ü§ñ MCP Agent Chat')
    console.log('Type \'quit/exit\' to exit the chat.')
    console.log('Type \'clear\' to clear conversation history')

    // Create readline interface
    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    })

    const question = (prompt: string): Promise<string> => {
        return new Promise((resolve) => {
            rl.question(prompt, resolve)
        })
    }

    try {
        while (true) {
            const userInput = await question('\nYou: ')

            if (['quit', 'exit'].includes(userInput.toLowerCase())) {
                console.log('üëã Goodbye!')
                break
            }

            if (userInput.toLowerCase() === 'clear') {
                agent.clearConversationHistory()
                console.log('üßπ Conversation history cleared.')
                continue
            }

            // Skip empty messages
            if (!userInput) {
                continue
            }

            try {
                process.stdout.write('\nü§ñ Assistant: ')
                const response = await agent.run(userInput)
                console.log(response)
            } catch (error) {
                console.error(`\n‚ùå Error: ${error}`)
                console.log('Please try again or type \'exit\' to quit.')
            }
        }
    } finally {
        rl.close()
        await client.closeAllSessions()
    }
}

basicChatLoop().catch(console.error)
```
</CodeGroup>

## Streaming Chat Loop

Here's a chat loop with streaming responses enabled:

<CodeGroup>
```python Python
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def streaming_chat_loop():
    """Chat loop with streaming responses with MCPAgent"""
    # Load environment variables
    load_dotenv()

    # MCP server configuration
    config = {
        "mcpServers": {
            "playwright": {
                "command": "npx",
                "args": ["@playwright/mcp@latest"],
                "env": {"DISPLAY": ":1"}
            }
        }
    }

    # Create client and agent
    client = MCPClient(config)
    llm = ChatOpenAI(model="gpt-4o")

    agent = MCPAgent(llm=llm,
        client=client,
        memory_enabled=True, # Enable memory to track conversation history
        max_steps=20)

    # Some initial messages
    print("ü§ñ MCP Agent Chat (Streaming)")
    print("Type 'quit/exit' to exit the chat.")
    print("Type 'clear' to clear conversation history")

    try:
        while True:
            user_input = input("\nYou: ")

            if user_input.lower() in ['quit', 'exit']:
                print("üëã Goodbye!")
                break

            if user_input.lower() == 'clear':
                agent.clear_conversation_history()
                print("üßπ Conversation history cleared.")
                continue

            if not user_input: # Skip empty messages
                continue

            try:
                print("\nü§ñ Assistant: ", end="", flush=True)

                # Stream the response
                async for chunk in agent.stream(user_input):
                    print(chunk, end="", flush=True)
                print()
            except KeyboardInterrupt: # Handle keyboard interrupt
                print("\n\n‚è∏Ô∏è Interrupted by user")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                print("Please try again or type 'exit' to quit.")
    finally:
        await client.close_all_sessions()

if __name__ == "__main__":
    asyncio.run(streaming_chat_loop())
```

```typescript TypeScript
import readline from 'node:readline'
import { config } from 'dotenv'
import { ChatOpenAI } from '@langchain/openai'
import { MCPAgent, MCPClient } from 'mcp-use'

config() // Load environment variables

async function streamingChatLoop() {
    // MCP server configuration
    const configuration = {
        mcpServers: {
            playwright: {
                command: 'npx',
                args: ['@playwright/mcp@latest'],
                env: { DISPLAY: ':1' }
            }
        }
    }

    // Create client and agent
    const client = new MCPClient(configuration)
    const llm = new ChatOpenAI({ model: 'gpt-4o' })

    const agent = new MCPAgent({
        llm,
        client,
        memoryEnabled: true, // Enable memory to track conversation history
        maxSteps: 20
    })

    // Some initial messages
    console.log('ü§ñ MCP Agent Chat (Streaming)')
    console.log('Type \'quit/exit\' to exit the chat.')
    console.log('Type \'clear\' to clear conversation history')

    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    })

    const question = (prompt: string): Promise<string> => {
        return new Promise((resolve) => {
            rl.question(prompt, resolve)
        })
    }

    try {
        while (true) {
            const userInput = await question('\nYou: ')

            if (['quit', 'exit'].includes(userInput.toLowerCase())) {
                console.log('üëã Goodbye!')
                break
            }

            if (userInput.toLowerCase() === 'clear') {
                agent.clearConversationHistory()
                console.log('üßπ Conversation history cleared.')
                continue
            }

            if (!userInput) { // Skip empty messages
                continue
            }

            try {
                process.stdout.write('\nü§ñ Assistant: ')

                // Stream the response
                for await (const step of agent.stream(userInput)) {
                    // In TypeScript, stream returns steps rather than text chunks
                    // You might want to use streamEvents for token-level streaming
                    console.log(`\nTool: ${step.action.tool}`)
                    console.log(`Result: ${step.observation}`)
                }
            } catch (error) {
                console.error(`\n‚ùå Error: ${error}`)
                console.log('Please try again or type \'exit\' to quit.')
            }
        }
    } finally {
        rl.close()
        await client.closeAllSessions()
    }
}

streamingChatLoop().catch(console.error)
```
</CodeGroup>

## Chat Loop with Structured I/O

It's possible to create a chat loop that can handle both natural language and structured inputs, allowing users to request specific tasks or analyses in a structured format. Here's an example of how to implement this:

<CodeGroup>
```python Python
import asyncio
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient
from typing import Optional

class TaskRequest(BaseModel):
    task_type: Optional[str] = Field(description="The type of task to perform")
    description: Optional[str] = Field(description="Detailed description of the task")
    priority: Optional[str] = Field(description="Priority level: low, medium, high")

async def structured_chat_loop():
    """Chat loop that can handle both natural language and structured inputs."""
    # Load environment variables
    load_dotenv()

    # MCP server configuration
    config = {
        "mcpServers": {
            "playwright": {
                "command": "npx",
                "args": ["@playwright/mcp@latest"],
                "env": {"DISPLAY": ":1"}
            }
        }
    }

    # Create client and agent
    client = MCPClient(config)
    llm = ChatOpenAI(model="gpt-4o")

    agent = MCPAgent(
        llm=llm,
        client=client,
        memory_enabled=True, # Enable memory to track conversation history
        max_steps=20
    )

    # Initial messages
    print("ü§ñ MCP Agent Chat (Structured)")
    print("You can chat naturally or request structured task analysis")
    print("Type 'task' to create a structured task request")

    try:
        while True:
            user_input = input("\nYou: ")
            if user_input.lower() in ['exit', 'quit']:
                print("üëã Goodbye!")
                break

            try:
                if user_input.lower() == 'task':
                    print("\nüìã Creating structured task...")
                    task_description = input("Describe your task: ")

                    task: TaskRequest = await agent.run(
                        f"Analyze a task with the following description: {task_description}",
                        output_schema=TaskRequest
                    )

                    # Print task analysis
                    print(f"\n‚úÖ Task Analysis:")
                    print(f"‚Ä¢ Type: {task.task_type}")
                    print(f"‚Ä¢ Description: {task.description}")
                    print(f"‚Ä¢ Priority: {task.priority or 'low'}")

                    proceed = input("\nDo you want to proceed with this task? (y/n)")
                    if proceed.lower() == 'y':
                        response = await agent.run(
                            f"Execute the following task: {task.description}"
                        )
                        print(f"\nü§ñ Assistant: {response}")
                else:
                    # Regular conversation
                    response = await agent.run(user_input)
                    print(f"\nü§ñ Assistant: {response}")
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
                print("Please try again or type 'exit' to quit.")

    finally:
        await client.close_all_sessions()

if __name__ == "__main__":
    asyncio.run(structured_chat_loop())
```

```typescript TypeScript
import readline from 'node:readline'
import { z } from 'zod'
import { config } from 'dotenv'
import { ChatOpenAI } from '@langchain/openai'
import { MCPAgent, MCPClient } from 'mcp-use'

config() // Load environment variables

// Define the schema using Zod
const TaskRequest = z.object({
    taskType: z.string().optional().describe('The type of task to perform'),
    description: z.string().optional().describe('Detailed description of the task'),
    priority: z.string().optional().describe('Priority level: low, medium, high')
})

type TaskRequest = z.infer<typeof TaskRequest>

async function structuredChatLoop() {
    // MCP server configuration
    const configuration = {
        mcpServers: {
            playwright: {
                command: 'npx',
                args: ['@playwright/mcp@latest'],
                env: { DISPLAY: ':1' }
            }
        }
    }

    // Create client and agent
    const client = new MCPClient(configuration)
    const llm = new ChatOpenAI({ model: 'gpt-4o' })

    const agent = new MCPAgent({
        llm,
        client,
        memoryEnabled: true, // Enable memory to track conversation history
        maxSteps: 20
    })

    // Initial messages
    console.log('ü§ñ MCP Agent Chat (Structured)')
    console.log('You can chat naturally or request structured task analysis')
    console.log('Type \'task\' to create a structured task request')

    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    })

    const question = (prompt: string): Promise<string> => {
        return new Promise((resolve) => {
            rl.question(prompt, resolve)
        })
    }

    try {
        while (true) {
            const userInput = await question('\nYou: ')
            if (['exit', 'quit'].includes(userInput.toLowerCase())) {
                console.log('üëã Goodbye!')
                break
            }

            try {
                if (userInput.toLowerCase() === 'task') {
                    console.log('\nüìã Creating structured task...')
                    const taskDescription = await question('Describe your task: ')

                    const task = await agent.run(
                        `Analyze a task with the following description: ${taskDescription}`,
                        undefined, // maxSteps
                        undefined, // manageConnector
                        undefined, // externalHistory
                        TaskRequest // output schema
                    )

                    // Print task analysis
                    console.log('\n‚úÖ Task Analysis:')
                    console.log(`‚Ä¢ Type: ${task.taskType}`)
                    console.log(`‚Ä¢ Description: ${task.description}`)
                    console.log(`‚Ä¢ Priority: ${task.priority || 'low'}`)

                    const proceed = await question('\nDo you want to proceed with this task? (y/n)')
                    if (proceed.toLowerCase() === 'y') {
                        const response = await agent.run(
                            `Execute the following task: ${task.description}`
                        )
                        console.log(`\nü§ñ Assistant: ${response}`)
                    }
                } else {
                    // Regular conversation
                    const response = await agent.run(userInput)
                    console.log(`\nü§ñ Assistant: ${response}`)
                }
            } catch (error) {
                console.error(`‚ùå Error: ${error}`)
                console.log('Please try again or type \'exit\' to quit.')
            }
        }
    } finally {
        rl.close()
        await client.closeAllSessions()
    }
}

structuredChatLoop().catch(console.error)
```
</CodeGroup>

## Next Steps

<CardGroup cols={3}>
  <Card title="Agent Configuration" icon="cloud" href="/agent/agent-configuration">
    Learn more about configuring agents for optimal streaming performance
  </Card>
  <Card title="Multi-Server Setup" icon="server" href="/advanced/multi-server-setup">
    Stream output from agents using multiple MCP servers
  </Card>
  <Card title="Security Best Practices" icon="shield" href="/advanced/security">
    Learn how to secure your MCP deployments
  </Card>
</CardGroup>
