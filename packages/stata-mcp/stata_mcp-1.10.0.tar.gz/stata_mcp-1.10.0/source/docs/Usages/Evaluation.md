# Evaluate
Want to evaluate your LLM? We offer a framework. Follow the steps below to evaluate your large language model.

> Reference: Tan, S., & Feng, M. (2025). How to use StataMCP improve your social science research? Shanghai Bayes Views Information Technology Co Ltd. 

Bibtex follows: 
```bibtex
@techreport{tan2025stataMCP,
  author = {Tan, Song and Feng, Muyao},
  title = {Stata-MCP: A research report on AI-assisted empirical research},
  year = {2025},
  month = {September},
  day = {21},
  language = {English},
  address = {Shanghai, China},
  institution = {Shanghai Bayes Views Information Technology Co., Ltd.},
  url = {https://www.statamcp.com/reports/2025/09/21/stata_mcp_a_research_report_on_ai_assisted_empirical_research}
}
```

## Step 1: Set your environment
Set api-key, base-url, and model-name
```bash
export OPENAI_API_KEY=<your-api-key>
export OPENAI_BASE_URL=https://api.openai.com/v1
export OPENAI_MODEL=gpt-3.5-turbo
export CHAT-MODEL=gpt-3.5-turbo
export THINKING_MODEL=gpt-5
```

## Step 2: Prepare your data
For example, you can prepare task with answer, then put task into a `Agent`, then run this agent.  
You will get the agent history message and final answer generated by LLM.

## Step3: Input necessary information into `ScoreModel`
```python
from stata_mcp.evaluate import ScoreModel

YOUR_TASK: str = ...
GIVEN_ANSWER: str = ...

HIST_MSG: str = ...
FINAL_ANSWER: str = ...

sm = ScoreModel(
    task=YOUR_TASK,
    reference_answer=GIVEN_ANSWER, 
    processer=HIST_MSG,  # Only support type <str> now
    results=FINAL_ANSWER, 
    task_id="YOU_CAN_SET_A_ID_OR_NOT"
)
score = sm.score_it()
```



