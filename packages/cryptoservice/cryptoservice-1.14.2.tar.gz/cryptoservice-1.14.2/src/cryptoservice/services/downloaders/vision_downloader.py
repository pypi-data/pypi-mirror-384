"""Binance Visionæ•°æ®ä¸‹è½½å™¨.

ä¸“é—¨å¤„ç†ä»Binance Vision S3å­˜å‚¨ä¸‹è½½å†å²æ•°æ®ã€‚
"""

import asyncio
import csv
import logging
import time
import zipfile
from datetime import datetime
from decimal import Decimal
from io import BytesIO

import aiohttp
from aiohttp import ClientConnectionError, ClientTimeout
from binance import AsyncClient

from cryptoservice.config import RetryConfig
from cryptoservice.exceptions import MarketDataFetchError
from cryptoservice.models import LongShortRatio, OpenInterest
from cryptoservice.storage.database import Database as AsyncMarketDB

from .base_downloader import BaseDownloader

logger = logging.getLogger(__name__)


class VisionDownloader(BaseDownloader):
    """Binance Visionæ•°æ®ä¸‹è½½å™¨."""

    def __init__(self, client: AsyncClient, request_delay: float = 0):
        """åˆå§‹åŒ–Binance Visionæ•°æ®ä¸‹è½½å™¨.

        Args:
            client: API å®¢æˆ·ç«¯å®ä¾‹.
            request_delay: è¯·æ±‚ä¹‹é—´çš„åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰.
        """
        super().__init__(client, request_delay)
        self.db: AsyncMarketDB | None = None
        self.base_url = "https://data.binance.vision/data/futures/um/daily/metrics"
        self._session: aiohttp.ClientSession | None = None
        self._session_lock: asyncio.Lock | None = None
        self._client_timeout = ClientTimeout(total=60, connect=10)

        # æ€§èƒ½ç»Ÿè®¡
        self._perf_stats = {
            "download_time": 0.0,
            "parse_time": 0.0,
            "db_time": 0.0,
            "download_count": 0,
            "concurrent_count": 0,
            "max_concurrent": 0,
        }

    async def download_metrics_batch(
        self,
        symbols: list[str],
        start_date: str,
        end_date: str,
        db_path: str,
        max_workers: int,
        request_delay: float,
    ) -> None:
        """æ‰¹é‡å¼‚æ­¥ä¸‹è½½æŒ‡æ ‡æ•°æ®.

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            db_path: æ•°æ®åº“è·¯å¾„
            request_delay: è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæ— å»¶è¿Ÿ
            max_workers: æœ€å¤§å¹¶å‘ä¸‹è½½æ•°ï¼ˆVision S3ä¸‹è½½å¯ä»¥é«˜å¹¶å‘ï¼‰
        """
        try:
            data_types = ["openInterest", "longShortRatio"]
            # é‡ç½®ç»Ÿè®¡
            self._perf_stats = {
                "download_time": 0.0,
                "parse_time": 0.0,
                "db_time": 0.0,
                "download_count": 0,
                "concurrent_count": 0,
                "max_concurrent": 0,
            }

            logger.info(f"å¼€å§‹ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®: {data_types}")

            if self.db is None:
                self.db = AsyncMarketDB(db_path)
            await self.db.initialize()

            import pandas as pd

            date_range = pd.date_range(start=start_date, end=end_date, freq="D")

            semaphore = asyncio.Semaphore(max_workers)
            tasks = []

            for date in date_range:
                date_str = date.strftime("%Y-%m-%d")
                for symbol in symbols:
                    task = asyncio.create_task(
                        self._download_and_process_symbol_for_date(symbol, date_str, semaphore, request_delay)
                    )
                    tasks.append(task)

            total_tasks = len(tasks)
            logger.info(f"ğŸ“¦ åˆ›å»ºäº† {total_tasks} ä¸ªä¸‹è½½ä»»åŠ¡ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")

            start_time = time.time()
            await asyncio.gather(*tasks)

            elapsed = time.time() - start_time
            logger.info("âœ… Binance Vision æŒ‡æ ‡æ•°æ®ä¸‹è½½å®Œæˆ")
            logger.info("ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            logger.info(f"   - æ€»è€—æ—¶: {elapsed:.2f}ç§’")
            logger.info(f"   - ä¸‹è½½ä»»åŠ¡: {self._perf_stats['download_count']}")
            logger.info(f"   - æœ€å¤§å¹¶å‘æ•°: {self._perf_stats['max_concurrent']}")

            dl_time = self._perf_stats["download_time"]
            dl_pct = dl_time / elapsed * 100 if elapsed > 0 else 0
            logger.info(f"   - ä¸‹è½½æ—¶é—´: {dl_time:.2f}ç§’ ({dl_pct:.1f}%)")

            parse_time = self._perf_stats["parse_time"]
            parse_pct = parse_time / elapsed * 100 if elapsed > 0 else 0
            logger.info(f"   - è§£ææ—¶é—´: {parse_time:.2f}ç§’ ({parse_pct:.1f}%)")

            db_time = self._perf_stats["db_time"]
            db_pct = db_time / elapsed * 100 if elapsed > 0 else 0
            logger.info(f"   - æ•°æ®åº“æ—¶é—´: {db_time:.2f}ç§’ ({db_pct:.1f}%)")

            if self._perf_stats["download_count"] > 0:
                avg_per_task = elapsed / self._perf_stats["download_count"]
                logger.info(f"   - å¹³å‡æ¯ä»»åŠ¡: {avg_per_task:.3f}ç§’")

        except Exception as e:
            logger.error(f"ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            raise MarketDataFetchError(f"ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}") from e
        finally:
            await self._close_session()

    async def _download_and_process_symbol_for_date(
        self,
        symbol: str,
        date_str: str,
        semaphore: asyncio.Semaphore,
        request_delay: float,
    ) -> None:
        """ä¸‹è½½å¹¶å¤„ç†å•ä¸ªäº¤æ˜“å¯¹åœ¨ç‰¹å®šæ—¥æœŸçš„æ•°æ®."""
        async with semaphore:
            # è®°å½•å¹¶å‘æ•°
            self._perf_stats["concurrent_count"] += 1
            current = self._perf_stats["concurrent_count"]
            if current > self._perf_stats["max_concurrent"]:
                self._perf_stats["max_concurrent"] = current
                if current % 10 == 0:  # æ¯10ä¸ªå¹¶å‘æ‰“å°ä¸€æ¬¡
                    logger.info(f"ğŸš€ å½“å‰å¹¶å‘: {current}")

            try:
                url = f"{self.base_url}/{symbol}/{symbol}-metrics-{date_str}.zip"
                logger.debug(f"â¬‡ï¸  ä¸‹è½½ {symbol} {date_str} æŒ‡æ ‡æ•°æ®")

                retry_config = RetryConfig(max_retries=3, base_delay=0)

                # è®¡æ—¶ï¼šä¸‹è½½
                dl_start = time.time()
                metrics_data = await self._download_and_parse_metrics_csv(url, symbol, retry_config)
                self._perf_stats["download_time"] += time.time() - dl_start

                if metrics_data and self.db:
                    # è®¡æ—¶ï¼šæ•°æ®åº“æ’å…¥
                    db_start = time.time()

                    if metrics_data.get("open_interest"):
                        await self.db.insert_open_interests(metrics_data["open_interest"])
                        logger.info(f"âœ… {symbol}: å­˜å‚¨äº† {date_str} {len(metrics_data['open_interest'])} æ¡æŒä»“é‡è®°å½•")
                    if metrics_data.get("long_short_ratio"):
                        await self.db.insert_long_short_ratios(metrics_data["long_short_ratio"])
                        logger.info(
                            f"âœ… {symbol}: å­˜å‚¨äº† {date_str} {len(metrics_data['long_short_ratio'])} æ¡å¤šç©ºæ¯”ä¾‹è®°å½•"
                        )

                    self._perf_stats["db_time"] += time.time() - db_start
                else:
                    logger.warning(f"âš ï¸ {symbol} on {date_str}: æ— æ³•è·å–æŒ‡æ ‡æ•°æ®")

                self._perf_stats["download_count"] += 1

            except Exception as e:
                logger.warning(f"ä¸‹è½½ {symbol} on {date_str} æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
                self._record_failed_download(symbol, str(e), {"url": url, "date": date_str, "data_type": "metrics"})

            finally:
                # å‡å°‘å¹¶å‘è®¡æ•°
                self._perf_stats["concurrent_count"] -= 1

        if request_delay > 0:
            await asyncio.sleep(request_delay)

    async def _download_and_parse_metrics_csv(  # noqa: C901
        self,
        url: str,
        symbol: str,
        retry_config: RetryConfig | None = None,
    ) -> dict[str, list] | None:
        """ä½¿ç”¨aiohttpä¸‹è½½å¹¶è§£ææŒ‡æ ‡CSVæ•°æ®."""
        if retry_config is None:
            retry_config = RetryConfig(max_retries=3, base_delay=0)

        # ä½¿ç”¨åŸºç±»çš„é‡è¯•æœºåˆ¶ä¸‹è½½ZIPæ–‡ä»¶
        async def _download_zip() -> bytes:
            """ä¸‹è½½ZIPæ–‡ä»¶çš„å†…éƒ¨å¼‚æ­¥å‡½æ•°."""
            session = await self._get_session()
            try:
                async with session.get(url) as response:
                    response.raise_for_status()
                    return await response.read()
            except ClientConnectionError:
                await self._reset_session()
                raise

        try:
            # ä½¿ç”¨åŸºç±»çš„å¼‚æ­¥é‡è¯•å¤„ç†æœºåˆ¶
            zip_content = await self._handle_async_request_with_retry(
                _download_zip,
                retry_config=retry_config,
            )
        except Exception as e:
            logger.error(f"ä¸‹è½½æŒ‡æ ‡æ•°æ®å¤±è´¥ {symbol}: {e}")
            return None

        try:
            # è®¡æ—¶ï¼šè§£æ
            parse_start = time.time()
            with zipfile.ZipFile(BytesIO(zip_content)) as zip_file:
                csv_files = [f for f in zip_file.namelist() if f.endswith(".csv")]

                if not csv_files:
                    logger.warning(f"ZIPæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶: {url}")
                    return None

                result: dict[str, list] = {"open_interest": [], "long_short_ratio": []}

                for csv_file in csv_files:
                    try:
                        with zip_file.open(csv_file) as f:
                            content = f.read().decode("utf-8")
                        csv_reader = csv.DictReader(content.splitlines())
                        rows = list(csv_reader)
                        if not rows:
                            continue

                        first_row = rows[0]
                        if "sum_open_interest" in first_row:
                            result["open_interest"].extend(self._parse_oi_data(rows, symbol))
                        if any(
                            field in first_row
                            for field in [
                                "sum_toptrader_long_short_ratio",
                                "count_long_short_ratio",
                                "sum_taker_long_short_vol_ratio",
                            ]
                        ):
                            result["long_short_ratio"].extend(self._parse_lsr_data(rows, symbol, csv_file))
                    except Exception as e:
                        logger.warning(f"è§£æCSVæ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
                        continue

                # è®°å½•è§£ææ—¶é—´
                self._perf_stats["parse_time"] += time.time() - parse_start

                return result if result["open_interest"] or result["long_short_ratio"] else None
        except Exception as e:
            logger.error(f"ä¸‹è½½å’Œè§£ææŒ‡æ ‡æ•°æ®å¤±è´¥ {symbol}: {e}")
            return None

    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–å¤ç”¨çš„aiohttpä¼šè¯å®ä¾‹."""
        if self._session_lock is None:
            self._session_lock = asyncio.Lock()

        async with self._session_lock:
            if self._session is None or self._session.closed:
                connector = aiohttp.TCPConnector(
                    limit=200,  # å¢åŠ TCPè¿æ¥æ± ä»¥æ”¯æŒæ›´é«˜å¹¶å‘
                    limit_per_host=100,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°é™åˆ¶
                    ttl_dns_cache=300,
                    enable_cleanup_closed=True,
                    force_close=True,  # å¼ºåˆ¶å…³é—­è¿æ¥ï¼Œé¿å…SSLè¶…æ—¶ï¼ˆä¸keepaliveäº’æ–¥ï¼‰
                )
                self._session = aiohttp.ClientSession(
                    timeout=self._client_timeout,
                    connector=connector,
                    connector_owner=True,
                    trust_env=True,
                )

        return self._session

    async def _close_session(self) -> None:
        """å…³é—­å½“å‰çš„aiohttpä¼šè¯."""
        session = self._session
        self._session = None

        if session and not session.closed:
            try:
                # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶å…³é—­ä¼šè¯ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
                await asyncio.wait_for(session.close(), timeout=5.0)
            except TimeoutError:
                logger.debug("å…³é—­aiohttpä¼šè¯è¶…æ—¶ï¼Œå¼ºåˆ¶å…³é—­")
            except ClientConnectionError as exc:
                logger.debug(f"å…³é—­aiohttpä¼šè¯æ—¶å‡ºç°SSLé—®é¢˜: {exc}")
            except Exception as exc:  # noqa: BLE001
                logger.debug(f"å…³é—­aiohttpä¼šè¯æ—¶å‡ºç°å¼‚å¸¸: {exc}")

    async def _reset_session(self) -> None:
        """é‡ç½®å®¢æˆ·ç«¯ä¼šè¯ä»¥ä¾¿ä¸‹æ¬¡é‡å»º."""
        await self._close_session()

    def _parse_oi_data(self, raw_data: list[dict], symbol: str) -> list[OpenInterest]:
        """è§£ææŒä»“é‡æ•°æ®."""
        open_interests = []

        for row in raw_data:
            try:
                # è§£ææ—¶é—´å­—æ®µï¼ˆBinance API è¿”å›çš„æ˜¯ UTC æ—¶é—´ï¼‰
                create_time = row["create_time"]
                from datetime import UTC

                timestamp = int(
                    datetime.strptime(create_time, "%Y-%m-%d %H:%M:%S").replace(tzinfo=UTC).timestamp() * 1000
                )

                # å®‰å…¨è·å–æŒä»“é‡å€¼
                oi_value = self._safe_decimal_convert(row.get("sum_open_interest"))
                oi_value_usd = self._safe_decimal_convert(row.get("sum_open_interest_value"))

                # åªæœ‰å½“ä¸»è¦å­—æ®µæœ‰æ•ˆæ—¶æ‰åˆ›å»ºè®°å½•
                if oi_value is not None:
                    open_interest = OpenInterest(
                        symbol=symbol,
                        open_interest=oi_value,
                        time=timestamp,
                        open_interest_value=oi_value_usd,
                    )
                    open_interests.append(open_interest)

            except (ValueError, KeyError) as e:
                logger.warning(f"è§£ææŒä»“é‡æ•°æ®è¡Œæ—¶å‡ºé”™: {e}, è¡Œæ•°æ®: {row}")
                continue

        return open_interests

    def _parse_lsr_data(self, raw_data: list[dict], symbol: str, file_name: str) -> list[LongShortRatio]:  # noqa: C901
        """è§£æå¤šç©ºæ¯”ä¾‹æ•°æ®."""
        long_short_ratios = []

        for row in raw_data:
            try:
                # è§£ææ—¶é—´å­—æ®µï¼ˆBinance API è¿”å›çš„æ˜¯ UTC æ—¶é—´ï¼‰
                create_time = row["create_time"]
                from datetime import UTC

                timestamp = int(
                    datetime.strptime(create_time, "%Y-%m-%d %H:%M:%S").replace(tzinfo=UTC).timestamp() * 1000
                )

                # å¤„ç†é¡¶çº§äº¤æ˜“è€…æ•°æ® - åˆ†åˆ«å¤„ç†ï¼Œç¡®ä¿æ— æŸ
                try:
                    if "sum_toptrader_long_short_ratio" in row:
                        ratio_sum_str = row["sum_toptrader_long_short_ratio"]
                        count_str = row.get("count_toptrader_long_short_ratio", "")

                        # å®‰å…¨è½¬æ¢æ•°å€¼ï¼Œå¤„ç†ç©ºå€¼
                        ratio_sum = self._safe_decimal_convert(ratio_sum_str)
                        count = self._safe_decimal_convert(count_str)

                        if ratio_sum is not None:
                            # è®¡ç®—å¹³å‡æ¯”ä¾‹
                            ratio_value = ratio_sum / count if count is not None and count > 0 else ratio_sum

                            # è®¡ç®—å¤šç©ºè´¦æˆ·æ¯”ä¾‹
                            if ratio_value > 0:
                                total = ratio_value + 1
                                long_account = ratio_value / total
                                short_account = Decimal("1") / total
                            else:
                                long_account = Decimal("0.5")
                                short_account = Decimal("0.5")

                            long_short_ratios.append(
                                LongShortRatio(
                                    symbol=symbol,
                                    long_short_ratio=ratio_value,
                                    long_account=long_account,
                                    short_account=short_account,
                                    timestamp=timestamp,
                                    ratio_type="account",
                                )
                            )
                except Exception as e:
                    logger.debug(f"è·³è¿‡é¡¶çº§äº¤æ˜“è€…æ•°æ®å¤„ç† {symbol} at {create_time}: {e}")

                # å¤„ç†Takeræ•°æ® - ç‹¬ç«‹å¤„ç†ï¼Œç¡®ä¿æ— æŸ
                try:
                    if "sum_taker_long_short_vol_ratio" in row:
                        taker_ratio_str = row["sum_taker_long_short_vol_ratio"]
                        taker_ratio = self._safe_decimal_convert(taker_ratio_str)

                        if taker_ratio is not None:
                            if taker_ratio > 0:
                                total = taker_ratio + 1
                                long_vol = taker_ratio / total
                                short_vol = Decimal("1") / total
                            else:
                                long_vol = Decimal("0.5")
                                short_vol = Decimal("0.5")

                            long_short_ratios.append(
                                LongShortRatio(
                                    symbol=symbol,
                                    long_short_ratio=taker_ratio,
                                    long_account=long_vol,
                                    short_account=short_vol,
                                    timestamp=timestamp,
                                    ratio_type="taker",
                                )
                            )
                except Exception as e:
                    logger.debug(f"è·³è¿‡Takeræ•°æ®å¤„ç† {symbol} at {create_time}: {e}")

            except (ValueError, KeyError) as e:
                logger.warning(f"è§£æå¤šç©ºæ¯”ä¾‹æ•°æ®è¡Œæ—¶å‡ºé”™: {e}, è¡Œæ•°æ®: {row}")
                continue

        return long_short_ratios

    def _safe_decimal_convert(self, value_str: str | None) -> Decimal | None:
        """å®‰å…¨è½¬æ¢å­—ç¬¦ä¸²ä¸ºDecimalï¼Œå¤„ç†ç©ºå€¼å’Œæ— æ•ˆå€¼.

        Args:
            value_str: è¦è½¬æ¢çš„å­—ç¬¦ä¸²å€¼

        Returns:
            è½¬æ¢åçš„Decimalå€¼ï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™è¿”å›None
        """
        if not value_str or value_str.strip() == "":
            return None

        try:
            return Decimal(str(value_str).strip())
        except (ValueError, TypeError):
            return None

    def download(self, *args, **kwargs):
        """å®ç°åŸºç±»çš„æŠ½è±¡æ–¹æ³•."""
        return self.download_metrics_batch(*args, **kwargs)
