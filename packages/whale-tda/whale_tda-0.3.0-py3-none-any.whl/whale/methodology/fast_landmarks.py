"""Fast landmark selection for LiDAR point clouds.

Quantum-inspired probabilistic sampling strategies optimized for automotive LiDAR.
Replaces expensive MaxMin/KDE with faster approximate methods while preserving
spatial coverage and topological representativeness.

Key optimizations:
1. Spatial hashing for O(1) neighbor lookups instead of O(n) distances
2. Quantum-inspired amplitude amplification (biased sampling without full KDE)
3. Multi-resolution grid sampling for guaranteed coverage
4. GPU-accelerated distance computations when available

Target: <50ms landmark selection for ~120k LiDAR points.
"""

import numpy as np
from typing import Tuple, Optional
import math


def spatial_hash_3d(points: np.ndarray, cell_size: float) -> Tuple[np.ndarray, dict]:
    """Fast spatial hashing for 3D point clouds.
    
    Args:
        points: (N, 3) point cloud
        cell_size: Grid cell size for hashing
        
    Returns:
        cell_indices: (N, 3) grid coordinates for each point
        cell_map: dict mapping cell_id -> list of point indices
    """
    # Compute grid cell indices
    cell_indices = np.floor(points / cell_size).astype(np.int32)
    
    # Build hash map: cell -> point indices
    cell_map = {}
    for i, cell in enumerate(cell_indices):
        cell_id = tuple(cell)
        if cell_id not in cell_map:
            cell_map[cell_id] = []
        cell_map[cell_id].append(i)
    
    return cell_indices, cell_map


def quantum_inspired_sampling(
    points: np.ndarray,
    m: int,
    cell_size: Optional[float] = None,
    alpha: float = 0.7,
    seed: int = 42
) -> np.ndarray:
    """Quantum-inspired landmark selection using amplitude amplification.
    
    This method mimics Grover's amplitude amplification by:
    1. Spatial partitioning (oracle marking desirable states)
    2. Biased sampling from sparse regions (amplitude amplification)
    3. Iterative refinement (Grover iterations)
    
    Much faster than MaxMin+KDE but maintains similar coverage properties.
    
    Args:
        points: (N, 3) point cloud
        m: Number of landmarks
        cell_size: Grid cell size (auto if None)
        alpha: Sparsity bias weight (0=uniform, 1=maximally sparse)
        seed: Random seed
        
    Returns:
        landmark_indices: (m,) array of selected point indices
    """
    rng = np.random.default_rng(seed)
    n = len(points)
    
    if m >= n:
        return np.arange(n, dtype=np.int64)
    
    # Auto-determine cell size based on point cloud extent and target m
    if cell_size is None:
        bbox_min = np.min(points, axis=0)
        bbox_max = np.max(points, axis=0)
        bbox_size = bbox_max - bbox_min
        # Target: ~m^(1/3) cells per dimension for roughly 1 landmark per cell
        cells_per_dim = max(2, int(np.ceil(m ** (1/3))))
        cell_size = np.max(bbox_size) / cells_per_dim
    
    # Phase 1: Spatial hashing (oracle marking) - vectorized
    cell_indices = np.floor(points / cell_size).astype(np.int32)
    
    # Fast cell grouping using unique
    cell_tuples = [tuple(row) for row in cell_indices]
    unique_cells, inverse = np.unique(cell_indices, axis=0, return_inverse=True)
    
    cell_map = {}
    for i, cell_tuple in enumerate(cell_tuples):
        if cell_tuple not in cell_map:
            cell_map[cell_tuple] = []
        cell_map[cell_tuple].append(i)
    
    # Phase 2: Compute cell occupation (measure state amplitudes)
    cell_occupation = {cell_id: len(indices) for cell_id, indices in cell_map.items()}
    
    # Phase 3: Amplitude amplification - bias toward sparse cells
    # Quantum analogy: amplify amplitudes of sparse states
    max_occupation = max(cell_occupation.values())
    cell_weights = {}
    for cell_id, count in cell_occupation.items():
        # Inverse density weighting (sparse cells get higher amplitude)
        sparsity = 1.0 - (count / max_occupation)
        cell_weights[cell_id] = alpha * sparsity + (1 - alpha) * (1.0 / len(cell_map))
    
    # Normalize weights
    total_weight = sum(cell_weights.values())
    cell_probs = {cell_id: w / total_weight for cell_id, w in cell_weights.items()}
    
    # Phase 4: Sample cells proportional to amplified amplitudes
    cells = list(cell_map.keys())
    cell_probabilities = np.array([cell_probs[c] for c in cells])
    
    # Sample m cells (with replacement, then deduplicate)
    n_samples = min(m * 2, len(cells))  # Oversample for deduplication
    sampled_cell_ids = rng.choice(
        len(cells),
        size=n_samples,
        replace=False,
        p=cell_probabilities
    )
    
    # Phase 5: Select one point per sampled cell (measurement collapse)
    landmarks = []
    for cell_idx in sampled_cell_ids:
        cell_id = cells[cell_idx]
        # Randomly pick one point from this cell
        point_idx = rng.choice(cell_map[cell_id])
        landmarks.append(point_idx)
        
        if len(landmarks) >= m:
            break
    
    # Phase 6: If not enough, fill with uniform random (decoherence)
    if len(landmarks) < m:
        remaining = np.setdiff1d(np.arange(n), landmarks)
        extras = rng.choice(remaining, size=m - len(landmarks), replace=False)
        landmarks.extend(extras)
    
    return np.array(landmarks[:m], dtype=np.int64)


def multi_resolution_grid_sampling(
    points: np.ndarray,
    m: int,
    n_levels: int = 3,
    seed: int = 42
) -> np.ndarray:
    """Multi-resolution grid sampling for guaranteed coverage.
    
    Uses hierarchical spatial subdivision (octree-like) to ensure:
    - Coarse coverage at large scales
    - Fine coverage at small scales
    - O(n) time complexity
    
    Args:
        points: (N, 3) point cloud
        m: Number of landmarks
        n_levels: Number of resolution levels
        seed: Random seed
        
    Returns:
        landmark_indices: (m,) array of selected point indices
    """
    rng = np.random.default_rng(seed)
    n = len(points)
    
    if m >= n:
        return np.arange(n, dtype=np.int64)
    
    bbox_min = np.min(points, axis=0)
    bbox_max = np.max(points, axis=0)
    bbox_size = bbox_max - bbox_min
    
    landmarks = []
    points_per_level = m // n_levels
    
    for level in range(n_levels):
        # Double resolution each level
        cells_per_dim = 2 ** (level + 1)
        cell_size = np.max(bbox_size) / cells_per_dim
        
        _, cell_map = spatial_hash_3d(points, cell_size)
        
        # Sample from non-empty cells
        cells = list(cell_map.keys())
        n_cells_to_sample = min(points_per_level, len(cells))
        
        sampled_cells = rng.choice(len(cells), size=n_cells_to_sample, replace=False)
        
        for cell_idx in sampled_cells:
            cell_id = cells[cell_idx]
            # Pick one point from this cell
            point_idx = rng.choice(cell_map[cell_id])
            if point_idx not in landmarks:
                landmarks.append(point_idx)
    
    # Fill remaining with random
    if len(landmarks) < m:
        remaining = np.setdiff1d(np.arange(n), landmarks)
        if len(remaining) > 0:
            extras = rng.choice(remaining, size=min(m - len(landmarks), len(remaining)), replace=False)
            landmarks.extend(extras)
    
    return np.array(landmarks[:m], dtype=np.int64)


def lidar_sector_sampling(
    points: np.ndarray,
    m: int,
    n_sectors: int = 16,
    n_rings: int = 4,
    seed: int = 42
) -> np.ndarray:
    """LiDAR-specific cylindrical sector sampling.
    
    Exploits the structure of automotive LiDAR scanners which have:
    - Rotational symmetry (azimuthal sectors)
    - Range-based rings (elevation/distance bins)
    
    This ensures:
    - Coverage of all viewing directions
    - Representation across near/far ranges
    - Natural handling of LiDAR scan patterns
    
    Args:
        points: (N, 3) point cloud in sensor frame (x forward, z up)
        m: Number of landmarks
        n_sectors: Number of azimuthal sectors
        n_rings: Number of radial distance rings
        seed: Random seed
        
    Returns:
        landmark_indices: (m,) array of selected point indices
    """
    rng = np.random.default_rng(seed)
    n = len(points)
    
    if m >= n:
        return np.arange(n, dtype=np.int64)
    
    # Fast vectorized cylindrical coordinates
    xy = points[:, :2]
    ranges = np.sqrt(xy[:, 0]**2 + xy[:, 1]**2)  # 2x faster than linalg.norm
    azimuths = np.arctan2(points[:, 1], points[:, 0])
    
    # Vectorized binning (faster than digitize)
    sector_width = 2 * np.pi / n_sectors
    sector_ids = ((azimuths + np.pi) / sector_width).astype(np.int32)
    sector_ids = np.clip(sector_ids, 0, n_sectors - 1)
    
    # Fast range binning using percentiles
    range_percentiles = np.linspace(0, 100, n_rings + 1)
    range_bins = np.percentile(ranges, range_percentiles)
    ring_ids = np.searchsorted(range_bins[:-1], ranges, side='right') - 1
    ring_ids = np.clip(ring_ids, 0, n_rings - 1)
    
    # Combine sector and ring into single cell ID for faster grouping
    cell_ids = sector_ids * n_rings + ring_ids
    n_cells = n_sectors * n_rings
    
    # Ultra-fast cell grouping using argsort (avoids Python loops)
    sort_idx = np.argsort(cell_ids)
    sorted_cells = cell_ids[sort_idx]
    
    # Find cell boundaries using searchsorted
    cell_boundaries = np.searchsorted(sorted_cells, np.arange(n_cells + 1))
    
    # Sample from non-empty cells (vectorized)
    points_per_cell = max(1, m // n_cells)
    landmarks = []
    
    for cell_idx in range(n_cells):
        start = cell_boundaries[cell_idx]
        end = cell_boundaries[cell_idx + 1]
        
        if start < end:  # Non-empty cell
            cell_points = sort_idx[start:end]
            n_to_sample = min(points_per_cell, len(cell_points))
            
            # Fast sampling using random indexing
            if n_to_sample == 1:
                landmarks.append(cell_points[rng.integers(len(cell_points))])
            else:
                sampled = rng.choice(cell_points, size=n_to_sample, replace=False)
                landmarks.extend(sampled)
            
            if len(landmarks) >= m:
                break
    
    # Fast fill remaining using set difference
    if len(landmarks) < m:
        landmarks_array = np.array(landmarks)
        all_indices = np.arange(n)
        mask = np.ones(n, dtype=bool)
        mask[landmarks_array] = False
        remaining = all_indices[mask]
        
        if len(remaining) > 0:
            n_extra = min(m - len(landmarks), len(remaining))
            extras = rng.choice(remaining, size=n_extra, replace=False)
            landmarks.extend(extras)
    
    return np.array(landmarks[:m], dtype=np.int64)


def fast_landmark_selection(
    points: np.ndarray,
    m: int,
    method: str = "quantum",
    seed: int = 42,
    **kwargs
) -> np.ndarray:
    """Fast landmark selection dispatcher.
    
    Args:
        points: (N, 3) point cloud
        m: Number of landmarks
        method: Selection method
            - 'quantum': Quantum-inspired amplitude amplification (best for general use)
            - 'grid': Multi-resolution grid (best for guaranteed coverage)
            - 'lidar': LiDAR sector sampling (best for automotive LiDAR)
        seed: Random seed
        **kwargs: Method-specific parameters
        
    Returns:
        landmark_indices: (m,) array of selected point indices
    """
    if method == "quantum":
        return quantum_inspired_sampling(points, m, seed=seed, **kwargs)
    elif method == "grid":
        return multi_resolution_grid_sampling(points, m, seed=seed, **kwargs)
    elif method == "lidar":
        return lidar_sector_sampling(points, m, seed=seed, **kwargs)
    else:
        raise ValueError(f"Unknown method: {method}")
