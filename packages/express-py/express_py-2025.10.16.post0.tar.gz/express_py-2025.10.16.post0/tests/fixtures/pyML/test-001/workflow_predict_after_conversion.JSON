{
    "name": "workflow:pyml_predict",
    "displayName": "[Predict Workflow] Python ML Train Regression",
    "creator": {
        "_id": "",
        "cls": "User",
        "slug": ""
    },
    "owner": {
        "_id": "",
        "cls": "Account",
        "slug": ""
    },
    "schemaVersion": "0.2.0",
    "exabyteId": "",
    "hash": "",
    "_id": "",
    "units": [
        {
            "type": "subworkflow",
            "_id": "1277856674cc647647a213cb",
            "name": "Set Up the Job",
            "status": "idle",
            "flowchartId": "77c7e674df08c663ce8ae7c5",
            "results": [],
            "monitors": [],
            "preProcessors": [],
            "postProcessors": [],
            "head": true,
            "next": "53c4948b2e22c649cd08030f",
            "statusTrack": []
        },
        {
            "type": "subworkflow",
            "_id": "727b9f5253842f378bb8bf7d",
            "name": "Machine Learning",
            "status": "idle",
            "flowchartId": "53c4948b2e22c649cd08030f",
            "results": [],
            "monitors": [],
            "preProcessors": [],
            "postProcessors": [],
            "head": false,
            "statusTrack": []
        }
    ],
    "subworkflows": [
        {
            "_id": "1277856674cc647647a213cb",
            "name": "Set Up the Job",
            "application": {
                "version": "3.8.6",
                "isDefault": true,
                "summary": "Python Script",
                "name": "python",
                "shortName": "py",
                "build": "Default"
            },
            "properties": [],
            "model": {
                "type": "unknown",
                "subtype": "unknown",
                "method": {
                    "type": "unknown",
                    "subtype": "unknown",
                    "data": {}
                }
            },
            "units": [
                {
                    "name": "Set Workflow Mode",
                    "type": "assignment",
                    "operand": "IS_WORKFLOW_RUNNING_TO_PREDICT",
                    "value": "True",
                    "input": [],
                    "flowchartId": "head-set-predict-status",
                    "tags": [
                        "pyml:workflow-type-setter"
                    ],
                    "status": "finished",
                    "results": [],
                    "monitors": [],
                    "preProcessors": [],
                    "postProcessors": [],
                    "head": true,
                    "next": "head-fetch-training-data",
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "name": "Fetch Dataset",
                    "type": "io",
                    "subtype": "input",
                    "source": "object_storage",
                    "input": [
                        {
                            "basename": "{{DATASET_BASENAME}}",
                            "objectData": {
                                "CONTAINER": "",
                                "NAME": "{{DATASET_FILEPATH}}",
                                "PROVIDER": "",
                                "REGION": ""
                            }
                        }
                    ],
                    "flowchartId": "head-fetch-training-data",
                    "enableRender": true,
                    "status": "finished",
                    "results": [],
                    "monitors": [],
                    "preProcessors": [],
                    "postProcessors": [],
                    "head": false,
                    "next": "head-branch-on-predict-status",
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "name": "Train or Predict?",
                    "preProcessors": [],
                    "postProcessors": [],
                    "results": [],
                    "type": "condition",
                    "input": [
                        {
                            "scope": "global",
                            "name": "IS_WORKFLOW_RUNNING_TO_PREDICT"
                        }
                    ],
                    "then": "head-fetch-trained-model",
                    "else": "end-of-ml-train-head",
                    "statement": "IS_WORKFLOW_RUNNING_TO_PREDICT",
                    "maxOccurrences": 100,
                    "flowchartId": "head-branch-on-predict-status",
                    "status": "finished",
                    "monitors": [],
                    "head": false,
                    "next": "head-fetch-trained-model",
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "name": "Fetch Trained Model as file",
                    "type": "io",
                    "subtype": "input",
                    "source": "object_storage",
                    "input": [
                        {
                            "basename": "train_target.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/train_target.pkl"
                            }
                        },
                        {
                            "basename": "workflow_context_file_mapping",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/workflow_context_file_mapping"
                            }
                        },
                        {
                            "basename": "test_target.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/test_target.pkl"
                            }
                        },
                        {
                            "basename": "train_descriptors.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/train_descriptors.pkl"
                            }
                        },
                        {
                            "basename": "test_descriptors.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/test_descriptors.pkl"
                            }
                        },
                        {
                            "basename": "descriptor_scaler.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/descriptor_scaler.pkl"
                            }
                        },
                        {
                            "basename": "target_scaler.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/target_scaler.pkl"
                            }
                        },
                        {
                            "basename": "sklearn_mlp.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/sklearn_mlp.pkl"
                            }
                        },
                        {
                            "basename": "train_predictions.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/train_predictions.pkl"
                            }
                        },
                        {
                            "basename": "test_predictions.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/test_predictions.pkl"
                            }
                        },
                        {
                            "basename": "RMSE.pkl",
                            "pathname": ".job_context",
                            "overwrite": false,
                            "objectData": {
                                "CONTAINER": "vagrant-cluster-001",
                                "PROVIDER": "aws",
                                "REGION": "us-east-1",
                                "NAME": "/cluster-001-home/demo/data/demo-default/testregression-CWcjBtrqvvKhAipBx/.job_context/RMSE.pkl"
                            }
                        }
                    ],
                    "flowchartId": "head-fetch-trained-model",
                    "enableRender": true,
                    "tags": [
                        "set-io-unit-filenames"
                    ],
                    "status": "idle",
                    "results": [],
                    "monitors": [],
                    "preProcessors": [],
                    "postProcessors": [],
                    "head": false,
                    "next": "end-of-ml-train-head",
                    "statusTrack": []
                },
                {
                    "name": "End Setup",
                    "type": "assignment",
                    "operand": "IS_SETUP_COMPLETE",
                    "value": "True",
                    "input": [],
                    "flowchartId": "end-of-ml-train-head",
                    "status": "finished",
                    "results": [],
                    "monitors": [],
                    "preProcessors": [],
                    "postProcessors": [],
                    "head": false,
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                }
            ]
        },
        {
            "_id": "727b9f5253842f378bb8bf7d",
            "name": "Machine Learning",
            "application": {
                "version": "3.8.6",
                "isDefault": true,
                "summary": "Python Script",
                "name": "python",
                "shortName": "py",
                "build": "Default"
            },
            "properties": [
                "workflow:pyml_predict",
                "file_content"
            ],
            "model": {
                "type": "unknown",
                "subtype": "unknown",
                "method": {
                    "type": "unknown",
                    "subtype": "unknown",
                    "data": {}
                }
            },
            "units": [
                {
                    "type": "execution",
                    "name": "Setup Variables and Packages",
                    "head": true,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "75308b98bac7fceb36d5e24d",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "version": "3.8.6",
                        "isDefault": true,
                        "summary": "Python Script",
                        "name": "python",
                        "shortName": "py",
                        "build": "Default"
                    },
                    "executable": {
                        "_id": "7T9bjm96pi6NCR9bK",
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [],
                        "name": "python",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "applicationId": [
                            "Asst5sW8bwCgiWHk5",
                            "CZPivmgLwfxY7MoP6",
                            "RQhSm7uYJR23rtQoB",
                            "9GCM8NdYYHtvBjqsv",
                            "ZRsdFKKGsHZfNi45e",
                            "rgcuSWcSKLeLy2tYT"
                        ],
                        "createdAt": "2021-04-10T00:32:39.353Z",
                        "updatedAt": "2021-04-10T00:44:38.676Z"
                    },
                    "flavor": {
                        "_id": "sarjPxBpNPF5NDsXz",
                        "input": [
                            {
                                "name": "settings.py",
                                "templateId": "zXHF2aSqQL4kdj4ST"
                            },
                            {
                                "name": "requirements.txt",
                                "templateId": "hHmyvDrw2jXjSDHWd"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:setup_variables_packages",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "executableId": "7T9bjm96pi6NCR9bK",
                        "createdAt": "2021-04-10T00:32:39.362Z",
                        "updatedAt": "2021-04-10T00:44:38.692Z"
                    },
                    "enableRender": true,
                    "status": "finished",
                    "input": [
                        {
                            "_id": "zXHF2aSqQL4kdj4ST",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   General settings for PythonML jobs on the Exabyte.io Platform   #\n#                                                                   #\n#   This file generally shouldn't be modified directly by users.    #\n#   The \"datafile\" and \"is_workflow_running_to_predict\" variables   #\n#   are defined in the head subworkflow, and are templated into     #\n#   this file. This helps facilitate the workflow's behavior        #\n#   differing whether it is in a \"train\" or \"predict\" mode.         #\n#                                                                   #\n#   Also in this file is the \"Context\" object, which helps maintain #\n#   certain Python objects between workflow units, and between      #\n#   predict runs.                                                   #\n#                                                                   #\n#   Whenever a python object needs to be stored for subsequent runs #\n#   (such as in the case of a trained model), context.save() can be #\n#   called to save it. The object can then be loaded again by using #\n#   context.load().                                                 #\n# ----------------------------------------------------------------- #\n\n\nimport pickle, os\n\n# =========================\n# User-modifiable variables\n# =========================\n# Variables in this section can (and oftentimes need to) be modified by the user\n\n# Target_column_name is used during training to identify the variable the model is traing to predict.\n# For example, consider a CSV containing three columns, \"Y\", \"X1\", and \"X2\". If the goal is to train a model\n# that will predict the value of \"Y,\" then target_column_name would be set to \"Y\"\ntarget_column_name = \"target\"\n\n# The type of ML problem being performed. Can be either \"Regression\", \"Classification,\" or \"Clustering.\"\nproblem_category = \"regression\"\n\n# =============================\n# Non user-modifiable variables\n# =============================\n# Variables in this section generally do not need to be modified.\n\n# The problem category, regression or classification or clustering. In regression, the target (predicted) variable\n# is continues. In classification, it is categorical. In clustering, there is no target - a set of labels is\n# automatically generated.\nis_regression = is_classification = is_clustering = False\nif problem_category.lower() == \"regression\":\n    is_regression = True\nelif problem_category.lower() == \"classification\":\n    is_classification = True\nelif problem_category.lower() == \"clustering\":\n    is_clustering = True\nelse:\n    raise ValueError(\n        \"Variable 'problem_category' must be either 'regression', 'classification', or 'clustering'. Check settings.py\")\n\n# The variables \"is_workflow_running_to_predict\" and \"is_workflow_running_to_train\" are used to control whether\n# the workflow is in a \"training\" mode or a \"prediction\" mode. The \"IS_WORKFLOW_RUNNING_TO_PREDICT\" variable is set by\n# an assignment unit in the \"Set Up the Job\" subworkflow that executes at the start of the job. It is automatically\n# changed when the predict workflow is generated, so users should not need to modify this variable.\nis_workflow_running_to_predict = {% raw %}{{IS_WORKFLOW_RUNNING_TO_PREDICT}}{% endraw %}\nis_workflow_running_to_train = not is_workflow_running_to_predict\n\n# Sets the datafile variable. The \"datafile\" is the data that will be read in, and will be used by subsequent\n# workflow units for either training or prediction, depending on the workflow mode.\nif is_workflow_running_to_predict:\n    datafile = \"{% raw %}{{DATASET_BASENAME}}{% endraw %}\"\nelse:\n    datafile = \"{% raw %}{{DATASET_BASENAME}}{% endraw %}\"\n\n# The \"Context\" class allows for data to be saved and loaded between units, and between train and predict runs.\n# Variables which have been saved using the \"Save\" method are written to disk, and the predict workflow is automatically\n# configured to obtain these files when it starts.\n#\n# IMPORTANT NOTE: Do *not* adjust the value of \"context_dir_pathname\" in the Context object. If the value is changed, then\n# files will not be correctly copied into the generated predict workflow. This will cause the predict workflow to be\n# generated in a broken state, and it will not be able to make any predictions.\nclass Context(object):\n    \"\"\"\n    Saves and loads objects from the disk, useful for preserving data between workflow units\n\n    Attributes:\n        context_paths (dict): Dictionary of the format {variable_name: path}, that governs where\n                              pickle saves files.\n\n    Methods:\n        save: Used to save objects to the context directory\n        load: Used to load objects from the context directory\n    \"\"\"\n\n    def __init__(self, context_file_basename=\"workflow_context_file_mapping\"):\n        \"\"\"\n        Constructor for Context objects\n\n        Args:\n            context_file_basename (str): Name of the file to store context paths in\n        \"\"\"\n\n        # Warning: DO NOT modify the context_dir_pathname variable below\n        # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n        context_dir_pathname = \"{% raw %}{{ CONTEXT_DIR_RELATIVE_PATH }}{% endraw %}\"\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._context_dir_pathname = context_dir_pathname\n        self._context_file = os.path.join(context_dir_pathname, context_file_basename)\n\n        # Make context dir if it does not exist\n        if not os.path.exists(context_dir_pathname):\n            os.makedirs(context_dir_pathname)\n\n        # Read in the context sources dictionary, if it exists\n        if os.path.exists(self._context_file):\n            with open(self._context_file, \"rb\") as file_handle:\n                self.context_paths: dict = pickle.load(file_handle)\n        else:\n            # Items is a dictionary of {varname: path}\n            self.context_paths = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._update_context()\n\n    def __contains__(self, item):\n        return item in self.context_paths\n\n    def _update_context(self):\n        with open(self._context_file, \"wb\") as file_handle:\n            pickle.dump(self.context_paths, file_handle)\n\n    def load(self, name: str):\n        \"\"\"\n        Returns a contextd object\n\n        Args:\n            name (str): The name in self.context_paths of the object\n        \"\"\"\n        path = self.context_paths[name]\n        with open(path, \"rb\") as file_handle:\n            obj = pickle.load(file_handle)\n        return obj\n\n    def save(self, obj: object, name: str):\n        \"\"\"\n        Saves an object to disk using pickle\n\n        Args:\n            name (str): Friendly name for the object, used for lookup in load() method\n            obj (object): Object to store on disk\n        \"\"\"\n        path = os.path.join(self._context_dir_pathname, f\"{name}.pkl\")\n        self.context_paths[name] = path\n        with open(path, \"wb\") as file_handle:\n            pickle.dump(obj, file_handle)\n        self._update_context()\n\n# Generate a context object, so that the \"with settings.context\" can be used by other units in this workflow.\ncontext = Context()\n\nis_using_train_test_split = \"is_using_train_test_split\" in context and (context.load(\"is_using_train_test_split\"))",
                            "name": "settings.py",
                            "contextProviders": [],
                            "applicationName": "python",
                            "executableName": "python",
                            "isDefault": false,
                            "schemaVersion": "0.2.0",
                            "inSet": [],
                            "tags": [],
                            "createdAt": "2021-04-10T00:32:36.987Z",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   General settings for PythonML jobs on the Exabyte.io Platform   #\n#                                                                   #\n#   This file generally shouldn't be modified directly by users.    #\n#   The \"datafile\" and \"is_workflow_running_to_predict\" variables   #\n#   are defined in the head subworkflow, and are templated into     #\n#   this file. This helps facilitate the workflow's behavior        #\n#   differing whether it is in a \"train\" or \"predict\" mode.         #\n#                                                                   #\n#   Also in this file is the \"Context\" object, which helps maintain #\n#   certain Python objects between workflow units, and between      #\n#   predict runs.                                                   #\n#                                                                   #\n#   Whenever a python object needs to be stored for subsequent runs #\n#   (such as in the case of a trained model), context.save() can be #\n#   called to save it. The object can then be loaded again by using #\n#   context.load().                                                 #\n# ----------------------------------------------------------------- #\n\n\nimport pickle, os\n\n# =========================\n# User-modifiable variables\n# =========================\n# Variables in this section can (and oftentimes need to) be modified by the user\n\n# Target_column_name is used during training to identify the variable the model is traing to predict.\n# For example, consider a CSV containing three columns, \"Y\", \"X1\", and \"X2\". If the goal is to train a model\n# that will predict the value of \"Y,\" then target_column_name would be set to \"Y\"\ntarget_column_name = \"target\"\n\n# The type of ML problem being performed. Can be either \"Regression\", \"Classification,\" or \"Clustering.\"\nproblem_category = \"regression\"\n\n# =============================\n# Non user-modifiable variables\n# =============================\n# Variables in this section generally do not need to be modified.\n\n# The problem category, regression or classification or clustering. In regression, the target (predicted) variable\n# is continues. In classification, it is categorical. In clustering, there is no target - a set of labels is\n# automatically generated.\nis_regression = is_classification = is_clustering = False\nif problem_category.lower() == \"regression\":\n    is_regression = True\nelif problem_category.lower() == \"classification\":\n    is_classification = True\nelif problem_category.lower() == \"clustering\":\n    is_clustering = True\nelse:\n    raise ValueError(\n        \"Variable 'problem_category' must be either 'regression', 'classification', or 'clustering'. Check settings.py\")\n\n# The variables \"is_workflow_running_to_predict\" and \"is_workflow_running_to_train\" are used to control whether\n# the workflow is in a \"training\" mode or a \"prediction\" mode. The \"IS_WORKFLOW_RUNNING_TO_PREDICT\" variable is set by\n# an assignment unit in the \"Set Up the Job\" subworkflow that executes at the start of the job. It is automatically\n# changed when the predict workflow is generated, so users should not need to modify this variable.\nis_workflow_running_to_predict = {{IS_WORKFLOW_RUNNING_TO_PREDICT}}\nis_workflow_running_to_train = not is_workflow_running_to_predict\n\n# Sets the datafile variable. The \"datafile\" is the data that will be read in, and will be used by subsequent\n# workflow units for either training or prediction, depending on the workflow mode.\nif is_workflow_running_to_predict:\n    datafile = \"{{DATASET_BASENAME}}\"\nelse:\n    datafile = \"{{DATASET_BASENAME}}\"\n\n# The \"Context\" class allows for data to be saved and loaded between units, and between train and predict runs.\n# Variables which have been saved using the \"Save\" method are written to disk, and the predict workflow is automatically\n# configured to obtain these files when it starts.\n#\n# IMPORTANT NOTE: Do *not* adjust the value of \"context_dir_pathname\" in the Context object. If the value is changed, then\n# files will not be correctly copied into the generated predict workflow. This will cause the predict workflow to be\n# generated in a broken state, and it will not be able to make any predictions.\nclass Context(object):\n    \"\"\"\n    Saves and loads objects from the disk, useful for preserving data between workflow units\n\n    Attributes:\n        context_paths (dict): Dictionary of the format {variable_name: path}, that governs where\n                              pickle saves files.\n\n    Methods:\n        save: Used to save objects to the context directory\n        load: Used to load objects from the context directory\n    \"\"\"\n\n    def __init__(self, context_file_basename=\"workflow_context_file_mapping\"):\n        \"\"\"\n        Constructor for Context objects\n\n        Args:\n            context_file_basename (str): Name of the file to store context paths in\n        \"\"\"\n\n        # Warning: DO NOT modify the context_dir_pathname variable below\n        # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n        context_dir_pathname = \"{{ CONTEXT_DIR_RELATIVE_PATH }}\"\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._context_dir_pathname = context_dir_pathname\n        self._context_file = os.path.join(context_dir_pathname, context_file_basename)\n\n        # Make context dir if it does not exist\n        if not os.path.exists(context_dir_pathname):\n            os.makedirs(context_dir_pathname)\n\n        # Read in the context sources dictionary, if it exists\n        if os.path.exists(self._context_file):\n            with open(self._context_file, \"rb\") as file_handle:\n                self.context_paths: dict = pickle.load(file_handle)\n        else:\n            # Items is a dictionary of {varname: path}\n            self.context_paths = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._update_context()\n\n    def __contains__(self, item):\n        return item in self.context_paths\n\n    def _update_context(self):\n        with open(self._context_file, \"wb\") as file_handle:\n            pickle.dump(self.context_paths, file_handle)\n\n    def load(self, name: str):\n        \"\"\"\n        Returns a contextd object\n\n        Args:\n            name (str): The name in self.context_paths of the object\n        \"\"\"\n        path = self.context_paths[name]\n        with open(path, \"rb\") as file_handle:\n            obj = pickle.load(file_handle)\n        return obj\n\n    def save(self, obj: object, name: str):\n        \"\"\"\n        Saves an object to disk using pickle\n\n        Args:\n            name (str): Friendly name for the object, used for lookup in load() method\n            obj (object): Object to store on disk\n        \"\"\"\n        path = os.path.join(self._context_dir_pathname, f\"{name}.pkl\")\n        self.context_paths[name] = path\n        with open(path, \"wb\") as file_handle:\n            pickle.dump(obj, file_handle)\n        self._update_context()\n\n# Generate a context object, so that the \"with settings.context\" can be used by other units in this workflow.\ncontext = Context()\n\nis_using_train_test_split = \"is_using_train_test_split\" in context and (context.load(\"is_using_train_test_split\"))"
                        },
                        {
                            "_id": "hHmyvDrw2jXjSDHWd",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  PythonML Package Requirements for use on the Exabyte.io Platform #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n#  The package set below is a stable working set of pymatgen and    #\n#  all of its dependencies.  Please adjust the list to include      #\n#  your preferred packages.                                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\n# Python 2 packages\nbackports.functools-lru-cache==1.6.1;python_version<\"3\"\ncertifi==2020.12.5;python_version<\"3\"\nchardet==4.0.0;python_version<\"3\"\ncycler==0.10.0;python_version<\"3\"\ndecorator==4.4.2;python_version<\"3\"\nenum34==1.1.10;python_version<\"3\"\nidna==2.10;python_version<\"3\"\nkiwisolver==1.1.0;python_version<\"3\"\nmatplotlib==2.2.5;python_version<\"3\"\nmonty==2.0.7;python_version<\"3\"\nmpmath==1.2.1;python_version<\"3\"\nnetworkx==2.2;python_version<\"3\"\nnumpy==1.16.6;python_version<\"3\"\npalettable==3.3.0;python_version<\"3\"\npandas==0.24.2;python_version<\"3\"\nPyDispatcher==2.0.5;python_version<\"3\"\npymatgen==2018.12.12;python_version<\"3\"\npyparsing==2.4.7;python_version<\"3\"\npython-dateutil==2.8.1;python_version<\"3\"\npytz==2021.1;python_version<\"3\"\nrequests==2.25.1;python_version<\"3\"\nruamel.ordereddict==0.4.15;python_version<\"3\"\nruamel.yaml==0.16.12;python_version<\"3\"\nruamel.yaml.clib==0.2.2;python_version<\"3\"\nscipy==1.2.3;python_version<\"3\"\nscikit-learn==0.20.4;python_version<\"3\"\nsix==1.15.0;python_version<\"3\"\nspglib==1.16.1;python_version<\"3\"\nsubprocess32==3.5.4;python_version<\"3\"\nsympy==1.5.1;python_version<\"3\"\ntabulate==0.8.7;python_version<\"3\"\nurllib3==1.26.3;python_version<\"3\"\n\n# Python 3 packages\ncertifi==2020.12.5;python_version>=\"3\"\nchardet==4.0.0;python_version>=\"3\"\ncycler==0.10.0;python_version>=\"3\"\ndecorator==4.4.2;python_version>=\"3\"\nfuture==0.18.2;python_version>=\"3\"\nidna==2.10;python_version>=\"3\"\nkiwisolver==1.3.1;python_version>=\"3\"\nmatplotlib==3.3.4;python_version>=\"3\"\nmonty==4.0.2;python_version>=\"3\"\nmpmath==1.2.1;python_version>=\"3\"\nnetworkx==2.5;python_version>=\"3\"\nnumpy==1.19.5;python_version>=\"3\"\npalettable==3.3.0;python_version>=\"3\"\npandas==1.1.5;python_version>=\"3\"\nPillow==8.1.0;python_version>=\"3\"\nplotly==4.14.3;python_version>=\"3\"\npymatgen==2021.2.8.1;python_version>=\"3\"\npyparsing==2.4.7;python_version>=\"3\"\npython-dateutil==2.8.1;python_version>=\"3\"\npytz==2021.1;python_version>=\"3\"\nrequests==2.25.1;python_version>=\"3\"\nretrying==1.3.3;python_version>=\"3\"\nruamel.yaml==0.16.12;python_version>=\"3\"\nruamel.yaml.clib==0.2.2;python_version>=\"3\"\nscikit-learn==0.24.1;python_version>=\"3\"\nscipy==1.5.4;python_version>=\"3\"\nsix==1.15.0;python_version>=\"3\"\nspglib==1.16.1;python_version>=\"3\"\nsympy==1.7.1;python_version>=\"3\"\ntabulate==0.8.7;python_version>=\"3\"\nuncertainties==3.1.5;python_version>=\"3\"\nurllib3==1.26.3;python_version>=\"3\"",
                            "name": "requirements.txt",
                            "contextProviders": [],
                            "applicationName": "python",
                            "executableName": "python",
                            "isDefault": false,
                            "schemaVersion": "0.2.0",
                            "inSet": [],
                            "tags": [],
                            "createdAt": "2021-04-10T00:32:36.985Z",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  PythonML Package Requirements for use on the Exabyte.io Platform #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n#  The package set below is a stable working set of pymatgen and    #\n#  all of its dependencies.  Please adjust the list to include      #\n#  your preferred packages.                                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\n# Python 2 packages\nbackports.functools-lru-cache==1.6.1;python_version<\"3\"\ncertifi==2020.12.5;python_version<\"3\"\nchardet==4.0.0;python_version<\"3\"\ncycler==0.10.0;python_version<\"3\"\ndecorator==4.4.2;python_version<\"3\"\nenum34==1.1.10;python_version<\"3\"\nidna==2.10;python_version<\"3\"\nkiwisolver==1.1.0;python_version<\"3\"\nmatplotlib==2.2.5;python_version<\"3\"\nmonty==2.0.7;python_version<\"3\"\nmpmath==1.2.1;python_version<\"3\"\nnetworkx==2.2;python_version<\"3\"\nnumpy==1.16.6;python_version<\"3\"\npalettable==3.3.0;python_version<\"3\"\npandas==0.24.2;python_version<\"3\"\nPyDispatcher==2.0.5;python_version<\"3\"\npymatgen==2018.12.12;python_version<\"3\"\npyparsing==2.4.7;python_version<\"3\"\npython-dateutil==2.8.1;python_version<\"3\"\npytz==2021.1;python_version<\"3\"\nrequests==2.25.1;python_version<\"3\"\nruamel.ordereddict==0.4.15;python_version<\"3\"\nruamel.yaml==0.16.12;python_version<\"3\"\nruamel.yaml.clib==0.2.2;python_version<\"3\"\nscipy==1.2.3;python_version<\"3\"\nscikit-learn==0.20.4;python_version<\"3\"\nsix==1.15.0;python_version<\"3\"\nspglib==1.16.1;python_version<\"3\"\nsubprocess32==3.5.4;python_version<\"3\"\nsympy==1.5.1;python_version<\"3\"\ntabulate==0.8.7;python_version<\"3\"\nurllib3==1.26.3;python_version<\"3\"\n\n# Python 3 packages\ncertifi==2020.12.5;python_version>=\"3\"\nchardet==4.0.0;python_version>=\"3\"\ncycler==0.10.0;python_version>=\"3\"\ndecorator==4.4.2;python_version>=\"3\"\nfuture==0.18.2;python_version>=\"3\"\nidna==2.10;python_version>=\"3\"\nkiwisolver==1.3.1;python_version>=\"3\"\nmatplotlib==3.3.4;python_version>=\"3\"\nmonty==4.0.2;python_version>=\"3\"\nmpmath==1.2.1;python_version>=\"3\"\nnetworkx==2.5;python_version>=\"3\"\nnumpy==1.19.5;python_version>=\"3\"\npalettable==3.3.0;python_version>=\"3\"\npandas==1.1.5;python_version>=\"3\"\nPillow==8.1.0;python_version>=\"3\"\nplotly==4.14.3;python_version>=\"3\"\npymatgen==2021.2.8.1;python_version>=\"3\"\npyparsing==2.4.7;python_version>=\"3\"\npython-dateutil==2.8.1;python_version>=\"3\"\npytz==2021.1;python_version>=\"3\"\nrequests==2.25.1;python_version>=\"3\"\nretrying==1.3.3;python_version>=\"3\"\nruamel.yaml==0.16.12;python_version>=\"3\"\nruamel.yaml.clib==0.2.2;python_version>=\"3\"\nscikit-learn==0.24.1;python_version>=\"3\"\nscipy==1.5.4;python_version>=\"3\"\nsix==1.15.0;python_version>=\"3\"\nspglib==1.16.1;python_version>=\"3\"\nsympy==1.7.1;python_version>=\"3\"\ntabulate==0.8.7;python_version>=\"3\"\nuncertainties==3.1.5;python_version>=\"3\"\nurllib3==1.26.3;python_version>=\"3\""
                        }
                    ],
                    "next": "c6acff722ea59396b6ef15ce",
                    "context": {},
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "type": "execution",
                    "name": "Data Input",
                    "head": false,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "c6acff722ea59396b6ef15ce",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "version": "3.8.6",
                        "isDefault": true,
                        "summary": "Python Script",
                        "name": "python",
                        "shortName": "py",
                        "build": "Default"
                    },
                    "executable": {
                        "_id": "7T9bjm96pi6NCR9bK",
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [],
                        "name": "python",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "applicationId": [
                            "Asst5sW8bwCgiWHk5",
                            "CZPivmgLwfxY7MoP6",
                            "RQhSm7uYJR23rtQoB",
                            "9GCM8NdYYHtvBjqsv",
                            "ZRsdFKKGsHZfNi45e",
                            "rgcuSWcSKLeLy2tYT"
                        ],
                        "createdAt": "2021-04-10T00:32:39.353Z",
                        "updatedAt": "2021-04-10T00:44:38.676Z"
                    },
                    "flavor": {
                        "_id": "tYRT3jnmMxmYALfHK",
                        "input": [
                            {
                                "name": "data_input_read_csv_pandas.py",
                                "templateId": "YEc3vMa6s9oz2BAqW"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:data_input:read_csv:pandas",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "executableId": "7T9bjm96pi6NCR9bK",
                        "createdAt": "2021-04-10T00:32:39.367Z",
                        "updatedAt": "2021-04-10T00:44:38.699Z"
                    },
                    "status": "finished",
                    "input": [
                        {
                            "_id": "YEc3vMa6s9oz2BAqW",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to read in data for the ML workflow.              #\n#                                                                   #\n#   Also showcased here is the concept of branching based on        #\n#   whether the workflow is in \"train\" or \"predict\" mode.           #\n#                                                                   #\n#   If the workflow is in \"training\" mode, it will read in the data #\n#   before converting it to a Numpy array and save it for use       #\n#   later. During training, we already have values for the output,  #\n#   and this gets saved to \"target.\"                                #\n#                                                                   #\n#   Finally, whether the workflow is in training or predict mode,   #\n#   it will always read in a set of descriptors from a datafile     #\n#   defined in settings.py                                          #\n# ----------------------------------------------------------------- #\n\n\nimport pandas\nimport sklearn.preprocessing\nimport settings\n\nwith settings.context as context:\n    data = pandas.read_csv(settings.datafile)\n\n    # Train\n    # By default, we don't do train/test splitting: the train and test represent the same dataset at first.\n    # Other units (such as a train/test splitter) down the line can adjust this as-needed.\n    if settings.is_workflow_running_to_train:\n\n        # Handle the case where we are clustering\n        if settings.is_clustering:\n            target = data.to_numpy()[:, 0]  # Just get the first column, it's not going to get used anyway\n        else:\n            target = data.pop(settings.target_column_name).to_numpy()\n\n        # Handle the case where we are classifying\n        if settings.is_classification:\n            target = target.astype(int)\n\n        target = target.reshape(-1, 1)  # Reshape array from a row vector into a column vector\n\n        context.save(target, \"train_target\")\n        context.save(target, \"test_target\")\n\n        descriptors = data.to_numpy()\n\n        context.save(descriptors, \"train_descriptors\")\n        context.save(descriptors, \"test_descriptors\")\n\n    else:\n        descriptors = data.to_numpy()\n        context.save(descriptors, \"descriptors\")",
                            "name": "data_input_read_csv_pandas.py",
                            "contextProviders": [],
                            "applicationName": "python",
                            "executableName": "python",
                            "isDefault": false,
                            "schemaVersion": "0.2.0",
                            "inSet": [],
                            "tags": [],
                            "createdAt": "2021-04-10T00:32:36.989Z",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to read in data for the ML workflow.              #\n#                                                                   #\n#   Also showcased here is the concept of branching based on        #\n#   whether the workflow is in \"train\" or \"predict\" mode.           #\n#                                                                   #\n#   If the workflow is in \"training\" mode, it will read in the data #\n#   before converting it to a Numpy array and save it for use       #\n#   later. During training, we already have values for the output,  #\n#   and this gets saved to \"target.\"                                #\n#                                                                   #\n#   Finally, whether the workflow is in training or predict mode,   #\n#   it will always read in a set of descriptors from a datafile     #\n#   defined in settings.py                                          #\n# ----------------------------------------------------------------- #\n\n\nimport pandas\nimport sklearn.preprocessing\nimport settings\n\nwith settings.context as context:\n    data = pandas.read_csv(settings.datafile)\n\n    # Train\n    # By default, we don't do train/test splitting: the train and test represent the same dataset at first.\n    # Other units (such as a train/test splitter) down the line can adjust this as-needed.\n    if settings.is_workflow_running_to_train:\n\n        # Handle the case where we are clustering\n        if settings.is_clustering:\n            target = data.to_numpy()[:, 0]  # Just get the first column, it's not going to get used anyway\n        else:\n            target = data.pop(settings.target_column_name).to_numpy()\n\n        # Handle the case where we are classifying\n        if settings.is_classification:\n            target = target.astype(int)\n\n        target = target.reshape(-1, 1)  # Reshape array from a row vector into a column vector\n\n        context.save(target, \"train_target\")\n        context.save(target, \"test_target\")\n\n        descriptors = data.to_numpy()\n\n        context.save(descriptors, \"train_descriptors\")\n        context.save(descriptors, \"test_descriptors\")\n\n    else:\n        descriptors = data.to_numpy()\n        context.save(descriptors, \"descriptors\")"
                        }
                    ],
                    "next": "7429a49e14b018e4545bc767",
                    "context": {},
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "type": "execution",
                    "name": "Data Standardize",
                    "head": false,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "7429a49e14b018e4545bc767",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "version": "3.8.6",
                        "isDefault": true,
                        "summary": "Python Script",
                        "name": "python",
                        "shortName": "py",
                        "build": "Default"
                    },
                    "executable": {
                        "_id": "7T9bjm96pi6NCR9bK",
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [],
                        "name": "python",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "applicationId": [
                            "Asst5sW8bwCgiWHk5",
                            "CZPivmgLwfxY7MoP6",
                            "RQhSm7uYJR23rtQoB",
                            "9GCM8NdYYHtvBjqsv",
                            "ZRsdFKKGsHZfNi45e",
                            "rgcuSWcSKLeLy2tYT"
                        ],
                        "createdAt": "2021-04-10T00:32:39.353Z",
                        "updatedAt": "2021-04-10T00:44:38.676Z"
                    },
                    "flavor": {
                        "_id": "4crbw8xrFbmTQwTw6",
                        "input": [
                            {
                                "name": "pre_processing_standardization_sklearn.py",
                                "templateId": "RM9JDLRs6DKkR26Rs"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:pre_processing:standardization:sklearn",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "executableId": "7T9bjm96pi6NCR9bK",
                        "createdAt": "2021-04-10T00:32:39.379Z",
                        "updatedAt": "2021-04-10T00:44:38.725Z"
                    },
                    "status": "finished",
                    "input": [
                        {
                            "_id": "RM9JDLRs6DKkR26Rs",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Sklearn Standard Scaler workflow unit                           #\n#                                                                   #\n#   This workflow unit scales the data such that it a mean of 0 and #\n#   a variance of 1. It then saves the data for use further down    #\n#   the road in the workflow, for use in un-transforming the data.  #\n#                                                                   #\n#   It is important that new predictions are made by scaling the    #\n#   new inputs using the mean and variance of the original training #\n#   set. As a result, the scaler gets saved in the Training phase.  #\n#                                                                   #\n#   During a predict workflow, the scaler is loaded, and the        #\n#   new examples are scaled using the stored scaler.                #\n# ----------------------------------------------------------------- #\n\n\nimport sklearn.preprocessing\n\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Descriptor Scaler\n        scaler = sklearn.preprocessing.StandardScaler\n        descriptor_scaler = scaler()\n        train_descriptors = descriptor_scaler.fit_transform(train_descriptors)\n        test_descriptors = descriptor_scaler.transform(test_descriptors)\n        context.save(descriptor_scaler, \"descriptor_scaler\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n        # Our target is only continuous if it's a regression problem\n        if settings.is_regression:\n            target_scaler = scaler()\n            train_target = target_scaler.fit_transform(train_target)\n            test_target = target_scaler.transform(test_target)\n            context.save(target_scaler, \"target_scaler\")\n            context.save(train_target, \"train_target\")\n            context.save(test_target, \"test_target\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Get the scaler\n        descriptor_scaler = context.load(\"descriptor_scaler\")\n\n        # Scale the data\n        descriptors = descriptor_scaler.transform(descriptors)\n\n        # Store the data\n        context.save(descriptors, \"descriptors\")",
                            "name": "pre_processing_standardization_sklearn.py",
                            "contextProviders": [],
                            "applicationName": "python",
                            "executableName": "python",
                            "isDefault": false,
                            "schemaVersion": "0.2.0",
                            "inSet": [],
                            "tags": [],
                            "createdAt": "2021-04-10T00:32:36.997Z",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Sklearn Standard Scaler workflow unit                           #\n#                                                                   #\n#   This workflow unit scales the data such that it a mean of 0 and #\n#   a variance of 1. It then saves the data for use further down    #\n#   the road in the workflow, for use in un-transforming the data.  #\n#                                                                   #\n#   It is important that new predictions are made by scaling the    #\n#   new inputs using the mean and variance of the original training #\n#   set. As a result, the scaler gets saved in the Training phase.  #\n#                                                                   #\n#   During a predict workflow, the scaler is loaded, and the        #\n#   new examples are scaled using the stored scaler.                #\n# ----------------------------------------------------------------- #\n\n\nimport sklearn.preprocessing\n\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Descriptor Scaler\n        scaler = sklearn.preprocessing.StandardScaler\n        descriptor_scaler = scaler()\n        train_descriptors = descriptor_scaler.fit_transform(train_descriptors)\n        test_descriptors = descriptor_scaler.transform(test_descriptors)\n        context.save(descriptor_scaler, \"descriptor_scaler\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n        # Our target is only continuous if it's a regression problem\n        if settings.is_regression:\n            target_scaler = scaler()\n            train_target = target_scaler.fit_transform(train_target)\n            test_target = target_scaler.transform(test_target)\n            context.save(target_scaler, \"target_scaler\")\n            context.save(train_target, \"train_target\")\n            context.save(test_target, \"test_target\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Get the scaler\n        descriptor_scaler = context.load(\"descriptor_scaler\")\n\n        # Scale the data\n        descriptors = descriptor_scaler.transform(descriptors)\n\n        # Store the data\n        context.save(descriptors, \"descriptors\")"
                        }
                    ],
                    "next": "d2e51a8241cd5c002d706927",
                    "context": {},
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "type": "execution",
                    "name": "Model Train and Predict",
                    "head": false,
                    "results": [
                        {
                            "name": "file_content",
                            "basename": "predictions.csv",
                            "filetype": "csv"
                        }
                    ],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "d2e51a8241cd5c002d706927",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "version": "3.8.6",
                        "isDefault": true,
                        "summary": "Python Script",
                        "name": "python",
                        "shortName": "py",
                        "build": "Default"
                    },
                    "executable": {
                        "_id": "7T9bjm96pi6NCR9bK",
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [],
                        "name": "python",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "applicationId": [
                            "Asst5sW8bwCgiWHk5",
                            "CZPivmgLwfxY7MoP6",
                            "RQhSm7uYJR23rtQoB",
                            "9GCM8NdYYHtvBjqsv",
                            "ZRsdFKKGsHZfNi45e",
                            "rgcuSWcSKLeLy2tYT"
                        ],
                        "createdAt": "2021-04-10T00:32:39.353Z",
                        "updatedAt": "2021-04-10T00:44:38.676Z"
                    },
                    "flavor": {
                        "_id": "gKkdeL5PzTjyPcCGo",
                        "input": [
                            {
                                "name": "model_mlp_sklearn.py",
                                "templateId": "QkweWeRibSZ2jRJcJ"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:model:multilayer_perceptron:sklearn",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "executableId": "7T9bjm96pi6NCR9bK",
                        "createdAt": "2021-04-10T00:32:39.396Z",
                        "updatedAt": "2021-04-10T00:44:38.753Z"
                    },
                    "tags": [
                        "remove-all-results",
                        "creates-predictions-csv-during-predict-phase"
                    ],
                    "status": "finished",
                    "input": [
                        {
                            "_id": "QkweWeRibSZ2jRJcJ",
                            "content": "# ---------------------------------------------------------------------------------------------------------- #\n#                                                                                                            #\n#   Workflow unit to train a simple feedforward neural network model on a regression problem                 #\n#   using Scikit-Learn. In this template, we use the default values for hidden_layer_sizes, activation,      #\n#   solver, and learning rate. Other parameters are available (consult the sklearn docs), but in this        #\n#   case, we only include those relevant to the Adam optimizer.                                              #\n#   Sklearn docs:http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html   #\n#                                                                                                            #\n#   When then workflow is in Training mode, the network is trained and the model is saved, along with the    #\n#   RMSE and some predictions made using the training data (e.g. for use in a parity plot or calculation     #\n#   of other error metrics).                                                                                 #\n#                                                                                                            #\n#   When the workflow is run in Predict mode, the network is loaded, predictions are made, they are          #\n#   un-transformed using the trained scaler from the training run, and they are written to a filed           #\n#   named \"predictions.csv\"                                                                                  #\n# ---------------------------------------------------------------------------------------------------------- #\n\nimport sklearn.neural_network\nimport sklearn.metrics\nimport numpy as np\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Flatten the targets\n        train_target = train_target.flatten()\n        test_target = test_target.flatten()\n\n        # Initialize the NN model\n        model = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,),\n                                                    activation=\"relu\",\n                                                    solver=\"adam\",\n                                                    max_iter=200,\n                                                    early_stopping=False,\n                                                    validation_fraction=0.1)\n\n        # Train the model and save\n        model.fit(train_descriptors, train_target)\n        context.save(model, \"sklearn_mlp\")\n        train_predictions = model.predict(train_descriptors)\n        test_predictions = model.predict(test_descriptors)\n\n        # Scale predictions so they have the same shape as the saved target\n        train_predictions = train_predictions.reshape(-1, 1)\n        test_predictions = test_predictions.reshape(-1, 1)\n        context.save(train_predictions, \"train_predictions\")\n        context.save(test_predictions, \"test_predictions\")\n\n        # Scale for RMSE calc on the test set\n        target_scaler = context.load(\"target_scaler\")\n        # Unflatten the target\n        test_target = test_target.reshape(-1, 1)\n        y_true = target_scaler.inverse_transform(test_target)\n        y_pred = target_scaler.inverse_transform(test_predictions)\n\n        # RMSE\n        mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n        rmse = np.sqrt(mse)\n        print(f\"RMSE = {rmse}\")\n        context.save(rmse, \"RMSE\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Restore model\n        model = context.load(\"sklearn_mlp\")\n\n        # Make some predictions and unscale\n        predictions = model.predict(descriptors)\n        predictions = predictions.reshape(-1, 1)\n        target_scaler = context.load(\"target_scaler\")\n\n        predictions = target_scaler.inverse_transform(predictions)\n\n        # Save the predictions to file\n        np.savetxt(\"predictions.csv\", predictions, header=\"prediction\", comments=\"\")",
                            "name": "model_mlp_sklearn.py",
                            "contextProviders": [],
                            "applicationName": "python",
                            "executableName": "python",
                            "isDefault": false,
                            "schemaVersion": "0.2.0",
                            "inSet": [],
                            "tags": [],
                            "createdAt": "2021-04-10T00:32:37.011Z",
                            "rendered": "# ---------------------------------------------------------------------------------------------------------- #\n#                                                                                                            #\n#   Workflow unit to train a simple feedforward neural network model on a regression problem                 #\n#   using Scikit-Learn. In this template, we use the default values for hidden_layer_sizes, activation,      #\n#   solver, and learning rate. Other parameters are available (consult the sklearn docs), but in this        #\n#   case, we only include those relevant to the Adam optimizer.                                              #\n#   Sklearn docs:http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html   #\n#                                                                                                            #\n#   When then workflow is in Training mode, the network is trained and the model is saved, along with the    #\n#   RMSE and some predictions made using the training data (e.g. for use in a parity plot or calculation     #\n#   of other error metrics).                                                                                 #\n#                                                                                                            #\n#   When the workflow is run in Predict mode, the network is loaded, predictions are made, they are          #\n#   un-transformed using the trained scaler from the training run, and they are written to a filed           #\n#   named \"predictions.csv\"                                                                                  #\n# ---------------------------------------------------------------------------------------------------------- #\n\nimport sklearn.neural_network\nimport sklearn.metrics\nimport numpy as np\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Flatten the targets\n        train_target = train_target.flatten()\n        test_target = test_target.flatten()\n\n        # Initialize the NN model\n        model = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,),\n                                                    activation=\"relu\",\n                                                    solver=\"adam\",\n                                                    max_iter=200,\n                                                    early_stopping=False,\n                                                    validation_fraction=0.1)\n\n        # Train the model and save\n        model.fit(train_descriptors, train_target)\n        context.save(model, \"sklearn_mlp\")\n        train_predictions = model.predict(train_descriptors)\n        test_predictions = model.predict(test_descriptors)\n\n        # Scale predictions so they have the same shape as the saved target\n        train_predictions = train_predictions.reshape(-1, 1)\n        test_predictions = test_predictions.reshape(-1, 1)\n        context.save(train_predictions, \"train_predictions\")\n        context.save(test_predictions, \"test_predictions\")\n\n        # Scale for RMSE calc on the test set\n        target_scaler = context.load(\"target_scaler\")\n        # Unflatten the target\n        test_target = test_target.reshape(-1, 1)\n        y_true = target_scaler.inverse_transform(test_target)\n        y_pred = target_scaler.inverse_transform(test_predictions)\n\n        # RMSE\n        mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n        rmse = np.sqrt(mse)\n        print(f\"RMSE = {rmse}\")\n        context.save(rmse, \"RMSE\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Restore model\n        model = context.load(\"sklearn_mlp\")\n\n        # Make some predictions and unscale\n        predictions = model.predict(descriptors)\n        predictions = predictions.reshape(-1, 1)\n        target_scaler = context.load(\"target_scaler\")\n\n        predictions = target_scaler.inverse_transform(predictions)\n\n        # Save the predictions to file\n        np.savetxt(\"predictions.csv\", predictions, header=\"prediction\", comments=\"\")"
                        }
                    ],
                    "next": "5fdc3cc7bbf164b9dd81c04e",
                    "context": {},
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                },
                {
                    "type": "execution",
                    "name": "Parity Plot",
                    "head": false,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "5fdc3cc7bbf164b9dd81c04e",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "version": "3.8.6",
                        "isDefault": true,
                        "summary": "Python Script",
                        "name": "python",
                        "shortName": "py",
                        "build": "Default"
                    },
                    "executable": {
                        "_id": "7T9bjm96pi6NCR9bK",
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [],
                        "name": "python",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "applicationId": [
                            "Asst5sW8bwCgiWHk5",
                            "CZPivmgLwfxY7MoP6",
                            "RQhSm7uYJR23rtQoB",
                            "9GCM8NdYYHtvBjqsv",
                            "ZRsdFKKGsHZfNi45e",
                            "rgcuSWcSKLeLy2tYT"
                        ],
                        "createdAt": "2021-04-10T00:32:39.353Z",
                        "updatedAt": "2021-04-10T00:44:38.676Z"
                    },
                    "flavor": {
                        "_id": "fAwEFiHABWEDjLbYF",
                        "input": [
                            {
                                "name": "post_processing_parity_plot_matplotlib.py",
                                "templateId": "AkEtY7aG3KwJEgap9"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:post_processing:parity_plot:matplotlib",
                        "isDefault": false,
                        "schemaVersion": "0.2.0",
                        "inSet": [],
                        "tags": [],
                        "executableId": "7T9bjm96pi6NCR9bK",
                        "createdAt": "2021-04-10T00:32:39.405Z",
                        "updatedAt": "2021-04-10T00:44:38.769Z"
                    },
                    "tags": [
                        "remove-all-results"
                    ],
                    "status": "finished",
                    "input": [
                        {
                            "_id": "AkEtY7aG3KwJEgap9",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Parity plot generation unit                                     #\n#                                                                   #\n#   This unit generates a parity plot based on the known values     #\n#   in the training data, and the predicted values generated        #\n#   using the training data.                                        #\n#                                                                   #\n#   Because this metric compares predictions versus a ground truth, #\n#   it doesn't make sense to generate the plot when a predict       #\n#   workflow is being run (because in that case, we generally don't #\n#   know the ground truth for the values being predicted). Hence,   #\n#   this unit does nothing if the workflow is in \"predict\" mode.    #\n# ----------------------------------------------------------------- #\n\n\nimport matplotlib.pyplot as plt\n\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_predictions = context.load(\"train_predictions\")\n        test_target = context.load(\"test_target\")\n        test_predictions = context.load(\"test_predictions\")\n\n        # Un-transform the data\n        target_scaler = context.load(\"target_scaler\")\n        train_target = target_scaler.inverse_transform(train_target)\n        train_predictions = target_scaler.inverse_transform(train_predictions)\n        test_target = target_scaler.inverse_transform(test_target)\n        test_predictions = target_scaler.inverse_transform(test_predictions)\n\n        # Plot the data\n        plt.scatter(train_target, train_predictions, c=\"#203d78\", label=\"Training Set\")\n        if settings.is_using_train_test_split:\n            plt.scatter(test_target, test_predictions, c=\"#67ac5b\", label=\"Testing Set\")\n        plt.xlabel(\"Actual Value\")\n        plt.ylabel(\"Predicted Value\")\n\n        # Scale the plot\n        target_range = (min(min(train_target), min(test_target)),\n                        max(max(train_target), max(test_target)))\n        predictions_range = (min(min(train_predictions), min(test_predictions)),\n                             max(max(train_predictions), max(test_predictions)))\n\n        limits = (min(min(target_range), min(target_range)),\n                  max(max(predictions_range), max(predictions_range)))\n        plt.xlim = (limits[0], limits[1])\n        plt.ylim = (limits[0], limits[1])\n\n        # Draw a parity line, as a guide to the eye\n        plt.plot((limits[0], limits[1]), (limits[0], limits[1]), c=\"black\", linestyle=\"dotted\", label=\"Parity\")\n        plt.legend()\n\n        # Save the figure\n        plt.savefig(\"my_parity_plot.png\", dpi=600)\n\n    # Predict\n    else:\n        # It might not make as much sense to draw a parity plot when predicting...\n        pass",
                            "name": "post_processing_parity_plot_matplotlib.py",
                            "contextProviders": [],
                            "applicationName": "python",
                            "executableName": "python",
                            "isDefault": false,
                            "schemaVersion": "0.2.0",
                            "inSet": [],
                            "tags": [],
                            "createdAt": "2021-04-10T00:32:37.018Z",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Parity plot generation unit                                     #\n#                                                                   #\n#   This unit generates a parity plot based on the known values     #\n#   in the training data, and the predicted values generated        #\n#   using the training data.                                        #\n#                                                                   #\n#   Because this metric compares predictions versus a ground truth, #\n#   it doesn't make sense to generate the plot when a predict       #\n#   workflow is being run (because in that case, we generally don't #\n#   know the ground truth for the values being predicted). Hence,   #\n#   this unit does nothing if the workflow is in \"predict\" mode.    #\n# ----------------------------------------------------------------- #\n\n\nimport matplotlib.pyplot as plt\n\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_predictions = context.load(\"train_predictions\")\n        test_target = context.load(\"test_target\")\n        test_predictions = context.load(\"test_predictions\")\n\n        # Un-transform the data\n        target_scaler = context.load(\"target_scaler\")\n        train_target = target_scaler.inverse_transform(train_target)\n        train_predictions = target_scaler.inverse_transform(train_predictions)\n        test_target = target_scaler.inverse_transform(test_target)\n        test_predictions = target_scaler.inverse_transform(test_predictions)\n\n        # Plot the data\n        plt.scatter(train_target, train_predictions, c=\"#203d78\", label=\"Training Set\")\n        if settings.is_using_train_test_split:\n            plt.scatter(test_target, test_predictions, c=\"#67ac5b\", label=\"Testing Set\")\n        plt.xlabel(\"Actual Value\")\n        plt.ylabel(\"Predicted Value\")\n\n        # Scale the plot\n        target_range = (min(min(train_target), min(test_target)),\n                        max(max(train_target), max(test_target)))\n        predictions_range = (min(min(train_predictions), min(test_predictions)),\n                             max(max(train_predictions), max(test_predictions)))\n\n        limits = (min(min(target_range), min(target_range)),\n                  max(max(predictions_range), max(predictions_range)))\n        plt.xlim = (limits[0], limits[1])\n        plt.ylim = (limits[0], limits[1])\n\n        # Draw a parity line, as a guide to the eye\n        plt.plot((limits[0], limits[1]), (limits[0], limits[1]), c=\"black\", linestyle=\"dotted\", label=\"Parity\")\n        plt.legend()\n\n        # Save the figure\n        plt.savefig(\"my_parity_plot.png\", dpi=600)\n\n    # Predict\n    else:\n        # It might not make as much sense to draw a parity plot when predicting...\n        pass"
                        }
                    ],
                    "context": {},
                    "statusTrack": [
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "active",
                            "repetition": 0
                        },
                        {
                            "trackedAt": 1618014959.0365415,
                            "status": "finished",
                            "repetition": 0
                        }
                    ]
                }
            ]
        }
    ],
    "isUsingDataset": true
}

