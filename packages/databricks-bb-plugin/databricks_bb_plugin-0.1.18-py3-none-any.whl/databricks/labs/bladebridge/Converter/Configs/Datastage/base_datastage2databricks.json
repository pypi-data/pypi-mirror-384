// base file for Datastage source
{
	"inherit_from" : ["base_etl2databricks_defs.json"],
	"script_header" : "# Databricks notebook source~
from datetime import datetime~
from pyspark.sql.functions import *~
from pyspark.sql.types import *~
from datetime import datetime~
from Mapplets import Mapplets~
starttime = datetime.now() #start timestamp of the script~
dbutils.widgets.text(name = 'starttime', defaultValue = str(starttime))~
~
~
# COMMAND ----------", // instruct the converter to generate this header and make it the default db.  Can be changed to anything else

	"comment_for_rowid_expression" : "-- for performance reasons, replace with list of natural key columns if known\n",

	"multistatement_separator" : ";[\s\n]*",
	"replicate_folder_stricture" : true,

	"generate_variable_declaration" : 1,
	//"variable_declaration_template" : "dbutils.widgets.text(name = '%VARNAME%', defaultValue = '%DEFAULT_VALUE%')\n%VARNAME% = dbutils.widgets.get(\"%VARNAME%\")\n",
	//"variable_declaration_comment" : "Variable declaration section",

	"dataset_creation_method" : "TABLE"
}
