{
    "inherit_from" : ["ic2dws.json", "base_infapc2databricks_pyspark.json"],
    // "header": "#Code converted on %CONVERTER_TIMESTAMP%\nimport os\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext;\nfrom pyspark.sql import SQLContext \nfrom pyspark.sql.session import SparkSession\nsc = SparkContext.getOrCreate()\nspark = SparkSession(sc)",
    
    "code_generation_module" : "CodeGeneration::PySpark",
    
        "header": "# Databricks notebook source~
# Code converted on %CONVERTER_TIMESTAMP%~
import os\nfrom pyspark.sql import *~
from pyspark.sql.functions import *~
from pyspark.sql.window import Window~
from pyspark.sql.types import *~
from datetime import datetime~
from pyspark.dbutils import DBUtils~
from delta.tables import DeltaTable~
from databricks_conversion_supplements import DatabricksConversionSupplements~
~
# COMMAND ----------~
~
# Set global variables~
starttime = datetime.now() #start timestamp of the script",

    "script_extension": "py",
    //"pre_node_line" : "# Processing node %NODE_NAME%, type %NODE_TYPE%\n# COLUMNS: %COLUMN_LIST%",
    "pre_node_line" : "# COMMAND ----------\n# Processing node %NODE_NAME%, type %NODE_TYPE%\n# COLUMN COUNT: %COLUMN_COUNT%\n", //# COLUMNS %COLUMN_LIST%
    "post_node_line" : "",
    "explicit_aliasing" : "1",
    "skip_rowid_generation" : "1", // omits generation of sys_row_id
    //"additional_trailing_fields" : ["load_cntl_no"],
    "column_aliasing_df_naming_pattern" : "PRE_%DF%", // in case column aliases are used, like prefixes and suffixes, create an additional dataframe with this name
    "implied_target_fields_enable_alpha_sort" : "1",
    //"force_multi_source_join_type" : "left_join",
    // use SelectExpr instead of pyspark
    // "use_selectExpr" : 1,
    // "remove_expression_comments" : "1", // removes inline comments in expressions before converting them

    // hook to convert target nodes to lowercase
    "CUSTOM_CONVERTER_MODULES" : ["target_lowercase_hook"],
    "initialize_hooks_call" : "::init_run",

    "component_handling" : {
        "TARGET" : "::special_handle_TARGET"
        //"JOINER" : "::special_handle_JOINER"
    },

    //use a template file.  this overrides any header/footer specs and the wrap specs below
    //"template_file" : "C:/Work/pyspark_template_v1.0.py",
    //"source_df_read_template" : "\t\t\t\t%DF% = dfs[%ZERO_BASED_SOURCE_INDEX%]",
    //"target_df_write_template" : "\t\t\t\t\ttrasfomedDF.append(%DF%)",
    //"source_target_exclude_from_body" : "1", //telling converter to exclude source and target code from the main body - because we need to have these statements placed separately in the pyspark code
    "field_rename_df_pattern" : "%DF%_TMP",
    "field_rename_df_comment" : "#Conforming layout of Hive to InfaCloud for %DF%",
    //"datatype_casting_catalog" : "C:/Users/Beso/Desktop/Gamma/datatype_casting_catalog.txt",
    //"object_path_catalog" : "C:/Users/Beso/Desktop/Gamma/object_path_catalog.txt",
    // "datatype_casting_catalog" : "C:/bladebridge/datatype_casting_catalog.txt",
    // "object_path_catalog" : "C:/bladebridge/object_path_catalog.txt",

    //"load_func":"def null_if(param1,param2):\n\tif param1 == param2:\n\t\treturn None\n\telse:\n\t\treturn param1",

    // Change filter to expr filter
    "convert_as_expr_pattens" : ["\bIF\b", "\bIIF\b", "\bDECODE\b"],
    "generate_router_as_expr" : "1",
    "generate_filter_as_expr" : "1",

    "sql_converter_config_file" : "mssql2sparksql.json",
    "etl_converter_config_file" : "base_infapc2pyspark.json",
    "expr_sql_converter_config_file" : "base_infa2databricks_expr.json",


    "expr_sql_converter_config_file" : "infa2databricks_expr.json",

	// "convert_as_expr_pattens" : ["\bIIF\b", "\bDECODE\b"],


    "exclude_from_lit_wrapping" : [
        "YYYY-MM-DD",
        "MM\/DD\/YYYY",
        "MM-DD-YYYY",
        "SYSDATE"
    ],

    "exclude_function_args_from_lit_wrapping" : [
        "REPLACESTR",
        "RPAD"
    ],

    "default_indent" : {
        "header" : "",
        "add_environmental_vars" : "",
        // "body" : "    ",
        "body" : "",
        "footer" : ""
    },
    
    // "body_wrap" : {
    //     "before" : "try:\n\n",
    //     "after" : "\n\nexcept Exception as e:\n\tprint(f'{type(e).__name__} at line {e.__traceback__.tb_lineno} of {dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()}: {e}')\n"
    // },
    
    "multiline_stmt_break" : " \ ",
    "null_assignment" : "lit(None).cast(NullType())",
    "sort_function" : "sort", //goes into the sorter node

    "expression_line_subst" : "1",
    
    "filter_subst" : {
        "LAST_N_DAYS" : { "expr" : "(((date_format(%TOKEN1%,'YYYY-MM-dd')==date_sub(date_format(current_timestamp(), 'YYYY-MM-dd'),%TOKEN2%))", "TOKEN1" : "(\w+)\s*=", "TOKEN2" : "\:(\d+)"},
        " IN\s*\(" : {"expr" : "%TOKEN1%.isin%TOKEN2", "TOKEN1" : "(.+)\s+in", "TOKEN2" : "in\s+(.+)"},
        " NOT IN\s*\(" : {"expr" : "%TOKEN1%.isin%TOKEN2 == False", "TOKEN1" : "(.+)\s+in", "TOKEN2" : "in\s+(.+)"}
    },

    //if threshold is met, introduce the registerTempTable code snippet
    "target_special_handling" : {
        "column_count_threshold" : "5000",
        "temp_df_name" : "%DF%_OUTPUT",
        "final_df_name" : "%DF%_FINAL",
        "final_df_population" : "sqlContext.sql('select * from %DF%')"
    },

    "default_flatfile_delimiter" : ",",

    "rowid_ref_expr_substitution" : "xxhash64('*')",

    "function_translation": { // inside pyspark select translate function name
        "UPPER" : "upper",
        "LOWER" : "lower",
        "RPAD" : "rpad" ,
        "LPAD" : "lpad" ,
        "RTRIM" : "rtrim" ,
        "Rtrim" : "rtrim" ,
        "RTrim" : "rtrim" ,
        "LTRIM" : "ltrim" ,
        "Ltrim" : "ltrim" ,
        "LTrim" : "ltrim" ,
        // "MAX" : "max" ,
        "SYSDATE" : "current_date" //this doesn't do anything
    },
    // TO_DATE(SUBSTR(SYSDATE,1,10),'MM-DD-YYYY')
    //TO_DATE(SUBSTR(SYSDATE , lit(1) , lit(10)) , lit('MM-DD-YYYY'))
    // TO_DATE(SUBSTR(lit(SYSDATE) , lit(1) , lit(10)) , 'MM-DD-YYYY')
    "final_file_visual_substitution" : [
        {"from" : "\bSYSDATE\.\w+\b", "to" : "(current_timestamp())"},
        // {"from" : "TO_DATE\(SUBSTR\(SYSDATE , lit\(1\) , lit\(10\)\) , lit\('MM-DD-YYYY'\)\)", "to" : "current_date"},
        // {"from" : "TO_DATE\(SUBSTR\(lit\(SYSDATE\) , lit\(1\) , lit\(10\)\) , 'MM-DD-YYYY'\)", "to" : "current_timestamp"}, worked
        {"from" : "YYYY-MM-DD HH24", "to" : "yyyy-MM-dd HH"},
        {"from" : "lit\(sysdate\)", "to" : "current_timestamp()"},
        {"from" : "\s*__DOT__\s*", "to" : "."}
    ],


    // "df_naming_template" : "%JOB_NAME%_%NODE_NAME%", //when not specified, the converter will use NODE_NAME
    "env_var_extraction": "os.environ.get('%VAR%')",

    "connection_code_translations" : {
        "Sample Salesforce Connection" : "SALESFORCE"
    },

    "cast_templates" : {
        "binary" : "%EXPR%.cast('binary')",
        "boolean" : "%EXPR%.cast('boolean')",
        "tinyint" : "%EXPR%.cast('tinyint')",
        "date" : "%EXPR%.cast('date')",
        "double" : "%EXPR%.cast('double')",
        "float" : "%EXPR%.cast('float')",
        "int" : "%EXPR%.cast('int')",
        "bigint" : "%EXPR%.cast('bigint')",
        "smallint" : "%EXPR%.cast('smallint')",
        "string" : "%EXPR%.cast('string')",
        "timestamp" : "%EXPR%.cast('timestamp')"
    }
}
