//this is a sample file for Datastage to PySpark conversion
//sequences are converted to databricks notebooks
{
	"inherit_from" : ["ds2dbks_subjob_instructions.json"],
	"script_extension" : "py",
	"pre_node_line" : "# COMMAND ----------\n# Processing node %NODE_NAME%, type %NODE_TYPE%\n# COLUMN COUNT: %COLUMN_COUNT%\n# %ADDITIONAL_COMMENT%", //# COLUMNS %COLUMN_LIST%

	"default_flatfile_delimiter" : ",",

	"pre_post_sql_wrapper" : "#Processing %PRE_POST_FLAG% for node %NODE_NAME%\nspark.sql(\"\"\"%INNER_SQL%\"\"\")",

	"pre_post_sql_subst" : [
		{"from" : ";", "to" : ""},
		{"from" : "spark\.sql\(\"\"\"INSERT\s+INTO\s+getArgument\s*\(\s*\"(\w+)\"\s*\)", "to" : "spark.sql(\"\"\" Truncate \"+$1+\" \"\"\")\n\nspark.sql(\"\"\""}
	],

	"multiline_stmt_break" : " ",
	"skip_rowid_generation" : "1",

	"exclude_function_args_from_lit_wrapping" : [
		"regexp_replace",
		"orderBy"
	],

	"random_value_expressions" : {
		"DATE" : "CURRENT_DATE",
		"DATETIME" : "CURRENT_TIMESTAMP",
		"TIMESTAMP" : "CURRENT_TIMESTAMP",
		"INT" : "100",
		"STRING" : "'A'",
		"DECIMAL" : "12345.67",
		"DEFAULT" : "0"
	},

	"generate_variable_declaration" : 1,
	//	"variable_declaration_template" : "dbutils.widgets.text(name = 'w%VARNAME%', defaultValue = '%DEFAULT_VALUE%')\ns%VARNAME% = dbutils.widgets.get(\"w%VARNAME%\")\n",
	"variable_declaration_template" : "dbutils.widgets.text(name = 'w%VARNAME%', defaultValue = '')\ns%VARNAME% = dbutils.widgets.get(\"w%VARNAME%\")\n",
	"variable_declaration_pattern" : "dbutils\.widgets\.text", //this is used but SUBJOB handler to eliminate duplicate declarations
	"variable_declaration_header" : "# COMMAND ----------\n# Variable_declaration_comment\n\ndbutils.widgets.text(name=\"wDebug\", defaultValue=\"FALSE\")\nsDebug = dbutils.widgets.get(\"wDebug\").upper()\n",
	//"skip_variable_patterns" : [ "_DIR", "DSN", "PASSWORD", "USER" ], // skip vars that have these naming patterns, case insensitive
	//"variable_assignment_template" : "%VARNAME% = spark.sql(\"\"\"SELECT %EXPRESSION% end as COL\"\"\")\n%VARNAME% = %VARNAME%.select(('COL')).first()[0]\n", // used by SET_VARIABLE handler in PySpark
	"GET_ARGUMENT_IN_SET_VARIABLE" : false, //use this when we need to substitute args
	//"header" : "# MAGIC %md~
	//# MAGIC~
	//# Folder %FOLDER%~
	//# Job %JOBNAME%~
	//# MAGIC~
	//#~
	//#~
	"header": "#Code converted on %CONVERTER_TIMESTAMP%\nimport os\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext\nfrom pyspark.sql.session import SparkSession\nsc = SparkContext('local')\nspark = SparkSession(sc)\n",
	//"footer": "quit()",

	"default_indent" : {
		"header" : "",
		//"body" : "\t",
		"footer" : ""
	},

	"body_wrap" : {
		"before" : ""
		//"after" : "\n\nexcept OSError:\n\tprint('Error Occurred')\n"
	},

	"operator_to_function_subst" : { //converting operators to functions
		":" : "concat"
	},
	// expression syntax handling using BB's standard parser

	"line_subst" : [
		{"from" : "cast\s*\(\s*([\s\S]+)\s*as\s*(\w+)\s*\)", "to" : "$1.cast($2)"},
		{"from" : "\bnull\b", "to" : "None"},
		{"from" : "\.equals\s*\(\"(.*?)\"\s*\)", "to" : " == \"$1\""},
		{"from" : "\.equals\s*\(lit\(\"(.*)\"\s*\)\)", "to" : " == lit(\"$1\")"},
		//{"from" : "context.(\w+)\b", "to" : "os.environ['$1']"},
		{"from" : ".INROWNUM", "to" : "monotonically_increasing_id()"},
		{"from" : ".PARTITIONNUM", "to" : "monotonically_increasing_id()"},
		{"from" : "\bIsNull\b", "to" : "isnull", "case_sensitive_match" : "1"},
		{"from" : "<>", "to" : "!="},
		{"from" : "\bor\b",  "to" : "|", "exclude_categories" : ["PYSPARK_FILTER"]},
		{"from" : "\band\b",  "to" : "&", "exclude_categories" : ["PYSPARK_FILTER"]},
		{"from" : "trim\s*\(\s*([\w|\.]*)\s*,\s*'(.*?)'", "to" : "trim($1, $2"} // get rid of single quotes in the 2nd arg of trim function. prepare for regexp_replace function_subst
	],

	"function_subst" : [
		{"from" : "IF", "to" : "IIF"},
		{"from" : "StringHandling.TRIM", "to" : "trim"},
		{"from" : "routines.DateConversion.julianToGregorian", "output_template" : "datetime.datetime.strptime($1, '%y%j').date().strftime('%Y-%m-%d')"},
		{"from" : "CurrentDate", "to" : "current_date"},
		{"from" : "DaysSinceFromDate", "output_template" : "datediff($1,$2)"},
		{"from" : "TRIM", "arg_pattern" : {"3" : "L"},  "output_template" : "regexp_replace($1, r'^[$2]*', '')"},
		{"from" : "TRIM", "to" : "trim"},
		{"from" : "TRIMLEADINGTRAILING", "to" : "trim"},
		{"from" : "generate", "output_template" : "monotonically_increasing_id()"}
	],

	// embedded sql conversion
	//,"sql_converter_config_file" : "!BB_CONFIG_CONVERTER!/db22.databricks.json"
	"etl_converter_config_file" : "!BB_CONFIG_CONVERTER!/datastage2deltalake.json",
	"sql_converter_config_file" : "C:/Work/Projects/DWS/BBPerl/backend-perl-shared/Config/Converter/sql_conv.json",
	"source_sql_converter_config_file" : "C:/Work/Projects/DWS/BBPerl/backend-perl-shared/Config/Converter/sql_conv_source.json",

	"generate_individual_subjobs" : true,
	"place_subjob_body_into_workflow" : false,
	// "subjob_script_extension" : "py", // so we can search for it in the target folder.
	"subjob_conversion_command" : "%EXE% %OPT_c% %OPT_d% %OPT_g% %OPT_o% %OPT_M% -u %CONFIG% %OPT_v% -f %SUBJOB_NAME%.xml", //%OPT_letter% means the original option
	// //"SUBJOB_main_config_file" : "!BB_CONFIG_CONVERTER!/ds2dbks_main_no_markdown.json",
	"subjob_config_file" : { //pattern based.  we want to execute a specific CONFIG for nested sequences, and default for everything else
		"SEQ" : "C:/Work/Projects/DWS/BBPerl/backend-perl-shared/Config/Converter/ds2dbks_seq_markdown_parallel_nomarkdown.json",
	 	"DEFAULT" : "C:/Work/Projects/DWS/BBPerl/backend-perl-shared/Config/Converter/ds2dbks_main_no_markdown.json"
	},

	//"use_generic_workflow_builder" : 1,
	"workflow_specs" : { // will kick in only if use_generic_workflow_builder is on
		//"workflow_class" : "CodeGeneration::DatabricksJobs",
		// any other attributes on the job level (tags)
		"workflow_component_mapping" : {
			"SESSION" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"notebook_task" : {
			 		"notebook_path" : "notebookTestPath/%MAPPING_NAME%.py"
			 	}
			},
			"SUBJOB" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"notebook_task" : {
			 		"notebook_path" : "notebookTestPath/%MAPPING_NAME%.py"
			 	}
			},
			"COMMAND" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			},
			"SET_VARIABLE" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			},
			"DIE" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			},
			"SYNCHRONIZE" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			},
			"CONTROL" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			},
			"DECISION" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			},
			"ASSIGNMENT" : {
				"task_key" : "%COMPONENT_NAME%",
				"description" : "%DESCRIPTION%",
				"spark_python_task" : {
			 		"python_file" : "somePath/%COMPONENT_NAME%.py"
			 	}
			}
		},

		"default_workflow_attr" : {
			"tags" : {
				"cost-center": "engineering",
				"team": "jobs"
			},
			"job_clusters" : [
				{
					"job_cluster_key": "auto_scaling_cluster",
					"new_cluster": {}
				}
			],
			"email_notifications" :  {
				"on_start": [
					"user.name@databricks.com"
				],
				"on_success": [
					"user.name@databricks.com"
				],
				"on_failure": [
					"user.name@databricks.com"
				],
				"no_alert_for_skipped_runs": false
			},
			"timeout_seconds" : 86400,
			"schedule" : {
				"quartz_cron_expression": "20 30 * * * ?",
				"timezone_id": "Europe/London",
				"pause_status": "PAUSED"
			},
			"max_concurrent_runs" : 10,
			"git_source" : null,
			"format" : "MULTI_TASK",
			"access_control_list" : [
				{
					"user_name": "jsmith@example.com",
					"permission_level": "CAN_MANAGE"
				}
			]
		},

		"default_task_attr" : {
			"timeout_seconds" : 86400,
			"max_retries" : 3,
			"min_retry_interval_millis" : 2000,
			"retry_on_timeout" : false
		},

		"output_workflow_filename_template" : "%JOB_NAME%.py",
		//"script_header" : "#this is a standard header for workflow %WORKFLOW_NAME%\n\ttry:", // can also provide "script_header_template" instead
		"script_header_template" : "!BB_CONFIG_WRITER_DIR!/Spark/pyspark_airflow_workflow_header.py",
		//"script_footer" : "\texcept:\n\t\tprint('An exception occurred')", // can also provide "script_footer_template" instead
		//"code_indent" : "            ", //code indent for workflow code - 12 spaces
		//"skip_component_name_patterns" : ["audit"],
		"skip_component_types" : ["email", "start"],

		"workflow_component_template_START" : "%WORKFLOW_TEMPLATE_PATH%/START_template.py",
		"workflow_component_template_SESSION" : "!BB_CONFIG_WRITER_DIR!/Spark/pyspark_airflow_SESSION.py",
		"workflow_component_template_COMMAND" : "!BB_CONFIG_WRITER_DIR!/Spark/databricksJobs_template_COMMAND.py",
		"workflow_component_template_SET_VARIABLE" : "!BB_CONFIG_WRITER_DIR!/Spark/databricksJobs_template_SET_VARIABLE.py",
		"workflow_component_template_DIE" : "!BB_CONFIG_WRITER_DIR!/Spark/databricksJobs_template_DIE.py",
		"workflow_component_template_SYNCHRONIZE" : "!BB_CONFIG_WRITER_DIR!/Spark/databricksJobs_template_SYNCHRONIZE.py",
		"workflow_component_template_EMAIL" : "%WORKFLOW_TEMPLATE_PATH%/EMAIL_template.py",
		"workflow_component_template_ASSIGNMENT" : "!BB_CONFIG_WRITER_DIR!/Spark/ASSIGNMENT_template.py",
		"workflow_component_template_CONTROL" : "!BB_CONFIG_WRITER_DIR!/Spark/CONTROL_template.py",
		"workflow_component_template_DECISION" : "!BB_CONFIG_WRITER_DIR!/Spark/DECISION_template.py",

		"flow_start" : "\n########### Flow definition ###########\n",
		"dependency_instruction_template" : "%COMPONENT_NAME% << %UPSTREAM_COMPONENT_LIST%",

		"component_list_spec" : {
			"list_enclosure" : "[,]", //specify start character sequence before comma and closing char sequence after comma
			"single_item_use_enclosure_flag" : "0"
		}
		//parameter passing.
		//"single_entry_template_COMMAND" : "#Command label %TOKEN1%\nos.system('%TOKEN2%')",
		//"single_entry_template_ASSIGNMENT" : "%TOKEN1% = %TOKEN2%"
	}
}
