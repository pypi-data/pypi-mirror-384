// base file for InfaPowercenter source
{
	"inherit_from" : ["base_etl2databricks_defs.json"],
	"script_header" : "# Databricks notebook source~
from datetime import datetime~
from pyspark.sql.functions import *~
from pyspark.sql.types import *~
from datetime import datetime~
from Mapplets import Mapplets~
starttime = datetime.now() #start timestamp of the script~
dbutils.widgets.text(name = \"starttime\", defaultValue = str(starttime))~
~
~
# COMMAND ----------", // instruct the converter to generate this header and make it the default db.  Can be changed to anything else

	"retrieve_rowid_ind" : "1", //retrieves unique row identifier from source
	"skip_variable_token_prefixes" : 1, //infa specific, keep it on
	"rowid_column_name" : "source_record_id",
	"comment_for_rowid_expression" : "-- for performance reasons, replace with list of natural key columns if known\n",
	//"rowid_expression" : "xxhash64(%DELIMITED_COLUMN_LIST%) as %ROWID_COL_NAME%",
	//"rowid_column_wrap" : "nvl(%COLUMN_NAME%,'__%COLUMN_NAME%__')", // do not enable this here
	"rowid_expression_use_pk_when_possible" : 1,

	"multistatement_separator" : ";[\s\n]*",
	"code_fragment_breakers": {
		"line_end": [";"]
	},

	"generate_variable_declaration" : 1,
	//"variable_declaration_template" : "dbutils.widgets.text(name = '%VARNAME%', defaultValue = '%DEFAULT_VALUE%')\n%VARNAME% = dbutils.widgets.get(\"%VARNAME%\")\n",
	//"variable_declaration_comment" : "Variable declaration section",

	"dataset_creation_method" : "TABLE"
}
