<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>scan API documentation</title>
<meta name="description" content="Module for scanning and extracting data from aplog-generated files." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scan</code></h1>
</header>
<section id="section-intro">
<p>Module for scanning and extracting data from aplog-generated files.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34; Module for scanning and extracting data from aplog-generated files.
&#34;&#34;&#34;
import sys, time, argparse, os
from timeit import default_timer as timer
#from pprint import pprint
import bisect
import numpy as np
from io import BytesIO
import msgpack
__version__ = &#39;v2.0.3 2021-08-11&#39;#

#````````````````````````````Globals``````````````````````````````````````````
Nano = 0.000000001
TimeFormat_in = &#39;%y%m%d_%H%M%S&#39;
TimeFormat_out = &#39;%y%m%d_%H%M%S&#39;
#````````````````````````````Helper functions`````````````````````````````````
def _printv(msg):
    if APScan.Verbosity &gt;= 1:
        print(f&#39;DBG_APSV: {msg}&#39;)
def _printvv(msg):
    if APScan.Verbosity &gt;= 2 :
        print(f&#39;DBG_APSVV: {msg}&#39;)

def _croppedText(txt, limit=200):
    if len(txt) &gt; limit:
        txt = txt[:limit]+&#39;...&#39;
    return txt

def _seconds2Datetime(ns:int):
    from datetime import datetime
    dt = datetime.fromtimestamp(ns*Nano)
    return dt.strftime(&#39;%y%m%d_%H%M%S&#39;) 

def _timeInterval(startTime, span):
    &#34;&#34;&#34;returns sections (string) and times (float) of time interval
    boundaries&#34;&#34;&#34;
    ttuple = time.strptime(startTime,TimeFormat_in)
    firstDataSection = time.strftime(TimeFormat_out, ttuple)
    startTime = time.mktime(ttuple)
    endTime = startTime +span
    endTime = min(endTime, 4102462799.)# 2099-12-31
    ttuple = time.localtime(endTime)
    endSection = time.strftime(TimeFormat_out, ttuple)
    return firstDataSection, int(startTime/Nano), endSection, int(endTime/Nano)

def _unpacknp(data):
    if not isinstance(data,(tuple,list)):
        return data
    if len(data) != 2:# expect two arrays: times and values
        return data
    #print( _croppedText(f&#39;unp: {data}&#39;))
    unpacked = []
    for i,item in enumerate(data):
        try:
            dtype = item[&#39;dtype&#39;]
            shape = item[&#39;shape&#39;]
            buf = item[&#39;bytes&#39;]
            arr = np.frombuffer(buf, dtype=dtype).reshape(shape)
            if i == 0:
                arr = arr * Nano#
            unpacked.append(arr)
        except Exception as e:
            print(f&#39;Exception in iter: {e}&#39;)
            if i == 0:
                print(f&#39;ERR in unpacknp: {e}&#39;)
                return data
            else:
                print(&#39;not np-packed data&#39;)
                unpacked.append(data)
    #print( _croppedText(f&#39;unpacked: {len(unpacked[0])} of {unpacked[0].dtype}, {len(unpacked[1])} of {unpacked[1].dtype}&#39;))
    return unpacked
#,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#````````````````````````````class APView`````````````````````````````````````
class APScan():
    Verbosity = 0
    &#34;&#34;&#34;Show dedugging messages.&#34;&#34;&#34;

    def __init__(self, fileName):
        &#34;&#34;&#34;Open logbook fileName, unpack headers, position file to data sections.&#34;&#34;&#34;
        self.logbookName = fileName
        try:
            self.logbookSize = os.path.getsize(fileName)
        except Exception as e:
            print(f&#39;ERROR opening file {fileName}: {e}&#39;)
            sys.exit()
        self.logbook = open(fileName,&#39;rb&#39;)

        # unpack logbook contents and set file position after it
        self.unpacker = msgpack.Unpacker(self.logbook, use_list=False
        ,strict_map_key=False) #use_list speeds up 20%, # does not help:, read_size=100*1024*1024)
        self.dirSize = 0
        self.directory = []
        for contents in self.unpacker:
            _printvv(_croppedText(f&#39;Table of contents: {contents}&#39;))
            try:
                self.dirSize = contents[&#39;contents&#39;][&#39;size&#39;]
            except:
                print(&#39;Warning: Table of contents is missing or wrong&#39;)
                break
            self.directory = contents[&#39;data&#39;]
            break

        # unpack two sections after the contents: Abstract and Index
        self.position = self.dirSize
        self.logbook.seek(self.position)
        self.unpacker = msgpack.Unpacker(self.logbook, use_list=False
        ,strict_map_key=False) #use_lis=False speeds up 20%
        nSections = 0
        for section in self.unpacker:
            #print(f&#39;section:{nSections}&#39;)
            nSections += 1
            if nSections == 1:# section: Abstract
                _printvv(f&#39;Abstract@{self.logbook.tell()}: {section}&#39;)
                self.abstract = section[&#39;abstract&#39;]
                self.compression = self.abstract.get(&#39;compression&#39;)
                if self.compression is None:
                    continue
                if self.compression != &#39;None&#39;:
                    module = __import__(self.compression)
                    self.decompress = module.decompress
                continue
            if nSections == 2:# section: Index
                #_printvv(f&#39;Index@{self.logbook.tell()}: {section}&#39;)
                par2key = section[&#39;index&#39;]
                #self.key2par = {value:key for key,value in self.par2key.items()}
                self.key2par = par2key
                _printvv(f&#39;Index@{self.logbook.tell()}: {self.key2par}&#39;)                
                break

    def get_headers(self):
        &#34;&#34;&#34;Returns dict of header sections: Directory, Abstract, Index&#34;&#34;&#34;
        return {&#39;Directory&#39;:self.directory, &#39;Abstract&#39;:self.abstract
        , &#39;Index&#39;:self.key2par}

    def extract_objects(self, span=0., items=[], startTime=None
        , bufSize=128*1024*1024):
        &#34;&#34;&#34;
        Returns correlated dict of times and values of the logged items during
        the selected time interval.
        
        **span**:   Time interval for data extraction in seconds. If 0, then
                the data will be extracted starting from the startTime and
                ending at the end of the logbook.
        
        **items**:  List of integer indexes of items to extract. The map of
         indexes to Control System parameters could be obtained using 
         get_headers()[&#39;Index&#39;].
        
        **startTime**: String for selecting start of the extraction interval. 
                Format: YYMMDD_HHMMSS. If None then extraction starts from the
                beginning. 

        **bufSize**:  Size of the bytesIO buffer. If file size is smaller than
                the bufSize, then the whole file will be read into the buffer.
                Otherwise each section will be read from the file sequentially.
                Note, the Python3 read() for binary files is using very
                effective buffering scheme, therefore using very large bufSize
                have almost no effect on performance.&#34;&#34;&#34;

        extracted = {}
        parameterStatistics = {}
        endPosition = self.logbookSize
        readerBufferSize = bufSize

        # create empty map for return
        if len(items) == 0: # enable handling of all items 
            #items = self.key2par.keys()
            items = [i for i in range(len(self.key2par))]
        #for key,par in self.key2par.items():
        for key,par in enumerate(self.key2par):
            if key not in parameterStatistics:
                #print(f&#39;add to stat[{len(parameterStatistics)+1}]: {key}&#39;) 
                parameterStatistics[key] = 0
            if par not in extracted and key in items:
                _printvv(f&#39;add extracted[{len(extracted)+1}]: {par}&#39;) 
                extracted[key] = {&#39;par&#39;:par, &#39;times&#39;:[], &#39;values&#39;:[]}
    
        if len(self.directory) == 0:
               print(&#39;ERROR. Directory is missing&#39;)
               sys.exit()

        # determine a part of the logbook for extraction
        keys = list(self.directory.keys())
        if startTime is  None:
            firstTStamp = keys[0]
            startTime = _seconds2Datetime(firstTStamp)
        firstDataSection, startTStamp, endSection, endTStamp\
        = _timeInterval(startTime, span)
        _printv(f&#39;start,end:{firstDataSection, int(startTStamp*Nano), endSection, int(endTStamp*Nano)}&#39;)

        # position logbook to first data section
        lk = len(keys)
        bt = timer()
        # find nearest_key ising bisect, that is fast, ~10us
        startSection_idx = bisect.bisect_left(keys, startTStamp)
        #print(f&#39;nidx: {startSection_idx,startTStamp,endTStamp}&#39;)
        startSectionTStamp = keys[startSection_idx]
        if startSectionTStamp &gt; startTStamp:
            startSection_idx -= 1
            startSectionTStamp = keys[max(startSection_idx,0)]
        endTStamp = startTStamp + span/Nano
        nearest_idx = min(bisect.bisect_left(keys, endTStamp),lk-1)
        lastSectionTStamp = keys[nearest_idx]
        if lastSectionTStamp &lt; endTStamp:
            lastSectionTStamp = keys[min(nearest_idx+1,lk-1)]
        self.position = self.directory[startSectionTStamp]
        endPosition = self.directory[lastSectionTStamp]
        _printvv(f&#39;first dsection {self.position}&#39;)
        _printvv(f&#39;last dsection {endPosition}&#39;)
        self.logbook.seek(self.position)
        _printvv(f&#39;logbook@{self.logbook.tell()}, offset={self.dirSize}&#39;)

        # Try to read required sections into a buffer. If successful, then
        # the streamReader for unpacker will be this buffer, otherwise
        # it will be the logbook file.
        toRead =  endPosition - self.logbook.tell()
        if toRead &lt; readerBufferSize:
            ts = timer()
            rbuf = self.logbook.read(toRead)
            ts1 = timer()
            dt1 = round(ts1 - ts,6)
            streamReader = BytesIO(rbuf)
            dt2 = round(timer() - ts1,6)
            print(f&#39;Read {round(toRead/1e6,3)}MB in {dt1}s, adopted in {dt2}s&#39;)
            _printv(f&#39;Read {round(toRead/1e6,3)}MB in {dt1}s, adopted in {dt2}s&#39;)
        else:
            print((f&#39;Read size {round(toRead/1e6,1)}MB &gt;&#39;
            f&#39; {round(readerBufferSize/1e6,1)}MB&#39;
            &#39;, processing it sequentially&#39;))
            streamReader = self.logbook

        # re-create the Unpacker to re-position it in the logbook
        self.unpacker = msgpack.Unpacker(streamReader, use_list=False
        ,strict_map_key=False) #use_list=False speeds up 20%

        # loop over sections in the logbook
        nSections = 0
        if APScan.Verbosity &gt;= 1:
            sectionTime = [0.]*3
        startTStampNS = startTStamp
        endTStampNS = endTStamp
        print(f&#39;sts,ets:{startTStampNS,endTStampNS}&#39;)
        extractionTime = 0.
        perfMonTime = 0.
        timerTotal = timer()
        for section in self.unpacker:
            extractionTStart = timer()
            nSections += 1
            # data sections
            _printv(f&#39;Data Section: {nSections+startSection_idx}&#39;)
            if nSections%60 == 0:
                dt = time.time() - extractionTime
                _printv((f&#39;Data sections: {nSections}&#39;
                f&#39;, elapsed time: {round(dt,4)}&#39;))#, paragraphs/s: {nParagraphs//dt}&#39;))
            try:# handle compressed data
                if self.compression != &#39;None&#39;:
                    ts = timer()
                    decompressed = self.decompress(section)
                    if APScan.Verbosity &gt;= 1:
                        sectionTime[0] += timer() - ts
                    ts = timer()
                    section = msgpack.unpackb(decompressed
                    ,strict_map_key=False)#ISSUE: strict_map_key does not work here
                    if APScan.Verbosity &gt;= 1:
                        sectionTime[1] += timer() - ts
            except Exception as e:
                print(f&#39;WARNING: wrong section {nSections}: {str(section)[:75]}...&#39;, {e})
                break
            _printv(f&#34;Data section {nSections}: {section[&#39;tstart&#39;]}&#34;)

            # iterate over parameters
            ts = timer()
            try:
                # the following loop takes 90% time
                for parIndex, tsValsNP in section[&#39;pars&#39;].items():
                    if not parIndex in items:
                        continue
                    tstamps, values = _unpacknp(tsValsNP)

                    # trim array if needed
                    if tstamps[0] &lt; startTStampNS:
                        first = bisect.bisect_left(tstamps, startTStampNS)
                        tstamps = tstamps[first:]
                        values = values[first:]
                    try:
                        if tstamps[-1] &gt; endTStampNS:
                            last = bisect.bisect_left(tstamps, endTStampNS)
                            tstamps = tstamps[:last]
                            values = values[:last]
                    except: pass
                    if APScan.Verbosity &gt;= 2:
                        print( _croppedText(f&#39;times{parIndex}[{len(tstamps)}]: {tstamps}&#39;))
                        try:    vshape = f&#39;of numpy arrays {values.dtype,values.shape}&#39;
                        except: vshape = &#39;&#39;
                        print(f&#39;vals{parIndex}[{len(values)}] {vshape}:&#39;)
                        print( _croppedText(f&#39;{values}&#39;))

                                        #`````````Concatenation of parameter lists.``````````````
                    # Using numpy.concatenate turned to be very slow.
                    # The best performance is using list.extend() 
                    extracted[parIndex][&#39;times&#39;].extend(list(tstamps))
                    ts2 = timer()
                    extracted[parIndex][&#39;values&#39;].extend(list(values))
                    perfMonTime += timer() - ts2
                    #,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,

                    n = len(extracted[parIndex][&#39;times&#39;])
                    _printvv(f&#34;par{parIndex}[{n}]&#34;)
                    parameterStatistics[parIndex] = n

            except Exception as e:
                print(f&#39;WARNING: in concatenation: {e}&#39;)

            dts = timer() - ts
            if APScan.Verbosity &gt;= 1:
                sectionTime[2] += dts
            extractionTime += timer() - extractionTStart

        if APScan.Verbosity &gt;= 1:
            print(f&#39;SectionTime: {[round(i/nSections,6) for i in sectionTime]}&#39;)
        print(f&#39;Deserialized from {self.logbookName}: {nSections} sections&#39;)
        print(f&#39;Sets/Parameter: {parameterStatistics}&#39;)
        ttime = timer()-timerTotal
        mbps = (f&#39; {round(toRead/1e6/extractionTime,1)} MB/s&#39;
        f&#39;, including disk: {round(ttime,3)} s, {round(toRead/1e6/ttime,1)} MB/s&#39;)
        print(f&#39;Processing time: {round(extractionTime,3)} s, {mbps}&#39;)
        print(f&#39;Spent {round(perfMonTime/extractionTime*100,1)}% in the monitored code.&#39;)
        return extracted</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scan.APScan"><code class="flex name class">
<span>class <span class="ident">APScan</span></span>
<span>(</span><span>fileName)</span>
</code></dt>
<dd>
<div class="desc"><p>Open logbook fileName, unpack headers, position file to data sections.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class APScan():
    Verbosity = 0
    &#34;&#34;&#34;Show dedugging messages.&#34;&#34;&#34;

    def __init__(self, fileName):
        &#34;&#34;&#34;Open logbook fileName, unpack headers, position file to data sections.&#34;&#34;&#34;
        self.logbookName = fileName
        try:
            self.logbookSize = os.path.getsize(fileName)
        except Exception as e:
            print(f&#39;ERROR opening file {fileName}: {e}&#39;)
            sys.exit()
        self.logbook = open(fileName,&#39;rb&#39;)

        # unpack logbook contents and set file position after it
        self.unpacker = msgpack.Unpacker(self.logbook, use_list=False
        ,strict_map_key=False) #use_list speeds up 20%, # does not help:, read_size=100*1024*1024)
        self.dirSize = 0
        self.directory = []
        for contents in self.unpacker:
            _printvv(_croppedText(f&#39;Table of contents: {contents}&#39;))
            try:
                self.dirSize = contents[&#39;contents&#39;][&#39;size&#39;]
            except:
                print(&#39;Warning: Table of contents is missing or wrong&#39;)
                break
            self.directory = contents[&#39;data&#39;]
            break

        # unpack two sections after the contents: Abstract and Index
        self.position = self.dirSize
        self.logbook.seek(self.position)
        self.unpacker = msgpack.Unpacker(self.logbook, use_list=False
        ,strict_map_key=False) #use_lis=False speeds up 20%
        nSections = 0
        for section in self.unpacker:
            #print(f&#39;section:{nSections}&#39;)
            nSections += 1
            if nSections == 1:# section: Abstract
                _printvv(f&#39;Abstract@{self.logbook.tell()}: {section}&#39;)
                self.abstract = section[&#39;abstract&#39;]
                self.compression = self.abstract.get(&#39;compression&#39;)
                if self.compression is None:
                    continue
                if self.compression != &#39;None&#39;:
                    module = __import__(self.compression)
                    self.decompress = module.decompress
                continue
            if nSections == 2:# section: Index
                #_printvv(f&#39;Index@{self.logbook.tell()}: {section}&#39;)
                par2key = section[&#39;index&#39;]
                #self.key2par = {value:key for key,value in self.par2key.items()}
                self.key2par = par2key
                _printvv(f&#39;Index@{self.logbook.tell()}: {self.key2par}&#39;)                
                break

    def get_headers(self):
        &#34;&#34;&#34;Returns dict of header sections: Directory, Abstract, Index&#34;&#34;&#34;
        return {&#39;Directory&#39;:self.directory, &#39;Abstract&#39;:self.abstract
        , &#39;Index&#39;:self.key2par}

    def extract_objects(self, span=0., items=[], startTime=None
        , bufSize=128*1024*1024):
        &#34;&#34;&#34;
        Returns correlated dict of times and values of the logged items during
        the selected time interval.
        
        **span**:   Time interval for data extraction in seconds. If 0, then
                the data will be extracted starting from the startTime and
                ending at the end of the logbook.
        
        **items**:  List of integer indexes of items to extract. The map of
         indexes to Control System parameters could be obtained using 
         get_headers()[&#39;Index&#39;].
        
        **startTime**: String for selecting start of the extraction interval. 
                Format: YYMMDD_HHMMSS. If None then extraction starts from the
                beginning. 

        **bufSize**:  Size of the bytesIO buffer. If file size is smaller than
                the bufSize, then the whole file will be read into the buffer.
                Otherwise each section will be read from the file sequentially.
                Note, the Python3 read() for binary files is using very
                effective buffering scheme, therefore using very large bufSize
                have almost no effect on performance.&#34;&#34;&#34;

        extracted = {}
        parameterStatistics = {}
        endPosition = self.logbookSize
        readerBufferSize = bufSize

        # create empty map for return
        if len(items) == 0: # enable handling of all items 
            #items = self.key2par.keys()
            items = [i for i in range(len(self.key2par))]
        #for key,par in self.key2par.items():
        for key,par in enumerate(self.key2par):
            if key not in parameterStatistics:
                #print(f&#39;add to stat[{len(parameterStatistics)+1}]: {key}&#39;) 
                parameterStatistics[key] = 0
            if par not in extracted and key in items:
                _printvv(f&#39;add extracted[{len(extracted)+1}]: {par}&#39;) 
                extracted[key] = {&#39;par&#39;:par, &#39;times&#39;:[], &#39;values&#39;:[]}
    
        if len(self.directory) == 0:
               print(&#39;ERROR. Directory is missing&#39;)
               sys.exit()

        # determine a part of the logbook for extraction
        keys = list(self.directory.keys())
        if startTime is  None:
            firstTStamp = keys[0]
            startTime = _seconds2Datetime(firstTStamp)
        firstDataSection, startTStamp, endSection, endTStamp\
        = _timeInterval(startTime, span)
        _printv(f&#39;start,end:{firstDataSection, int(startTStamp*Nano), endSection, int(endTStamp*Nano)}&#39;)

        # position logbook to first data section
        lk = len(keys)
        bt = timer()
        # find nearest_key ising bisect, that is fast, ~10us
        startSection_idx = bisect.bisect_left(keys, startTStamp)
        #print(f&#39;nidx: {startSection_idx,startTStamp,endTStamp}&#39;)
        startSectionTStamp = keys[startSection_idx]
        if startSectionTStamp &gt; startTStamp:
            startSection_idx -= 1
            startSectionTStamp = keys[max(startSection_idx,0)]
        endTStamp = startTStamp + span/Nano
        nearest_idx = min(bisect.bisect_left(keys, endTStamp),lk-1)
        lastSectionTStamp = keys[nearest_idx]
        if lastSectionTStamp &lt; endTStamp:
            lastSectionTStamp = keys[min(nearest_idx+1,lk-1)]
        self.position = self.directory[startSectionTStamp]
        endPosition = self.directory[lastSectionTStamp]
        _printvv(f&#39;first dsection {self.position}&#39;)
        _printvv(f&#39;last dsection {endPosition}&#39;)
        self.logbook.seek(self.position)
        _printvv(f&#39;logbook@{self.logbook.tell()}, offset={self.dirSize}&#39;)

        # Try to read required sections into a buffer. If successful, then
        # the streamReader for unpacker will be this buffer, otherwise
        # it will be the logbook file.
        toRead =  endPosition - self.logbook.tell()
        if toRead &lt; readerBufferSize:
            ts = timer()
            rbuf = self.logbook.read(toRead)
            ts1 = timer()
            dt1 = round(ts1 - ts,6)
            streamReader = BytesIO(rbuf)
            dt2 = round(timer() - ts1,6)
            print(f&#39;Read {round(toRead/1e6,3)}MB in {dt1}s, adopted in {dt2}s&#39;)
            _printv(f&#39;Read {round(toRead/1e6,3)}MB in {dt1}s, adopted in {dt2}s&#39;)
        else:
            print((f&#39;Read size {round(toRead/1e6,1)}MB &gt;&#39;
            f&#39; {round(readerBufferSize/1e6,1)}MB&#39;
            &#39;, processing it sequentially&#39;))
            streamReader = self.logbook

        # re-create the Unpacker to re-position it in the logbook
        self.unpacker = msgpack.Unpacker(streamReader, use_list=False
        ,strict_map_key=False) #use_list=False speeds up 20%

        # loop over sections in the logbook
        nSections = 0
        if APScan.Verbosity &gt;= 1:
            sectionTime = [0.]*3
        startTStampNS = startTStamp
        endTStampNS = endTStamp
        print(f&#39;sts,ets:{startTStampNS,endTStampNS}&#39;)
        extractionTime = 0.
        perfMonTime = 0.
        timerTotal = timer()
        for section in self.unpacker:
            extractionTStart = timer()
            nSections += 1
            # data sections
            _printv(f&#39;Data Section: {nSections+startSection_idx}&#39;)
            if nSections%60 == 0:
                dt = time.time() - extractionTime
                _printv((f&#39;Data sections: {nSections}&#39;
                f&#39;, elapsed time: {round(dt,4)}&#39;))#, paragraphs/s: {nParagraphs//dt}&#39;))
            try:# handle compressed data
                if self.compression != &#39;None&#39;:
                    ts = timer()
                    decompressed = self.decompress(section)
                    if APScan.Verbosity &gt;= 1:
                        sectionTime[0] += timer() - ts
                    ts = timer()
                    section = msgpack.unpackb(decompressed
                    ,strict_map_key=False)#ISSUE: strict_map_key does not work here
                    if APScan.Verbosity &gt;= 1:
                        sectionTime[1] += timer() - ts
            except Exception as e:
                print(f&#39;WARNING: wrong section {nSections}: {str(section)[:75]}...&#39;, {e})
                break
            _printv(f&#34;Data section {nSections}: {section[&#39;tstart&#39;]}&#34;)

            # iterate over parameters
            ts = timer()
            try:
                # the following loop takes 90% time
                for parIndex, tsValsNP in section[&#39;pars&#39;].items():
                    if not parIndex in items:
                        continue
                    tstamps, values = _unpacknp(tsValsNP)

                    # trim array if needed
                    if tstamps[0] &lt; startTStampNS:
                        first = bisect.bisect_left(tstamps, startTStampNS)
                        tstamps = tstamps[first:]
                        values = values[first:]
                    try:
                        if tstamps[-1] &gt; endTStampNS:
                            last = bisect.bisect_left(tstamps, endTStampNS)
                            tstamps = tstamps[:last]
                            values = values[:last]
                    except: pass
                    if APScan.Verbosity &gt;= 2:
                        print( _croppedText(f&#39;times{parIndex}[{len(tstamps)}]: {tstamps}&#39;))
                        try:    vshape = f&#39;of numpy arrays {values.dtype,values.shape}&#39;
                        except: vshape = &#39;&#39;
                        print(f&#39;vals{parIndex}[{len(values)}] {vshape}:&#39;)
                        print( _croppedText(f&#39;{values}&#39;))

                                        #`````````Concatenation of parameter lists.``````````````
                    # Using numpy.concatenate turned to be very slow.
                    # The best performance is using list.extend() 
                    extracted[parIndex][&#39;times&#39;].extend(list(tstamps))
                    ts2 = timer()
                    extracted[parIndex][&#39;values&#39;].extend(list(values))
                    perfMonTime += timer() - ts2
                    #,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,

                    n = len(extracted[parIndex][&#39;times&#39;])
                    _printvv(f&#34;par{parIndex}[{n}]&#34;)
                    parameterStatistics[parIndex] = n

            except Exception as e:
                print(f&#39;WARNING: in concatenation: {e}&#39;)

            dts = timer() - ts
            if APScan.Verbosity &gt;= 1:
                sectionTime[2] += dts
            extractionTime += timer() - extractionTStart

        if APScan.Verbosity &gt;= 1:
            print(f&#39;SectionTime: {[round(i/nSections,6) for i in sectionTime]}&#39;)
        print(f&#39;Deserialized from {self.logbookName}: {nSections} sections&#39;)
        print(f&#39;Sets/Parameter: {parameterStatistics}&#39;)
        ttime = timer()-timerTotal
        mbps = (f&#39; {round(toRead/1e6/extractionTime,1)} MB/s&#39;
        f&#39;, including disk: {round(ttime,3)} s, {round(toRead/1e6/ttime,1)} MB/s&#39;)
        print(f&#39;Processing time: {round(extractionTime,3)} s, {mbps}&#39;)
        print(f&#39;Spent {round(perfMonTime/extractionTime*100,1)}% in the monitored code.&#39;)
        return extracted</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="scan.APScan.Verbosity"><code class="name">var <span class="ident">Verbosity</span></code></dt>
<dd>
<div class="desc"><p>Show dedugging messages.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scan.APScan.extract_objects"><code class="name flex">
<span>def <span class="ident">extract_objects</span></span>(<span>self, span=0.0, items=[], startTime=None, bufSize=134217728)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns correlated dict of times and values of the logged items during
the selected time interval.</p>
<p><strong>span</strong>:
Time interval for data extraction in seconds. If 0, then
the data will be extracted starting from the startTime and
ending at the end of the logbook.</p>
<p><strong>items</strong>:
List of integer indexes of items to extract. The map of
indexes to Control System parameters could be obtained using
get_headers()['Index'].</p>
<p><strong>startTime</strong>: String for selecting start of the extraction interval.
Format: YYMMDD_HHMMSS. If None then extraction starts from the
beginning. </p>
<p><strong>bufSize</strong>:
Size of the bytesIO buffer. If file size is smaller than
the bufSize, then the whole file will be read into the buffer.
Otherwise each section will be read from the file sequentially.
Note, the Python3 read() for binary files is using very
effective buffering scheme, therefore using very large bufSize
have almost no effect on performance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_objects(self, span=0., items=[], startTime=None
    , bufSize=128*1024*1024):
    &#34;&#34;&#34;
    Returns correlated dict of times and values of the logged items during
    the selected time interval.
    
    **span**:   Time interval for data extraction in seconds. If 0, then
            the data will be extracted starting from the startTime and
            ending at the end of the logbook.
    
    **items**:  List of integer indexes of items to extract. The map of
     indexes to Control System parameters could be obtained using 
     get_headers()[&#39;Index&#39;].
    
    **startTime**: String for selecting start of the extraction interval. 
            Format: YYMMDD_HHMMSS. If None then extraction starts from the
            beginning. 

    **bufSize**:  Size of the bytesIO buffer. If file size is smaller than
            the bufSize, then the whole file will be read into the buffer.
            Otherwise each section will be read from the file sequentially.
            Note, the Python3 read() for binary files is using very
            effective buffering scheme, therefore using very large bufSize
            have almost no effect on performance.&#34;&#34;&#34;

    extracted = {}
    parameterStatistics = {}
    endPosition = self.logbookSize
    readerBufferSize = bufSize

    # create empty map for return
    if len(items) == 0: # enable handling of all items 
        #items = self.key2par.keys()
        items = [i for i in range(len(self.key2par))]
    #for key,par in self.key2par.items():
    for key,par in enumerate(self.key2par):
        if key not in parameterStatistics:
            #print(f&#39;add to stat[{len(parameterStatistics)+1}]: {key}&#39;) 
            parameterStatistics[key] = 0
        if par not in extracted and key in items:
            _printvv(f&#39;add extracted[{len(extracted)+1}]: {par}&#39;) 
            extracted[key] = {&#39;par&#39;:par, &#39;times&#39;:[], &#39;values&#39;:[]}

    if len(self.directory) == 0:
           print(&#39;ERROR. Directory is missing&#39;)
           sys.exit()

    # determine a part of the logbook for extraction
    keys = list(self.directory.keys())
    if startTime is  None:
        firstTStamp = keys[0]
        startTime = _seconds2Datetime(firstTStamp)
    firstDataSection, startTStamp, endSection, endTStamp\
    = _timeInterval(startTime, span)
    _printv(f&#39;start,end:{firstDataSection, int(startTStamp*Nano), endSection, int(endTStamp*Nano)}&#39;)

    # position logbook to first data section
    lk = len(keys)
    bt = timer()
    # find nearest_key ising bisect, that is fast, ~10us
    startSection_idx = bisect.bisect_left(keys, startTStamp)
    #print(f&#39;nidx: {startSection_idx,startTStamp,endTStamp}&#39;)
    startSectionTStamp = keys[startSection_idx]
    if startSectionTStamp &gt; startTStamp:
        startSection_idx -= 1
        startSectionTStamp = keys[max(startSection_idx,0)]
    endTStamp = startTStamp + span/Nano
    nearest_idx = min(bisect.bisect_left(keys, endTStamp),lk-1)
    lastSectionTStamp = keys[nearest_idx]
    if lastSectionTStamp &lt; endTStamp:
        lastSectionTStamp = keys[min(nearest_idx+1,lk-1)]
    self.position = self.directory[startSectionTStamp]
    endPosition = self.directory[lastSectionTStamp]
    _printvv(f&#39;first dsection {self.position}&#39;)
    _printvv(f&#39;last dsection {endPosition}&#39;)
    self.logbook.seek(self.position)
    _printvv(f&#39;logbook@{self.logbook.tell()}, offset={self.dirSize}&#39;)

    # Try to read required sections into a buffer. If successful, then
    # the streamReader for unpacker will be this buffer, otherwise
    # it will be the logbook file.
    toRead =  endPosition - self.logbook.tell()
    if toRead &lt; readerBufferSize:
        ts = timer()
        rbuf = self.logbook.read(toRead)
        ts1 = timer()
        dt1 = round(ts1 - ts,6)
        streamReader = BytesIO(rbuf)
        dt2 = round(timer() - ts1,6)
        print(f&#39;Read {round(toRead/1e6,3)}MB in {dt1}s, adopted in {dt2}s&#39;)
        _printv(f&#39;Read {round(toRead/1e6,3)}MB in {dt1}s, adopted in {dt2}s&#39;)
    else:
        print((f&#39;Read size {round(toRead/1e6,1)}MB &gt;&#39;
        f&#39; {round(readerBufferSize/1e6,1)}MB&#39;
        &#39;, processing it sequentially&#39;))
        streamReader = self.logbook

    # re-create the Unpacker to re-position it in the logbook
    self.unpacker = msgpack.Unpacker(streamReader, use_list=False
    ,strict_map_key=False) #use_list=False speeds up 20%

    # loop over sections in the logbook
    nSections = 0
    if APScan.Verbosity &gt;= 1:
        sectionTime = [0.]*3
    startTStampNS = startTStamp
    endTStampNS = endTStamp
    print(f&#39;sts,ets:{startTStampNS,endTStampNS}&#39;)
    extractionTime = 0.
    perfMonTime = 0.
    timerTotal = timer()
    for section in self.unpacker:
        extractionTStart = timer()
        nSections += 1
        # data sections
        _printv(f&#39;Data Section: {nSections+startSection_idx}&#39;)
        if nSections%60 == 0:
            dt = time.time() - extractionTime
            _printv((f&#39;Data sections: {nSections}&#39;
            f&#39;, elapsed time: {round(dt,4)}&#39;))#, paragraphs/s: {nParagraphs//dt}&#39;))
        try:# handle compressed data
            if self.compression != &#39;None&#39;:
                ts = timer()
                decompressed = self.decompress(section)
                if APScan.Verbosity &gt;= 1:
                    sectionTime[0] += timer() - ts
                ts = timer()
                section = msgpack.unpackb(decompressed
                ,strict_map_key=False)#ISSUE: strict_map_key does not work here
                if APScan.Verbosity &gt;= 1:
                    sectionTime[1] += timer() - ts
        except Exception as e:
            print(f&#39;WARNING: wrong section {nSections}: {str(section)[:75]}...&#39;, {e})
            break
        _printv(f&#34;Data section {nSections}: {section[&#39;tstart&#39;]}&#34;)

        # iterate over parameters
        ts = timer()
        try:
            # the following loop takes 90% time
            for parIndex, tsValsNP in section[&#39;pars&#39;].items():
                if not parIndex in items:
                    continue
                tstamps, values = _unpacknp(tsValsNP)

                # trim array if needed
                if tstamps[0] &lt; startTStampNS:
                    first = bisect.bisect_left(tstamps, startTStampNS)
                    tstamps = tstamps[first:]
                    values = values[first:]
                try:
                    if tstamps[-1] &gt; endTStampNS:
                        last = bisect.bisect_left(tstamps, endTStampNS)
                        tstamps = tstamps[:last]
                        values = values[:last]
                except: pass
                if APScan.Verbosity &gt;= 2:
                    print( _croppedText(f&#39;times{parIndex}[{len(tstamps)}]: {tstamps}&#39;))
                    try:    vshape = f&#39;of numpy arrays {values.dtype,values.shape}&#39;
                    except: vshape = &#39;&#39;
                    print(f&#39;vals{parIndex}[{len(values)}] {vshape}:&#39;)
                    print( _croppedText(f&#39;{values}&#39;))

                                    #`````````Concatenation of parameter lists.``````````````
                # Using numpy.concatenate turned to be very slow.
                # The best performance is using list.extend() 
                extracted[parIndex][&#39;times&#39;].extend(list(tstamps))
                ts2 = timer()
                extracted[parIndex][&#39;values&#39;].extend(list(values))
                perfMonTime += timer() - ts2
                #,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,

                n = len(extracted[parIndex][&#39;times&#39;])
                _printvv(f&#34;par{parIndex}[{n}]&#34;)
                parameterStatistics[parIndex] = n

        except Exception as e:
            print(f&#39;WARNING: in concatenation: {e}&#39;)

        dts = timer() - ts
        if APScan.Verbosity &gt;= 1:
            sectionTime[2] += dts
        extractionTime += timer() - extractionTStart

    if APScan.Verbosity &gt;= 1:
        print(f&#39;SectionTime: {[round(i/nSections,6) for i in sectionTime]}&#39;)
    print(f&#39;Deserialized from {self.logbookName}: {nSections} sections&#39;)
    print(f&#39;Sets/Parameter: {parameterStatistics}&#39;)
    ttime = timer()-timerTotal
    mbps = (f&#39; {round(toRead/1e6/extractionTime,1)} MB/s&#39;
    f&#39;, including disk: {round(ttime,3)} s, {round(toRead/1e6/ttime,1)} MB/s&#39;)
    print(f&#39;Processing time: {round(extractionTime,3)} s, {mbps}&#39;)
    print(f&#39;Spent {round(perfMonTime/extractionTime*100,1)}% in the monitored code.&#39;)
    return extracted</code></pre>
</details>
</dd>
<dt id="scan.APScan.get_headers"><code class="name flex">
<span>def <span class="ident">get_headers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns dict of header sections: Directory, Abstract, Index</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_headers(self):
    &#34;&#34;&#34;Returns dict of header sections: Directory, Abstract, Index&#34;&#34;&#34;
    return {&#39;Directory&#39;:self.directory, &#39;Abstract&#39;:self.abstract
    , &#39;Index&#39;:self.key2par}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scan.APScan" href="#scan.APScan">APScan</a></code></h4>
<ul class="">
<li><code><a title="scan.APScan.Verbosity" href="#scan.APScan.Verbosity">Verbosity</a></code></li>
<li><code><a title="scan.APScan.extract_objects" href="#scan.APScan.extract_objects">extract_objects</a></code></li>
<li><code><a title="scan.APScan.get_headers" href="#scan.APScan.get_headers">get_headers</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>