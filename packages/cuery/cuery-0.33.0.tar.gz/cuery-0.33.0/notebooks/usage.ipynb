{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import instructor\n",
    "import pandas as pd\n",
    "from pydantic import Field\n",
    "\n",
    "from cuery import Prompt, Response, Task, pprint\n",
    "from cuery.utils import set_env\n",
    "\n",
    "set_env(apify_secrets=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310808de",
   "metadata": {},
   "source": [
    "# Create a prompt from simple string\n",
    "The `Prompt` class expects a list of (jinja) messages with their roles. But it can also be instantiated from a simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Hello {{name}}! {% for item in ingredients %} {{ item }} {% endfor %}\"\n",
    "p = Prompt.from_string(t)\n",
    "pprint(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81097421",
   "metadata": {},
   "source": [
    "# Simplified client/model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5db557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docstring descriptions will be passed via the response model to the LLM\n",
    "class Recipe(Response):\n",
    "    ingredients: list[str]\n",
    "    \"\"\"A list of ingredients for the dish.\"\"\"\n",
    "\n",
    "\n",
    "prompt = Prompt.from_string(\"Generate a list of recipe ingredients to make '{{dish}}'.\")\n",
    "task = Task(prompt=prompt, response=Recipe)\n",
    "pprint(task)\n",
    "\n",
    "model = \"openai/gpt-4.1-mini\"  # or e.g. \"perplexity/sonar\"\n",
    "responses = await task(context=[{\"dish\": \"spaghetti carbonara\"}], model=model)\n",
    "# responses.to_pandas(explode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.to_pandas(explode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2d1d0",
   "metadata": {},
   "source": [
    "# Inspect LLM queries (containing final prompt send to LLM)\n",
    "\n",
    "Only available (for now), when multiple rows were processed!\n",
    "\n",
    "Each task maintains a log of errors and the queries to the LLM provider. Note that the structure of what's sent to the provider may be different for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f73f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.queries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e92fd9",
   "metadata": {},
   "source": [
    "# Choices (enum)\n",
    "Require LLM to respond with one of N _options_ (fixed categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34664d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Role(Enum):\n",
    "    PRINCIPAL = \"PRINCIPAL\"\n",
    "    TEACHER = \"TEACHER\"\n",
    "    STUDENT = \"STUDENT\"\n",
    "    OTHER = \"OTHER\"\n",
    "\n",
    "\n",
    "class UserDetail(Response):\n",
    "    age: int\n",
    "    name: str\n",
    "    role: Role = Field(description=\"Correctly assign one of the predefined roles to the user.\")\n",
    "\n",
    "\n",
    "prompt = Prompt.from_string(\"Please a create a synthetic user profile with age, name and role.\")\n",
    "task = Task(prompt=prompt, response=UserDetail)\n",
    "\n",
    "response = await task(model=\"openai/gpt-3.5-turbo\")\n",
    "print(response)\n",
    "response.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a9b70",
   "metadata": {},
   "source": [
    "Or using the Literal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class UserDetail(Response):\n",
    "    age: int\n",
    "    name: str\n",
    "    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n",
    "    \"\"\"Correctly assign one of the predefined roles to the user.\"\"\"\n",
    "\n",
    "\n",
    "response = await Task(prompt=prompt, response=UserDetail)(model=\"openai/gpt-3.5-turbo\")\n",
    "response.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183a07a",
   "metadata": {},
   "source": [
    "# Simple Multivalued fields\n",
    "Require LLM to respond with a _list_ of values (unconstrained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ingredients(Response):\n",
    "    items: list[str] = Field(description=\"List of ingredients for the recipe.\")\n",
    "\n",
    "\n",
    "prompt = Prompt.from_string(\"List the ingredients for the following dish: {{dish}}.\")\n",
    "context = [{\"dish\": \"pasta bolognese\"}, {\"dish\": \"chocolate cake\"}]\n",
    "\n",
    "task = Task(prompt=prompt, response=Ingredients)\n",
    "responses = await task(context=context)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain the original structure of the responses\n",
    "responses.to_pandas(explode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39652a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the list of ingredients into separate rows\n",
    "responses.to_pandas(explode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to simple python records\n",
    "responses.to_records(explode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb90a17",
   "metadata": {},
   "source": [
    "# Nested models\n",
    "Define a more complicated output structure by referencing another response model. \n",
    "\n",
    "In this case a list of certain length containing instances of pre-defined response model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sector(Response):\n",
    "    sector: str = Field(\n",
    "        description=\"Human-readable title(!) of the industrical sector (in NAICS taxonomy)\",\n",
    "        min_length=10,\n",
    "        max_length=150,\n",
    "    )\n",
    "    subsector: str = Field(\n",
    "        description=\"Human-readable title(!) of the industrial SUBsector (in NAICS taxonomy)\",\n",
    "        min_length=5,\n",
    "        max_length=150,\n",
    "    )\n",
    "    sector_automation_potential: int = Field(\n",
    "        description=\"A score from 1 to 10 indicating the sector's potential for automation\",\n",
    "        ge=0,\n",
    "        le=10,\n",
    "    )\n",
    "\n",
    "\n",
    "class Sectors(Response):\n",
    "    sectors: list[Sector] = Field(\n",
    "        description=\"A list of 1 to 5 NAIC industrial sectors with their AI automation potential\",\n",
    "        min_length=1,\n",
    "        max_length=5,\n",
    "    )\n",
    "\n",
    "\n",
    "sectors_prompt = Prompt.from_string(\n",
    "    \"List some industrial sector in the country of {{country}} that have great AI automation potential.\"\n",
    ")\n",
    "\n",
    "context = [{\"country\": \"Germany\"}, {\"country\": \"United States\"}, {\"country\": \"Japan\"}]\n",
    "sectors_task = Task(prompt=sectors_prompt, response=Sectors)\n",
    "responses = await sectors_task(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.to_pandas(explode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074200d",
   "metadata": {},
   "source": [
    "# Chain tasks together\n",
    "Run multiple tasks one after the other, collecting the results in a single DataFrame.\n",
    "\n",
    "Keep in mind here that the names of inputs of one task must be the same as the names of outputs in the previous one.\n",
    "\n",
    "Here we extract first some industrial sectors for each input country, and then some job roles within each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3773406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-uses \"sectors\" task from previous code cell (!)\n",
    "\n",
    "from cuery import Chain\n",
    "\n",
    "\n",
    "class Job(Response):\n",
    "    job_role: str\n",
    "    \"\"\"Name of the job role (job title, less than 50 characters)\"\"\"\n",
    "    job_description: str\n",
    "    \"\"\"A short description of the job role (less than 200 characters)\"\"\"\n",
    "    job_automation_potential: int = Field(\n",
    "        description=\"A score from 1 to 10 indicating the job's potential for automation\",\n",
    "        ge=0,\n",
    "        le=10,\n",
    "    )\n",
    "\n",
    "\n",
    "class Jobs(Response):\n",
    "    jobs: list[Job]\n",
    "    \"\"\"A list of jobs with their AI automation potential and reasons for that potential\"\"\"\n",
    "\n",
    "\n",
    "jobs_prompt = Prompt.from_string(\n",
    "    \"List some job roles with great AI automation potential in the country of {{country}} and the sector '{{sector}}'\"\n",
    ")\n",
    "\n",
    "context = pd.DataFrame(\n",
    "    {\n",
    "        \"country\": [\"Germany\", \"United States\", \"Japan\"],\n",
    "        \"PIB\": [4.0, 5.0, 3.5],\n",
    "    }\n",
    ")\n",
    "\n",
    "jobs_task = Task(prompt=jobs_prompt, response=Jobs)\n",
    "chain = Chain(sectors_task, jobs_task)\n",
    "responses = await chain(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daef255",
   "metadata": {},
   "source": [
    "# Tools\n",
    "\n",
    "`Tools` are another thin level of abstraction to make `Tasks` configurable with a clear input interface (the output is already defined by a `Response` model). They're mostly useful with tasks (prompts and response models) that can be customized, i.e. which have configurable parameters that don't depend on the context of the data over which it will be iterated.\n",
    "\n",
    "We use pydantic again to define the interface. This has the advantage that we can re-use a tool's interface directly for FastAPI endpoints, and therefore also directly as an MCP interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f22a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import ClassVar\n",
    "\n",
    "from cuery.cli import set_env_vars\n",
    "from cuery.prompt import Prompt\n",
    "from cuery.response import Response, ResponseClass\n",
    "from cuery.tool import Tool\n",
    "\n",
    "set_env_vars(apify_secrets=False)\n",
    "\n",
    "\n",
    "class Jokes(Response):\n",
    "    jokes: list[str]\n",
    "\n",
    "\n",
    "class Joker(Tool):\n",
    "    n_jokes: int\n",
    "    topics: list[str]\n",
    "\n",
    "    response_model: ClassVar[ResponseClass] = Jokes\n",
    "\n",
    "    @property\n",
    "    def prompt(self):\n",
    "        # ${vars} will be substituted once initially. and so values will be constant when iterating over data\n",
    "        # Jinja variables (and other Jinja syntax) will be evaluated for each request/row/context item\n",
    "        instructions = \"Create ${n_jokes} one-liners about {{topic}}.\"\n",
    "        return Prompt(messages=instructions).substitute(n_jokes=self.n_jokes)\n",
    "\n",
    "    @property\n",
    "    def context(self):\n",
    "        return [{\"topic\": topic} for topic in self.topics]\n",
    "\n",
    "\n",
    "joker = Joker(n_jokes=3, topics=[\"cats\", \"nerds\", \"youths\"])\n",
    "result = await joker(n_concurrent=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d954fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_records(explode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "joker.task.queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuery.cli import set_env_vars\n",
    "from cuery.tools.flex import generic\n",
    "\n",
    "set_env_vars(apify_secrets=False)\n",
    "\n",
    "p = \"I need a schema for users having a name and email\"\n",
    "t = generic.SchemaGenerator(instructions=p, model=\"openai/gpt-4.1\")\n",
    "r = await t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf396013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuery import pprint\n",
    "\n",
    "pprint(r.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a4ede",
   "metadata": {},
   "source": [
    "# Web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13704b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "from cuery import Field, Prompt, Response, Task, pprint\n",
    "\n",
    "\n",
    "class Citation(Response):\n",
    "    id: int\n",
    "    url: str\n",
    "\n",
    "\n",
    "class Place(Response):\n",
    "    name: str = Field(..., description=\"Name of the restaurant.\")\n",
    "    address: str = Field(..., description=\"Address of the restaurant.\")\n",
    "    telephone: str = Field(..., description=\"Telephone number of the restaurant.\")\n",
    "\n",
    "\n",
    "class Places(Response):\n",
    "    summary: str\n",
    "    citations: list[Citation]\n",
    "\n",
    "\n",
    "client = instructor.from_provider(\n",
    "    \"openai/gpt-4.1-mini\",\n",
    "    mode=instructor.Mode.RESPONSES_TOOLS_WITH_INBUILT_TOOLS,\n",
    "    async_client=True,\n",
    ")\n",
    "\n",
    "response, completion = await client.responses.create_with_completion(\n",
    "    input=\"What are some of the best places to eat Paella in Madrid, Spain? Return a list of restaurants with their name, address and telephone number.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"web_search_preview\",\n",
    "            \"search_context_size\": \"low\",\n",
    "            \"user_location\": {\n",
    "                \"type\": \"approximate\",\n",
    "                \"country\": \"ES\",\n",
    "                \"city\": \"Madrid\",\n",
    "                \"region\": \"Madrid\",\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    response_model=Places,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c705d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.output[1].content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f869318",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [(ann.start_index, ann.end_index) for ann in completion.output[1].content[0].annotations]\n",
    "print(idx)\n",
    "response.summary[idx[0][0] : idx[0][1]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuery (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
