CREATE TRIGGER `{{ trigger_name }}` AFTER INSERT ON `{{ table_name }}` FOR EACH ROW BEGIN
    DECLARE changed_data JSON;
    DECLARE columns TEXT;
    SET changed_data = JSON_OBJECT();
    SELECT GROUP_CONCAT(CONCAT('"', column_name, '"')) INTO columns
        FROM information_schema.columns WHERE table_name = '{{ table_name }}';

    {% for column in column_names %}
        {# Guard against accessing columns that are dropped during migrations, but haven't yet
           been removed from this trigger. #}
        IF LOCATE('"{{ column }}"', columns) > 0 THEN
            SET changed_data = JSON_SET(changed_data, '$.{{ column }}', NEW.{{ column }});
        END IF;
    {% endfor %}

    INSERT INTO audit_log SET
        timestamp=CURRENT_TIMESTAMP,
        {# `updated_by_type` should always be present in inserts generated by SQLAlchemy, so if it's
           missing, we assume the insert was executed via a manual SQL query. Log the the MySQL user
           in this case.

           The above assumption may not be true in all cases, but due to the lack of information
           available in MySQL triggers about the origin of the query, it is the best we can do. #}
        updated_by_type=IF(NEW.updated_by_type IS NULL, 'mysql', NEW.updated_by_type),
        updated_by_id=IF(NEW.updated_by_type IS NULL, SUBSTRING_INDEX(USER(), '@', 1), NEW.updated_by_id),
        updated_by_ip=IF(NEW.updated_by_type IS NULL, SUBSTRING_INDEX(USER(), '@', -1), NEW.updated_by_ip),
        table_name='{{ table_name }}',
        object_id=NEW.id,
        operation='insert',
        old_data=NULL,
        changed_data=changed_data;
END
