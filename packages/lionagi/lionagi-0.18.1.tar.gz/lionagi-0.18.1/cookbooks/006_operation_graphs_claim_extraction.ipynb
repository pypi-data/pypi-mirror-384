{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 006: Operation Graphs - Academic Claim Validation\n",
    "\n",
    "Building on ReAct patterns from tutorial 005, this demonstrates sequential coordination using Operation Graphs to orchestrate ReaderTool workflows for academic claim validation.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Sequential Coordination**: Building context step-by-step through dependent operations\n",
    "2. **ReaderTool Integration**: Document chunking and progressive reading strategies  \n",
    "3. **Structured Extraction**: Using Pydantic models for reliable claim extraction\n",
    "4. **Operation Dependencies**: How operations build on previous results\n",
    "\n",
    "## Use Case: Validating a Theoretical Framework Paper\n",
    "\n",
    "We'll validate claims in an academic paper about capability-based security by:\n",
    "- Reading document chunks progressively with ReaderTool\n",
    "- Building understanding through sequential analysis\n",
    "- Extracting verifiable claims with structured output formats\n",
    "- Demonstrating how each operation builds on the previous one's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment setup complete\n",
      "ðŸ“„ Target: 006_lion_proof_ch2.md\n",
      "ðŸŽ¯ Goal: Validate academic claims using coordinated ReAct workflows\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lionagi import Branch, Session, Builder, types, iModel\n",
    "from lionagi.tools.types import ReaderTool\n",
    "\n",
    "# Target document - complex theoretical framework\n",
    "here = Path().cwd()\n",
    "document_path = here / \"data\" / \"006_lion_proof_ch2.md\"\n",
    "\n",
    "print(\"âœ… Environment setup complete\")\n",
    "print(f\"ðŸ“„ Target: {document_path.name}\")\n",
    "print(\"ðŸŽ¯ Goal: Validate academic claims using coordinated ReAct workflows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data models defined\n"
     ]
    }
   ],
   "source": [
    "# Data models for structured responses\n",
    "class Claim(BaseModel):\n",
    "    claim: str\n",
    "    type: Literal[\"citation\", \"performance\", \"technical\", \"other\"]\n",
    "    location: str = Field(..., description=\"Section/paragraph reference\")\n",
    "    verifiability: Literal[\"high\", \"medium\", \"low\"]\n",
    "    search_strategy: str = Field(..., description=\"How to verify this claim\")\n",
    "\n",
    "\n",
    "class ClaimExtraction(BaseModel):\n",
    "    claims: list[Claim]\n",
    "\n",
    "\n",
    "print(\"âœ… Data models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from pydantic import Field, HttpUrl\n",
    "\n",
    "from lionagi.models import HashableModel\n",
    "\n",
    "\n",
    "class Source(HashableModel):\n",
    "    \"\"\"\n",
    "    Represents a citation or external source, such as:\n",
    "     - a website,\n",
    "     - documentation link,\n",
    "     - research paper,\n",
    "     - or any external resource.\n",
    "    \"\"\"\n",
    "\n",
    "    title: str = Field(\n",
    "        ...,\n",
    "        description=\"Short label or title for the reference (e.g. 'Pydantic Docs', 'RFC 3986').\",\n",
    "    )\n",
    "\n",
    "    url: str | HttpUrl | None = Field(\n",
    "        None,\n",
    "        description=\"Full URL or local path pointing to the resource. Must conform to standard URL format.\",\n",
    "    )\n",
    "\n",
    "    note: str | None = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Optional additional note explaining why this reference is relevant or what it contains.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class SnippetType(str, Enum):\n",
    "    TEXT = \"text\"\n",
    "    CODE = \"code\"\n",
    "\n",
    "\n",
    "class TextSnippet(HashableModel):\n",
    "    \"\"\"\n",
    "    Specialized snippet for textual/prose content.\n",
    "    \"\"\"\n",
    "\n",
    "    type: SnippetType = Field(\n",
    "        SnippetType.TEXT,\n",
    "        description=(\n",
    "            \"Must be 'text' for textual snippets. Ensures explicit type distinction.\"\n",
    "        ),\n",
    "    )\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The actual text. Can be a paragraph, bullet list, or any narrative content.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class CodeSnippet(HashableModel):\n",
    "    \"\"\"\n",
    "    Specialized snippet for source code or command-line examples.\n",
    "    \"\"\"\n",
    "\n",
    "    type: SnippetType = Field(\n",
    "        SnippetType.CODE,\n",
    "        description=(\n",
    "            \"Must be 'code' for code snippets. Allows separate handling or formatting.\"\n",
    "        ),\n",
    "    )\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The actual code or command sequence. Should be well-formatted so it can be rendered properly.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Section(HashableModel):\n",
    "    \"\"\"\n",
    "    A single section of a document or article. Each section has:\n",
    "     - A title\n",
    "     - A sequential list of content snippets (text or code),\n",
    "       which appear in the intended reading order.\n",
    "     - Optional sources specifically cited in this section.\n",
    "    \"\"\"\n",
    "\n",
    "    title: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The section heading or label, e.g., 'Introduction', 'Implementation Steps'.\"\n",
    "        ),\n",
    "    )\n",
    "    snippets: list[TextSnippet | CodeSnippet] = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"Ordered list of content snippets. Could be multiple text blocks, code examples, etc.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    sources: list[Source] = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"References specifically cited in this section. \"\n",
    "            \"If sources are stored at the doc-level, this can be omitted.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class OutlineItem(HashableModel):\n",
    "    \"\"\"\n",
    "    Represents a single outline item, which could become a full section later.\n",
    "    \"\"\"\n",
    "\n",
    "    heading: str = Field(\n",
    "        ...,\n",
    "        description=\"Short name or label for this item, e.g., 'Chapter 1: Basics'.\",\n",
    "    )\n",
    "    summary: str | None = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"A brief description of what this section will cover, if known.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Outline(HashableModel):\n",
    "    \"\"\"\n",
    "    A top-level outline for a document or article.\n",
    "    \"\"\"\n",
    "\n",
    "    topic: str = Field(\n",
    "        ..., description=\"Working title or overarching topic of the document.\"\n",
    "    )\n",
    "    items: list[OutlineItem] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of major outline points or sections planned.\",\n",
    "    )\n",
    "    notes: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Any additional remarks, questions, or brainstorming notes for the outline.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Sequential Document Analysis\n",
    "\n",
    "Build understanding step-by-step: Open â†’ Analyze â†’ Extract claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Executing sequential analysis...\n",
      "Pre-allocated 2 branches\n",
      "Warning: Operation b42e456a using default branch (not pre-allocated)\n",
      "Executing operation: b42e456a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 13:31:53,665 - INFO - detected formats: [<InputFormat.MD: 'md'>]\n",
      "2025-10-15 13:31:53,667 - INFO - Going to convert document batch...\n",
      "2025-10-15 13:31:53,668 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-10-15 13:31:53,679 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-15 13:31:53,681 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-15 13:31:53,681 - INFO - Processing document 006_lion_proof_ch2.md\n",
      "2025-10-15 13:31:54,843 - INFO - Finished converting document 006_lion_proof_ch2.md in 1.18 sec.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: To understand the theoretical framework document /Users/lion/projects/lionagi/cookbooks/data/006_lion_proof_ch2.md, the crucial first step is to open the document and examine its structure. This will allow identification of major sections, headings, and where verifiable claimsâ€”such as hypothesis statements, referenced theories, or empirical resultsâ€”are presented. This initial opening is required before any partial reading or deeper analysis can happen.\n",
       "planned_actions:\n",
       "  - action_type: reader_tool\n",
       "    description: Open the theoretical framework document located at /Users/lion/projects/lionagi/cookbooks/data/006_lion_proof_ch2.md to analyze its structure and identify sections containing verifiable claims.\n",
       "extension_needed: True\n",
       "milestone: Document opened and initial overview obtained.\n",
       "action_strategy: sequential\n",
       "action_required: True\n",
       "reason:\n",
       "  title: Need to open document before detailed reading\n",
       "  content: Without opening the document, no structural or content analysis...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.2 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: The document is large, with 53,001 characters and 12,245 tokens. To understand its structure and identify sections containing verifiable claims, the next step is to read the table of contents or the initial portion (e.g., the first 2000 characters) to extract headings, subheadings and any explicit summary sections. This will provide an overview of the framework's organization and highlight areas likely to contain verifiable claims such as hypotheses, evidence, or citations. Reading the initial segment is necessary before deeper content analysis and claim verification can proceed.\n",
       "planned_actions:\n",
       "  - action_type: reader_tool\n",
       "    description: Read the first 2000 characters of the document DOC_9143726367203047801 to capture the structure including table of contents and section headings.\n",
       "extension_needed: True\n",
       "milestone: Obtained document structure and identified candidate sections for verifiable claims.\n",
       "action_strategy: sequential\n",
       "action_required: True\n",
       "reason:\n",
       "  title: ...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.3 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  The initial 2000 characters reveal a clear and organized structure for the theoretical framework document. It starts with the introduction (2.1 Introduction) including motivation and contribution overview subsections. The contribution overview lists four main theorems (2.1.2 Contribution Overview), each with a formal proof and related lemmas. These theorems represent key verifiable claims in the text. This introduction also references supporting mechanized models (TLA+ and Lean).\n",
       "  \n",
       "  To systematically identify all sections with verifiable claims, the next logical step is to read the table of contents or the next portion of the document that likely contains the section headings for the entire chapter 2. This will allow me to pinpoint all theorem statements and related proofs scattered throughout the document.\n",
       "  \n",
       "  Therefore, I will read the next 2000 characters to capture the document outline or table of contents and early parts of the chapter sections beyond intr...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "\n",
       "```yaml\n",
       "answer: |\n",
       "  The theoretical framework document is structured into well-defined sections and subsections, enabling a clear understanding of its content and locating verifiable claims. The major structure includes:\n",
       "  \n",
       "  1. **2.1 Introduction**\n",
       "     - 2.1.1 Motivation: Discusses fundamental security challenges in distributed systems.\n",
       "     - 2.1.2 Contribution Overview: Lists four main theorems representing key verifiable claims:\n",
       "       - Theorem 2.1 (Cross-Component Capability Flow)\n",
       "       - Theorem 2.2 (Security Composition)\n",
       "       - Theorem 2.3 (Confused Deputy Prevention)\n",
       "       - Theorem 2.4 (Automatic POLA Enforcement)\n",
       "  \n",
       "  2. **2.2 System Model and Formal Definitions**\n",
       "     - 2.2.1 Lion Ecosystem Architecture: Describes core components of the Lion ecosystem.\n",
       "     - 2.2.2 Formal System Definition: Contains formal definitions such as Definition 2.1 (Lion Capability System as a 7-tuple) and Definition 2.2 (Cross-Component Capability as a 5-tuple).\n",
       "  \n",
       "  The sections beginning at...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed operation: b42e456a\n",
      "Operation 5b2324ec waiting for b42e456a\n",
      "Operation 5b2324ec inherited context from b42e456a\n",
      "Executing operation: 5b2324ec\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  To identify citations, technical claims, and performance metrics that can be verified in the theoretical framework document, I will focus on key sections that contain formal theorems, definitions, proofs, and any results or evaluation discussions typically found in a chapter on theory and implementation.\n",
       "  \n",
       "  From prior analysis, the document features:\n",
       "  - Section 2.1 with introduction and theorem overview\n",
       "  - Section 2.2 with the system model and formal definitions\n",
       "  \n",
       "  Since the contribution overview listed four main theorems, I will read the sections containing these theorem statements and their supporting lemmas and proofs. Additionally, I will check for any empirical or performance evaluation sections if present, as they may contain verifiable performance metrics.\n",
       "  \n",
       "  Given the size of the document, I propose reading the next key parts where Theorem 2.1 and Theorem 2.2 should be discussed and proven. These sections are likely immediately after the formal def...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.2 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  The examined section (offsets 4000-8000) reveals detailed formal definitions, theorem statements, and proofs that contain technical claims suitable for verification. Notable content includes:\n",
       "  \n",
       "  - Definitions 2.3 to 2.5, which formalize capability authority, component composition compatibility, and security properties respectively. These set the ground for rigorous reasoning about system security.\n",
       "  \n",
       "  - The full statement of Theorem 2.1 (Cross-Component Capability Flow Preservation), which asserts that capability authority and unforgeability are preserved across component communications.\n",
       "  \n",
       "  - The beginning of Theorem 2.1's proof, outlining lemmas such as Lemma 2.1.1 on WebAssembly isolation preserving capability references. This includes technical claims about host memory separation and cryptographically secure handles.\n",
       "  \n",
       "  This content contains concrete formal claims and some references to verified security properties (e.g., unforgeability via cryptographic...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.3 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  The content from offsets 8000 to 12000 continues detailing the proof of Theorem 2.1 through lemmas that make explicit technical claims and include a concrete Rust code snippet for capability serialization and deserialization. The claims here are highly verifiable, given the code and cryptographic guarantees discussed. Lemma 2.1.2 demonstrates the authority preservation via cryptographic HMAC signatures on capability handles during transfer. Lemma 2.1.3 outlines policy compliance during capability transfers, reinforcing security constraints. This section contains technical claims grounded in formal methods and implementation details but lacks explicit citations or performance metrics.\n",
       "  \n",
       "  The next logical step is to proceed reading further segments (offsets 12000-16000) to locate Theorem 2.2 security composition proofs, look for any citations supporting the theoretical claims, and search for performance evaluation or metrics that demonstrate practical impacts.\n",
       "  \n",
       "...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.4 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  The content from offsets 12000 to 16000 covers the conclusion of Theorem 2.1 and introduces Theorem 2.2 on Security Composition. It details the theorem statement and an outline of the proof with associated lemmas, describing exactly how security properties such as unforgeable references, authority confinement, least privilege, and policy compliance are preserved when components are composed. The section includes formal statements and reasoning but does not explicitly include citations or performance metrics.\n",
       "  \n",
       "  Mechanization notes for Theorem 2.1 mention the use of TLA+ for model checking, providing confidence in the formal verification process. However, explicit bibliographic citations or performance evaluations have not appeared so far.\n",
       "  \n",
       "  To continue identifying verifiable claims, citations, and performance metrics, the next logical step is to read on from character offset 16000 onward. This may include further lemma proofs, any explicit references to relat...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "\n",
       "```yaml\n",
       "topic: Analysis of Verifiable Citations, Technical Claims, and Performance Metrics in the Lion Ecosystem Theoretical Framework\n",
       "items:\n",
       "  - heading: Theorem 2.1: Cross-Component Capability Flow\n",
       "    summary: Contains technical claims about preservation of capability authority and unforgeability across component boundaries, supported by formal lemmas. Includes proof sketches utilizing WebAssembly memory model and cryptographic HMAC to ensure authority integrity. Code snippets in Rust demonstrate capability serialization and deserialization with proof of integrity. No explicit bibliographic citations or performance metrics present.\n",
       "  - heading: Theorem 2.2: Security Composition\n",
       "    summary: Establishes that composing secure Lion components via compatible capability interfaces preserves overall system security properties (unforgeability, authority confinement, least privilege, and policy compliance). The proof uses lemmas about compositional security and interface compatibility. Mech...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed operation: 5b2324ec\n",
      "Operation 80cba960 waiting for 5b2324ec\n",
      "Operation 80cba960 inherited context from 5b2324ec\n",
      "Executing operation: 80cba960\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  Based on the prior comprehensive reading and analysis of the document sections covering the introduction, formal system definitions, theorem statements and proofs, I have identified several specific, verifiable claims that include technical assertions and mechanized proof notes. While explicit bibliographic citations were scarce in the analyzed text, the document references mechanized models (TLA+ and Lean) which serve as verifiable formal methods evidence. No explicit performance metrics have been encountered so far; these may appear later in evaluation chapters but are not part of the theoretical framework document analyzed.\n",
       "  \n",
       "  The specific verifiable claims extracted are:\n",
       "  \n",
       "  1. **Theorem 2.1: Cross-Component Capability Flow Preservation**\n",
       "     - The capability authority and unforgeability are preserved precisely across component boundaries during communication.\n",
       "     - Mechanized verification via TLA+ confirms state-based authority consistency.\n",
       "  \n",
       "  2. **Rus...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "\n",
       "```yaml\n",
       "claims:\n",
       "  - claim: Theorem 2.1 states that capability authority and unforgeability are preserved precisely across component boundaries during communication in the Lion ecosystem.\n",
       "    type: technical\n",
       "    location: Section 2.3.1 Theorem 2.1: Cross-Component Capability Flow\n",
       "    verifiability: high\n",
       "    search_strategy: Review formal statement of Theorem 2.1 and mechanized TLA+ model checking described in Appendix A.2 to confirm preservation of authority and unforgeability.\n",
       "  - claim: Rust code implementing capability serialization and deserialization guarantees integrity by using cryptographic HMAC signatures to protect capability handles against forgery or tampering.\n",
       "    type: technical\n",
       "    location: Section 2.3.2 Lemma 2.1.2: Capability Transfer Protocol Preserves Authority\n",
       "    verifiability: high\n",
       "    search_strategy: Audit the Rust functions serialize_capability and deserialize_capability and verify the use of HMAC signatures for integrity checks as described in the proof.\n",
       "  - c...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed operation: 80cba960\n"
     ]
    }
   ],
   "source": [
    "from lionagi.fields import Instruct\n",
    "\n",
    "\n",
    "async def sequential_analysis():\n",
    "    \"\"\"Sequential workflow: open â†’ analyze structure â†’ extract claims.\"\"\"\n",
    "\n",
    "    # Create branch with ReaderTool\n",
    "    branch = Branch(\n",
    "        tools=[ReaderTool], chat_model=iModel(model=\"openai/gpt-4.1-mini\")\n",
    "    )\n",
    "    session = Session(default_branch=branch)\n",
    "    builder = Builder()\n",
    "\n",
    "    # Step 1: Open and understand document\n",
    "    doc_reader = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"open_document\",\n",
    "        instruct=Instruct(\n",
    "            instruction=\"Use ReaderTool to open and analyze the theoretical framework document. Understand its structure and identify sections containing verifiable claims.\",\n",
    "            context={\"document_path\": str(document_path)},\n",
    "        ),\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=2,\n",
    "        verbose=True,\n",
    "        verbose_length=1000,\n",
    "    )\n",
    "\n",
    "    # Step 2: Progressive content analysis\n",
    "    content_analyzer = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"analyze_content\",\n",
    "        depends_on=[doc_reader],\n",
    "        instruct=Instruct(\n",
    "            instruction=\"Read through key sections to identify citations, technical claims, and performance metrics that can be verified.\"\n",
    "        ),\n",
    "        response_format=Outline,\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=3,\n",
    "        verbose=True,\n",
    "        verbose_length=1000,\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract specific claims\n",
    "    claim_extractor = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"extract_claims\",\n",
    "        depends_on=[content_analyzer],\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Extract 5-7 specific, verifiable claims. Prioritize citations, performance metrics, and technical assertions.\"\n",
    "        ),\n",
    "        response_format=ClaimExtraction,\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=3,\n",
    "        verbose=True,\n",
    "        verbose_length=1000,\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Execute workflow\n",
    "    graph = builder.get_graph()\n",
    "    print(\"ðŸ”— Executing sequential analysis...\")\n",
    "\n",
    "    result = await session.flow(graph, parallel=False, verbose=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Execute sequential analysis\n",
    "result = await sequential_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ“„ Document Structure (5b2324ec-2cf2-438c-baf4-756b664c3d71)\n",
       "\n",
       "**Topic:** Analysis of Verifiable Citations, Technical Claims, and Performance Metrics in the Lion Ecosystem Theoretical Framework\n",
       "\n",
       "### Key Sections:\n",
       "- **Theorem 2.1: Cross-Component Capability Flow**: Contains technical claims about preservation of capability authority and unforgeability across component boundaries, supported by formal lemmas. Includes proof sketches utilizing WebAssembly memory model and cryptographic HMAC to ensure authority integrity. Code snippets in Rust demonstrate capability serialization and deserialization with proof of integrity. No explicit bibliographic citations or performance metrics present.\n",
       "- **Theorem 2.2: Security Composition**: Establishes that composing secure Lion components via compatible capability interfaces preserves overall system security properties (unforgeability, authority confinement, least privilege, and policy compliance). The proof uses lemmas about compositional security and interface compatibility. Mechanized proofs are noted in Lean for formal verification. No explicit external citations or performance data yet.\n",
       "- **Theorem 2.3: Confused Deputy Prevention**: Formalizes prevention of confused deputy attacks by requiring explicit capability transfers for any privileged action. The theorem states that no component can perform unauthorized actions on behalf of others without proper capabilities. This section begins formal statements and proof outlines but details are truncated at the read limit. No citations or performance metrics identified yet.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ“‘ Extracted Claims (80cba960-cd4d-407d-b06d-0d692826328b)\n",
       "\n",
       "Found **7** verifiable claims:\n",
       "\n",
       "\n",
       "### 1. [TECHNICAL] Theorem 2.1 states that capability authority and unforgeability are preserved precisely across component boundaries during communication in the Lion ecosystem.\n",
       "\n",
       "- **Location:** Section 2.3.1 Theorem 2.1: Cross-Component Capability Flow  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Review formal statement of Theorem 2.1 and mechanized TLA+ model checking described in Appendix A.2 to confirm preservation of authority and unforgeability.\n",
       "\n",
       "\n",
       "### 2. [TECHNICAL] Rust code implementing capability serialization and deserialization guarantees integrity by using cryptographic HMAC signatures to protect capability handles against forgery or tampering.\n",
       "\n",
       "- **Location:** Section 2.3.2 Lemma 2.1.2: Capability Transfer Protocol Preserves Authority  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Audit the Rust functions serialize_capability and deserialize_capability and verify the use of HMAC signatures for integrity checks as described in the proof.\n",
       "\n",
       "\n",
       "### 3. [TECHNICAL] Theorem 2.2 demonstrates that composing individually secure Lion components via compatible capability interfaces preserves overall system security properties such as unforgeability, authority confinement, least privilege, and policy compliance.\n",
       "\n",
       "- **Location:** Section 2.4 Theorem 2.2: Security Composition  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Examine the formal theorem statement, supporting lemmas 2.2.1 and 2.2.2, and the mechanized proof in Lean that checks compositional security invariants.\n",
       "\n",
       "\n",
       "### 4. [TECHNICAL] WebAssembly isolation enforces memory separation, ensuring that capability references stored in host memory are isolated from WebAssembly module memory, preventing unauthorized tampering; handles are injective, unguessable, and unforgeable due to cryptographic properties.\n",
       "\n",
       "- **Location:** Section 2.3.2 Lemma 2.1.1: WebAssembly Isolation Preserves Capability References  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Validate the formal definitions of memory separation, properties of handles, and supporting argument that WebAssembly cannot forge capability references.\n",
       "\n",
       "\n",
       "### 5. [TECHNICAL] Theorem 2.3 formally claims that no component can perform actions on behalf of another without explicit capability transfer, thus preventing classic confused deputy attacks.\n",
       "\n",
       "- **Location:** Section 2.5 Theorem 2.3: Confused Deputy Prevention  \n",
       "- **Verifiability:** medium\n",
       "- **Search Strategy:** Check the formal statement and ensuing proof that any privileged action requires a corresponding capability explicitly held by the performing component.\n",
       "\n",
       "\n",
       "### 6. [TECHNICAL] Capability transfers are mediated by a policy engine which enforces system policies, permitting transfers only when policy_allows(source, target, capability) is true, ensuring all capability transfers comply with security policies.\n",
       "\n",
       "- **Location:** Section 2.3.2 Lemma 2.1.3: Policy Compliance During Transfer  \n",
       "- **Verifiability:** medium\n",
       "- **Search Strategy:** Review the formal modeling of send events and policy evaluation as described; verify system implementation for policy mediation during capability transfer.\n",
       "\n",
       "\n",
       "### 7. [OTHER] Mechanized proofs of key theorems are encoded using formal tools TLA+ (for Theorem 2.1) and Lean (for Theorem 2.2), which provide algorithmic verification beyond textual argumentation.\n",
       "\n",
       "- **Location:** Sections 2.3.1 Mechanization note; 2.4 Mechanization note  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Access and review the mechanized models in TLA+ and Lean referenced, examining proof scripts and model checking results for confirmation.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## âœ… Sequential analysis completed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display results\n",
    "for node_id, data in result[\"operation_results\"].items():\n",
    "    if isinstance(data, Outline):\n",
    "        md_content = f\"\"\"\n",
    "## ðŸ“„ Document Structure ({node_id})\n",
    "\n",
    "**Topic:** {data.topic}\n",
    "\n",
    "### Key Sections:\n",
    "\"\"\"\n",
    "        for item in data.items[:3]:  # Show first 3\n",
    "            md_content += f\"- **{item.heading}**: {item.summary}\\n\"\n",
    "\n",
    "        display(Markdown(md_content))\n",
    "\n",
    "    elif isinstance(data, ClaimExtraction):\n",
    "        md_content = f\"\"\"\n",
    "## ðŸ“‘ Extracted Claims ({node_id})\n",
    "\n",
    "Found **{len(data.claims)}** verifiable claims:\n",
    "\n",
    "\"\"\"\n",
    "        for i, claim in enumerate(data.claims, 1):\n",
    "            md_content += f\"\"\"\n",
    "### {i}. [{claim.type.upper()}] {claim.claim}\n",
    "\n",
    "- **Location:** {claim.location}  \n",
    "- **Verifiability:** {claim.verifiability}\n",
    "- **Search Strategy:** {claim.search_strategy}\n",
    "\n",
    "\"\"\"\n",
    "        display(Markdown(md_content))\n",
    "\n",
    "display(Markdown(\"## âœ… Sequential analysis completed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
