import inspect
import json
import re
from datetime import UTC, datetime
from pathlib import Path

import aiofiles
import black

import plastron

FIELD_ARG_NAME_MAP = {"primary_key": "pk", "db_index": "index"}

FIELD_ARGS = [
    "null",
    "unique",
    "primary_key",
    "db_index",
    "default",
    "source_field",
    "generated",
]

FIELD_ARG_ORDER = {
    "IntField": [*FIELD_ARGS],
    "BigIntField": [*FIELD_ARGS],
    "SmallIntField": [*FIELD_ARGS],
    "CharField": ["max_length", *FIELD_ARGS],
    "TextField": [*FIELD_ARGS],
    "BooleanField": [*FIELD_ARGS],
    "DecimalField": ["max_digits", "decimal_places", *FIELD_ARGS],
    "DatetimeField": ["auto_now", "auto_now_add", *FIELD_ARGS],
    "DateField": [*FIELD_ARGS],
    "TimeField": ["auto_now", "auto_now_add", *FIELD_ARGS],
    "TimeDeltaField": [*FIELD_ARGS],
    "FloatField": [*FIELD_ARGS],
    "JSONField": [*FIELD_ARGS],
    "UUIDField": [*FIELD_ARGS],
    "BinaryField": [*FIELD_ARGS],
    # # Relation fields (basic support)
    # "ForeignKeyField": [
    #     "model_name",
    #     "related_name",
    #     "on_delete",
    #     "null",
    #     "default",
    #     "pk",
    #     "index",
    #     "description",
    #     "generated",
    #     "source_field",
    # ],
    # "OneToOneField": [
    #     "model_name",
    #     "related_name",
    #     "on_delete",
    #     "null",
    #     "default",
    #     "pk",
    #     "unique",
    #     "index",
    #     "description",
    #     "generated",
    #     "source_field",
    # ],
    # "ManyToManyField": [
    #     "model_name",
    #     "related_name",
    #     "through",
    #     "forward_key",
    #     "backward_key",
    #     "description",
    # ],
}


MIGRATE_TEMPLATE = """# Generated by Plastron {version} on {dt}
from tortoise import fields

from plastron import operations as op
{additional_imports}

revision = "{revision}"
down_revision = {down_revision}

upgrade_operations = [
    {upgrade_operations}
]

downgrade_operations = [
    {downgrade_operations}
]

"""


class Serializer:
    def __init__(self, operations, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._operations = operations

        self.additional_imports = []
        self.upgrade_operations = []
        self.downgrade_operations = []

    def _value_to_code(self, value):
        # TODO: currently we don't support functions, enums and classes in field kwargs
        # Callables -> get bare name
        # if callable(value) and hasattr(value, "__name__"):
        #     self.additional_imports.append(f"import {value.__module__}")
        #     return f"{value.__module__}.{value.__name__}"

        # # Enums -> Class.Member
        # if (
        #     hasattr(value, "__class__")
        #     and hasattr(value, "name")
        #     and hasattr(value, "value")
        # ):
        #     # heuristic for Enum-like objects
        #     cls = value.__class__.__name__
        #     return f"{cls}.{value.name}"

        if isinstance(value, str):
            # Wrap string values in quotes
            return f'"{value}"'

        return value

    def _construct_base_field(self, field):
        cls = field.__class__
        sig = inspect.signature(field.__class__.__init__)
        required = {}
        for name in sig.parameters:
            # only a handful of parameters are without default values, so only get those
            params = ("max_length", "max_digits", "decimal_places")
            if name in params and hasattr(field, name):
                required[name] = getattr(field, name)

        # when auto_now is set to True, auto_now_add can't be set to true as well
        # as Tortoise doesn't like that
        if required.get("auto_now"):
            required["auto_now_add"] = False
        return cls(**required)

    def _gather_field_kwargs(self, field):
        cls_name = field.__class__.__name__

        base_field = self._construct_base_field(field)
        kwargs = {}
        for k in FIELD_ARG_ORDER[cls_name]:
            field_arg_name = FIELD_ARG_NAME_MAP.get(k, k)
            if hasattr(field, field_arg_name):
                value = getattr(field, field_arg_name)
                # If value is the default value of the field (just for base Field
                # kwargs) don't add them to the signature to keep migration file code
                # bloat to a minimum
                base_value = getattr(base_field, field_arg_name)
                if k in FIELD_ARGS and value == base_value:
                    continue

                kwargs[k] = value

        # when auto_now is set to True, auto_now_add can't be set to true as well
        # as Tortoise doesn't like that
        if kwargs.get("auto_now"):
            kwargs["auto_now_add"] = False
        return kwargs

    def _field_to_code(self, field):
        kwargs = self._gather_field_kwargs(field)
        parts = []
        for key, value in kwargs.items():
            if value is None:
                continue
            parts.append(f"{key}={self._value_to_code(value)}")

        return f"fields.{field.__class__.__name__}({', '.join(parts)})"

    def _create_table_op(self, table, data):
        fields = []
        for column_name, field in data["fields"]:
            fields.append(f'("{column_name}", {self._field_to_code(field)})')

        return (
            f'op.CreateTable("{table}", '
            f"fields=[{','.join(fields)}], "
            f"unique_together={json.dumps(data['unique_together'])})"
        )

    def _add_index_op(self, table, columns):
        return f'op.AddIndex("{table}", columns={json.dumps(columns)})'

    def _drop_index_op(self, table, columns):
        return f'op.DropIndex("{table}", columns={json.dumps(columns)})'

    def _drop_table_op(self, table):
        return f'op.DropTable("{table}")'

    def _add_unique_together_op(self, table, columns):
        return f'op.AddUniqueTogether("{table}", columns={json.dumps(columns)})'

    def _drop_unique_together_op(self, table, columns):
        return f'op.DropUniqueTogether("{table}", columns={json.dumps(columns)})'

    def _add_column_op(self, table, data):
        return (
            f'op.AddColumn("{table}", '
            f'"{data["name"]}", {self._field_to_code(data["field"])})'
        )

    def _drop_column_op(self, table, column_name):
        return f'op.DropColumn("{table}", "{column_name}")'

    def _alter_column_op(self, table, field_name, field):
        return (
            f'op.AlterColumn("{table}", "{field_name}", {self._field_to_code(field)})'
        )

    def _serialize_operation(self, operation):
        op, table, data = operation
        match op:
            case "CreateTable":
                self.upgrade_operations.append(self._create_table_op(table, data))
                self.downgrade_operations.append(self._drop_table_op(table))

                for columns in data["indexes"]:
                    self.upgrade_operations.append(self._add_index_op(table, columns))
                    self.downgrade_operations.append(
                        self._drop_index_op(table, columns)
                    )

            case "DropTable":
                for columns in data["indexes"]:
                    self.upgrade_operations.append(self._drop_index_op(table, columns))

                self.upgrade_operations.append(self._drop_table_op(table))
                self.downgrade_operations.append(self._create_table_op(table, data))

                for columns in data["indexes"]:
                    self.downgrade_operations.append(self._add_index_op(table, columns))

            case "AddUniqueTogether":
                self.upgrade_operations.append(
                    self._add_unique_together_op(table, data)
                )
                self.downgrade_operations.append(
                    self._drop_unique_together_op(table, data)
                )

            case "DropUniqueTogether":
                self.upgrade_operations.append(
                    self._drop_unique_together_op(table, data)
                )
                self.downgrade_operations.append(
                    self._add_unique_together_op(table, data)
                )

            case "AddIndex":
                self.upgrade_operations.append(self._add_index_op(table, data))
                self.downgrade_operations.append(self._drop_index_op(table, data))

            case "DropIndex":
                self.upgrade_operations.append(self._drop_index_op(table, data))
                self.downgrade_operations.append(self._add_index_op(table, data))

            case "AddColumn":
                self.upgrade_operations.append(self._add_column_op(table, data))
                self.downgrade_operations.append(
                    self._drop_column_op(table, data["name"])
                )

            case "DropColumn":
                self.upgrade_operations.append(
                    self._drop_column_op(table, data["name"])
                )
                self.downgrade_operations.append(self._add_column_op(table, data))

            case "AlterColumn":
                self.upgrade_operations.append(
                    self._alter_column_op(table, data["name"], data["new_field"])
                )
                self.downgrade_operations.append(
                    self._alter_column_op(table, data["name"], data["old_field"])
                )

            case _:
                raise NotImplementedError(f"Missing serializer for operation: {op}")

    def _serialize(self):
        for operation in self._operations:
            self._serialize_operation(operation)

    def _clean_migration_name(self, name):
        # First strip empty characters in case it's a string
        name = name.strip() if name else name
        if not name:
            return "auto"
        if not re.fullmatch(r"[A-Za-z0-9 _-]+", name):
            raise ValueError(f"Invalid migration name: {name}")
        return "_".join([x for x in name.split() if x.strip()]).lower()

    async def write_migration_file(
        self,
        revision,
        down_revision,
        migrations_folder,
        migration_name=None,
    ):
        self._serialize()

        down = None
        if len(down_revision) > 1:
            down = tuple(down_revision)
        elif down_revision:
            down = f'"{down_revision[0]}"'

        content = MIGRATE_TEMPLATE.format(
            revision=revision,
            down_revision=down,
            version=plastron.__version__,
            dt=datetime.now(UTC).replace(microsecond=0).isoformat(),
            upgrade_operations=",\n    ".join(self.upgrade_operations),
            downgrade_operations=",\n    ".join(reversed(self.downgrade_operations)),
            additional_imports="\n ".join(self.additional_imports)
            if self.additional_imports
            else "",
        )
        mode = black.Mode()
        content = black.format_str(content, mode=mode)

        filename = f"{revision}_{self._clean_migration_name(migration_name)}.py"
        path = Path(migrations_folder) / filename

        async with aiofiles.open(path, mode="w") as f:
            await f.write(content)

        return filename
