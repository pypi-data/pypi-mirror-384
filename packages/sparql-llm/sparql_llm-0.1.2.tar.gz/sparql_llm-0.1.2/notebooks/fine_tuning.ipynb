{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ Generating files for fine-tuning from SPARQL endpoints\n",
    "\n",
    "Using OpenAI JSONL schema: https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "Questions:\n",
    "* Which info can I get from the UniProt endpoint?\n",
    "* Give me example of queries to retrieve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing endpoint https://sparql.uniprot.org/sparql/\n",
      "Found 51 queries\n",
      "Processing endpoint https://www.bgee.org/sparql/\n",
      "Found 0 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7873/2708457306.py:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from SPARQLWrapper import JSON, SPARQLWrapper\n",
    "\n",
    "# system_prompt = \"Expasy is an assistant that helps you query the databases from the Swiss Institute of Bioinformatics, such as UniProt or Bgee.\"\n",
    "# system_prompt = \"Expasy is an assistant that helps you query the databases from the Swiss Institute of Bioinformatics, such as UniProt or Bgee. It provides high-quality SPARQL queries to retrieve information from these databases, and that all prefixes are well defined\"\n",
    "system_prompt = \"Expasy is an assistant that helps you query the databases from the Swiss Institute of Bioinformatics, such as UniProt or Bgee. It learns how to answer questions by using the questions/queries pairs provided from a catalog of examples.\"\n",
    "\n",
    "endpoints = [\n",
    "    \"https://sparql.uniprot.org/sparql/\",\n",
    "    \"https://www.bgee.org/sparql/\",\n",
    "]\n",
    "\n",
    "jsonl_str: str = \"\"\n",
    "for endpoint_url in endpoints:\n",
    "    print(f\"Processing endpoint {endpoint_url}\")\n",
    "    sparql_endpoint = SPARQLWrapper(endpoint_url)\n",
    "    sparql_endpoint.setReturnFormat(JSON)\n",
    "\n",
    "    def remove_a_tags(html_text: str) -> str:\n",
    "        \"\"\"Remove all <a> tags from the queries descriptions\"\"\"\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        for a_tag in soup.find_all(\"a\"):\n",
    "            a_tag.replace_with(a_tag.text)\n",
    "        return soup.get_text()\n",
    "\n",
    "    get_queries = \"\"\"PREFIX sh: <http://www.w3.org/ns/shacl#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?comment ?query\n",
    "    WHERE\n",
    "    {\n",
    "        ?sq a sh:SPARQLExecutable ;\n",
    "            rdfs:comment ?comment ;\n",
    "            sh:select ?query .\n",
    "    }\"\"\"\n",
    "\n",
    "    get_prefixes = \"\"\"PREFIX sh: <http://www.w3.org/ns/shacl#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?prefix ?namespace\n",
    "    WHERE {\n",
    "        [] sh:namespace ?namespace ;\n",
    "            sh:prefix ?prefix .\n",
    "    } ORDER BY ?prefix\"\"\"\n",
    "\n",
    "    sparql_endpoint.setQuery(get_prefixes)\n",
    "    results = sparql_endpoint.query().convert()\n",
    "    prefix_map = {}\n",
    "    for row in results[\"results\"][\"bindings\"]:\n",
    "        prefix_map[row[\"prefix\"][\"value\"]] = row[\"namespace\"][\"value\"]\n",
    "\n",
    "    # print(f\"Found {len(prefix_map)} prefixes\")\n",
    "\n",
    "    sparql_endpoint.setQuery(get_queries)\n",
    "    results = sparql_endpoint.query().convert()\n",
    "    queries_list = []\n",
    "    for row in results[\"results\"][\"bindings\"]:\n",
    "        queries_list.append(\n",
    "            {\n",
    "                \"comment\": row[\"comment\"][\"value\"],\n",
    "                \"query\": row[\"query\"][\"value\"],\n",
    "            }\n",
    "        )\n",
    "        # queries_map[row[\"comment\"][\"value\"]] = row[\"query\"][\"value\"]\n",
    "\n",
    "    print(f\"Found {len(queries_list)} queries\")\n",
    "\n",
    "    for q in queries_list:\n",
    "        query = q[\"query\"]\n",
    "        # Add prefixes to queries\n",
    "        for prefix, namespace in prefix_map.items():\n",
    "            prefix_str = f\"PREFIX {prefix}: <{namespace}>\"\n",
    "            if not re.search(prefix_str, query) and re.search(\n",
    "                f\"[(| |\\u00a0|/]{prefix}:\", query\n",
    "            ):\n",
    "                query = f\"{prefix_str}\\n{query}\"\n",
    "\n",
    "        bot_resp = f\"This question can be answered by executing the query below on the endpoint available at {endpoint_url}:\\n\\n```sparql\\n{query}\\n```\"\n",
    "\n",
    "        jsonl_str += (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": remove_a_tags(q[\"comment\"])},\n",
    "                        {\"role\": \"assistant\", \"content\": bot_resp},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "    # print(jsonl_str)\n",
    "\n",
    "with open(\"../data/finetuning_queries.jsonl\", \"w\") as f:\n",
    "    f.write(jsonl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-6IH2oUWN9xTmCgtkmZe72GBy\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "do_fine_tune = False\n",
    "\n",
    "if do_fine_tune:\n",
    "    file = client.files.create(\n",
    "        file=open(\"../data/finetuning_queries.jsonl\", \"rb\"), purpose=\"fine-tune\"\n",
    "    )\n",
    "    print(file.id)\n",
    "\n",
    "    ft_job = client.fine_tuning.jobs.create(\n",
    "        training_file=file.id,\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        # model=\"gpt-3.5-turbo-0125\",\n",
    "        hyperparameters={\n",
    "            \"n_epochs\": 20,\n",
    "            \"batch_size\": 1,\n",
    "            \"learning_rate\": 0.1,\n",
    "        },\n",
    "    )\n",
    "    print(ft_job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expasy-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
