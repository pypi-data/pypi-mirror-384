"""Sequential method based on asymptotic solution to optimal stopping PDE

This module defines the sequential test for the 2x2 Bernoulli contingency table as a special
case of the analysis presented in Lai (1988), "Nearly Optimal Sequential Tests of Composite
Hypotheses" (Annals of Statistics 16(2): 856-886 [June, 1988]).
DOI: 10.1214/aos/1176350840
URL: https://projecteuclid.org/journals/annals-of-statistics/volume-16/issue-2/Nearly-Optimal-Sequential-Tests-of-Composite-Hypotheses/10.1214/aos/1176350840.full
"""

import concurrent.futures
import os
import warnings
from functools import partial
from typing import Dict, Tuple, Union

import numpy as np
from numpy.typing import ArrayLike
from scipy import interpolate, stats
from tqdm import tqdm

from sequentialized_barnard_tests.base import (
    Decision,
    Hypothesis,
    SequentialTestBase,
    TestResult,
)
from sequentialized_barnard_tests.utils.utils_lai import (
    calculate_gamma,
    calculate_robust_zeta,
    run_test_step_gamma_uniparameter,
)


class LaiTest(SequentialTestBase):
    """Lai test for comparing two Bernoulli distributions (2x2 Contingency Table).

    This class defines the sequential test for the 2x2 Bernoulli contingency table as a special
    case of the analysis presented in Lai (1988), "Nearly Optimal Sequential Tests of Composite
    Hypotheses" (Annals of Statistics 16(2): 856-886 [June, 1988]).

    Attributes:
        alternative (Hypothesis): Specification of the alternative hypothesis.
        alpha (float): Significance level of the test, lies in (0., 1.).
        calibration_correction (float): Significance level of calibration, lies in (0., alpha).
        minimum_gap (float): Minimal allowed gap, >= 0. Defaults to 0.
        n_max (int): Maximal allowed length of the test sequence. Must be greater than 0.
        c (float): Regularized cost calibrated to (n_max, alpha). Must lie in (0., 1.)
    """

    def __init__(
        self,
        alternative: Hypothesis,
        n_max: int,
        alpha: float,
        minimum_gap: float = 0.0,
        calibrate_regularizer: bool = True,
        use_offline_calibration: bool = True,
        default_c: float = 4.3321e-05,
        n_calibration_sequences: int = 10000,
        calibration_seed: int = 42,
        verbose: bool = False,
    ) -> None:
        """Initializes the test object.

        Args:
            alternative (Hypothesis): Specification of the alternative hypothesis.
            n_max (int): Maximal sequence length. Must be greater than 0.
            alpha (float): Significance level of the test. Must lie in (0., 1.)
            minimum_gap (float, optional): Minimal gap in the alternative
                space. Nonnegative. Defaults to 0.0 (robust solution).
            calibrate_regularizer (bool, optional): Toggle whether to calibrate the
                value of c or set a default value of default_c. Defaults to True.
            use_offline_calibration (bool, optional): If True, use offline calibration
                data generated by `scripts/calibrate_lai_offline.py`. Defaults to True.
            default_c (float, optional): If calibrate_regularizer is False, self.c is
                set to default_c. Defaults to 4.3321e-05, which is tuned for
                n_max = 500, alpha = 0.05, and minimum_gap = 0.0.
            n_calibration_sequences (int, optional): If calibrate_regularizer is True,
                self.c is tuned via calibration, where the number of sequences equals
                n_calibration_sequences. Defaults to 10000.
            calibration_seed (int, optional): If calibrating the Lai procedure,
                use this seed in order to generate the sequences. Defaults to 42.
            verbose (bool, optional): If True, print the outputs to stdout.
                Defaults to False.

        Raise:
            ValueError: If the inputs are invalid.
        """

        # Handle erroneous inputs
        try:
            assert n_max > 0
            assert 0.0 < alpha < 1.0
            assert minimum_gap >= 0.0 and minimum_gap <= 1.0
        except:
            raise ValueError(
                "Invalid inputs: MUST HAVE n_max > 0, alpha in (0., 1.), minimum_gap in [0., 1.]"
            )
        # Assign attributes
        self.n_max = n_max
        self.alpha = alpha
        self.alternative = alternative
        self.minimum_gap = minimum_gap

        # Assign derived attributes
        self.calibration_correction = np.minimum(alpha / 50.0, 1e-3)

        # Initialize Lai uniparameter test attributes
        self._zeta = calculate_robust_zeta(minimum_gap)
        self._theta_0 = 0.5
        self._theta_1 = 0.5 + self._zeta

        # Initialize Lai procedure optimization regularizers
        self.c = None
        self._gamma = None  # Note that self._gamma is updated automatically whenever self.c is updated.

        # Assign values to these regularizers either via calibration or by explicitly setting c.
        if calibrate_regularizer:
            # User requested to calibrate.
            if not np.isclose(default_c, 4.3321e-05):
                warnings.warn(
                    (
                        "User tried to set `default_c` but this will be ignored "
                        "since `calibrate_regularizer` is True."
                    )
                )
            if use_offline_calibration:
                # User requested to use offline data.
                try:
                    calibration_path = os.path.join(
                        os.path.dirname(__file__),
                        "data/lai_calibration_data.npy",
                    )
                    calibration_data_dict = np.load(
                        calibration_path, allow_pickle=True
                    ).item()
                    self.calibrate_c_offline(calibration_data_dict, verbose=verbose)
                except FileNotFoundError:
                    warnings.warn(
                        (
                            f"Calibration file not found at {calibration_path}. "
                            "Run `scripts/calibrate_lai_offline.py` to generate it. "
                            "Switching to online calibration for now."
                        )
                    )
                    self.calibrate_c(
                        n_calibration_sequences=n_calibration_sequences,
                        seed=calibration_seed,
                        verbose=verbose,
                    )
            else:
                # Use requested to perform online calibration.
                self.calibrate_c(
                    n_calibration_sequences=n_calibration_sequences,
                    seed=calibration_seed,
                    verbose=verbose,
                )
        elif use_offline_calibration:
            # User requested to not calibrate but demanded to use offline data.
            warnings.warn(
                (
                    "User requested to use offline data for calibration but "
                    "`calibrate_regularizer` is set to False. The request is ignored."
                )
            )
            self.set_c(default_c, verbose)
        else:
            # User requested to set c to a specified value.
            if n_calibration_sequences != 10000:
                warnings.warn(
                    (
                        "User tried to set `n_calibration_sequences` but this will be "
                        "ignored since `calibrate_regularizer` is False."
                    )
                )
            self.set_c(default_c, verbose)

        # Run the reset method by default
        self._state = None
        self._t = None
        self._current_decision = None
        self.reset(verbose)

    def step(
        self,
        datum_0: Union[bool, int, float],
        datum_1: Union[bool, int, float],
        verbose: bool = False,
    ) -> TestResult:
        """Runs the test procedure on a single pair of Bernoulli data.

        Args:
            datum_0: Bernoulli datum from the first source.
            datum_1: Bernoulli datum from the second source.
            verbose (optional): If True, print the outputs to stdout. Defaults to False.

        Returns:
            TestResult: Result of the hypothesis test.

        Raise:
            ValueError: If the input data take non-Bernoulli values.
        """
        is_bernoulli_0 = datum_0 in [0, 1]
        is_bernoulli_1 = datum_1 in [0, 1]
        if not (is_bernoulli_0 and is_bernoulli_1):
            raise (ValueError("Input data are not interpretable as Bernoulli."))
        if verbose:
            print(
                (
                    "Update the Lai process given new "
                    f"datum_0 == {datum_0} and datum_1 == {datum_1}."
                )
            )
        # Iterate time (total number of pairs seen)
        self._t += 1
        if self._t > self.n_max:
            warnings.warn(
                "Have exceeded the allowed number of evals; not updating internal states."
            )
            info = {"Time": self._t, "State": self._state}
            result = TestResult(self._current_decision, info)

            return result

        # Update state {#1s, (#1s + #-1s), #-1s}
        if np.isclose(datum_1, datum_0):
            # Got a zero, so no update
            info = {"Time": self._t, "State": self._state}
            result = TestResult(self._current_decision, info)

            return result

        else:
            eval_result = self._update_state_and_run_lai(datum_0, datum_1)

            if eval_result > 0.5:
                # Reject Null and Accept Alternative
                self._current_decision = Decision.AcceptAlternative
            else:
                self._current_decision = Decision.FailToDecide

            info = {"Time": self._t, "State": self._state}
            result = TestResult(self._current_decision, info)

            return result

    def reset(
        self,
        verbose: bool = False,
    ) -> None:
        """Resets the underlying Lai process.

        Args:
            verbose (bool, optional): If True, print the outputs to stdout.
                Defaults to False.
        """
        self._state = np.zeros(3)
        self._t = int(0)
        self._current_decision = Decision.FailToDecide

        if self.alternative == Hypothesis.P0MoreThanP1:
            if verbose:
                print("    Null:        P0 <= P1")
                print("    Alternative: P0 >  P1")
        elif self.alternative == Hypothesis.P0LessThanP1:
            if verbose:
                print("    Null:        P0 >= P1")
                print("    Alternative: P0 <  P1")

    def set_c(self, new_c: float, verbose: bool = False) -> None:
        """Set the optimization regularizer c in (0., 1.) to a user-specified value.
        Doing so updates the derived parameter (gamma) in the optimization schema.

        Args:
            new_c (float): Optimization regularizer. Lies in (0., 1.)
            verbose (optional): If True, print the outputs to stdout. Defaults to False.
        """
        try:
            assert new_c > 0.0 and new_c < 1.0
        except:
            raise ValueError("Regularizer c must be in (0., 1.)")
        if verbose:
            print(f"Setting the regularizer term to {new_c}.")
        self.c = new_c
        self._gamma = calculate_gamma(self._theta_0, self._theta_1, self.c)

    def calibrate_c(
        self,
        n_calibration_sequences: int = 10000,
        seed: int = 42,
        verbose: bool = False,
    ) -> None:
        """Set the optimization regularizer c in (0., 1.) using Monte Carlo estimation
        and a high-probability upper bound. Doing so updates the derived parameter
        (gamma) in the optimization schema.

        Args:
            n_calibration_sequences (int, optional): Number of Monte Carlo
                sequences. Must be greater than 100. Defaults to 10000.
            seed (int, optional): Seed for the numpy random Generator object, from which
                the calibration sequences are drawn. This ensures reproducibility.
                Defaults to 42.
            verbose (optional): If True, print the outputs to stdout. Defaults to False.
        Raise:
            ValueError: If the number of calibration sequences does not exceed 100.
        """
        try:
            assert n_calibration_sequences >= 100
        except:
            raise ValueError(
                "Insufficient calibration sequences. Must have at least 100."
            )
        if verbose:
            print("Calibrating the regularizer term using Monte Carlo sampling.")
        # Define the Generator object
        rng = np.random.default_rng(seed=seed)

        # Generate calibration sequences
        sequences = rng.binomial(1, 0.5, size=(n_calibration_sequences, self.n_max, 2))

        # Track erroneous decisions
        erroneous_decisions = int(0)

        # High-probability lower bound error count
        target_error_count = stats.binom.ppf(
            self.calibration_correction,
            n_calibration_sequences,
            self.alpha - self.calibration_correction,
        )

        # Binary search to converge on c
        log_c_max = 0.0
        log_c_min = -16.0

        fpr_error = n_calibration_sequences
        # Add termination guarantee via tightness bound on log_c_error
        minimum_log_c_error = 1e-5
        maximum_number_of_iterations = int(
            np.floor(np.log((log_c_max - log_c_min) / minimum_log_c_error) + 1)
        )

        calibration_progress_bar = tqdm(total=maximum_number_of_iterations)
        while (
            np.abs(fpr_error) >= 1.5
            and np.abs(log_c_max - log_c_min) > minimum_log_c_error
        ):
            # Update estimate of c
            log_c_mid = 0.5 * (log_c_max + log_c_min)
            c = np.exp(log_c_mid)
            self.set_c(c, verbose=False)
            n_workers = max(os.cpu_count() - 2, 1)
            # Use ProcessPoolExecutor to parallelize the inner loop
            with concurrent.futures.ProcessPoolExecutor(
                max_workers=n_workers
            ) as executor:
                # Bind self to the helper function using partial
                func = partial(worker_run_test_for_calibration, self)
                # Prepare the list of sequences to process
                seq_list = [sequences[k, :, :] for k in range(n_calibration_sequences)]
                # Map the helper function over all calibration sequences
                count_list = list(executor.map(func, seq_list))
            # Sum the results to count the erroneous decisions.
            erroneous_decisions = sum(count_list)

            fpr_error = erroneous_decisions - target_error_count

            if fpr_error >= 1.5:
                # Empirical FPR is too high --> reduce c
                log_c_max = log_c_mid
            elif fpr_error <= -1.5:
                # Empirical FPR is too low --> increase c
                log_c_min = log_c_mid
            else:
                break

            calibration_progress_bar.update(1)

        # Store in self.c and update self.gamma
        self.set_c(c, verbose=verbose)

    def calibrate_c_offline(
        self, data_dict: Dict[Tuple[float, int], float], verbose: float = False
    ):
        if verbose:
            print("Calibrating the regularizer term using offline data.")
        alpha_list = sorted(set(key[0] for key in data_dict.keys()))
        n_max_list = sorted(set(key[1] for key in data_dict.keys()))

        # Fit a regression curve in the (log_n_max, log_c) space for each alpha.
        log_slope_list = []
        log_intercept_list = []
        for alpha in alpha_list:
            c_array = np.array([data_dict[(alpha, n_max)] for n_max in n_max_list])
            log_c_array = np.log(c_array)
            log_n_max_array = np.log(n_max_list)
            # log_c is empirically linear as a function of log_n_max
            regression_result = stats.linregress(log_n_max_array, log_c_array)
            log_slope_list.append(regression_result.slope)
            log_intercept_list.append(regression_result.intercept)

        # Interpolate the slope and the intercept as a function of alpha.
        log_slope = interpolate.interp1d(
            alpha_list, log_slope_list, kind="cubic", fill_value="extrapolate"
        )
        log_intercept = interpolate.interp1d(
            alpha_list,
            log_intercept_list,
            kind="cubic",
            fill_value="extrapolate",
        )

        # Obtain the value of c using the model.
        log_c = log_slope(self.alpha) * np.log(self.n_max) + log_intercept(self.alpha)
        # Make sure log_c < 0 so c < 1.
        log_c_clipped = np.clip(log_c, a_min=None, a_max=-1e-10)

        self.set_c(np.exp(log_c_clipped), verbose=verbose)

    def _update_state_and_run_lai(
        self,
        datum_0: Union[bool, int, float],
        datum_1: Union[bool, int, float],
    ) -> float:
        """Update the internal state and return the Lai eval result.

        Args:
            datum_0: Bernoulli datum from the first source.
            datum_1: Bernoulli datum from the second source.

        Returns:
            Output of `run_test_step_gamma_uniparameter`.
        """
        self._state[1] += 1
        if datum_1 > datum_0:  # Difference of 1
            self._state[0] += 1
        else:  # Difference of -1
            self._state[2] += 1

        if self.alternative == Hypothesis.P0LessThanP1:
            # Test statistic is #1s / (#-1s + #1s)
            statistic = self._state[0] / self._state[1]
        elif self.alternative == Hypothesis.P0MoreThanP1:
            # Test statistic is #-1s / (#-1s + #1s)
            statistic = self._state[2] / self._state[1]

        # Run general procedure using the statistic
        eval_result = run_test_step_gamma_uniparameter(
            self.c,
            self._state[1],
            statistic,
            self._theta_0,
            gamma=self._gamma,
        )
        return eval_result


class MirroredLaiTest(LaiTest):
    """Mirrored Lai Test for comparing two Bernoulli distribiutions (2x2 Contingency
    Table).

    This class defines the mirrored version of the Lai Test, which has the ability to
    accept the Null Hypothesis in addition to reject it. Otherwise it is the same as
    LaiTest.

    Attributes:
        alternative (Hypothesis): Specification of the alternative hypothesis.
        alpha (float): Significance level of the test, lies in (0., 1.).
        calibration_correction (float): Significance level of calibration, lies in (0., alpha).
        minimum_gap (float): Minimal allowed gap, >= 0. Defaults to 0.
        n_max (int): Maximal allowed length of the test sequence. Must be greater than 0.
        c (float): Regularized cost calibrated to (n_max, alpha). Must lie in (0., 1.)
    """

    def __init__(
        self,
        alternative: Hypothesis,
        n_max: int,
        alpha: float,
        minimum_gap: float = 0.0,
        calibrate_regularizer: bool = True,
        use_offline_calibration: bool = True,
        default_c: float = 4.3321e-05,
        n_calibration_sequences: int = 10000,
        calibration_seed: int = 42,
        verbose: bool = False,
    ) -> None:
        """Initializes the test object.

        Args:
            alternative (Hypothesis): Specification of the alternative hypothesis.
            n_max (int): Maximal sequence length. Must be greater than 0.
            alpha (float): Significance level of the test. Must lie in (0., 1.)
            minimum_gap (float, optional): Minimal gap in the alternative
                space. Nonnegative. Defaults to 0.0 (robust solution).
            calibrate_regularizer (bool, optional): Toggle whether to calibrate the
                value of c or set a default value of default_c. Defaults to True.
            use_offline_calibration (bool, optional): If True, use offline calibration
                data generated by `scripts/calibrate_lai_offline.py`. Defaults to True.
            default_c (float, optional): If calibrate_regularizer is False, self.c is
                set to default_c. Defaults to 4.3321e-05, which is tuned for
                n_max = 500, alpha = 0.05, and minimum_gap = 0.0.
            n_calibration_sequences (int, optional): If calibrate_regularizer is True,
                self.c is tuned via calibration, where the number of sequences equals
                n_calibration_sequences. Defaults to 10000.
            calibration_seed (int, optional): If calibrating the Lai procedure,
                use this seed in order to generate the sequences. Defaults to 42.
            verbose (bool, optional): If True, print the outputs to stdout.
                Defaults to False.

        Raise:
            ValueError: If the inputs are invalid.
        """

        super().__init__(
            alternative,
            n_max,
            alpha,
            minimum_gap,
            calibrate_regularizer,
            use_offline_calibration,
            default_c,
            n_calibration_sequences,
            calibration_seed,
            verbose,
        )

    def step(
        self,
        datum_0: Union[bool, int, float],
        datum_1: Union[bool, int, float],
        verbose: bool = False,
    ) -> TestResult:
        """Runs the test procedure on a single pair of Bernoulli data.

        Args:
            datum_0: Bernoulli datum from the first source.
            datum_1: Bernoulli datum from the second source.
            verbose (optional): If True, print the outputs to stdout. Defaults to False.

        Returns:
            TestResult: Result of the hypothesis test.

        Raise:
            ValueError: If the input data take non-Bernoulli values.
        """
        is_bernoulli_0 = datum_0 in [0, 1]
        is_bernoulli_1 = datum_1 in [0, 1]
        if not (is_bernoulli_0 and is_bernoulli_1):
            raise (ValueError("Input data are not interpretable as Bernoulli."))
        if verbose:
            print(
                (
                    "Update the Lai process given new "
                    f"datum_0 == {datum_0} and datum_1 == {datum_1}."
                )
            )
        # Iterate time (total number of pairs seen)
        self._t += 1
        if self._t > self.n_max:
            warnings.warn(
                "Have exceeded the allowed number of evals; not updating internal states."
            )
            info = {"Time": self._t, "State": self._state}
            result = TestResult(self._current_decision, info)

            return result

        # Update state {#1s, (#1s + #-1s), #-1s}
        if np.isclose(datum_1, datum_0):
            # Got a zero, so no update
            info = {"Time": self._t, "State": self._state}
            result = TestResult(self._current_decision, info)

            return result

        else:
            eval_result = self._update_state_and_run_lai(datum_0, datum_1)

            if eval_result > 0.5:
                # Reject Null and Accept Alternative
                self._current_decision = Decision.AcceptAlternative
            elif eval_result < -0.5:
                self._current_decision = Decision.AcceptNull
            else:
                self._current_decision = Decision.FailToDecide

            info = {"Time": self._t, "State": self._state}
            result = TestResult(self._current_decision, info)

            return result


# Define this helper function at the module level so it's picklable.
def worker_run_test_for_calibration(instance: LaiTest, paired_sequence: ArrayLike):
    """The worker function for calibrtion of the regularizer `c`.

    Args:
        instance: A test object.
        sequence: A paired Bernoulli sequence of shape (n_max, 2).

    Returns:
        1 if AcceptAlternative, 0 if otherwise.
    """
    # seq is a 2D numpy array of shape (n_max, 2)
    result = instance.run_on_sequence(paired_sequence[:, 0], paired_sequence[:, 1])
    # Return 1 if the decision is AcceptAlternative, else 0.
    return 1 if result.decision == Decision.AcceptAlternative else 0
