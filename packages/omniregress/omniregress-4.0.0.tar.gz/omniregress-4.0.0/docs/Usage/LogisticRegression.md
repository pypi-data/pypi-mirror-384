
# ğŸš€ Logistic Regression: Quick & Cool Guide  

## ğŸŒŸ Overview  
The `LogisticRegression` class provides an implementation of logistic regression for binary classification. Key features:  

- ğŸ¯ Binary classification with probability outputs  
- ğŸ“Š Sigmoid function for probability estimation  
- âš™ï¸ Configurable learning rate and iterations  
- ğŸ” Coefficient and intercept interpretation  

## ğŸ“ Code Example  

```python
from omniregress import LogisticRegression

def test_logistic_regression():
    print("=== Testing Logistic Regression ===")

    # Binary classification data
    X = [[0.5], [1.0], [1.5], [2.0], [2.5], [3.0]]
    y = [0, 0, 0, 1, 1, 1]

    # Initialize and fit model
    model = LogisticRegression(learning_rate=0.1, max_iter=1000)
    model.fit(X, y)

    # Model parameters
    print(f"Coefficients: {model.coefficients}")
    print(f"Intercept: {model.intercept}")

    # Predictions
    test_X = [[1.0], [2.0], [3.5]]
    probabilities = model.predict_proba(test_X)
    predictions = model.predict(test_X)

    print(f"Probabilities: {probabilities}")
    print(f"Predictions: {predictions}")

    # Verification
    assert predictions[0] == 0, "Should predict class 0"
    assert predictions[1] == 1, "Should predict class 1"
    assert predictions[2] == 1, "Should predict class 1"
    assert all(0 <= p <= 1 for p in probabilities), "Invalid probabilities"

    print("Logistic Regression Test Passed!\n")
```

## ğŸ”‘ Key Features  

### ğŸ“ˆ Probability Estimation  
Uses sigmoid function to output probabilities between 0 and 1:  
`probability = 1 / (1 + e^(-(wx + b)))`  

### âš™ï¸ Hyperparameters  
- `learning_rate`: Controls step size in gradient descent  
- `max_iter`: Maximum number of training iterations  

### ğŸ“Š Outputs  
- `predict_proba()`: Returns class probabilities  
- `predict()`: Returns class labels (0/1)  

## ğŸ§  How It Works  
1. Initializes weights randomly  
2. Computes gradients using cross-entropy loss  
3. Updates weights via gradient descent  
4. Applies sigmoid activation for predictions  

## ğŸ† When to Use  
âœ”ï¸ Binary classification problems  
âœ”ï¸ Probabilistic interpretation needed  
âœ”ï¸ Linear decision boundary is sufficient  
