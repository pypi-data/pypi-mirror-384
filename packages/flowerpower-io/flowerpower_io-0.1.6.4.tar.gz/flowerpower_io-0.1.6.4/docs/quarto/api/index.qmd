---
title: "API Reference"
description: "Complete API reference for flowerpower-io library"
---

# API Reference

Welcome to the flowerpower-io API reference documentation. This comprehensive guide covers all public classes, functions, and methods available in the library.

## Overview

The flowerpower-io library provides a unified interface for data I/O operations across multiple formats and storage systems. It supports:

- **File Formats**: CSV, JSON, Parquet, Delta Lake
- **Databases**: SQLite, PostgreSQL, MySQL, Oracle, MSSQL, DuckDB
- **Data Processing**: Polars, Pandas, PyArrow, DataFusion
- **Cloud Storage**: AWS S3, Google Cloud Storage, Azure Blob Storage
- **Message Queues**: MQTT

## Quick Navigation

### Base Classes

The foundation of the library consists of several base classes that provide common functionality:

- [`BaseFileReader`](base.qmd#basefilereader) - Base class for file readers
- [`BaseFileWriter`](base.qmd#basefilewriter) - Base class for file writers
- [`BaseDatasetReader`](base.qmd#basedatasetreader) - Base class for dataset readers
- [`BaseDatasetWriter`](base.qmd#basedatasetwriter) - Base class for dataset writers
- [`BaseDatabaseReader`](base.qmd#basedatabasereader) - Base class for database readers
- [`BaseDatabaseWriter`](base.qmd#basedatabasewriter) - Base class for database writers
- [`BaseMQTTReader`](base.qmd#basemqtreader) - Base class for MQTT readers
- [`BaseMQTTWriter`](base.qmd#basemqttwriter) - Base class for MQTT writers

### Loaders

Data loaders provide convenient interfaces for reading data from various sources:

#### File Loaders
- [`CSVFileReader`](loader.qmd#csvfilereader) - Read CSV files
- [`JsonFileReader`](loader.qmd#jsonfilereader) - Read JSON files
- [`ParquetFileReader`](loader.qmd#parquetfilereader) - Read Parquet files
- [`DeltaTableFileReader`](loader.qmd#deltatablefilereader) - Read Delta Lake files

#### Dataset Loaders
- [`CSVDatasetReader`](loader.qmd#csvdatasetreader) - Read CSV datasets
- [`JsonDatasetReader`](loader.qmd#jsondatasetreader) - Read JSON datasets
- [`ParquetDatasetReader`](loader.qmd#parquetdatasetreader) - Read Parquet datasets
- [`DeltaTableDatasetReader`](loader.qmd#deltatabledatasetreader) - Read Delta Lake datasets

#### Database Loaders
- [`SQLiteReader`](loader.qmd#sqlitereader) - Read SQLite databases
- [`PostgreSQLReader`](loader.qmd#postgresqlreader) - Read PostgreSQL databases
- [`MySQLReader`](loader.qmd#mysqlreader) - Read MySQL databases
- [`OracleDBReader`](loader.qmd#oracledbreader) - Read Oracle databases
- [`MSSQLReader`](loader.qmd#mssqlreader) - Read Microsoft SQL Server databases
- [`DuckDBReader`](loader.qmd#duckdbreader) - Read DuckDB databases

#### Specialized Loaders
- [`PydalaDatasetReader`](loader.qmd#pydaladatasetreader) - Read Pydala datasets
- [`MQTTReader`](loader.qmd#mqttreader) - Read from MQTT topics

### Savers

Data savers provide convenient interfaces for writing data to various destinations:

#### File Writers
- [`CSVFileWriter`](saver.qmd#csvfilewriter) - Write CSV files
- [`JsonFileWriter`](saver.qmd#jsonfilewriter) - Write JSON files

#### Dataset Writers
- [`CSVDatasetWriter`](saver.qmd#csvdatasetwriter) - Write CSV datasets
- [`JsonDatasetWriter`](saver.qmd#jsondatasetwriter) - Write JSON datasets
- [`DeltaTableWriter`](saver.qmd#deltatablewriter) - Write Delta Lake tables
- [`PydalaDatasetWriter`](saver.qmd#pydaladatasetwriter) - Write Pydala datasets

#### Database Writers
- [`DuckDBWriter`](saver.qmd#duckdbwriter) - Write DuckDB databases
- [`MSSQLWriter`](saver.qmd#mssqlwriter) - Write Microsoft SQL Server databases
- [`MySQLWriter`](saver.qmd#mysqlwriter) - Write MySQL databases
- [`OracleDBWriter`](saver.qmd#oracledbwriter) - Write Oracle databases
- [`PostgreSQLWriter`](saver.qmd#postgresqlwriter) - Write PostgreSQL databases


### Metadata Functions

Comprehensive metadata extraction capabilities:

- [`get_dataframe_metadata()`](metadata.qmd#get_dataframe_metadata) - Extract DataFrame metadata
- [`get_file_metadata()`](metadata.qmd#get_file_metadata) - Extract file metadata
- [`get_table_metadata()`](metadata.qmd#get_table_metadata) - Extract table metadata
- [`get_column_metadata()`](metadata.qmd#get_column_metadata) - Extract column metadata
- [`get_schema_metadata()`](metadata.qmd#get_schema_metadata) - Extract schema metadata
- [`get_statistics_metadata()`](metadata.qmd#get_statistics_metadata) - Extract statistics metadata
- [`get_quality_metadata()`](metadata.qmd#get_quality_metadata) - Extract data quality metadata
- [`get_completeness_metadata()`](metadata.qmd#get_completeness_metadata) - Extract completeness metadata
- [`get_uniqueness_metadata()`](metadata.qmd#get_uniqueness_metadata) - Extract uniqueness metadata
- [`get_consistency_metadata()`](metadata.qmd#get_consistency_metadata) - Extract consistency metadata
- [`get_timeliness_metadata()`](metadata.qmd#get_timeliness_metadata) - Extract timeliness metadata
- [`get_validity_metadata()`](metadata.qmd#get_validity_metadata) - Extract validity metadata
- [`get_accuracy_metadata()`](metadata.qmd#get_accuracy_metadata) - Extract accuracy metadata

## Usage Patterns

### Basic Data Loading

```python
from flowerpower_io.loader import CSVFileReader, ParquetFileReader

# Read CSV file
csv_reader = CSVFileReader("data.csv")
df = csv_reader.to_pandas()

# Read Parquet file
parquet_reader = ParquetFileReader("data.parquet")
df = parquet_reader.to_polars()
```

### Basic Data Writing

```python
from flowerpower_io.saver import CSVFileWriter, DeltaTableWriter

# Write CSV file
csv_writer = CSVFileWriter("output.csv")
csv_writer.write(df)

# Write Delta Lake table
delta_writer = DeltaTableWriter("delta_table/")
delta_writer.write(df)
```

### Database Operations

```python
from flowerpower_io.loader import PostgreSQLReader
from flowerpower_io.saver import PostgreSQLWriter

# Read from PostgreSQL
pg_reader = PostgreSQLReader(
    table_name="users",
    host="localhost",
    port=5432,
    username="user",
    password="password",
    database="mydb"
)
df = pg_reader.to_pyarrow_table()

# Write to PostgreSQL
pg_writer = PostgreSQLWriter(
    table_name="users",
    host="localhost",
    port=5432,
    username="user",
    password="password",
    database="mydb"
)
pg_writer.write(df)
```

### Metadata Extraction

```python
from flowerpower_io.metadata import get_dataframe_metadata, get_quality_metadata

# Get basic metadata
metadata = get_dataframe_metadata(df)

# Get quality metadata
quality = get_quality_metadata(df)
```


## Advanced Features

### Custom Storage Options

```python
from flowerpower_io.loader import ParquetFileReader

# Read from S3
reader = ParquetFileReader(
    path="s3://bucket/data.parquet",
    storage_options={
        "key": "your-access-key",
        "secret": "your-secret-key",
        "client_kwargs": {
            "region_name": "us-east-1"
        }
    }
)
df = reader.to_polars(lazy=True)
```

### Delta Lake Advanced Features

```python
from flowerpower_io.saver import DeltaTableWriter

# Write with partitioning and schema evolution
writer = DeltaTableWriter("delta_table/")
writer.write(
    df,
    mode="append",
    partition_by=["date"],
    schema_mode="merge",
    predicate="active = true"
)
```

### Database Connection Management


## Error Handling

The library provides comprehensive error handling:

```python
from flowerpower_io.loader import CSVFileReader

try:
    reader = CSVFileReader("data.csv")
    df = reader.to_duckdb()
except FileNotFoundError:
    print("File not found")
except Exception as e:
    print(f"Error reading file: {e}")
```

## Performance Considerations

- Use lazy loading with large datasets where possible
- Consider partitioning for large Delta Lake tables
- Use appropriate compression settings for file formats
- Leverage connection pooling for database operations

## Best Practices

1. **Use appropriate data types** for your data to optimize storage and performance
2. **Implement proper error handling** for production applications
3. **Use connection pooling** for database operations
4. **Consider data partitioning** for large datasets
5. **Use metadata functions** for data quality monitoring
6. **Leverage lazy evaluation** for large datasets when possible

## Contributing

If you find any issues or have suggestions for improvements, please refer to the main documentation for contribution guidelines.

## License

This library is licensed under the MIT License. See the LICENSE file for details.