[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "This guide will walk you through a simple example of using flowerpower-io to load data from a JSON file and save it to a CSV file.\n\n\nFirst, let’s create a sample JSON file named data.json:\n[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"age\": 30\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"age\": 24\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Charlie\",\n    \"age\": 35\n  }\n]\nSave this content to a file named data.json in the same directory where you’ll run your Python script.\n\n\n\nNow, create a Python script (e.g., quickstart_example.py) with the following content:\n\nfrom fsspec_utils.utils.types import dict_to_dataframe\nfrom flowerpower_io.loader import ParquetFileReader\nfrom flowerpower_io.saver import CSVFileWriter, ParquetFileWriter\nimport json\nimport os\n\n# Create dummy JSON data\njson_data = \"\"\"\n[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"age\": 30\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"age\": 24\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Charlie\",\n    \"age\": 35\n  }\n]\n\"\"\"\n\n# Create dataframe from json data\ndf = dict_to_dataframe(json.loads(json_data))\nsaver1 = ParquetFileWriter(\"data.parquet\")\nsaver1.write(df)\n\n# Load data from Parquet\nloader = ParquetFileReader(path=\"data.parquet\")\ndata_frame = loader.to_pandas()\n\nprint(\"Data loaded from Parquet:\")\nprint(data_frame)\n\n# Save data to CSV\nsaver = CSVFileWriter(path=\"output.csv\")\nsaver.write(data_frame)\n\nprint(\"\\nData saved to output.csv\")\n\n# Clean up generated files (optional)\nos.remove(\"data.parquet\")\nos.remove(\"output.csv\")\n\n\n\nExecute the Python script. If you’ve installed flowerpower-io in a virtual environment, make sure it’s activated:\nuv run python quickstart_example.py\nYou should see output similar to this, indicating the data was loaded and saved successfully:\nData loaded from JSON:\nshape: (3, 3)\n┌─────┬─────────┬─────┐\n│ id  ┆ name    ┆ age │\n│ --- ┆ ---     ┆ --- │\n│ i64 ┆ str     ┆ i64 │\n╞═════╪═════════╪═════╡\n│ 1   ┆ Alice   ┆ 30  │\n│ 2   ┆ Bob     ┆ 24  │\n│ 3   ┆ Charlie ┆ 35  │\n└─────┴─────────┴─────┘\n\nData saved to output.csv\nThis example demonstrates the basic workflow of using flowerpower-io to move data between different formats. You can extend this to various other supported formats and sources.",
    "crumbs": [
      "Home",
      "Overview",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#prepare-your-data",
    "href": "quickstart.html#prepare-your-data",
    "title": "Quickstart",
    "section": "",
    "text": "First, let’s create a sample JSON file named data.json:\n[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"age\": 30\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"age\": 24\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Charlie\",\n    \"age\": 35\n  }\n]\nSave this content to a file named data.json in the same directory where you’ll run your Python script.",
    "crumbs": [
      "Home",
      "Overview",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#write-the-python-script",
    "href": "quickstart.html#write-the-python-script",
    "title": "Quickstart",
    "section": "",
    "text": "Now, create a Python script (e.g., quickstart_example.py) with the following content:\n\nfrom fsspec_utils.utils.types import dict_to_dataframe\nfrom flowerpower_io.loader import ParquetFileReader\nfrom flowerpower_io.saver import CSVFileWriter, ParquetFileWriter\nimport json\nimport os\n\n# Create dummy JSON data\njson_data = \"\"\"\n[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"age\": 30\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"age\": 24\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Charlie\",\n    \"age\": 35\n  }\n]\n\"\"\"\n\n# Create dataframe from json data\ndf = dict_to_dataframe(json.loads(json_data))\nsaver1 = ParquetFileWriter(\"data.parquet\")\nsaver1.write(df)\n\n# Load data from Parquet\nloader = ParquetFileReader(path=\"data.parquet\")\ndata_frame = loader.to_pandas()\n\nprint(\"Data loaded from Parquet:\")\nprint(data_frame)\n\n# Save data to CSV\nsaver = CSVFileWriter(path=\"output.csv\")\nsaver.write(data_frame)\n\nprint(\"\\nData saved to output.csv\")\n\n# Clean up generated files (optional)\nos.remove(\"data.parquet\")\nos.remove(\"output.csv\")",
    "crumbs": [
      "Home",
      "Overview",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#run-the-script",
    "href": "quickstart.html#run-the-script",
    "title": "Quickstart",
    "section": "",
    "text": "Execute the Python script. If you’ve installed flowerpower-io in a virtual environment, make sure it’s activated:\nuv run python quickstart_example.py\nYou should see output similar to this, indicating the data was loaded and saved successfully:\nData loaded from JSON:\nshape: (3, 3)\n┌─────┬─────────┬─────┐\n│ id  ┆ name    ┆ age │\n│ --- ┆ ---     ┆ --- │\n│ i64 ┆ str     ┆ i64 │\n╞═════╪═════════╪═════╡\n│ 1   ┆ Alice   ┆ 30  │\n│ 2   ┆ Bob     ┆ 24  │\n│ 3   ┆ Charlie ┆ 35  │\n└─────┴─────────┴─────┘\n\nData saved to output.csv\nThis example demonstrates the basic workflow of using flowerpower-io to move data between different formats. You can extend this to various other supported formats and sources.",
    "crumbs": [
      "Home",
      "Overview",
      "Quickstart"
    ]
  },
  {
    "objectID": "api/base.html",
    "href": "api/base.html",
    "title": "Base Classes",
    "section": "",
    "text": "This section documents the foundational base classes within flowerpower-io, which provide the core functionality for file and database I/O operations.",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basefileio",
    "href": "api/base.html#basefileio",
    "title": "Base Classes",
    "section": "BaseFileIO",
    "text": "BaseFileIO\n\n\n\n\n\n\nBaseFileIO\n\n\n\nBaseFileIO is the base class for file I/O operations, supporting various storage backends like AWS S3, Google Cloud Storage, Azure Blob Storage, GitHub, and GitLab. It handles filesystem initialization and provides methods for listing files.\n\n\nDefinition:\nclass BaseFileIO(msgspec.Struct):\n    path: str | list[str]\n    storage_options: (\n        StorageOptions\n        | AwsStorageOptions\n        | AzureStorageOptions\n        | GcsStorageOptions\n        | GitLabStorageOptions\n        | GitHubStorageOptions\n        | dict[str, Any]\n        | None\n    ) = field(default=None)\n    fs: AbstractFileSystem | None = field(default=None)\n    format: str | None = None\nArguments:\n\npath (str | list[str]): Path or list of paths to file(s).\nstorage_options (StorageOptions | ... | dict[str, Any] | None, optional): Storage-specific options for accessing remote filesystems.\nfs (AbstractFileSystem | None, optional): Filesystem instance for handling file operations.\nformat (str | None, optional): File format extension (without dot).\n\nMethods:\n\nlist_files(): Lists files based on the configured path and filesystem.\n\nExample:\n\n\nShow the code\nfrom flowerpower_io.base import BaseFileIO\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\n# Example 1: Local file system\nfile_io_local = BaseFileIO(path=\"data/my_file.csv\")\nprint(f\"Local file protocol: {file_io_local.protocol}\")\n\n# Example 2: S3 bucket with storage options\n# In a real scenario, replace with your actual key and secret\nfile_io_s3 = BaseFileIO(\n    path=\"s3://my-bucket/path/to/data/\",\n    storage_options=AwsStorageOptions(key=\"YOUR_ACCESS_KEY\", secret=\"YOUR_SECRET_KEY\")\n)\nprint(f\"S3 file protocol: {file_io_s3.protocol}\")\n\n# Example 3: Listing files (hypothetical, requires actual files)\n# Assuming 'data/' contains 'file1.txt', 'file2.txt'\n# from fsspec import filesystem\n# fs = filesystem(\"file\")\n# fs.makedirs(\"data\", exist_ok=True)\n# with fs.open(\"data/file1.txt\", \"w\") as f: f.write(\"hello\")\n# with fs.open(\"data/file2.txt\", \"w\") as f: f.write(\"world\")\n# file_io_local_list = BaseFileIO(path=\"data/\")\n# print(f\"Files in data/: {file_io_local_list.list_files()}\")",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basefilereader",
    "href": "api/base.html#basefilereader",
    "title": "Base Classes",
    "section": "BaseFileReader",
    "text": "BaseFileReader\n\n\n\n\n\n\nBaseFileReader\n\n\n\nBaseFileReader extends BaseFileIO to provide a foundation for reading data from various file formats (CSV, Parquet, JSON, etc.) into different data structures like Pandas DataFrames, Polars DataFrames, or PyArrow Tables. It supports batch processing and integration with DuckDB and DataFusion.\n\n\nDefinition:\nclass BaseFileReader(BaseFileIO):\n    include_file_path: bool = field(default=False)\n    concat: bool = field(default=True)\n    batch_size: int | None = field(default=None)\n    opt_dtypes: bool = field(default=False)\n    use_threads: bool = field(default=True)\n    conn: duckdb.DuckDBPyConnection | None = field(default=None)\n    ctx: datafusion.SessionContext | None = field(default=None)\n    jsonlines: bool | None = field(default=None)\n    partitioning: str | list[str] | pds.Partitioning | None = field(default=None)\n    verbose: bool | None = field(default=None)\nArguments:\n\npath (str | list[str]): Path or list of paths to file(s). (Inherited from BaseFileIO)\nformat (str, optional): File format extension (without dot). (Inherited from BaseFileIO)\nfs (AbstractFileSystem, optional): Filesystem instance. (Inherited from BaseFileIO)\ninclude_file_path (bool, optional): Include file path in the output DataFrame. Defaults to False.\nconcat (bool, optional): Concatenate multiple files into a single DataFrame. Defaults to True.\nbatch_size (int | None, optional): Batch size for iteration.\nopt_dtypes (bool, optional): Optimize data types. Defaults to False.\nuse_threads (bool, optional): Use threads for reading data. Defaults to True.\nconn (duckdb.DuckDBPyConnection | None, optional): DuckDB connection instance.\nctx (datafusion.SessionContext | None, optional): DataFusion session context instance.\njsonlines (bool | None, optional): Treat JSON files as JSON lines.\npartitioning (str | list[str] | pds.Partitioning | None, optional): Dataset partitioning scheme.\nverbose (bool | None, optional): Enable verbose output.\n\nMethods:\n\nto_pandas(...): Converts data to Pandas DataFrame(s).\niter_pandas(...): Iterates over Pandas DataFrames.\nto_polars(lazy=False, ...): Converts data to Polars DataFrame(s).\nto_polars(lazy=True, ...): Converts data to Polars LazyFrame(s).\niter_polars(lazy=False, ...): Iterates over Polars DataFrames.\niter_polars(lazy=True, ...): Iterates over Polars LazyFrames.\nto_pyarrow_table(...): Converts data to PyArrow Table(s).\niter_pyarrow_table(...): Iterates over PyArrow Tables.\nto_duckdb_relation(...): Converts data to a DuckDB relation.\nregister_in_duckdb(...): Registers data in a DuckDB connection.\nto_duckdb(...): Converts data to DuckDB relation or registers it.\nregister_in_datafusion(...): Registers data in a DataFusion session context.\nfilter(filter_expr): Filters data based on a given expression.\nmetadata: Property that returns metadata about the loaded data.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nimport os\nfrom flowerpower_io.base import BaseFileReader\n\n# Create a dummy CSV file\ndummy_csv_path = \"temp_reader_data.csv\"\npd.DataFrame({'col1': [1, 2, 3], 'col2': ['A', 'B', 'C']}).to_csv(dummy_csv_path, index=False)\n\n# Initialize BaseFileReader\nreader = BaseFileReader(path=dummy_csv_path, format=\"csv\")\n\n# Convert to Pandas DataFrame\ndf_pandas = reader.to_pandas()\nprint(\"Pandas DataFrame:\")\nprint(df_pandas)\n\n# Convert to Polars DataFrame\ndf_polars = reader.to_polars()\nprint(\"\\nPolars DataFrame:\")\nprint(df_polars)\n\n# Get metadata\nprint(\"\\nMetadata:\")\n_, meta = reader.to_pandas(metadata=True)\nprint(meta)\n\n# Clean up\nos.remove(dummy_csv_path)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basedatasetreader",
    "href": "api/base.html#basedatasetreader",
    "title": "Base Classes",
    "section": "BaseDatasetReader",
    "text": "BaseDatasetReader\n\n\n\n\n\n\nBaseDatasetReader\n\n\n\nBaseDatasetReader is specialized for reading datasets, particularly useful for partitioned data. It builds upon BaseFileReader and provides specific methods for working with PyArrow Datasets and Pydala ParquetDatasets.\n\n\nDefinition:\nclass BaseDatasetReader(BaseFileReader):\n    schema_: pa.Schema | None = field(default=None)\n    _dataset: pds.Dataset | None = field(default=None)\n    _pydala_dataset: Any | None = field(default=None)\nArguments:\n\nInherits all arguments from BaseFileReader.\nschema_ (pa.Schema | None, optional): PyArrow schema for the dataset.\npartitioning (Inherited from BaseFileReader): Dataset partitioning scheme.\n\nMethods:\n\nto_pyarrow_dataset(...): Converts data to PyArrow Dataset.\nto_pandas(...): Converts data to Pandas DataFrame.\nto_polars(lazy=True, ...): Converts data to Polars LazyFrame.\nto_polars(lazy=False, ...): Converts data to Polars DataFrame.\nto_pyarrow_table(...): Converts data to PyArrow Table.\nto_pydala_dataset(...): Converts data to Pydala ParquetDataset.\nto_duckdb_relation(...): Converts data to DuckDB relation.\nregister_in_duckdb(...): Registers data in DuckDB.\nto_duckdb(...): Converts data to DuckDB relation or registers it.\nregister_in_datafusion(...): Registers data in DataFusion.\nfilter(filter_expr): Filters data based on a given expression.\nmetadata: Property that returns metadata about the loaded data.\n\nExample:\n\n\nShow the code\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nimport shutil\nfrom flowerpower_io.base import BaseDatasetReader\n\n# Create a dummy partitioned dataset\ndataset_path = \"temp_dataset\"\nos.makedirs(os.path.join(dataset_path, \"year=2023\"), exist_ok=True)\nos.makedirs(os.path.join(dataset_path, \"year=2024\"), exist_ok=True)\n\ntable1 = pa.table({'col1': [1, 2], 'col2': ['A', 'B'], 'year': [2023, 2023]})\ntable2 = pa.table({'col1': [3, 4], 'col2': ['C', 'D'], 'year': [2024, 2024]})\n\npq.write_table(table1, os.path.join(dataset_path, \"year=2023\", \"part1.parquet\"))\npq.write_table(table2, os.path.join(dataset_path, \"year=2024\", \"part2.parquet\"))\n\n# Initialize BaseDatasetReader for a partitioned dataset\nreader = BaseDatasetReader(\n    path=dataset_path,\n    format=\"parquet\",\n    partitioning=\"hive\" # Specify hive style partitioning\n)\n\n# Load as Polars DataFrame\ndf_dataset = reader.to_polars()\nprint(\"Polars DataFrame from partitioned dataset:\")\nprint(df_dataset)\n\n# Clean up\nshutil.rmtree(dataset_path)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basefilewriter",
    "href": "api/base.html#basefilewriter",
    "title": "Base Classes",
    "section": "BaseFileWriter",
    "text": "BaseFileWriter\n\n\n\n\n\n\nBaseFileWriter\n\n\n\nBaseFileWriter is the base class for writing data to files across various storage backends and formats. It supports different write modes (append, overwrite) and data deduplication.\n\n\nDefinition:\nclass BaseFileWriter(BaseFileIO):\n    basename: str | None = field(default=None)\n    concat: bool = field(default=False)\n    mode: str = field(default=\"append\")  # append, overwrite, delete_matching, error_if_exists\n    unique: bool | list[str] | str = field(default=False)\nArguments:\n\nInherits all arguments from BaseFileIO.\nbasename (str | None, optional): Basename for the output file(s).\nconcat (bool, optional): Concatenate multiple files into a single DataFrame. Defaults to False.\nmode (str, optional): Write mode (\"append\", \"overwrite\", \"delete_matching\", \"error_if_exists\"). Defaults to \"append\".\nunique (bool | list[str] | str, optional): Unique columns for deduplication. Defaults to False.\n\nMethods:\n\nwrite(data, ...): Writes the provided data to the configured path.\nmetadata: Property that returns metadata about the written data.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nimport os\nfrom flowerpower_io.base import BaseFileWriter\n\n# Create dummy data\ndf_to_write = pd.DataFrame({'colA': [1, 2, 3], 'colB': ['X', 'Y', 'Z']})\noutput_csv_path = \"temp_writer_output.csv\"\n\n# Initialize BaseFileWriter\nwriter = BaseFileWriter(path=output_csv_path, format=\"csv\", mode=\"overwrite\")\n\n# Write data\nwriter.write(data=df_to_write)\nprint(f\"Data written to {output_csv_path}\")\nprint(f\"File exists: {os.path.exists(output_csv_path)}\")\n\n# Clean up\nos.remove(output_csv_path)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basedatasetwriter",
    "href": "api/base.html#basedatasetwriter",
    "title": "Base Classes",
    "section": "BaseDatasetWriter",
    "text": "BaseDatasetWriter\n\n\n\n\n\n\nBaseDatasetWriter\n\n\n\nBaseDatasetWriter is designed for writing datasets, with enhanced control over partitioning, compression, and file sizing, building on BaseFileWriter. It’s particularly useful for creating optimized data lakes.\n\n\nDefinition:\nclass BaseDatasetWriter(BaseFileWriter):\n    schema_: pa.Schema | None = None\n    partition_by: str | list[str] | pds.Partitioning | None = None\n    partitioning_flavor: str | None = None\n    compression: str = \"zstd\"\n    row_group_size: int | None = 250_000\n    max_rows_per_file: int | None = 2_500_000\n    is_pydala_dataset: bool = False\nArguments:\n\nInherits all arguments from BaseFileWriter.\nschema_ (pa.Schema | None, optional): PyArrow schema for the dataset.\npartition_by (str | list[str] | pds.Partitioning | None, optional): Dataset partitioning scheme.\npartitioning_flavor (str | None, optional): Partitioning flavor (e.g., \"hive\").\ncompression (str, optional): Compression codec. Defaults to \"zstd\".\nrow_group_size (int | None, optional): Row group size for Parquet. Defaults to 250_000.\nmax_rows_per_file (int | None, optional): Maximum rows per output file. Defaults to 2_500_000.\nis_pydala_dataset (bool, optional): Whether to write as a Pydala ParquetDataset. Defaults to False.\n\nMethods:\n\nwrite(data, ...): Writes the provided data as a dataset.\nmetadata: Property that returns metadata about the written data.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nimport pyarrow as pa\nimport os\nimport shutil\nfrom flowerpower_io.base import BaseDatasetWriter\n\n# Create dummy data\ndf_dataset_to_write = pd.DataFrame({\n    'col1': [1, 2, 3, 4],\n    'col2': ['A', 'B', 'C', 'D'],\n    'partition_col': ['P1', 'P1', 'P2', 'P2']\n})\noutput_dataset_path = \"temp_dataset_output\"\n\n# Initialize BaseDatasetWriter for a partitioned Parquet dataset\nwriter = BaseDatasetWriter(\n    path=output_dataset_path,\n    format=\"parquet\",\n    mode=\"overwrite\",\n    partition_by=\"partition_col\",\n    partitioning_flavor=\"hive\",\n    compression=\"snappy\"\n)\n\n# Write data\nwriter.write(data=df_dataset_to_write)\nprint(f\"Data written to partitioned dataset at {output_dataset_path}\")\nprint(f\"Directory exists: {os.path.exists(output_dataset_path)}\")\nprint(f\"Partition P1 exists: {os.path.exists(os.path.join(output_dataset_path, 'partition_col=P1'))}\")\n\n# Clean up\nshutil.rmtree(output_dataset_path)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basedatabaseio",
    "href": "api/base.html#basedatabaseio",
    "title": "Base Classes",
    "section": "BaseDatabaseIO",
    "text": "BaseDatabaseIO\n\n\n\n\n\n\nBaseDatabaseIO\n\n\n\nBaseDatabaseIO serves as the base class for database input/output operations, abstracting away the specifics of connecting to various database systems like SQLite, DuckDB, PostgreSQL, MySQL, MSSQL, and Oracle. It handles connection string generation and query execution.\n\n\nDefinition:\nclass BaseDatabaseIO(msgspec.Struct):\n    type_: str\n    table_name: str = field(default=\"\")\n    path: str | None = field(default=None)\n    username: str | None = field(default=None)\n    password: str | None = field(default=None)\n    server: str | None = field(default=None)\n    port: str | int | None = field(default=None)\n    database: str | None = field(default=None)\n    ssl: bool = field(default=False)\n    connection_string: str | None = field(default=None)\nArguments:\n\ntype_ (str): Database type (e.g., \"sqlite\", \"duckdb\", \"postgres\").\ntable_name (str, optional): Table name in the database. Defaults to \"\".\npath (str | None, optional): File path for file-based databases (SQLite, DuckDB).\nusername (str | None, optional): Username for the database connection.\npassword (str | None, optional): Password for the database connection.\nserver (str | None, optional): Server address for the database.\nport (str | int | None, optional): Port number for the database.\ndatabase (str | None, optional): Database name.\nssl (bool, optional): Enable SSL connection. Defaults to False.\nconnection_string (str | None, optional): Full connection string (if provided, overrides other connection parameters).\n\nMethods:\n\nexecute(query, cursor=True, ...): Executes a SQL query.\nconnect(): Establishes a database connection.\n\nExample:\n\n\nShow the code\nimport os\nfrom flowerpower_io.base import BaseDatabaseIO\nimport sqlite3\n\n# Example with SQLite\ndb_path_io = \"temp_db_io.db\"\nconn_str_io = f\"sqlite:///{db_path_io}\"\n\n# Create a dummy table and insert data using raw SQLite\nconn = sqlite3.connect(db_path_io)\nconn.execute(\"CREATE TABLE IF NOT EXISTS my_table (id INTEGER, name TEXT);\")\nconn.execute(\"INSERT INTO my_table (id, name) VALUES (1, 'Test User');\")\nconn.commit()\nconn.close()\n\ndb_io = BaseDatabaseIO(type_=\"sqlite\", table_name=\"my_table\", path=db_path_io)\n\n# Execute a query\nresult = db_io.execute(\"SELECT * FROM my_table;\")\nprint(\"Query result (from BaseDatabaseIO.execute):\")\nfor row in result:\n    print(row)\n\n# Clean up\nos.remove(db_path_io)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basedatabasewriter",
    "href": "api/base.html#basedatabasewriter",
    "title": "Base Classes",
    "section": "BaseDatabaseWriter",
    "text": "BaseDatabaseWriter\n\n\n\n\n\n\nBaseDatabaseWriter\n\n\n\nBaseDatabaseWriter extends BaseDatabaseIO to provide functionality for writing data to various database systems. It supports different write modes like append, replace, and fail.\n\n\nDefinition:\nclass BaseDatabaseWriter(BaseDatabaseIO):\n    mode: str = field(default=\"append\")  # append, replace, fail\n    concat: bool = field(default=False)\n    unique: bool | list[str] | str = field(default=False)\nArguments:\n\nInherits all arguments from BaseDatabaseIO.\nmode (str, optional): Write mode (\"append\", \"replace\", \"fail\"). Defaults to \"append\".\nconcat (bool, optional): Concatenate multiple files into a single DataFrame (relevant for internal processing). Defaults to False.\nunique (bool | list[str] | str, optional): Unique columns for deduplication. Defaults to False.\n\nMethods:\n\nwrite(data, ...): Writes the provided data to the database table.\nmetadata: Property that returns metadata about the written data.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nimport os\nfrom flowerpower_io.base import BaseDatabaseWriter\n\ndb_path_writer = \"temp_writer_db.db\"\ndf_to_db = pd.DataFrame({'id': [101, 102], 'value': ['Alpha', 'Beta']})\n\n# Initialize BaseDatabaseWriter\nwriter = BaseDatabaseWriter(type_=\"sqlite\", table_name=\"new_table\", path=db_path_writer)\n\n# Write data\nwriter.write(data=df_to_db, mode=\"replace\") # Use \"replace\" to create/overwrite table\nprint(f\"Data written to {db_path_writer} in table 'new_table'.\")\n\n# Clean up\nos.remove(db_path_writer)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/base.html#basedatabasereader",
    "href": "api/base.html#basedatabasereader",
    "title": "Base Classes",
    "section": "BaseDatabaseReader",
    "text": "BaseDatabaseReader\n\n\n\n\n\n\nBaseDatabaseReader\n\n\n\nBaseDatabaseReader extends BaseDatabaseIO to focus on reading data from databases. It provides methods to load data into various data structures and supports executing custom SQL queries.\n\n\nDefinition:\nclass BaseDatabaseReader(BaseDatabaseIO):\n    query: str | None = None\nArguments:\n\nInherits all arguments from BaseDatabaseIO.\nquery (str | None, optional): SQL query to execute. If None, loads all data from the specified table_name.\n\nMethods:\n\nto_polars(...): Converts data to Polars DataFrame.\nto_pandas(...): Converts data to Pandas DataFrame.\nto_pyarrow_table(...): Converts data to PyArrow Table.\nto_duckdb_relation(...): Converts data to DuckDB relation.\nregister_in_duckdb(...): Registers data in DuckDB.\nregister_in_datafusion(...): Registers data in DataFusion.\nmetadata: Property that returns metadata about the loaded data.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nimport os\nimport sqlite3\nfrom flowerpower_io.base import BaseDatabaseReader\n\ndb_path_reader = \"temp_reader_db.db\"\n# Create a dummy table and insert data\nconn = sqlite3.connect(db_path_reader)\nconn.execute(\"CREATE TABLE IF NOT EXISTS sales (product TEXT, amount INTEGER);\")\nconn.execute(\"INSERT INTO sales (product, amount) VALUES ('A', 100), ('B', 150), ('A', 200);\")\nconn.commit()\nconn.close()\n\n# Initialize BaseDatabaseReader\nreader = BaseDatabaseReader(type_=\"sqlite\", table_name=\"sales\", path=db_path_reader)\n\n# Read all data\ndf_all_sales = reader.to_pandas()\nprint(\"All sales data:\")\nprint(df_all_sales)\n\n# Read data with a custom query\ndf_product_a = reader.to_pandas(query=\"SELECT * FROM sales WHERE product = 'A'\")\nprint(\"\\nSales for product 'A':\")\nprint(df_product_a)\n\n# Clean up\nos.remove(db_path_reader)",
    "crumbs": [
      "Home",
      "API Reference",
      "Base Classes"
    ]
  },
  {
    "objectID": "api/loader.html",
    "href": "api/loader.html",
    "title": "Loaders",
    "section": "",
    "text": "Loaders in flowerpower-io are responsible for reading data from various sources and formats, and converting them into a standardized data structure (typically a Polars DataFrame, but also supporting Pandas DataFrames and PyArrow Tables). All loader classes inherit from BaseFileReader or BaseDatabaseReader, providing a consistent API for data ingestion.",
    "crumbs": [
      "Home",
      "API Reference",
      "Loaders"
    ]
  },
  {
    "objectID": "api/loader.html#available-loader-classes",
    "href": "api/loader.html#available-loader-classes",
    "title": "Loaders",
    "section": "Available Loader Classes",
    "text": "Available Loader Classes\nHere’s a list of the currently supported loader classes:\n\nCSVFileReader and CSVDatasetReader: For reading data from CSV files and CSV datasets (e.g., partitioned CSVs).\nDeltaTableReader: For reading data from Delta Lake tables.\nDuckDBReader: For reading data from DuckDB databases.\nJsonFileReader and JsonDatasetReader: For reading data from JSON files and JSON datasets.\nMSSQLReader: For reading data from Microsoft SQL Server databases.\nMySQLReader: For reading data from MySQL databases.\nOracleDBReader: For reading data from Oracle databases.\nParquetFileReader and ParquetDatasetReader: For reading data from Parquet files and Parquet datasets.\nPostgreSQLReader: For reading data from PostgreSQL databases.\nPydalaDatasetReader: For reading data using Pydala’s dataset capabilities.\nSQLiteReader: For reading data from SQLite databases.\nMQTTLoader: For subscribing to and reading data from MQTT topics.",
    "crumbs": [
      "Home",
      "API Reference",
      "Loaders"
    ]
  },
  {
    "objectID": "api/loader.html#examples",
    "href": "api/loader.html#examples",
    "title": "Loaders",
    "section": "Examples",
    "text": "Examples\n\nCSV Loader (CSVFileReader)\nThe CSVFileReader allows you to easily load data from a CSV file.\n\nimport pandas as pd\nimport os\nfrom flowerpower_io.loader import CSVFileReader\n\n# Create a dummy CSV file\ndummy_csv_path = \"temp_data.csv\"\npd.DataFrame({'id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']}).to_csv(dummy_csv_path, index=False)\n\n# Load data from CSV\nloader = CSVFileReader(path=dummy_csv_path)\ndf = loader.to_pandas()\n\nprint(\"Data loaded from CSV:\")\nprint(df)\n\n# Clean up\nos.remove(dummy_csv_path)\n\n\n\nJSON Loader (JsonFileReader)\nThe JsonFileReader is used to load data from JSON files.\n\nimport pandas as pd\nimport os\nfrom flowerpower_io.loader import JsonFileReader\n\n# Create a dummy JSON file\ndummy_json_path = \"temp_data.json\"\njson_content = \"\"\"\n[\n  {\"product\": \"Laptop\", \"price\": 1200},\n  {\"product\": \"Mouse\", \"price\": 25},\n  {\"product\": \"Keyboard\", \"price\": 75}\n]\n\"\"\"\nwith open(dummy_json_path, \"w\") as f:\n    f.write(json_content)\n\n# Load data from JSON\nloader = JsonFileReader(path=dummy_json_path)\ndf_polars = loader.to_polars()\n\nprint(\"Data loaded from JSON (Polars DataFrame):\")\nprint(df_polars)\n\n# Clean up\nos.remove(dummy_json_path)\n\n\n\nSQLite Loader (SQLiteReader)\nThe SQLiteReader enables reading data from SQLite databases, including the execution of custom SQL queries.\n#| eval: false #| echo: true\nimport pandas as pd import os import sqlite3 from flowerpower_io.loader import SQLiteReader",
    "crumbs": [
      "Home",
      "API Reference",
      "Loaders"
    ]
  },
  {
    "objectID": "api/saver.html",
    "href": "api/saver.html",
    "title": "Savers",
    "section": "",
    "text": "Savers in flowerpower-io are responsible for writing data from a standardized data structure (typically a Polars DataFrame, but also supporting Pandas DataFrames and PyArrow Tables) to various destinations and formats. All saver classes inherit from BaseFileWriter or BaseDatabaseWriter, providing a consistent API for data output.",
    "crumbs": [
      "Home",
      "API Reference",
      "Savers"
    ]
  },
  {
    "objectID": "api/saver.html#available-saver-classes",
    "href": "api/saver.html#available-saver-classes",
    "title": "Savers",
    "section": "Available Saver Classes",
    "text": "Available Saver Classes\nHere’s a list of the currently supported saver classes:\n\nCSVFileWriter and CSVDatasetWriter: For writing data to CSV files and CSV datasets (e.g., partitioned CSVs).\nDeltaTableWriter: For writing data to Delta Lake tables.\nDuckDBWriter: For writing data to DuckDB databases.\nJsonFileWriter and JsonDatasetWriter: For writing data to JSON files and JSON datasets.\nMSSQLWriter: For writing data to Microsoft SQL Server databases.\nMySQLWriter: For writing data to MySQL databases.\nOracleDBWriter: For writing data to Oracle databases.\nParquetFileWriter and ParquetDatasetWriter: For writing data to Parquet files and Parquet datasets.\nPostgreSQLWriter: For writing data to PostgreSQL databases.\nPydalaDatasetWriter: For writing data using Pydala’s dataset capabilities.\nSQLiteWriter: For writing data to SQLite databases.\nMQTTSaver: For publishing data to MQTT topics.",
    "crumbs": [
      "Home",
      "API Reference",
      "Savers"
    ]
  },
  {
    "objectID": "api/saver.html#examples",
    "href": "api/saver.html#examples",
    "title": "Savers",
    "section": "Examples",
    "text": "Examples\n\nCSV Saver (CSVFileWriter)\nThe CSVFileWriter allows you to easily save data to a CSV file.\n\n\nShow the code\nimport pandas as pd\nimport os\nfrom flowerpower_io.saver import CSVFileWriter\n\n# Create a dummy DataFrame\ndf_to_save = pd.DataFrame({'id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\noutput_csv_path = \"output_data.csv\"\n\n# Save data to CSV\nwriter = CSVFileWriter(path=output_csv_path)\nwriter.write(df_to_save)\n\nprint(f\"Data saved to {output_csv_path}\")\nprint(f\"File exists: {os.path.exists(output_csv_path)}\")\n\n# Clean up\nos.remove(output_csv_path)\n\n\n\n\nJSON Saver (JsonFileWriter)\nThe JsonFileWriter is used to save data to JSON files.\n\n\nShow the code\nimport pandas as pd\nimport os\nfrom flowerpower_io.saver import JsonFileWriter\n\n# Create a dummy DataFrame\ndf_products = pd.DataFrame([\n  {\"product\": \"Laptop\", \"price\": 1200},\n  {\"product\": \"Mouse\", \"price\": 25},\n  {\"product\": \"Keyboard\", \"price\": 75}\n])\noutput_json_path = \"output_products.json\"\n\n# Save data to JSON\nwriter = JsonFileWriter(path=output_json_path)\nwriter.write(df_products)\n\nprint(f\"Data saved to {output_json_path}\")\nprint(f\"File exists: {os.path.exists(output_json_path)}\")\n\n# Clean up\nos.remove(output_json_path)\n\n\n\n\nSQLite Saver (SQLiteWriter)\nThe SQLiteWriter enables writing data to SQLite databases.\n\n\nShow the code\nimport pandas as pd\nimport os\nimport sqlite3\nfrom flowerpower_io.saver import SQLiteWriter\n\n# Create a dummy DataFrame\ndf_orders = pd.DataFrame({'order_id': [1, 2, 3], 'item': ['Book', 'Pen', 'Book'], 'quantity': [2, 5, 1]})\ndb_path = \"output_sales.db\"\n\n# Initialize SQLiteWriter\nwriter = SQLiteWriter(path=db_path, table_name=\"orders\", mode=\"replace\")\n\n# Save data to SQLite\nwriter.write(df_orders)\n\nprint(f\"Data saved to {db_path} in table 'orders'.\")\nprint(f\"Database file exists: {os.path.exists(db_path)}\")\n\n# Verify data by reading it back (optional)\nconn = sqlite3.connect(db_path)\ndf_verify = pd.read_sql(\"SELECT * FROM orders\", conn)\nprint(\"\\nData verified from SQLite:\")\nprint(df_verify)\nconn.close()\n\n# Clean up\nos.remove(db_path)",
    "crumbs": [
      "Home",
      "API Reference",
      "Savers"
    ]
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced Features",
    "section": "",
    "text": "flowerpower-io offers several advanced features and integration capabilities to handle complex data workflows, optimize performance, and connect with external systems.",
    "crumbs": [
      "Home",
      "Overview",
      "Advanced Features"
    ]
  },
  {
    "objectID": "advanced.html#performance-optimization",
    "href": "advanced.html#performance-optimization",
    "title": "Advanced Features",
    "section": "Performance Optimization",
    "text": "Performance Optimization\nflowerpower-io leverages efficient Python libraries like Polars and PyArrow for data processing, ensuring high performance, especially with large datasets.\n\nLazy Loading/Processing: For formats like Parquet, flowerpower-io can utilize lazy evaluation where supported, deferring computation until necessary, which can significantly reduce memory usage and improve performance.\nBatch Processing: When dealing with very large files, consider processing data in batches to manage memory effectively.",
    "crumbs": [
      "Home",
      "Overview",
      "Advanced Features"
    ]
  },
  {
    "objectID": "advanced.html#sql-integration",
    "href": "advanced.html#sql-integration",
    "title": "Advanced Features",
    "section": "SQL Integration",
    "text": "SQL Integration\nflowerpower-io provides robust integration with SQL databases, allowing you to not only load and save data but also execute custom SQL queries directly.\n\n\nShow the code\nfrom flowerpower_io.loader import SQLiteReader\nfrom flowerpower_io.saver import SQLiteWriter\nimport pandas as pd\nimport os\n\n# Setup a dummy SQLite database\ndb_path = \"temp_advanced.db\"\nconn_str = f\"sqlite:///{db_path}\"\ndf_data = pd.DataFrame({\n    'id': [1, 2, 3, 4, 5],\n    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'age': [25, 30, 35, 40, 45],\n    'city': ['New York', 'London', 'New York', 'Paris', 'London']\n})\n\n# Write data to SQLite\nsaver = SQLiteWriter(path=conn_str, table_name=\"users\")\nsaver.save(df_data)\nprint(f\"Data written to {db_path} in table 'users'.\")\n\n# Read all data\nreader = SQLiteReader(path=conn_str, table_name=\"users\")\ndf_all = reader.load()\nprint(\"\\nAll data from 'users' table:\")\nprint(df_all)\n\n# Execute a custom SQL query\nquery = \"SELECT name, age FROM users WHERE city = 'New York'\"\ndf_filtered = reader.load(query=query)\nprint(\"\\nUsers from New York:\")\nprint(df_filtered)\n\n# Clean up\nos.remove(db_path)\nprint(f\"\\nCleaned up {db_path}\")",
    "crumbs": [
      "Home",
      "Overview",
      "Advanced Features"
    ]
  },
  {
    "objectID": "advanced.html#duckdb-and-datafusion-integration",
    "href": "advanced.html#duckdb-and-datafusion-integration",
    "title": "Advanced Features",
    "section": "DuckDB and DataFusion Integration",
    "text": "DuckDB and DataFusion Integration\nflowerpower-io supports DuckDB and DataFusion, enabling powerful in-process analytical processing directly on your data files. This allows for SQL-like querying of various file formats without needing to load them entirely into memory.\n\nDuckDB Example\n\n\nShow the code\nfrom flowerpower_io.loader import DuckDBLoader\nfrom flowerpower_io.saver import DuckDBSaver\nimport pandas as pd\nimport os\n\n# Create a dummy CSV file for demonstration\ncsv_file = \"temp_data.csv\"\npd.DataFrame({'a': [1, 2, 3], 'b': ['x', 'y', 'z']}).to_csv(csv_file, index=False)\nprint(f\"Created dummy CSV: {csv_file}\")\n\n# Load data using DuckDBLoader and perform a SQL query\n# DuckDBLoader can query directly on files\nloader = DuckDBLoader(path=csv_file, table_name=\"temp_csv_table\")\n# You can query the CSV file directly as a table\ndf_result = loader.load(query=f\"SELECT a, b FROM '{csv_file}' WHERE a &gt; 1\")\nprint(\"\\nResult from DuckDB query on CSV:\")\nprint(df_result)\n\n# Save data to DuckDB\nduckdb_path = \"temp_duckdb.db\"\nsaver = DuckDBSaver(path=duckdb_path, table_name=\"my_table\")\nsaver.save(df_result)\nprint(f\"\\nData saved to DuckDB at {duckdb_path} in table 'my_table'.\")\n\n# Clean up\nos.remove(csv_file)\nif os.path.exists(duckdb_path):\n    os.remove(duckdb_path)\nprint(f\"Cleaned up {csv_file} and {duckdb_path}\")\n\n\n\n\n\n\n\n\nDataFusion\n\n\n\nDataFusion integration is primarily handled internally by the library for specific file formats (like Parquet). While there isn’t a direct DataFusionLoader or DataFusionSaver class, the library leverages DataFusion’s capabilities for efficient data manipulation when reading certain file types, especially when predicates are pushed down to the source.",
    "crumbs": [
      "Home",
      "Overview",
      "Advanced Features"
    ]
  },
  {
    "objectID": "advanced.html#integration-with-external-systems",
    "href": "advanced.html#integration-with-external-systems",
    "title": "Advanced Features",
    "section": "Integration with External Systems",
    "text": "Integration with External Systems\nflowerpower-io can be extended to integrate with various external systems.\n\nMessage Queues (e.g., MQTT)\nThe library includes MQTTLoader and MQTTSaver for real-time data streaming.\n\n\nShow the code\nfrom flowerpower_io.loader import MQTTLoader\nfrom flowerpower_io.saver import MQTTSaver\nimport polars as pl\nimport time\n\n# This example requires an MQTT broker running.\n# For demonstration, we'll simulate the process without a live broker connection.\n\n# Simulate receiving data\ndef simulate_mqtt_publish(topic, messages):\n    print(f\"\\nSimulating MQTT publish to topic: {topic}\")\n    for i, msg in enumerate(messages):\n        print(f\"Publishing: {msg}\")\n        # In a real scenario, this would be mqtt_client.publish(topic, msg)\n        time.sleep(0.1) # Simulate network delay\n\n# Simulate MQTT loader\nclass MockMQTTLoader(MQTTLoader):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._received_data = []\n\n    def load(self, timeout=1):\n        print(f\"Simulating MQTT load from topic: {self.topic}\")\n        # In a real scenario, this would connect and subscribe\n        # For demo, just return simulated data\n        if not self._received_data:\n            print(\"No simulated data to load.\")\n            return pl.DataFrame()\n        df = pl.DataFrame(self._received_data)\n        self._received_data = [] # Clear after loading\n        return df\n\n    def _on_message(self, client, userdata, msg):\n        # This method would parse the message payload\n        payload = msg.payload.decode('utf-8')\n        print(f\"Simulated MQTT message received: {payload}\")\n        self._received_data.append({\"message\": payload, \"timestamp\": time.time()})\n\n# Simulate MQTT saver\nclass MockMQTTSaver(MQTTSaver):\n    def save(self, data_frame):\n        print(f\"\\nSimulating MQTT save to topic: {self.topic}\")\n        for row in data_frame.iter_rows(named=True):\n            print(f\"Simulating publish: {row}\")\n            # In a real scenario, this would be mqtt_client.publish(self.topic, json.dumps(row))\n\n# Example usage with mock classes\ntopic = \"flowerpower/data\"\nmock_messages = [{\"value\": i} for i in range(3)]\ndf_to_publish = pl.DataFrame(mock_messages)\n\n# Use MockMQTTSaver\nsaver = MockMQTTSaver(topic=topic)\nsaver.save(df_to_publish)\n\n# Use MockMQTTLoader (assuming some data was published to it)\n# For this demo, we'll manually add some data to its internal buffer\nloader = MockMQTTLoader(topic=topic)\nloader._received_data = [{\"value\": 10}, {\"value\": 20}] # Manually add data for demo\ndf_loaded = loader.load()\nprint(\"\\nSimulated loaded data:\")\nprint(df_loaded)\n\n\n\n\n\n\n\n\nOther External Systems\n\n\n\nflowerpower-io can be integrated with other external systems like cloud storage (S3, GCS, Azure Blob Storage) or data warehouses (Snowflake, BigQuery) by leveraging underlying libraries (e.g., PyArrow for S3/GCS, specific database connectors). While direct flowerpower-io classes for these might not be explicitly listed, the modular design allows for custom implementations or direct use of the underlying libraries where flowerpower-io acts as an orchestrator.",
    "crumbs": [
      "Home",
      "Overview",
      "Advanced Features"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "We welcome contributions to flowerpower-io! Whether it’s reporting a bug, suggesting a new feature, or submitting a pull request, your help is valuable.\n\n\nIf you find a bug or have a feature request, please open an issue on our GitHub repository. When reporting a bug, please include:\n\nA clear and concise description of the issue.\nSteps to reproduce the behavior.\nExpected behavior.\nScreenshots or error messages, if applicable.\nYour operating system and Python version.\n\n\n\n\nWe encourage you to submit pull requests for bug fixes, new features, or improvements. Before submitting a pull request, please:\n\nFork the repository and clone it to your local machine.\nCreate a new branch for your changes: git checkout -b feature/your-feature-name or git checkout -b bugfix/issue-description.\nMake your changes and ensure your code adheres to the project’s coding style and conventions.\nWrite clear commit messages.\nWrite or update tests to cover your changes.\nEnsure all tests pass locally.\nUpdate documentation as needed.\nPush your branch to your forked repository.\nOpen a pull request to the main branch of the upstream repository.\n\n\n\n\nTo set up your development environment:\n\nClone the repository: bash     git clone https://github.com/flowerpower/flowerpower-io.git     cd flowerpower-io\nInstall dependencies: We recommend using uv or pixi for dependency management.\nUsing uv:\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev,test]\"\nUsing pixi:\npixi install\nRun tests: bash     uv run pytest or if using pixi: bash     pixi run pytest\n\nThank you for contributing to flowerpower-io!",
    "crumbs": [
      "Home",
      "Overview",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#reporting-issues",
    "href": "contributing.html#reporting-issues",
    "title": "Contributing",
    "section": "",
    "text": "If you find a bug or have a feature request, please open an issue on our GitHub repository. When reporting a bug, please include:\n\nA clear and concise description of the issue.\nSteps to reproduce the behavior.\nExpected behavior.\nScreenshots or error messages, if applicable.\nYour operating system and Python version.",
    "crumbs": [
      "Home",
      "Overview",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#submitting-pull-requests",
    "href": "contributing.html#submitting-pull-requests",
    "title": "Contributing",
    "section": "",
    "text": "We encourage you to submit pull requests for bug fixes, new features, or improvements. Before submitting a pull request, please:\n\nFork the repository and clone it to your local machine.\nCreate a new branch for your changes: git checkout -b feature/your-feature-name or git checkout -b bugfix/issue-description.\nMake your changes and ensure your code adheres to the project’s coding style and conventions.\nWrite clear commit messages.\nWrite or update tests to cover your changes.\nEnsure all tests pass locally.\nUpdate documentation as needed.\nPush your branch to your forked repository.\nOpen a pull request to the main branch of the upstream repository.",
    "crumbs": [
      "Home",
      "Overview",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#development-setup",
    "href": "contributing.html#development-setup",
    "title": "Contributing",
    "section": "",
    "text": "To set up your development environment:\n\nClone the repository: bash     git clone https://github.com/flowerpower/flowerpower-io.git     cd flowerpower-io\nInstall dependencies: We recommend using uv or pixi for dependency management.\nUsing uv:\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[dev,test]\"\nUsing pixi:\npixi install\nRun tests: bash     uv run pytest or if using pixi: bash     pixi run pytest\n\nThank you for contributing to flowerpower-io!",
    "crumbs": [
      "Home",
      "Overview",
      "Contributing"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "This section provides various examples demonstrating how to use flowerpower-io for common data loading, saving, and conversion tasks.",
    "crumbs": [
      "Home",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "examples.html#setup-for-examples",
    "href": "examples.html#setup-for-examples",
    "title": "Examples",
    "section": "Setup for Examples",
    "text": "Setup for Examples\nTo run these examples, you’ll need pandas, polars, and pyarrow installed, in addition to flowerpower-io.\n\n\nShow the code\nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nimport tempfile\nimport os\nfrom pathlib import Path\n\n# Import FlowerPower IO classes\nfrom flowerpower_io.loader.csv import CSVFileReader\nfrom flowerpower_io.saver.parquet import ParquetFileWriter\nfrom flowerpower_io.loader.sqlite import SQLiteReader\nfrom flowerpower_io.saver.sqlite import SQLiteWriter\n\n# Create sample data\nsample_data = {\n    'id': range(1, 101),\n    'name': [f'Person_{i}' for i in range(1, 101)],\n    'age': [20 + (i % 50) for i in range(1, 101)],\n    'city': ['New York', 'London', 'Tokyo', 'Paris', 'Berlin'] * 20,\n    'salary': [50000 + (i * 1000) for i in range(1, 101)]\n}\n\n# Create a temporary directory for our demo files\ntemp_dir = tempfile.mkdtemp()\ncsv_path = os.path.join(temp_dir, 'sample_data.csv')\nparquet_path = os.path.join(temp_dir, 'sample_data.parquet')\ndb_path = os.path.join(temp_dir, 'sample_data.db')\n\n# Create CSV file using pandas\ndf_pandas_original = pd.DataFrame(sample_data)\ndf_pandas_original.to_csv(csv_path, index=False)\n\nprint(f\"Created sample CSV file at: {csv_path}\")\nprint(f\"Sample data shape: {df_pandas_original.shape}\")",
    "crumbs": [
      "Home",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "examples.html#reading-csv-and-converting-formats",
    "href": "examples.html#reading-csv-and-converting-formats",
    "title": "Examples",
    "section": "1. Reading CSV and Converting Formats",
    "text": "1. Reading CSV and Converting Formats\nThis example demonstrates how to read a CSV file using CSVFileReader and convert the data into Pandas DataFrame, Polars DataFrame, and PyArrow Table formats.\n\n\nShow the code\n# Reading CSV Files with CSVFileReader\ncsv_reader = CSVFileReader(path=csv_path)\n\n# Convert to Pandas DataFrame\ndf_pandas_converted = csv_reader.to_pandas()\nprint(\"Pandas DataFrame (first 3 rows):\")\nprint(df_pandas_converted.head(3))\n\n# Convert to Polars DataFrame\ndf_polars = csv_reader.to_polars()\nprint(\"\\nPolars DataFrame (first 3 rows):\")\nprint(df_polars.head(3))\n\n# Convert to PyArrow Table\narrow_table = csv_reader.to_pyarrow_table()\nprint(\"\\nPyArrow Table (first 3 rows):\")\nprint(arrow_table.slice(0, 3).to_pandas())",
    "crumbs": [
      "Home",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "examples.html#writing-to-parquet",
    "href": "examples.html#writing-to-parquet",
    "title": "Examples",
    "section": "2. Writing to Parquet",
    "text": "2. Writing to Parquet\nThis example shows how to write a Pandas DataFrame to a Parquet file using ParquetFileWriter.\n\n\nShow the code\nparquet_writer = ParquetFileWriter(path=parquet_path)\nprint(\"Writing Pandas DataFrame to Parquet...\")\nparquet_writer.write(df_pandas_original)\nprint(f\"Parquet file created at: {parquet_path}\")\nprint(f\"Parquet file exists: {os.path.exists(parquet_path)}\")",
    "crumbs": [
      "Home",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "examples.html#reading-from-and-writing-to-sqlite",
    "href": "examples.html#reading-from-and-writing-to-sqlite",
    "title": "Examples",
    "section": "3. Reading from and Writing to SQLite",
    "text": "3. Reading from and Writing to SQLite\nThis example demonstrates how to write data to a SQLite database using SQLiteWriter and then read it back using SQLiteReader. It also includes an advanced querying example.\n\n\nShow the code\n# Writing to SQLite Database\nsqlite_writer = SQLiteWriter(\n    table_name=\"employees\",\n    path=db_path\n)\nprint(\"Writing data to SQLite Database...\")\nsqlite_writer.write(df_pandas_original)\nprint(f\"SQLite database created at: {db_path}\")\n\n# Reading from SQLite Database\nsqlite_reader = SQLiteReader(\n    table_name=\"employees\",\n    path=db_path\n)\ndf_from_sqlite_pandas = sqlite_reader.to_pandas()\nprint(\"\\nData read from SQLite (first 5 rows):\")\nprint(df_from_sqlite_pandas.head())\n\n# Advanced Querying: Employees older than 50\nprint(\"\\nQuerying: Employees older than 50\")\nquery = \"SELECT * FROM employees WHERE age &gt; 50\"\ndf_older_employees = sqlite_reader.to_pandas(query=query)\nprint(f\"Number of employees older than 50: {len(df_older_employees)}\")\nprint(df_older_employees)\n\n# Advanced Querying: Average salary by city\nprint(\"\\nQuerying: Average salary by city\")\nquery = \"SELECT city, AVG(salary) as avg_salary, COUNT(*) as count FROM employees GROUP BY city ORDER BY avg_salary DESC\"\ndf_salary_by_city = sqlite_reader.to_pandas(query=query)\nprint(\"Average salary by city:\")\nprint(df_salary_by_city)",
    "crumbs": [
      "Home",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "examples.html#cleanup",
    "href": "examples.html#cleanup",
    "title": "Examples",
    "section": "Cleanup",
    "text": "Cleanup\nAfter running the examples, you can clean up the temporary files created:\n#| eval: false #| echo: true\nimport shutil\nprint(“up temporary files…”) if os.path.exists(csv_path): os.remove(csv_path) if os.path.exists(parquet_path): os.remove(parquet_path) if os.path.exists(db_path): os.remove(db_path) if os.path.exists(temp_dir): shutil.rmtree(temp_dir) print(“Cleanup complete.”)",
    "crumbs": [
      "Home",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to flowerpower-io!",
    "section": "",
    "text": "flowerpower-io is a powerful and flexible Python library designed to simplify data loading, saving, and conversion across various formats and systems. Whether you’re dealing with CSVs, Parquet files, JSON, or integrating with databases like SQLite, PostgreSQL, or even message queues like MQTT, flowerpower-io provides a unified and intuitive interface to manage your data workflows.\n\n\n\nUnified Interface: Load and save data from diverse sources with a consistent API.\nBroad Format Support: Seamlessly handle CSV, Parquet, JSON, Delta Lake, and more.\nDatabase Integration: Connect and interact with popular databases including SQLite, PostgreSQL, MySQL, MSSQL, and Oracle.\nMessage Queue Compatibility: Integrate with MQTT for real-time data streaming.\nExtensible Architecture: Easily add support for new data sources and formats.\nPerformance Optimized: Built with efficient underlying libraries for fast data operations.\n\n\n\n\nIn today’s data-driven world, moving and transforming data efficiently is crucial. flowerpower-io eliminates the complexity of managing multiple data connectors and formats, allowing developers and data engineers to focus on what matters most: extracting insights and building robust data pipelines.\n\n\n\nReady to simplify your data operations? Head over to the Quickstart guide to begin using flowerpower-io in your projects.",
    "crumbs": [
      "Home",
      "Overview",
      "Welcome to flowerpower-io!"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Welcome to flowerpower-io!",
    "section": "",
    "text": "Unified Interface: Load and save data from diverse sources with a consistent API.\nBroad Format Support: Seamlessly handle CSV, Parquet, JSON, Delta Lake, and more.\nDatabase Integration: Connect and interact with popular databases including SQLite, PostgreSQL, MySQL, MSSQL, and Oracle.\nMessage Queue Compatibility: Integrate with MQTT for real-time data streaming.\nExtensible Architecture: Easily add support for new data sources and formats.\nPerformance Optimized: Built with efficient underlying libraries for fast data operations.",
    "crumbs": [
      "Home",
      "Overview",
      "Welcome to flowerpower-io!"
    ]
  },
  {
    "objectID": "index.html#why-flowerpower-io",
    "href": "index.html#why-flowerpower-io",
    "title": "Welcome to flowerpower-io!",
    "section": "",
    "text": "In today’s data-driven world, moving and transforming data efficiently is crucial. flowerpower-io eliminates the complexity of managing multiple data connectors and formats, allowing developers and data engineers to focus on what matters most: extracting insights and building robust data pipelines.",
    "crumbs": [
      "Home",
      "Overview",
      "Welcome to flowerpower-io!"
    ]
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Welcome to flowerpower-io!",
    "section": "",
    "text": "Ready to simplify your data operations? Head over to the Quickstart guide to begin using flowerpower-io in your projects.",
    "crumbs": [
      "Home",
      "Overview",
      "Welcome to flowerpower-io!"
    ]
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "flowerpower-io is designed with a modular and extensible architecture, making it easy to integrate with various data sources and formats. The core design principles revolve around separation of concerns, flexibility, and ease of use.\n\n\n\nModularity: The library is divided into distinct modules for loaders, savers, and common utilities, allowing for independent development and maintenance.\nExtensibility: New data formats or sources can be easily added by implementing specific loader or saver classes.\nAbstraction: Users interact with high-level interfaces, abstracting away the complexities of underlying data handling mechanisms.\nPerformance: Where possible, operations leverage efficient libraries (e.g., Polars, PyArrow) for optimized data processing.\n\n\n\n\nLoaders are responsible for reading data from various sources and converting it into a standardized format (typically a Polars DataFrame). Each loader is designed to handle a specific data source or format.\n\n\n\nCSV\nDeltaTable\nDuckDB\nJSON\nMQTT\nMSSQL\nMySQL\nOracle\nParquet\nPostgreSQL\nPydala\nSQLite\n\n\n\n\n\nSavers are responsible for writing data from the standardized format (Polars DataFrame) to various destinations or formats. Each saver is designed to handle a specific data destination or format.\n\n\n\nCSV\nDeltaTable\nDuckDB\nJSON\nMQTT\nMSSQL\nMySQL\nOracle\nParquet\nPostgreSQL\nPydala\nSQLite",
    "crumbs": [
      "Home",
      "Overview",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#core-design-principles",
    "href": "architecture.html#core-design-principles",
    "title": "Architecture",
    "section": "",
    "text": "Modularity: The library is divided into distinct modules for loaders, savers, and common utilities, allowing for independent development and maintenance.\nExtensibility: New data formats or sources can be easily added by implementing specific loader or saver classes.\nAbstraction: Users interact with high-level interfaces, abstracting away the complexities of underlying data handling mechanisms.\nPerformance: Where possible, operations leverage efficient libraries (e.g., Polars, PyArrow) for optimized data processing.",
    "crumbs": [
      "Home",
      "Overview",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#loaders",
    "href": "architecture.html#loaders",
    "title": "Architecture",
    "section": "",
    "text": "Loaders are responsible for reading data from various sources and converting it into a standardized format (typically a Polars DataFrame). Each loader is designed to handle a specific data source or format.\n\n\n\nCSV\nDeltaTable\nDuckDB\nJSON\nMQTT\nMSSQL\nMySQL\nOracle\nParquet\nPostgreSQL\nPydala\nSQLite",
    "crumbs": [
      "Home",
      "Overview",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#savers",
    "href": "architecture.html#savers",
    "title": "Architecture",
    "section": "",
    "text": "Savers are responsible for writing data from the standardized format (Polars DataFrame) to various destinations or formats. Each saver is designed to handle a specific data destination or format.\n\n\n\nCSV\nDeltaTable\nDuckDB\nJSON\nMQTT\nMSSQL\nMySQL\nOracle\nParquet\nPostgreSQL\nPydala\nSQLite",
    "crumbs": [
      "Home",
      "Overview",
      "Architecture"
    ]
  },
  {
    "objectID": "api/metadata.html",
    "href": "api/metadata.html",
    "title": "Metadata Utilities",
    "section": "",
    "text": "The metadata.py module in flowerpower-io provides utility functions for extracting and managing metadata from various data structures. This metadata includes schema information, data dimensions, file paths, and timestamps, which are crucial for understanding data characteristics and lineage."
  },
  {
    "objectID": "api/metadata.html#functions",
    "href": "api/metadata.html#functions",
    "title": "Metadata Utilities",
    "section": "Functions",
    "text": "Functions\n\nget_serializable_schema\n\n\n\n\n\n\nget_serializable_schema\n\n\n\nConverts the schema (data types) of a given DataFrame or data structure into a serializable dictionary format. This is useful for storing schema information in a persistent, human-readable way.\n\n\nDefinition:\ndef get_serializable_schema(\n    data: (\n        pd.DataFrame\n        | pl.DataFrame\n        | pl.LazyFrame\n        | duckdb.DuckDBPyRelation\n        | pa.Table\n        | pa.Schema\n        | pa.RecordBatch\n        | pa.RecordBatchReader\n        | pds.Dataset\n    ),\n) -&gt; dict[str, str]:\nArguments:\n\ndata: The input data structure (Pandas DataFrame, Polars DataFrame, PyArrow Table, DuckDB Relation, etc.) from which to extract the schema.\n\nReturns:\n\ndict[str, str]: A dictionary where keys are column names and values are their corresponding data types as strings.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nfrom flowerpower_io.metadata import get_serializable_schema\n\n# Example with Pandas DataFrame\ndf_pandas = pd.DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})\npandas_schema = get_serializable_schema(df_pandas)\nprint(\"Pandas Schema:\", pandas_schema)\n\n# Example with Polars DataFrame\ndf_polars = pl.DataFrame({'col3': [True, False], 'col4': [1.1, 2.2]})\npolars_schema = get_serializable_schema(df_polars)\nprint(\"Polars Schema:\", polars_schema)\n\n# Example with PyArrow Table\ntable_arrow = pa.table({'col5': [10, 20], 'col6': ['X', 'Y']})\npyarrow_schema = get_serializable_schema(table_arrow)\nprint(\"PyArrow Schema:\", pyarrow_schema)\n\n\n\n\nget_dataframe_metadata\n\n\n\n\n\n\nget_dataframe_metadata\n\n\n\nGenerates a comprehensive metadata dictionary for a DataFrame, including information such as path, format, timestamp, schema, number of columns, rows, and files.\n\n\nDefinition:\ndef get_dataframe_metadata(\n    df: pd.DataFrame\n    | pl.DataFrame\n    | pl.LazyFrame\n    | pa.Table\n    | pa.RecordBatch\n    | pa.RecordBatchReader\n    | list[\n        pd.DataFrame\n        | pl.DataFrame\n        | pl.LazyFrame\n        | pa.Table\n        | pa.RecordBatch\n        | pa.RecordBatchReader\n    ],\n    path: str | list[str] | None = None,\n    format: str | None = None,\n    topic: str | None = None,\n    num_files: int | None = None,\n    partition_columns: list[str] | None = None,\n    fs: AbstractFileSystem | None = None,\n    **kwargs,\n) -&gt; dict:\nArguments:\n\ndf: The input DataFrame or list of DataFrames.\npath (str | list[str] | None, optional): Path to the file(s) from which the DataFrame was loaded.\nformat (str | None, optional): Format of the data (e.g., \"csv\", \"parquet\").\ntopic (str | None, optional): Topic name for stream-based data (e.g., MQTT).\nnum_files (int | None, optional): Number of files if data is from multiple files.\npartition_columns (list[str] | None, optional): List of columns used for partitioning.\nfs (AbstractFileSystem | None, optional): Filesystem instance.\n**kwargs: Additional key-value pairs to include in the metadata.\n\nReturns:\n\ndict: A dictionary containing the extracted metadata.\n\nExample:\n\n\nShow the code\nimport pandas as pd\nfrom flowerpower_io.metadata import get_dataframe_metadata\nimport datetime as dt\n\ndf_sample = pd.DataFrame({'colA': [1, 2], 'colB': ['X', 'Y']})\nmetadata = get_dataframe_metadata(\n    df_sample,\n    path=\"/data/sample.csv\",\n    format=\"csv\",\n    custom_field=\"value\"\n)\nprint(\"DataFrame Metadata:\", metadata)\n\n\n\n\nget_duckdb_metadata\n\n\n\n\n\n\nget_duckdb_metadata\n\n\n\nRetrieves metadata specifically for DuckDB relations, including schema, shape, and file information if applicable.\n\n\nDefinition:\ndef get_duckdb_metadata(\n    rel: duckdb.DuckDBPyRelation,\n    path: str,\n    format: str,\n    fs: AbstractFileSystem | None = None,\n    include_shape: bool = False,\n    include_num_files: bool = False,\n    partition_columns: list[str] | None = None,\n    **kwargs,\n) -&gt; dict:\nArguments:\n\nrel: The DuckDBPyRelation object.\npath (str): Path to the file(s) that the DuckDBPyRelation was loaded from.\nformat (str): Format of the data.\nfs (AbstractFileSystem | None, optional): Filesystem instance.\ninclude_shape (bool, optional): Whether to include the shape (rows, columns) in the metadata. Defaults to False.\ninclude_num_files (bool, optional): Whether to include the number of files in the metadata. Defaults to False.\npartition_columns (list[str] | None, optional): List of columns used for partitioning.\n**kwargs: Additional key-value pairs to include in the metadata.\n\nReturns:\n\ndict: A dictionary containing the extracted DuckDB metadata.\n\nExample:\n\n\nShow the code\nimport duckdb\nfrom flowerpower_io.metadata import get_duckdb_metadata\n\nconn = duckdb.connect(database=\":memory:\")\nrel = conn.from_dict({'a': [1, 2], 'b': ['x', 'y']})\nduckdb_meta = get_duckdb_metadata(\n    rel,\n    path=\"memory_db\",\n    format=\"duckdb\",\n    include_shape=True\n)\nprint(\"DuckDB Metadata:\", duckdb_meta)\nconn.close()\n\n\n\n\nget_pyarrow_dataset_metadata\n\n\n\n\n\n\nget_pyarrow_dataset_metadata\n\n\n\nExtracts metadata from a PyArrow Dataset, including schema, file paths, and partitioning information.\n\n\nDefinition:\ndef get_pyarrow_dataset_metadata(\n    ds: pds.Dataset,\n    path: str,\n    format: str,\n    **kwargs,\n) -&gt; dict:\nArguments:\n\nds: The PyArrow Dataset object.\npath (str): Path to the dataset.\nformat (str): Format of the dataset.\n**kwargs: Additional key-value pairs to include in the metadata.\n\nReturns:\n\ndict: A dictionary containing the extracted PyArrow Dataset metadata.\n\nExample:\n\n\nShow the code\nimport pyarrow.dataset as pds\nimport pyarrow as pa\nimport os\nimport shutil\nfrom flowerpower_io.metadata import get_pyarrow_dataset_metadata\n\n# Create a dummy dataset directory\ndataset_dir = \"temp_pyarrow_dataset\"\nos.makedirs(dataset_dir, exist_ok=True)\npa.csv.write_csv(pa.table({'a': [1, 2]}), os.path.join(dataset_dir, \"data.csv\"))\n\n# Create a PyArrow Dataset\ndataset = pds.dataset(dataset_dir, format=\"csv\")\npyarrow_dataset_meta = get_pyarrow_dataset_metadata(\n    dataset,\n    path=dataset_dir,\n    format=\"csv\"\n)\nprint(\"PyArrow Dataset Metadata:\", pyarrow_dataset_meta)\n\n# Clean up\nshutil.rmtree(dataset_dir)\n\n\n\n\nget_delta_metadata\n\n\n\n\n\n\nget_delta_metadata\n\n\n\nExtracts metadata specifically from a DeltaTable object, providing details like name, description, ID, schema, partition columns, and number of files.\n\n\nDefinition:\ndef get_delta_metadata(\n    dtable: DeltaTable,\n    path: str,\n    **kwargs,\n) -&gt; dict:\nArguments:\n\ndtable: The DeltaTable object.\npath (str): Path to the Delta Lake table.\n**kwargs: Additional key-value pairs to include in the metadata.\n\nReturns:\n\ndict: A dictionary containing the extracted DeltaTable metadata.\n\nExample:\n\n\nShow the code\n# This example requires a Delta Lake table to exist.\n# For demonstration, we'll show the function call.\nfrom deltalake import DeltaTable\nfrom flowerpower_io.metadata import get_delta_metadata\nimport os\nimport shutil\n\n# Create a dummy Delta Lake table (requires deltalake package and rust toolchain)\n# from delta import DeltaTable, configure_spark_with_delta_pip\n# builder = configure_spark_with_delta_pip()\n# spark = builder.getOrCreate()\n# data = spark.range(0, 5).toDF(\"id\")\n# delta_path = \"temp_delta_table\"\n# data.write.format(\"delta\").save(delta_path)\n\n# Assuming a DeltaTable exists at 'path_to_delta_table'\n# delta_table = DeltaTable(\"path_to_delta_table\")\n# delta_meta = get_delta_metadata(delta_table, path=\"path_to_delta_table\")\n# print(\"DeltaTable Metadata:\", delta_meta)\n\n# Placeholder for demonstration without actual DeltaTable setup\nprint(\"DeltaTable example skipped as it requires Delta Lake setup.\")\n\n\n\n\nget_mqtt_metadata\n\n\n\n\n\n\nget_mqtt_metadata\n\n\n\nGenerates metadata for MQTT payloads, including topic, format, timestamp, schema, and basic data dimensions. This function requires the orjson library.\n\n\nDefinition:\ndef get_mqtt_metadata(\n    payload: bytes | dict[str, any],\n    topic: str | None = None,\n    **kwargs,\n) -&gt; dict:\nArguments:\n\npayload (bytes | dict[str, any]): The MQTT message payload, either as bytes or a parsed dictionary.\ntopic (str | None, optional): The MQTT topic from which the payload was received.\n**kwargs: Additional key-value pairs to include in the metadata.\n\nReturns:\n\ndict: A dictionary containing the extracted MQTT metadata.\n\nExample:\n\n\nShow the code\nimport orjson\nimport datetime as dt\nfrom flowerpower_io.metadata import get_mqtt_metadata\n\n# Example with a dictionary payload\nmqtt_payload_dict = {\"sensor_id\": \"temp_01\", \"temperature\": 25.5, \"unit\": \"C\"}\nmqtt_meta_dict = get_mqtt_metadata(\n    mqtt_payload_dict,\n    topic=\"sensors/temperature\"\n)\nprint(\"MQTT Metadata (Dict Payload):\", mqtt_meta_dict)\n\n# Example with a bytes payload\nmqtt_payload_bytes = orjson.dumps({\"event\": \"door_open\", \"timestamp\": dt.datetime.now().isoformat()})\nmqtt_meta_bytes = get_mqtt_metadata(\n    mqtt_payload_bytes,\n    topic=\"home/security\"\n)\nprint(\"MQTT Metadata (Bytes Payload):\", mqtt_meta_bytes)"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Welcome to the flowerpower-io API reference documentation. This comprehensive guide covers all public classes, functions, and methods available in the library.\n\n\nThe flowerpower-io library provides a unified interface for data I/O operations across multiple formats and storage systems. It supports:\n\nFile Formats: CSV, JSON, Parquet, Delta Lake\nDatabases: SQLite, PostgreSQL, MySQL, Oracle, MSSQL, DuckDB\nData Processing: Polars, Pandas, PyArrow, DataFusion\nCloud Storage: AWS S3, Google Cloud Storage, Azure Blob Storage\nMessage Queues: MQTT\n\n\n\n\n\n\nThe foundation of the library consists of several base classes that provide common functionality:\n\nBaseFileReader - Base class for file readers\nBaseFileWriter - Base class for file writers\nBaseDatasetReader - Base class for dataset readers\nBaseDatasetWriter - Base class for dataset writers\nBaseDatabaseReader - Base class for database readers\nBaseDatabaseWriter - Base class for database writers\nBaseMQTTReader - Base class for MQTT readers\nBaseMQTTWriter - Base class for MQTT writers\n\n\n\n\nData loaders provide convenient interfaces for reading data from various sources:\n\n\n\nCSVFileReader - Read CSV files\nJsonFileReader - Read JSON files\nParquetFileReader - Read Parquet files\nDeltaTableFileReader - Read Delta Lake files\n\n\n\n\n\nCSVDatasetReader - Read CSV datasets\nJsonDatasetReader - Read JSON datasets\nParquetDatasetReader - Read Parquet datasets\nDeltaTableDatasetReader - Read Delta Lake datasets\n\n\n\n\n\nSQLiteReader - Read SQLite databases\nPostgreSQLReader - Read PostgreSQL databases\nMySQLReader - Read MySQL databases\nOracleDBReader - Read Oracle databases\nMSSQLReader - Read Microsoft SQL Server databases\nDuckDBReader - Read DuckDB databases\n\n\n\n\n\nPydalaDatasetReader - Read Pydala datasets\nMQTTReader - Read from MQTT topics\n\n\n\n\n\nData savers provide convenient interfaces for writing data to various destinations:\n\n\n\nCSVFileWriter - Write CSV files\nJsonFileWriter - Write JSON files\n\n\n\n\n\nCSVDatasetWriter - Write CSV datasets\nJsonDatasetWriter - Write JSON datasets\nDeltaTableWriter - Write Delta Lake tables\nPydalaDatasetWriter - Write Pydala datasets\n\n\n\n\n\nDuckDBWriter - Write DuckDB databases\nMSSQLWriter - Write Microsoft SQL Server databases\nMySQLWriter - Write MySQL databases\nOracleDBWriter - Write Oracle databases\nPostgreSQLWriter - Write PostgreSQL databases\n\n\n\n\n\nComprehensive metadata extraction capabilities:\n\nget_dataframe_metadata() - Extract DataFrame metadata\nget_file_metadata() - Extract file metadata\nget_table_metadata() - Extract table metadata\nget_column_metadata() - Extract column metadata\nget_schema_metadata() - Extract schema metadata\nget_statistics_metadata() - Extract statistics metadata\nget_quality_metadata() - Extract data quality metadata\nget_completeness_metadata() - Extract completeness metadata\nget_uniqueness_metadata() - Extract uniqueness metadata\nget_consistency_metadata() - Extract consistency metadata\nget_timeliness_metadata() - Extract timeliness metadata\nget_validity_metadata() - Extract validity metadata\nget_accuracy_metadata() - Extract accuracy metadata\n\n\n\n\n\n\n\nfrom flowerpower_io.loader import CSVFileReader, ParquetFileReader\n\n# Read CSV file\ncsv_reader = CSVFileReader(\"data.csv\")\ndf = csv_reader.to_pandas()\n\n# Read Parquet file\nparquet_reader = ParquetFileReader(\"data.parquet\")\ndf = parquet_reader.to_polars()\n\n\n\nfrom flowerpower_io.saver import CSVFileWriter, DeltaTableWriter\n\n# Write CSV file\ncsv_writer = CSVFileWriter(\"output.csv\")\ncsv_writer.write(df)\n\n# Write Delta Lake table\ndelta_writer = DeltaTableWriter(\"delta_table/\")\ndelta_writer.write(df)\n\n\n\nfrom flowerpower_io.loader import PostgreSQLReader\nfrom flowerpower_io.saver import PostgreSQLWriter\n\n# Read from PostgreSQL\npg_reader = PostgreSQLReader(\n    table_name=\"users\",\n    host=\"localhost\",\n    port=5432,\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\ndf = pg_reader.to_pyarrow_table()\n\n# Write to PostgreSQL\npg_writer = PostgreSQLWriter(\n    table_name=\"users\",\n    host=\"localhost\",\n    port=5432,\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\npg_writer.write(df)\n\n\n\nfrom flowerpower_io.metadata import get_dataframe_metadata, get_quality_metadata\n\n# Get basic metadata\nmetadata = get_dataframe_metadata(df)\n\n# Get quality metadata\nquality = get_quality_metadata(df)\n\n\n\n\n\n\nfrom flowerpower_io.loader import ParquetFileReader\n\n# Read from S3\nreader = ParquetFileReader(\n    path=\"s3://bucket/data.parquet\",\n    storage_options={\n        \"key\": \"your-access-key\",\n        \"secret\": \"your-secret-key\",\n        \"client_kwargs\": {\n            \"region_name\": \"us-east-1\"\n        }\n    }\n)\ndf = reader.to_polars(lazy=True)\n\n\n\nfrom flowerpower_io.saver import DeltaTableWriter\n\n# Write with partitioning and schema evolution\nwriter = DeltaTableWriter(\"delta_table/\")\nwriter.write(\n    df,\n    mode=\"append\",\n    partition_by=[\"date\"],\n    schema_mode=\"merge\",\n    predicate=\"active = true\"\n)\n\n\n\n\n\n\n\nThe library provides comprehensive error handling:\nfrom flowerpower_io.loader import CSVFileReader\n\ntry:\n    reader = CSVFileReader(\"data.csv\")\n    df = reader.to_duckdb()\nexcept FileNotFoundError:\n    print(\"File not found\")\nexcept Exception as e:\n    print(f\"Error reading file: {e}\")\n\n\n\n\nUse lazy loading with large datasets where possible\nConsider partitioning for large Delta Lake tables\nUse appropriate compression settings for file formats\nLeverage connection pooling for database operations\n\n\n\n\n\nUse appropriate data types for your data to optimize storage and performance\nImplement proper error handling for production applications\nUse connection pooling for database operations\nConsider data partitioning for large datasets\nUse metadata functions for data quality monitoring\nLeverage lazy evaluation for large datasets when possible\n\n\n\n\nIf you find any issues or have suggestions for improvements, please refer to the main documentation for contribution guidelines.\n\n\n\nThis library is licensed under the MIT License. See the LICENSE file for details."
  },
  {
    "objectID": "api/index.html#overview",
    "href": "api/index.html#overview",
    "title": "API Reference",
    "section": "",
    "text": "The flowerpower-io library provides a unified interface for data I/O operations across multiple formats and storage systems. It supports:\n\nFile Formats: CSV, JSON, Parquet, Delta Lake\nDatabases: SQLite, PostgreSQL, MySQL, Oracle, MSSQL, DuckDB\nData Processing: Polars, Pandas, PyArrow, DataFusion\nCloud Storage: AWS S3, Google Cloud Storage, Azure Blob Storage\nMessage Queues: MQTT"
  },
  {
    "objectID": "api/index.html#quick-navigation",
    "href": "api/index.html#quick-navigation",
    "title": "API Reference",
    "section": "",
    "text": "The foundation of the library consists of several base classes that provide common functionality:\n\nBaseFileReader - Base class for file readers\nBaseFileWriter - Base class for file writers\nBaseDatasetReader - Base class for dataset readers\nBaseDatasetWriter - Base class for dataset writers\nBaseDatabaseReader - Base class for database readers\nBaseDatabaseWriter - Base class for database writers\nBaseMQTTReader - Base class for MQTT readers\nBaseMQTTWriter - Base class for MQTT writers\n\n\n\n\nData loaders provide convenient interfaces for reading data from various sources:\n\n\n\nCSVFileReader - Read CSV files\nJsonFileReader - Read JSON files\nParquetFileReader - Read Parquet files\nDeltaTableFileReader - Read Delta Lake files\n\n\n\n\n\nCSVDatasetReader - Read CSV datasets\nJsonDatasetReader - Read JSON datasets\nParquetDatasetReader - Read Parquet datasets\nDeltaTableDatasetReader - Read Delta Lake datasets\n\n\n\n\n\nSQLiteReader - Read SQLite databases\nPostgreSQLReader - Read PostgreSQL databases\nMySQLReader - Read MySQL databases\nOracleDBReader - Read Oracle databases\nMSSQLReader - Read Microsoft SQL Server databases\nDuckDBReader - Read DuckDB databases\n\n\n\n\n\nPydalaDatasetReader - Read Pydala datasets\nMQTTReader - Read from MQTT topics\n\n\n\n\n\nData savers provide convenient interfaces for writing data to various destinations:\n\n\n\nCSVFileWriter - Write CSV files\nJsonFileWriter - Write JSON files\n\n\n\n\n\nCSVDatasetWriter - Write CSV datasets\nJsonDatasetWriter - Write JSON datasets\nDeltaTableWriter - Write Delta Lake tables\nPydalaDatasetWriter - Write Pydala datasets\n\n\n\n\n\nDuckDBWriter - Write DuckDB databases\nMSSQLWriter - Write Microsoft SQL Server databases\nMySQLWriter - Write MySQL databases\nOracleDBWriter - Write Oracle databases\nPostgreSQLWriter - Write PostgreSQL databases\n\n\n\n\n\nComprehensive metadata extraction capabilities:\n\nget_dataframe_metadata() - Extract DataFrame metadata\nget_file_metadata() - Extract file metadata\nget_table_metadata() - Extract table metadata\nget_column_metadata() - Extract column metadata\nget_schema_metadata() - Extract schema metadata\nget_statistics_metadata() - Extract statistics metadata\nget_quality_metadata() - Extract data quality metadata\nget_completeness_metadata() - Extract completeness metadata\nget_uniqueness_metadata() - Extract uniqueness metadata\nget_consistency_metadata() - Extract consistency metadata\nget_timeliness_metadata() - Extract timeliness metadata\nget_validity_metadata() - Extract validity metadata\nget_accuracy_metadata() - Extract accuracy metadata"
  },
  {
    "objectID": "api/index.html#usage-patterns",
    "href": "api/index.html#usage-patterns",
    "title": "API Reference",
    "section": "",
    "text": "from flowerpower_io.loader import CSVFileReader, ParquetFileReader\n\n# Read CSV file\ncsv_reader = CSVFileReader(\"data.csv\")\ndf = csv_reader.to_pandas()\n\n# Read Parquet file\nparquet_reader = ParquetFileReader(\"data.parquet\")\ndf = parquet_reader.to_polars()\n\n\n\nfrom flowerpower_io.saver import CSVFileWriter, DeltaTableWriter\n\n# Write CSV file\ncsv_writer = CSVFileWriter(\"output.csv\")\ncsv_writer.write(df)\n\n# Write Delta Lake table\ndelta_writer = DeltaTableWriter(\"delta_table/\")\ndelta_writer.write(df)\n\n\n\nfrom flowerpower_io.loader import PostgreSQLReader\nfrom flowerpower_io.saver import PostgreSQLWriter\n\n# Read from PostgreSQL\npg_reader = PostgreSQLReader(\n    table_name=\"users\",\n    host=\"localhost\",\n    port=5432,\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\ndf = pg_reader.to_pyarrow_table()\n\n# Write to PostgreSQL\npg_writer = PostgreSQLWriter(\n    table_name=\"users\",\n    host=\"localhost\",\n    port=5432,\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\npg_writer.write(df)\n\n\n\nfrom flowerpower_io.metadata import get_dataframe_metadata, get_quality_metadata\n\n# Get basic metadata\nmetadata = get_dataframe_metadata(df)\n\n# Get quality metadata\nquality = get_quality_metadata(df)"
  },
  {
    "objectID": "api/index.html#advanced-features",
    "href": "api/index.html#advanced-features",
    "title": "API Reference",
    "section": "",
    "text": "from flowerpower_io.loader import ParquetFileReader\n\n# Read from S3\nreader = ParquetFileReader(\n    path=\"s3://bucket/data.parquet\",\n    storage_options={\n        \"key\": \"your-access-key\",\n        \"secret\": \"your-secret-key\",\n        \"client_kwargs\": {\n            \"region_name\": \"us-east-1\"\n        }\n    }\n)\ndf = reader.to_polars(lazy=True)\n\n\n\nfrom flowerpower_io.saver import DeltaTableWriter\n\n# Write with partitioning and schema evolution\nwriter = DeltaTableWriter(\"delta_table/\")\nwriter.write(\n    df,\n    mode=\"append\",\n    partition_by=[\"date\"],\n    schema_mode=\"merge\",\n    predicate=\"active = true\"\n)"
  },
  {
    "objectID": "api/index.html#error-handling",
    "href": "api/index.html#error-handling",
    "title": "API Reference",
    "section": "",
    "text": "The library provides comprehensive error handling:\nfrom flowerpower_io.loader import CSVFileReader\n\ntry:\n    reader = CSVFileReader(\"data.csv\")\n    df = reader.to_duckdb()\nexcept FileNotFoundError:\n    print(\"File not found\")\nexcept Exception as e:\n    print(f\"Error reading file: {e}\")"
  },
  {
    "objectID": "api/index.html#performance-considerations",
    "href": "api/index.html#performance-considerations",
    "title": "API Reference",
    "section": "",
    "text": "Use lazy loading with large datasets where possible\nConsider partitioning for large Delta Lake tables\nUse appropriate compression settings for file formats\nLeverage connection pooling for database operations"
  },
  {
    "objectID": "api/index.html#best-practices",
    "href": "api/index.html#best-practices",
    "title": "API Reference",
    "section": "",
    "text": "Use appropriate data types for your data to optimize storage and performance\nImplement proper error handling for production applications\nUse connection pooling for database operations\nConsider data partitioning for large datasets\nUse metadata functions for data quality monitoring\nLeverage lazy evaluation for large datasets when possible"
  },
  {
    "objectID": "api/index.html#contributing",
    "href": "api/index.html#contributing",
    "title": "API Reference",
    "section": "",
    "text": "If you find any issues or have suggestions for improvements, please refer to the main documentation for contribution guidelines."
  },
  {
    "objectID": "api/index.html#license",
    "href": "api/index.html#license",
    "title": "API Reference",
    "section": "",
    "text": "This library is licensed under the MIT License. See the LICENSE file for details."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "flowerpower-io can be installed using pip, uv, or pixi. Choose the method that best suits your environment.\n\n\nBefore installing flowerpower-io, ensure you have Python 3.8 or higher installed on your system.\n\n\n\nTo install the latest stable version of flowerpower-io using pip:\npip install flowerpower-io\nTo install with all optional dependencies for full functionality (e.g., database connectors, advanced file formats):\npip install \"flowerpower-io[all]\"\nYou can also install specific optional dependencies:\npip install \"flowerpower-io[csv,parquet,json]\"\n\n\n\nuv is a fast Python package installer and resolver. If you have uv installed, you can use it to install flowerpower-io:\nuv pip install flowerpower-io\nTo install with all optional dependencies:\nuv pip install \"flowerpower-io[all]\"\n\n\n\npixi is a modern package manager for polyglot projects. If you are using pixi, add flowerpower-io to your project:\npixi add flowerpower-io\n\n\n\n\n\nIt’s highly recommended to use a virtual environment (like venv or conda) to manage your project dependencies. This prevents conflicts with system-wide Python packages.\npython -m venv .venv\nsource .venv/bin/activate\npip install flowerpower-io\n\n\n\n\nModuleNotFoundError: Ensure flowerpower-io is installed in your active Python environment. If using a virtual environment, activate it first.\nDependency Conflicts: If you encounter dependency conflicts, try installing flowerpower-io in a fresh virtual environment or using uv which has a more robust dependency resolver.\nPermissions Errors: On some systems, you might need to use sudo (Linux/macOS) or run your terminal as an administrator (Windows) if you’re installing globally, though this is generally not recommended.",
    "crumbs": [
      "Home",
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#prerequisites",
    "href": "installation.html#prerequisites",
    "title": "Installation",
    "section": "",
    "text": "Before installing flowerpower-io, ensure you have Python 3.8 or higher installed on your system.",
    "crumbs": [
      "Home",
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#installation-with-pip",
    "href": "installation.html#installation-with-pip",
    "title": "Installation",
    "section": "",
    "text": "To install the latest stable version of flowerpower-io using pip:\npip install flowerpower-io\nTo install with all optional dependencies for full functionality (e.g., database connectors, advanced file formats):\npip install \"flowerpower-io[all]\"\nYou can also install specific optional dependencies:\npip install \"flowerpower-io[csv,parquet,json]\"",
    "crumbs": [
      "Home",
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#installation-with-uv",
    "href": "installation.html#installation-with-uv",
    "title": "Installation",
    "section": "",
    "text": "uv is a fast Python package installer and resolver. If you have uv installed, you can use it to install flowerpower-io:\nuv pip install flowerpower-io\nTo install with all optional dependencies:\nuv pip install \"flowerpower-io[all]\"",
    "crumbs": [
      "Home",
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#installation-with-pixi",
    "href": "installation.html#installation-with-pixi",
    "title": "Installation",
    "section": "",
    "text": "pixi is a modern package manager for polyglot projects. If you are using pixi, add flowerpower-io to your project:\npixi add flowerpower-io",
    "crumbs": [
      "Home",
      "Overview",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installation",
    "section": "",
    "text": "It’s highly recommended to use a virtual environment (like venv or conda) to manage your project dependencies. This prevents conflicts with system-wide Python packages.\npython -m venv .venv\nsource .venv/bin/activate\npip install flowerpower-io\n\n\n\n\nModuleNotFoundError: Ensure flowerpower-io is installed in your active Python environment. If using a virtual environment, activate it first.\nDependency Conflicts: If you encounter dependency conflicts, try installing flowerpower-io in a fresh virtual environment or using uv which has a more robust dependency resolver.\nPermissions Errors: On some systems, you might need to use sudo (Linux/macOS) or run your terminal as an administrator (Windows) if you’re installing globally, though this is generally not recommended.",
    "crumbs": [
      "Home",
      "Overview",
      "Installation"
    ]
  }
]