{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Custom Quantity Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install Salient SDK with: pip install salientsdk\")\n",
    "\n",
    "sk.set_file_destination(\"custom_quantities\")\n",
    "\n",
    "sk.login(\"SALIENT_USERNAME\", \"SALIENT_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Defining and Uploading a Custom Quantity\n",
    "\n",
    "The goal of this notebook is to show you how to make your own custom quantities and get both historical occurences of the quantity and forecasts from GEM and NOAA GEFS. Custom quantities can be both continuous and binary and involve 1 or more core variables. These quantities involve extra processing steps to calculate, but can be more direct in answering the questions that matter most for your business; one example of a custom quantity is actually cooling and heating degree days, because these apply extra processing on the minimum and maximum temperatures. Although the processing for CDDs/HDDs is relatively simple, it is still has \"extra steps\" one must to take to get at.\n",
    "\n",
    "To demonstrate our system, let's walk through the process of defining a complex or compound event: days with **both** high temperatures and low winds. At a high level, we are going to do the following:\n",
    "1. Upload our definition of high temperatures\n",
    "2. Upload our definition of low winds\n",
    "3. Upload our definition for the complex event, which chains these two individual events together.\n",
    "\n",
    "### Single Quantity\n",
    "First, high temperatures. The `/upload_file` endpoint also has the ability to take CSVs with the parameters defining the quantity. **Key: to differentiate custom quantity files from location_files, there must be 2 and only 2 columns titled parameter and value**. For more information, please see the \"Custom Quantity\" section in our Notion help page: https://salientpredictions.notion.site/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"name\": \"hot_temps_example\",\n",
    "    \"description\": \"my example of hot temperatures\",\n",
    "    \"variable\": \"tmax\",\n",
    "    \"daily_threshold_value\": 0.90,\n",
    "    \"daily_threshold_type\": \"seasonal_quantile\",\n",
    "    \"daily_threshold_direction\": \"above\",\n",
    "}\n",
    "\n",
    "hot_temps_file_path = f\"{sk.get_file_destination()}/hot_temps.csv\"\n",
    "df = pd.DataFrame.from_dict(params, orient=\"index\").reset_index()\n",
    "df.columns = [\"parameter\", \"value\"]\n",
    "df.to_csv(hot_temps_file_path, index=False)\n",
    "sk.upload_file(hot_temps_file_path, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "One of a latitude/longitude pair, location_file, or shapefile can be included in the definition if you are interested in adding\n",
    "spatial aggregations (such as population-weighted) or simply always look at a specfic area(s). These can also simply be added when calling an endpoint, as seen below. This gives you the flexibility to see maps in the UI over an entire region, such as North America, and then limit API calls. The name of the variable in the returned dataset is the same as the name passed for the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = sk.Location(shapefile=\"CONUS.geojson\")\n",
    "file_dsc = sk.data_timeseries(\n",
    "    loc=loc, custom_quantity=\"hot_temps_example\", start=\"2020-10-16\", end=\"2024-12-31\"\n",
    ")\n",
    "hot_historical = xr.load_dataset(file_dsc)\n",
    "hot_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"name\": \"low_winds_example\",\n",
    "    \"description\": \"my example of low winds\",\n",
    "    \"variable\": \"wspd100\",\n",
    "    \"daily_threshold_value\": 0.10,\n",
    "    \"daily_threshold_type\": \"seasonal_quantile\",\n",
    "    \"daily_threshold_direction\": \"below\",\n",
    "}\n",
    "\n",
    "low_winds_file_path = f\"{sk.get_file_destination()}/low_winds.csv\"\n",
    "df = pd.DataFrame.from_dict(params, orient=\"index\").reset_index()\n",
    "df.columns = [\"parameter\", \"value\"]\n",
    "df.to_csv(low_winds_file_path, index=False)\n",
    "sk.upload_file(low_winds_file_path, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dsc = sk.data_timeseries(\n",
    "    loc=loc, custom_quantity=\"low_winds_example\", start=\"2020-10-16\", end=\"2024-12-31\"\n",
    ")\n",
    "wind_historical = xr.load_dataset(file_dsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_historical = xr.concat(\n",
    "    [hot_historical[\"hot_temps_example\"], wind_historical[\"low_winds_example\"]], dim=\"quantity\"\n",
    ")\n",
    "both_historical = both_historical.assign_coords(quantity=[\"hot_days\", \"low_wind\"])\n",
    "both_historical.name = \"combined_event\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Making a Complex Event\n",
    "\n",
    "Custom quantities can also be complex or compound events where two or more individual events are chained together. To define a complex event, first be sure the individual components are all uploaded. Then, the key is to do something like: \n",
    "\n",
    "`combine`,event1&event2&event3&...\n",
    "\n",
    "where \"&\" is used to chain the individual components. Finally, there is an `operator` keyword which can be \"and\" or \"or\"; note that `combine` always uses \"&\" despite the operator also having an \"or\" option.\n",
    "\n",
    "Because the complex event uses the names of the individual components, if you make a change to one of them, the complex event will automatically be updated and use the new definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"name\": \"complex_example\",\n",
    "    \"description\": \"my example of complex event: high temps and low winds\",\n",
    "    \"combine\": \"hot_temps_example&low_winds_example\",\n",
    "    \"operator\": \"and\",\n",
    "}\n",
    "\n",
    "complex_file_path = f\"{sk.get_file_destination()}/complex.csv\"\n",
    "df = pd.DataFrame.from_dict(params, orient=\"index\").reset_index()\n",
    "df.columns = [\"parameter\", \"value\"]\n",
    "df.to_csv(complex_file_path, index=False)\n",
    "sk.upload_file(complex_file_path, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Calling Endpoints\n",
    "\n",
    "Once a custom quantity is uploaded, you can call `data_timeseries` to get historical occurances of the events, and `forecast_timeseries` to get forecasts from both GEM and NOAA GEFS. There are many parameters that are now no longer needed for the call, since things like `variable`, `field`, `units`, etc. are all defined within the event. Simply add `custom_quantity=name` to the call!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dsc = sk.data_timeseries(\n",
    "    loc=loc, custom_quantity=\"complex_example\", start=\"2020-10-16\", end=\"2024-12-31\"\n",
    ")\n",
    "complex_historical = xr.load_dataset(file_dsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Let's look at one specific example for forecasts: late August 2023. Here, we see warm temperatures across much of South and Central Texas, with low winds seen across parts of West Texas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dsc = sk.forecast_timeseries(\n",
    "    loc=loc,\n",
    "    date=\"2023/08/25\",\n",
    "    model=\"gem\",\n",
    "    timescale=\"daily\",\n",
    "    format=\"nc\",\n",
    "    custom_quantity=[\"hot_temps_example\", \"low_winds_example\"],\n",
    ")\n",
    "forecast = xr.open_mfdataset(file_dsc.file_name)\n",
    "combined_forecast = xr.concat([forecast[var] for var in forecast.data_vars], dim=\"quantity\")\n",
    "combined_forecast = combined_forecast.assign_coords(quantity=[\"hot_days\", \"low_winds\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mcolors.ListedColormap([\"white\", \"green\"])\n",
    "subplot_proj = dict(projection=ccrs.AlbersEqualArea(central_latitude=35, central_longitude=-100))\n",
    "\n",
    "im = both_historical.sel(time=slice(\"2023/08/27\", \"2023/08/31\")).plot(\n",
    "    row=\"quantity\",\n",
    "    col=\"time\",\n",
    "    add_colorbar=False,\n",
    "    cmap=cmap,\n",
    "    subplot_kws=subplot_proj,\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "for a in im.axs.flatten():\n",
    "    a.add_feature(cfeature.STATES, linewidth=0.5)\n",
    "    a.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    a.set_extent([-110, -90, 24, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Here, we can see that only areas that are shaded green show up in the complex event, owing to the \"and\" operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = (\n",
    "    complex_historical[\"complex_example\"]\n",
    "    .sel(time=slice(\"2023/08/27\", \"2023/08/31\"))\n",
    "    .plot(\n",
    "        col=\"time\",\n",
    "        cmap=cmap,\n",
    "        add_colorbar=False,\n",
    "        subplot_kws=subplot_proj,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    ")\n",
    "for a in im.axs.flatten():\n",
    "    a.add_feature(cfeature.STATES, linewidth=0.5)\n",
    "    a.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    a.set_extent([-110, -90, 24, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "A few days prior, GEM captured the areas with elevated risk of both events well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = combined_forecast.sel(valid_time=slice(\"2023/08/27\", \"2023/08/31\")).plot(\n",
    "    col=\"valid_time\",\n",
    "    row=\"quantity\",\n",
    "    cbar_kwargs=dict(location=\"bottom\", pad=0.02),\n",
    "    subplot_kws=subplot_proj,\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "for a in im.axs.flatten():\n",
    "    a.add_feature(cfeature.STATES, linewidth=0.5)\n",
    "    a.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    a.set_extent([-110, -90, 24, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Multi-Day Event\n",
    "\n",
    "Events can also be defined using some rolling aggregations to get mean weekly temperature, 5-day precipitation totals, etc. Here, we reproduce an example shown in our April webinar for the GEM launch: 3-day HDD totals exceeding 50 HDDs. Note that if the `accumulated_threshold_*` parameters were not present, this would return **continuous data** and instead of getting a probability, you'd get our standard set of quantiles (0.01, 0.025, 0.05, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"name\": \"multi_day_example\",\n",
    "    \"description\": \"my example of multi-day event\",\n",
    "    \"variable\": \"hdd\",\n",
    "    \"rolling_window\": 3,\n",
    "    \"rolling_window_unit\": \"D\",\n",
    "    \"rolling_aggregation\": \"sum\",\n",
    "    \"accumulated_threshold_value\": 50,\n",
    "    \"accumulated_threshold_type\": \"absolute\",\n",
    "    \"accumulated_threshold_direction\": \"above\",\n",
    "}\n",
    "\n",
    "multi_day_hdd_file_path = f\"{sk.get_file_destination()}/multi_day_hdd.csv\"\n",
    "df = pd.DataFrame.from_dict(params, orient=\"index\").reset_index()\n",
    "df.columns = [\"parameter\", \"value\"]\n",
    "df.to_csv(multi_day_hdd_file_path, index=False)\n",
    "sk.upload_file(multi_day_hdd_file_path, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = sk.Location(lat=32.7767, lon=-96.7970)  # dallas, tx\n",
    "file_dsc = sk.data_timeseries(\n",
    "    loc=loc, custom_quantity=\"multi_day_example\", start=\"2024-01-01\", end=\"2025-02-28\"\n",
    ")\n",
    "multi_day_historical = xr.load_dataset(file_dsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(ds, date):\n",
    "    return ds.sel(valid_time=date)\n",
    "\n",
    "\n",
    "dates = xr.date_range(start=\"2023/12/28\", end=\"2024/01/29\", freq=\"D\").to_list()\n",
    "prep = partial(_preprocess, date=\"2024/01/29\")\n",
    "file_dsc = sk.forecast_timeseries(\n",
    "    loc=loc,\n",
    "    date=dates,\n",
    "    model=\"gem\",\n",
    "    timescale=\"daily\",\n",
    "    format=\"nc\",\n",
    "    custom_quantity=\"multi_day_example\",\n",
    ")\n",
    "multi_day_forecast_gem = xr.open_mfdataset(\n",
    "    file_dsc.file_name, preprocess=prep, combine=\"nested\", concat_dim=\"forecast_date\"\n",
    ").squeeze()\n",
    "\n",
    "file_dsc = sk.forecast_timeseries(\n",
    "    loc=loc,\n",
    "    date=dates,\n",
    "    model=\"noaa_gefs\",\n",
    "    timescale=\"daily\",\n",
    "    format=\"nc\",\n",
    "    custom_quantity=\"multi_day_example\",\n",
    ")\n",
    "multi_day_forecast_gefs = xr.open_mfdataset(\n",
    "    file_dsc.file_name, preprocess=prep, combine=\"nested\", concat_dim=\"forecast_date\"\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4), constrained_layout=True)\n",
    "multi_day_forecast_gem[\"multi_day_example\"].plot(ax=ax, label=\"GEM\")\n",
    "multi_day_forecast_gefs[\"multi_day_example\"].plot(ax=ax, label=\"GEFS\")\n",
    "ax.set_title(\"Forecast for 3-day HDD starting 2024-01-29\", fontsize=13, fontweight=\"bold\", pad=1)\n",
    "ax.set_xlabel(\"Forecast Initialization\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Probability (%)\", fontsize=11, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salient",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
