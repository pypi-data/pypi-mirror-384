{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscaling Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f9c9003f5d0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "sk.set_file_destination(\"downscale_example\")\n",
    "\n",
    "sk.login(\"SALIENT_USERNAME\", \"SALIENT_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set geographic bounds\n",
    "\n",
    "The Salient SDK uses a \"Location\" object to specify the geographic bounds of a request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37.7749, -122.4194)\n"
     ]
    }
   ],
   "source": [
    "if False:  # lat/lon\n",
    "    loc = sk.Location(37.7749, -122.4194)\n",
    "elif False:  # \"shapefile\"\n",
    "    shp_file = sk.upload_bounding_box(-109.1, -102.0, 37.8, 41.0, \"Colorado\")\n",
    "    loc = sk.Location(shapefile=shp_file)\n",
    "elif True:  # \"location_file\"\n",
    "    loc_file = sk.upload_location_file(\n",
    "        lats=[37.7749, 33.9416, 32.7336],\n",
    "        lons=[-122.4194, -118.4085, -117.1897],\n",
    "        names=[\"SFO\", \"LAX\", \"SAN\"],\n",
    "        geoname=\"CA_Airports\",\n",
    "    )\n",
    "    loc = sk.Location(location_file=loc_file)\n",
    "else:\n",
    "    raise ValueError(\"Invalid location type\")\n",
    "\n",
    "print(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscale\n",
    "\n",
    "The `downscale` API endpoint converts Salient's temporally granular weekly/monthly/quarterly forecasts into a daily timeseries ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 103kB\n",
      "Dimensions:        (time: 366, location: 1, ensemble: 11)\n",
      "Coordinates:\n",
      "  * time           (time) datetime64[ns] 3kB 2021-01-01 ... 2022-01-01\n",
      "    analog         (ensemble, time) datetime64[ns] 32kB 2021-01-01 ... 1982-1...\n",
      "    lat            (location) float64 8B 37.77\n",
      "    lon            (location) float64 8B -122.4\n",
      "    forecast_date  datetime64[ns] 8B 2021-01-01\n",
      "Dimensions without coordinates: location, ensemble\n",
      "Data variables:\n",
      "    precip_clim    (time, location) float32 1kB 4.549 4.276 ... 5.884 4.549\n",
      "    temp_clim      (time, location) float32 1kB 9.539 9.476 9.243 ... 9.8 9.539\n",
      "    precip_anom    (ensemble, time, location) float32 16kB -3.9 -1.504 ... 7.514\n",
      "    temp_anom      (ensemble, time, location) float32 16kB -0.3568 ... 0.9146\n",
      "    precip         (ensemble, time, location) float32 16kB 0.6491 ... 12.06\n",
      "    temp           (ensemble, time, location) float32 16kB 9.182 10.54 ... 10.45\n"
     ]
    }
   ],
   "source": [
    "vars = \"temp,precip\"\n",
    "start_date = \"2021-01-01\"\n",
    "debias = False\n",
    "\n",
    "file_dsc = sk.downscale(\n",
    "    loc=loc,\n",
    "    variables=vars,\n",
    "    date=start_date,\n",
    "    members=11,\n",
    "    debias=debias,\n",
    "    verbose=False,\n",
    "    force=False,\n",
    ")\n",
    "\n",
    "dsc = xr.load_dataset(file_dsc)\n",
    "# Later on we'll be merging with data_history, which has a time coordinate:\n",
    "dsc = dsc.rename({\"forecast_day\": \"time\"})\n",
    "\n",
    "print(dsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name variable\n",
      "0  downscale_example/data_timeseries_fc636fcbaf40...     temp\n",
      "1  downscale_example/data_timeseries_981833b8b2e7...   precip\n",
      "<xarray.Dataset> Size: 9kB\n",
      "Dimensions:  (time: 366, location: 1)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 3kB 2021-01-01 2021-01-02 ... 2022-01-01\n",
      "    lat      (location) float64 8B 37.77\n",
      "    lon      (location) float64 8B -122.4\n",
      "Dimensions without coordinates: location\n",
      "Data variables:\n",
      "    temp     (time, location) float64 3kB 9.404 10.78 10.96 ... 8.064 7.243\n",
      "    precip   (time, location) float64 3kB 0.000208 1.486 0.2287 ... 0.002722 0.0\n"
     ]
    }
   ],
   "source": [
    "# Match the start and end dates of the downscale timeseries\n",
    "end_date = dsc.time.max().dt.strftime(\"%Y-%m-%d\").values\n",
    "\n",
    "file_hst = sk.data_timeseries(\n",
    "    loc=loc,\n",
    "    variable=vars,\n",
    "    field=\"all\",\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    debias=debias,\n",
    "    verbose=False,\n",
    "    force=False,\n",
    ")\n",
    "# Each variable will download to its own file\n",
    "print(file_hst)\n",
    "\n",
    "# Load all of the history files into a single dataset:\n",
    "hst = sk.load_multihistory(file_hst, fields=[\"vals\"])\n",
    "print(hst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Downscale vs History\n",
    "\n",
    "Compare the downscale and history datasets to see how well they match. This is a simple example, but you can use any of the xarray functions to analyze the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 9kB\n",
      "Dimensions:        (time: 366, location: 1)\n",
      "Coordinates:\n",
      "  * time           (time) datetime64[ns] 3kB 2021-01-01 ... 2022-01-01\n",
      "    lat            (location) float64 8B 37.77\n",
      "    lon            (location) float64 8B -122.4\n",
      "    forecast_date  datetime64[ns] 8B 2021-01-01\n",
      "Dimensions without coordinates: location\n",
      "Data variables:\n",
      "    precip         (time, location) float64 3kB 0.6489 1.216 ... 0.9117 10.88\n",
      "    temp           (time, location) float64 3kB 0.1796 0.3127 ... 1.16 2.494\n"
     ]
    }
   ],
   "source": [
    "dif = dsc - hst\n",
    "\n",
    "rmse = (dif**2).mean(dim=[\"ensemble\"]) ** 0.5\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salientsdk-54A4kIpb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
