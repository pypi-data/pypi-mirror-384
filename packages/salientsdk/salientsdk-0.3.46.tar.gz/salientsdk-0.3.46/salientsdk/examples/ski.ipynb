{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salient Predictions Ski-Cast\n",
    "\n",
    "In November, Salient predicts snow accumulation at 90 IKON and Epic resorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f1af51ad990>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "# Prevent wrapping on tables for readability\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "sk.set_file_destination(\"ski_example\")\n",
    "sk.login(\"SALIENT_USERNAME\", \"SALIENT_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize the calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the timeframe:\n",
    "year = 2024  # datetime.datetime.now().year\n",
    "norm_years = 15  # number of years to define baseline \"normal\" behavior\n",
    "start_month = 10  # first month of the northern hemisphere snow accumulation season\n",
    "end_month = 5  # final month of the ski season\n",
    "today = pd.to_datetime(datetime.date.today())\n",
    "fcst_date = pd.to_datetime(f\"{year}-12-01\")\n",
    "hist_start = pd.to_datetime(f\"{year-norm_years-1}-{start_month}-01\")\n",
    "fcst_end = pd.to_datetime(f\"{year+1}-{end_month}-01\") - pd.Timedelta(days=1)\n",
    "\n",
    "# Balance accuracy vs execution speed:\n",
    "fast = True\n",
    "freq = \"daily\" if fast else \"hourly\"\n",
    "# ensemble_count = 31 if fast else 101\n",
    "ensemble_count = 101\n",
    "force = False\n",
    "\n",
    "\n",
    "# These are the minimum variables we need to calculate snowpack:\n",
    "vars = [\"temp\", \"precip\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all of the IKON/EPIC resorts worldwide\n",
    "\n",
    "We'll define all the locations of interest in code here to keep the example self-contained. Often you'll list your locations in a separate CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location file: resorts.csv\n",
      "           lon        lat         name   region  pass    color\n",
      "0   137.861000  36.690000       Hakuba    japan  epic  #004488\n",
      "1   140.874000  42.739000      Rusutsu    japan  epic  #004488\n",
      "2   140.688000  42.864000       Niseko    japan  ikon  #004488\n",
      "3   138.175000  37.027000   Lotte Arai    japan  ikon  #004488\n",
      "4     6.863275  45.924065     Chamonix     alps  ikon  #33BBEE\n",
      "..         ...        ...          ...      ...   ...      ...\n",
      "86  -75.656331  41.109169   Jack Frost  na_east  epic  #777777\n",
      "87  -75.601282  41.050189  Big Boulder  na_east  epic  #777777\n",
      "88  -74.585253  46.209642    Tremblant  na_east  ikon  #777777\n",
      "89  -74.256712  42.293730      Windham  na_east  ikon  #777777\n",
      "90  -74.224640  42.202881       Hunter  na_east  epic  #777777\n",
      "\n",
      "[91 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# We define a \"region\" for each ski resort to group them for plotting purposes,\n",
    "# using Tol's colorblind-friendly \"vibrant\" palette.\n",
    "# https://cran.r-project.org/web/packages/khroma/vignettes/tol.html\n",
    "geo_colors = {\n",
    "    \"japan\": \"#004488\",  # blue\n",
    "    \"alps\": \"#33BBEE\",  # cyan\n",
    "    \"pnw\": \"#009988\",  # teal\n",
    "    \"rockies\": \"#CC3311\",  # red\n",
    "    \"new_england\": \"#DDAA33\",  # yellow\n",
    "    \"europe\": \"#555555\",  # dark grey\n",
    "    \"na_west\": \"#666666\",  # grey\n",
    "    \"na_east\": \"#777777\",  # light grey\n",
    "}\n",
    "\n",
    "csv_all = os.path.join(sk.get_file_destination(), \"resorts.csv\")\n",
    "geo_all = pd.DataFrame(\n",
    "    [  # lon      lat     name      region    pass\n",
    "        (137.861, 36.690, \"Hakuba\", \"japan\", \"epic\"),\n",
    "        (140.874, 42.739, \"Rusutsu\", \"japan\", \"epic\"),\n",
    "        (140.688, 42.864, \"Niseko\", \"japan\", \"ikon\"),\n",
    "        (138.175, 37.027, \"Lotte Arai\", \"japan\", \"ikon\"),\n",
    "        (6.8632749, 45.924065, \"Chamonix\", \"alps\", \"ikon\"),\n",
    "        (7.7522747, 46.0222204, \"Zermatt\", \"alps\", \"ikon\"),\n",
    "        (8.5916293, 46.6324621, \"Andermatt-Sedrun\", \"alps\", \"epic\"),\n",
    "        (12.3925407, 47.4492375, \"Kitzbuhel\", \"alps\", \"ikon\"),\n",
    "        (-123.204545, 49.396018, \"Cypress\", \"pnw\", \"ikon\"),\n",
    "        (-121.6781891, 44.0024701, \"Bachelor\", \"pnw\", \"ikon\"),\n",
    "        (-121.0890197, 47.7448119, \"Stevens Pass\", \"pnw\", \"epic\"),\n",
    "        (-122.9486474, 50.1149639, \"Whistler\", \"pnw\", \"epic\"),\n",
    "        (-121.4747533, 46.9352963, \"Crystal Mtn\", \"pnw\", \"ikon\"),\n",
    "        (-121.4257485, 47.4442426, \"Alpental\", \"pnw\", \"ikon\"),\n",
    "        (-121.4164161, 47.4245711, \"Snoqualmie\", \"pnw\", \"ikon\"),\n",
    "        # (-106.9490961, 39.2083984, \"Aspen Snowmass\",\"rockies\",\"ikon\"), # redundant Ajax\n",
    "        # (-106.8610687, 39.2058029, \"Buttermilk\",\"rockies\",\"ikon\"),  # redundant Ajax\n",
    "        # (-106.8553613, 39.1824124, \"Aspen Highlands\",\"rockies\",\"ikon\"),  # redundant Ajax\n",
    "        (-106.818227, 39.1862601, \"Aspen Mtn\", \"rockies\", \"ikon\"),\n",
    "        (-106.8045169, 40.4571991, \"Steamboat\", \"rockies\", \"ikon\"),\n",
    "        # (-106.5167109, 39.6042863, \"Beaver Creek\",\"rockies\",\"epic\"), # redundant Vail\n",
    "        (-106.3549717, 39.6061444, \"Vail\", \"rockies\", \"epic\"),\n",
    "        (-106.1516265, 39.501419, \"Copper\", \"rockies\", \"ikon\"),\n",
    "        (-106.0676088, 39.4808351, \"Breckenridge\", \"rockies\", \"epic\"),\n",
    "        (-105.9437656, 39.6075962, \"Keystone\", \"rockies\", \"epic\"),\n",
    "        (-105.8719397, 39.6425118, \"A-Basin\", \"rockies\", \"ikon\"),\n",
    "        (-105.762488, 39.8868392, \"Winter Park\", \"rockies\", \"ikon\"),\n",
    "        (-105.5826786, 39.9372203, \"Eldora\", \"rockies\", \"ikon\"),\n",
    "        (-72.9278443, 43.0906207, \"Stratton\", \"new_england\", \"ikon\"),\n",
    "        (-72.9204014, 42.9602444, \"Mt Snow\", \"new_england\", \"epic\"),\n",
    "        (-72.8944139, 44.1359019, \"Sugarbush\", \"new_england\", \"ikon\"),\n",
    "        # (-72.842512, 43.6621499, \"Pico\",\"new_england\",\"ikon\"), # redundant Killington\n",
    "        (-72.7967531, 43.6262922, \"Killington\", \"new_england\", \"ikon\"),\n",
    "        (-72.7814124, 44.5303066, \"Stowe\", \"new_england\", \"epic\"),\n",
    "        (-72.7170416, 43.4018257, \"Okemo\", \"new_england\", \"epic\"),\n",
    "        (-72.08014, 43.331889, \"Sunapee\", \"new_england\", \"epic\"),\n",
    "        (-71.8655176, 43.0198715, \"Crotched\", \"new_england\", \"epic\"),\n",
    "        (-71.6336041, 44.0563456, \"Loon\", \"new_england\", \"ikon\"),\n",
    "        (-71.2393036, 44.2640724, \"Wildcat\", \"new_england\", \"epic\"),\n",
    "        (-71.229443, 44.082771, \"Attitash\", \"new_england\", \"epic\"),\n",
    "        (-70.8568727, 44.4734182, \"Sunday River\", \"new_england\", \"ikon\"),\n",
    "        (-70.3085109, 45.0541811, \"Sugarloaf\", \"new_england\", \"ikon\"),\n",
    "        (1.4707674, 42.5729217, \"Arinsal\", \"europe\", \"ikon\"),\n",
    "        (1.499825, 42.6317345, \"Ordino ArcalÃ­s\", \"europe\", \"ikon\"),\n",
    "        (1.6462281, 42.5783833, \"Grandvalira\", \"europe\", \"ikon\"),\n",
    "        (11.6520936, 46.5739752, \"Dolomiti\", \"europe\", \"ikon\"),\n",
    "        (-120.2483913, 39.1906091, \"Palisades Tahoe\", \"na_west\", \"ikon\"),\n",
    "        (-120.1210934, 39.2745678, \"Northstar\", \"na_west\", \"epic\"),\n",
    "        (-120.0651665, 38.6847514, \"Kirkwood\", \"na_west\", \"epic\"),\n",
    "        (-119.9428424, 38.9569241, \"Heavenly\", \"na_west\", \"epic\"),\n",
    "        (-119.8859331, 50.8844311, \"Sun Peaks\", \"na_west\", \"ikon\"),\n",
    "        (-119.0906293, 37.7679169, \"June\", \"na_west\", \"ikon\"),\n",
    "        (-119.0267806, 37.6510972, \"Mammoth\", \"na_west\", \"ikon\"),\n",
    "        (-118.1630779, 50.9583858, \"Revelstoke\", \"na_west\", \"ikon\"),\n",
    "        (-117.8194705, 49.1024147, \"RED\", \"na_west\", \"ikon\"),\n",
    "        (-117.036177, 34.2248821, \"Snow Valley\", \"na_west\", \"ikon\"),\n",
    "        (-116.8892717, 34.2364081, \"Snow Summit\", \"na_west\", \"ikon\"),\n",
    "        (-116.8608572, 34.2276766, \"Bear Mtn\", \"na_west\", \"ikon\"),\n",
    "        (-116.6227441, 48.3679757, \"Schweitzer\", \"na_west\", \"ikon\"),\n",
    "        (-116.2380671, 50.4602801, \"Panorama\", \"na_west\", \"ikon\"),\n",
    "        (-116.1621717, 51.4419206, \"Lake Louise\", \"na_west\", \"ikon\"),\n",
    "        (-115.7840699, 51.0780997, \"Banff\", \"na_west\", \"ikon\"),\n",
    "        (-115.5982699, 51.2037624, \"Norquay\", \"na_west\", \"ikon\"),\n",
    "        (-115.5707632, 51.1751675, \"SkiBig3\", \"na_west\", \"ikon\"),\n",
    "        (-114.3542874, 43.6949128, \"Sun Valley\", \"na_west\", \"ikon\"),\n",
    "        (-114.3461537, 43.6820566, \"Dollar Mtn\", \"na_west\", \"ikon\"),\n",
    "        (-111.8571529, 41.2161404, \"Snowbasin\", \"na_west\", \"ikon\"),\n",
    "        (-111.6385807, 40.5884218, \"Alta\", \"na_west\", \"ikon\"),\n",
    "        # (-111.6563885, 40.5810814, \"Snowbird\",\"na_west\"), # redundant Alta\n",
    "        # (-111.591885, 40.619852, \"Solitude\",\"na_west\"), # redundant Alta\n",
    "        # (-111.583187, 40.598019, \"Brighton\",\"na_west\"), # redundant Alta\n",
    "        (-111.5079947, 40.6514199, \"Park City\", \"na_west\", \"epic\"),\n",
    "        # (-111.478306, 40.63738, \"Deer Valley\", \"na_west\", \"ikon\"), # redundant Park City\n",
    "        (-111.4012076, 45.2857289, \"Big Sky\", \"na_west\", \"ikon\"),\n",
    "        (-110.8279183, 43.5875453, \"Jackson Hole\", \"na_west\", \"ikon\"),\n",
    "        (-106.9878231, 38.8697146, \"Crested Butte\", \"na_west\", \"ikon\"),\n",
    "        (-105.4545, 36.5959999, \"Taos\", \"na_west\", \"ikon\"),\n",
    "        (-94.9707416, 39.4673048, \"Snow Creek\", \"na_east\", \"epic\"),  # Kansas City\n",
    "        (-92.7878062, 44.8576608, \"Afton\", \"na_east\", \"epic\"),  # Minneapolis\n",
    "        (-90.6506898, 38.5353168, \"Hidden Valley\", \"na_east\", \"epic\"),\n",
    "        (-88.1876602, 42.4989548, \"Wilmot\", \"na_east\", \"epic\"),\n",
    "        (-86.5122305, 38.5555868, \"Paoli\", \"na_east\", \"epic\"),\n",
    "        (-84.930067, 45.162884, \"Boyne\", \"na_east\", \"ikon\"),\n",
    "        # (-84.926535, 45.4647239, \"Boyne Highlands\", \"na_east\", \"ikon\"), # redundant Boyne\n",
    "        (-83.8115217, 42.54083, \"Mt. Brighton\", \"na_east\", \"epic\"),\n",
    "        (-83.6777778, 40.3180556, \"Mad River\", \"na_east\", \"epic\"),\n",
    "        (-81.5632108, 41.2640987, \"Boston Mills\", \"na_east\", \"epic\"),\n",
    "        (-81.259745, 41.52687, \"Alpine Valley\", \"na_east\", \"epic\"),\n",
    "        (-80.3122216, 44.5037818, \"Blue Mtn\", \"na_east\", \"ikon\"),\n",
    "        (-79.9960444, 38.4118566, \"Snowshoe\", \"na_east\", \"ikon\"),\n",
    "        (-79.2977032, 40.0229768, \"7 Springs\", \"na_east\", \"epic\"),\n",
    "        (-79.2581204, 40.058031, \"Hidden Valley 2\", \"na_east\", \"epic\"),\n",
    "        (-79.1657908, 40.1638728, \"Laurel\", \"na_east\", \"epic\"),\n",
    "        (-77.9333126, 39.7417652, \"Whitetail\", \"na_east\", \"epic\"),\n",
    "        (-77.375459, 39.76366, \"Liberty\", \"na_east\", \"epic\"),\n",
    "        (-76.9275492, 40.1094506, \"Roundtop\", \"na_east\", \"epic\"),\n",
    "        (-75.6563315, 41.1091686, \"Jack Frost\", \"na_east\", \"epic\"),\n",
    "        (-75.601282, 41.050189, \"Big Boulder\", \"na_east\", \"epic\"),\n",
    "        (-74.5852526, 46.2096417, \"Tremblant\", \"na_east\", \"ikon\"),\n",
    "        (-74.2567116, 42.2937298, \"Windham\", \"na_east\", \"ikon\"),\n",
    "        (-74.2246402, 42.2028811, \"Hunter\", \"na_east\", \"epic\"),\n",
    "    ],\n",
    "    columns=[\"lon\", \"lat\", \"name\", \"region\", \"pass\"],\n",
    ")\n",
    "assert not geo_all.duplicated(\n",
    "    subset=[\"lon\", \"lat\"]\n",
    ").any(), f\"Duplicate coordinates found: {geo_all[geo_all.duplicated(subset=['lon', 'lat'], keep=False)].sort_values(['lon', 'lat'])}\"\n",
    "geo_all[\"color\"] = geo_all[\"region\"].map(geo_colors)\n",
    "geo_all.to_csv(csv_all, index=False)\n",
    "loc_all = sk.Location(location_file=csv_all)\n",
    "print(loc_all)\n",
    "print(loc_all.load_location_file().drop(\"geometry\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster resort locations\n",
    "\n",
    "The `downscale` function requires that any one `location_file` contain geographic points from a single continent, and this is a global list. Let's break this into continental clusters and target 16 resorts per regional cluster to keep the downscale operation fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location file: ['resorts_6_00.csv', 'resorts_3_00.csv', 'resorts_1_03.csv', 'resorts_1_02.csv', 'resorts_1_01.csv', 'resorts_1_00.csv', 'resorts_1_04.csv', 'resorts_1_05.csv']\n"
     ]
    }
   ],
   "source": [
    "loc = loc_all.cluster(cluster_size=16, upload=\"changed\", verbose=False)\n",
    "# If the any of the clustered CSVs changed, we'll want to not load caches\n",
    "force = force or loc.any_changed\n",
    "# loc.plot_locations(title=\"global clustered locations\", names=False)\n",
    "print(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the data\n",
    "\n",
    "For each of the ski resorts, we will get the daily forecast of temperature and precipitation as of the beginning of the season. Then we will also get the historical observed conditions, calculate snowfall, and merge them into a single dataset for later analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downscale Daily Forecast\n",
    "\n",
    "In contrast to the probabilistic `forecast_timeseries` function, `downscale` samples historical analogs from the forecast distribution to create ensemble timeseries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name     location_file\n",
      "0  ski_example/downscale_227e38ce454c542b780868cf...  resorts_6_00.csv\n",
      "1  ski_example/downscale_8845053c079795255ad9f91e...  resorts_3_00.csv\n",
      "2  ski_example/downscale_6dd85bb42c06fdbd7db364dc...  resorts_1_03.csv\n",
      "3  ski_example/downscale_df4d9a188f4ac4f456962ff1...  resorts_1_02.csv\n",
      "4  ski_example/downscale_e0f8561b841f20737336ce00...  resorts_1_01.csv\n",
      "5  ski_example/downscale_6a7ba8d69890f03ff59ea6cc...  resorts_1_00.csv\n",
      "6  ski_example/downscale_c8e8eb455ab0031645f5ad72...  resorts_1_04.csv\n",
      "7  ski_example/downscale_5daf1924685d0010ea65d8a3...  resorts_1_05.csv\n"
     ]
    }
   ],
   "source": [
    "fcst_files = sk.downscale(\n",
    "    loc=loc,\n",
    "    variables=vars,\n",
    "    members=ensemble_count,\n",
    "    frequency=freq,\n",
    "    date=fcst_date,\n",
    "    force=force,\n",
    "    verbose=False,\n",
    "    strict=False,\n",
    ")\n",
    "print(fcst_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are requesting multiple location_files, `fcst_files` is a\n",
    "table with multiple downscale files. Let's combine all of them, and bring in the extra meta-information like `color` from the original `location_file` csvs using the `sk.merge_location_data` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_location_metadata(ds):\n",
    "    \"\"\"Merge additional columns from location_file.csv into the xarray datasets.\"\"\"\n",
    "    source = ds.encoding[\"source\"]\n",
    "    source_base = os.path.basename(source)\n",
    "    idx = fcst_files.index[fcst_files[\"file_name\"].apply(os.path.basename) == source_base][0]\n",
    "    loc_file = os.path.join(sk.get_file_destination(), fcst_files.loc[idx, \"location_file\"])\n",
    "    # get \"color\" and \"pass\" as coordinates:\n",
    "    return sk.merge_location_data(ds, loc_file, as_data_vars=False)\n",
    "\n",
    "\n",
    "fcst_raw = xr.open_mfdataset(\n",
    "    fcst_files[\"file_name\"].values,\n",
    "    concat_dim=\"location\",\n",
    "    combine=\"nested\",\n",
    "    preprocess=preprocess_location_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define add_season\n",
    "\n",
    "This utility function will add `season` and `season_elapsed` coordinates to a `time`-denominated `Dataset`. This will be useful later as an input to `stack_by_season`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season(\n",
    "    ds: xr.Dataset,\n",
    "    season_start_month: int,\n",
    "    season_end_month: int,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Add season and season_elapsed coordinates to a dataset based on time.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset with a 'time' coordinate\n",
    "        season_start_month: Month when season starts (e.g. 10 for October)\n",
    "        season_end_month: Month when season ends (e.g. 5 for May)\n",
    "\n",
    "    Returns:\n",
    "        xarray Dataset with new 'season' and 'season_elapsed' coordinates\n",
    "    \"\"\"\n",
    "    time = ds.time\n",
    "    month = time.dt.month.values\n",
    "    year = time.dt.year.values\n",
    "\n",
    "    # Calculate season_elapsed relative to each year's season start\n",
    "    season_starts = pd.to_datetime([f\"{y}-{season_start_month:02d}-01\" for y in year])\n",
    "    time_np = time.values.astype(\"datetime64[h]\")\n",
    "    season_starts_np = season_starts.values.astype(\"datetime64[h]\")\n",
    "\n",
    "    # For dates before October, use previous year's October 1\n",
    "    # Account for leap years in the offset\n",
    "    is_leap = pd.to_datetime(season_starts_np).is_leap_year\n",
    "    year_hours = np.where(is_leap, 366 * 24, 365 * 24)\n",
    "\n",
    "    hours = (\n",
    "        (\n",
    "            time_np\n",
    "            - np.where(\n",
    "                month < season_start_month,\n",
    "                season_starts_np - np.timedelta64(1, \"h\") * year_hours,\n",
    "                season_starts_np,\n",
    "            )\n",
    "        )\n",
    "        .astype(\"timedelta64[h]\")\n",
    "        .astype(int)\n",
    "    )\n",
    "    is_summer = (month >= season_end_month) & (month < season_start_month)\n",
    "    season_elapsed = np.where(is_summer, np.nan, hours)\n",
    "\n",
    "    # Calculate season year\n",
    "    season = year\n",
    "    season = np.where(month < season_start_month, season - 1, season)\n",
    "    season = np.where(is_summer, np.nan, season)\n",
    "\n",
    "    # Add new coordinates\n",
    "    ds = ds.assign_coords({\"season\": (\"time\", season), \"season_elapsed\": (\"time\", season_elapsed)})\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify the forecast\n",
    "\n",
    "Now let's simplify the downscale file by removing information we don't need and reformatting it to be \"day of season\" aware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 11MB\n",
      "Dimensions:         (ensemble: 101, time: 151, location: 91, dayofyear: 366)\n",
      "Coordinates:\n",
      "    region          (location) object 728B 'japan' 'japan' ... 'na_east'\n",
      "    pass            (location) object 728B 'epic' 'epic' ... 'epic' 'epic'\n",
      "    color           (location) object 728B '#004488' '#004488' ... '#777777'\n",
      "  * location        (location) object 728B 'Hakuba' 'Rusutsu' ... 'Big Boulder'\n",
      "    lat             (location) float64 728B dask.array<chunksize=(4,), meta=np.ndarray>\n",
      "    lon             (location) float64 728B dask.array<chunksize=(4,), meta=np.ndarray>\n",
      "    forecast_date   datetime64[ns] 8B 2024-12-01\n",
      "  * dayofyear       (dayofyear) float64 3kB 1.0 2.0 3.0 ... 364.0 365.0 366.0\n",
      "  * time            (time) datetime64[ns] 1kB 2024-12-01 ... 2025-04-30\n",
      "    season          (time) float64 1kB 2.024e+03 2.024e+03 ... 2.024e+03\n",
      "    season_elapsed  (time) float64 1kB 1.464e+03 1.488e+03 ... 5.064e+03\n",
      "Dimensions without coordinates: ensemble\n",
      "Data variables:\n",
      "    precip          (ensemble, time, location) float32 6MB dask.array<chunksize=(101, 151, 4), meta=np.ndarray>\n",
      "    temp            (ensemble, time, location) float32 6MB dask.array<chunksize=(101, 151, 4), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Remove things we don't need:\n",
    "unwanted_vars = [\"temp_clim\", \"precip_clim\", \"temp_anom\", \"precip_anom\", \"analog\"]\n",
    "fcst = fcst_raw.drop_vars([var for var in unwanted_vars if var in fcst_raw])\n",
    "\n",
    "# rename \"forecast_day\" to \"time\" for consistency with data_timeseries\n",
    "# Note that the coordinate is already called \"time\" when frequency=hourly\n",
    "fcst = fcst.rename({\"forecast_day\": \"time\"}) if \"forecast_day\" in fcst else fcst\n",
    "\n",
    "# Trim data beyond the end of the ski season:\n",
    "fcst = fcst.sel(time=slice(fcst_date, fcst_end))\n",
    "\n",
    "# Add \"day of season\" coordinate so we can later use it as the primary coord\n",
    "fcst = add_season(fcst, start_month, end_month)\n",
    "\n",
    "\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Historical Data\n",
    "\n",
    "The `data_timeseries` function will load the historical daily ERA5 timeseries, which we can later compare to the `downscale` timeseries ensembles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 9MB\n",
      "Dimensions:         (time: 6023, location: 91)\n",
      "Coordinates:\n",
      "  * time            (time) datetime64[ns] 48kB 2008-10-01 ... 2025-03-28\n",
      "  * location        (location) <U16 6kB '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "    lat             (location) float64 728B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon             (location) float64 728B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file   (location) object 728B 'resorts_1_04.csv' ... 'resorts_3_...\n",
      "    season          (time) float64 48kB 2.008e+03 2.008e+03 ... 2.024e+03\n",
      "    season_elapsed  (time) float64 48kB 0.0 24.0 48.0 ... 4.248e+03 4.272e+03\n",
      "Data variables:\n",
      "    temp            (time, location) float64 4MB 11.47 6.112 ... 3.247 -0.2834\n",
      "    precip          (time, location) float64 4MB 4.697 0.1293 ... 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get historical observed performance for each ski resort\n",
    "hist_files = sk.data_timeseries(\n",
    "    loc=loc,\n",
    "    variable=vars,\n",
    "    frequency=freq,\n",
    "    field=\"vals\",\n",
    "    start=hist_start,\n",
    "    end=min(fcst_end, today),\n",
    "    force=True,  # loc.any_changed,  # ignore caches if location_files have changed\n",
    ")\n",
    "\n",
    "# Assemble each historical file into a single xarray dataset\n",
    "hist = sk.load_multihistory(hist_files)\n",
    "hist = add_season(hist, start_month, end_month)\n",
    "\n",
    "\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepend history to forecast\n",
    "\n",
    "We begin analyzing each ski season in October to account for snow accumulation before the mountains become skiable. If the forecast was generated after October, prepend the observed history to the forecast timeseries so that we account for weather before the forecast was generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 31MB\n",
      "Dimensions:         (ensemble: 101, time: 212, location: 91, dayofyear: 366)\n",
      "Coordinates: (12/13)\n",
      "  * location        (location) object 728B '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "  * ensemble        (ensemble) int64 808B 0 1 2 3 4 5 6 ... 95 96 97 98 99 100\n",
      "  * time            (time) datetime64[ns] 2kB 2024-10-01 ... 2025-04-30\n",
      "    lat             (location) float64 728B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon             (location) float64 728B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file   (location) object 728B 'resorts_1_04.csv' ... 'resorts_3_...\n",
      "    ...              ...\n",
      "    season_elapsed  (time) float64 2kB 0.0 24.0 48.0 ... 5.04e+03 5.064e+03\n",
      "  * dayofyear       (dayofyear) float64 3kB 1.0 2.0 3.0 ... 364.0 365.0 366.0\n",
      "    region          (location) object 728B 'na_east' 'rockies' ... 'alps'\n",
      "    pass            (location) object 728B 'epic' 'ikon' ... 'ikon' 'ikon'\n",
      "    color           (location) object 728B '#777777' '#CC3311' ... '#33BBEE'\n",
      "    forecast_date   datetime64[ns] 8B 2024-12-01\n",
      "Data variables:\n",
      "    temp            (ensemble, time, location) float64 16MB dask.array<chunksize=(101, 61, 11), meta=np.ndarray>\n",
      "    precip          (ensemble, time, location) float64 16MB dask.array<chunksize=(101, 61, 11), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "season_start = f\"{year}-{start_month}-01\"\n",
    "if fcst.time[0] > np.datetime64(season_start):\n",
    "    fcst = xr.concat(\n",
    "        [\n",
    "            hist.sel(time=slice(season_start, fcst.time[0] - pd.Timedelta(days=1)))\n",
    "            .expand_dims(ensemble=fcst.ensemble)\n",
    "            .transpose(\"ensemble\", \"time\", \"location\"),\n",
    "            fcst,\n",
    "        ],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevation & Attributes\n",
    "\n",
    "The snow model has an optional elevation parameter to increase accuracy. Let's use the Salient `geo` function to get the elevation at each location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_files = sk.geo(loc=loc, variables=\"elevation\", force=force)\n",
    "elev = xr.open_mfdataset(\n",
    "    paths=elev_files[\"file_name\"],\n",
    "    concat_dim=\"location\",\n",
    "    combine=\"nested\",\n",
    ").load()\n",
    "elev = elev.set_coords(elev.data_vars)\n",
    "elev.attrs = {}  # prevent conflicts\n",
    "elev = elev.drop_vars([\"lat\", \"lon\"]).reindex(location=fcst.location)\n",
    "\n",
    "fcst = xr.merge([fcst, elev])\n",
    "hist = xr.merge([hist, elev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make units more readable for plotting purposes instead of \"mmm day-1\"\n",
    "fcst[\"precip\"].attrs[\"units\"] = hist[\"precip\"].attrs[\"units\"] = \"mm/day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we're done preprocessing the data, let's un-chunk it\n",
    "fcst = fcst.compute()\n",
    "hist = hist.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Snow Water Equivalent\n",
    "\n",
    "The `calc_swe` function builds on the `snow17` model to calculate the snow water equivalent (SWE) at each location and for each ensemble. It requires that the dataset input has data values `precip` and `temp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_swe(\n",
    "    ds: xr.Dataset, src_files: pd.DataFrame, name: str, force: bool = False\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Load or calculate snow water equivalent (SWE) with caching.\"\"\"\n",
    "    if force or \"swe\" not in ds:\n",
    "        cache_path = os.path.join(\n",
    "            sk.get_file_destination(),\n",
    "            f\"{name}_swe_{hashlib.md5(str(src_files).encode()).hexdigest()}.nc\",\n",
    "        )\n",
    "        if os.path.exists(cache_path) and not force:\n",
    "            ds[\"swe\"] = xr.load_dataarray(cache_path)\n",
    "        else:\n",
    "            ds[\"swe\"] = sk.hydro.calc_swe(ds, \"time\")\n",
    "            ds[\"swe\"].to_netcdf(path=cache_path, encoding={\"location\": {\"dtype\": str}})\n",
    "    return ds\n",
    "\n",
    "\n",
    "fcst = cached_swe(fcst, fcst_files, \"fcst\", force=True)\n",
    "print(fcst.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp          (location, season, season_elapsed) float64 3MB 11.47 ... nan\n",
      "    precip        (location, season, season_elapsed) float64 3MB 4.697 ... nan\n",
      "    swe           (location, season, season_elapsed) float64 3MB 0.0 0.0 ... nan\n",
      "    time          (season, season_elapsed) datetime64[ns] 29kB 2008-10-01 ......\n",
      "    season_start  (location) float64 728B 1.944e+03 504.0 ... 624.0 336.0\n",
      "    precipc       (location, season, season_elapsed) float64 3MB 0.0 ... 427.3\n"
     ]
    }
   ],
   "source": [
    "hist = cached_swe(hist, hist_files, \"hist\", force=force)\n",
    "print(hist.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `stack_by_season`\n",
    "\n",
    "To calculate a seasonal average similar to the forecast's per-ensemble average we need to break the single linear `time` dimension into `season` + `season_elapsed` dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_by_season(ds):\n",
    "    \"\"\"Convert a dataset from time dimension to season/season_elapsed dimensions.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset with time dimension and season/season_elapsed coordinates\n",
    "\n",
    "    Returns:\n",
    "        xarray Dataset with season and season_elapsed dimensions instead of time\n",
    "    \"\"\"\n",
    "    valid_mask = ~ds[\"season\"].isnull() & ~ds[\"season_elapsed\"].isnull()\n",
    "    ds_clean = ds.isel(time=valid_mask).copy()\n",
    "    ds_clean[\"season\"] = ds_clean.season.astype(int)\n",
    "    ds_clean[\"season_elapsed\"] = ds_clean.season_elapsed.astype(int)\n",
    "    time_df = pd.DataFrame(\n",
    "        {\n",
    "            \"season\": ds_clean.season.values,\n",
    "            \"season_elapsed\": ds_clean.season_elapsed.values,\n",
    "            \"time\": ds_clean.time.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds_stacked = ds_clean.set_index(time=[\"season\", \"season_elapsed\"])\n",
    "    ds_final = ds_stacked.unstack(\"time\")\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [ds_final.season.values, ds_final.season_elapsed.values],\n",
    "        names=[\"season\", \"season_elapsed\"],\n",
    "    )\n",
    "    time_series = time_df.set_index([\"season\", \"season_elapsed\"])[\"time\"].reindex(idx)\n",
    "    ds_final[\"time\"] = xr.DataArray(\n",
    "        time_series.values.reshape(len(ds_final.season), len(ds_final.season_elapsed)),\n",
    "        dims=[\"season\", \"season_elapsed\"],\n",
    "    )\n",
    "\n",
    "    return ds_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack timeseries by season\n",
    "\n",
    "So we can later merge, let's denominate the forecast and historical by `season_elapsed` instead of `time`. The `season` coordinate of `hist` represents each historical season, and the `ensemble` coordinate of `fcst` represents each potential future for this season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical ------\n",
      "Data variables:\n",
      "    temp     (location, season, season_elapsed) float64 3MB 11.47 7.944 ... nan\n",
      "    precip   (location, season, season_elapsed) float64 3MB 4.697 2.747 ... nan\n",
      "    swe      (location, season, season_elapsed) float64 3MB 0.0 0.0 ... nan nan\n",
      "    time     (season, season_elapsed) datetime64[ns] 29kB 2008-10-01 ... NaT\n",
      "Forecast ------\n",
      "Data variables:\n",
      "    temp     (ensemble, location, season_elapsed) float64 16MB 15.71 ... 0.4002\n",
      "    precip   (ensemble, location, season_elapsed) float64 16MB 22.06 ... 0.9323\n",
      "    swe      (ensemble, location, season_elapsed) float64 16MB 0.0 0.0 ... 765.0\n",
      "    time     (season_elapsed) datetime64[ns] 2kB 2024-10-01 ... 2025-04-30\n"
     ]
    }
   ],
   "source": [
    "hist = stack_by_season(hist)\n",
    "fcst = stack_by_season(fcst).squeeze(\"season\")\n",
    "\n",
    "print(\"Historical ------\")\n",
    "print(hist.data_vars)\n",
    "print(\"Forecast ------\")\n",
    "print(fcst.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate season start\n",
    "\n",
    "Each location has a different historical start of its snow accumulation season. We'll later calculate historical averages only once the season has begun, because we only want to start the `temp` and `precip` anomaly clock once the snow accumulation season starts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first hour that historically has meaningful snow accumulation\n",
    "SNOW_THRESHOLD = 10  # mm\n",
    "WINTER_SOLSTICE = 82 * 24  # Hours since October 1\n",
    "hist_swe_avg = hist[\"swe\"].mean(\"season\")\n",
    "season_start = (\n",
    "    hist_swe_avg.where(hist_swe_avg > SNOW_THRESHOLD)\n",
    "    .idxmin(\"season_elapsed\")\n",
    "    .clip(max=WINTER_SOLSTICE)\n",
    ")\n",
    "hist[\"season_start\"] = fcst[\"season_start\"] = season_start\n",
    "\n",
    "# visualize -----\n",
    "season_start_plt = (season_start / 24).to_pandas().sort_values()\n",
    "ax = season_start_plt.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(15, 5),\n",
    "    color=[fcst.sel(location=loc).color.item() for loc in season_start_plt.index],\n",
    ")\n",
    "plt.yticks([0, 31, (30 + 31), (30 + 31 + 31)], [\"Oct\", \"Nov\", \"Dec\", \"Jan\"])\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "ax.set_xticklabels(season_start_plt.index)\n",
    "ax.set_ylabel(f\"Days since {start_month}-01\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Accumulation Start\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulate precipitation since season start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical ------\n",
      "Data variables:\n",
      "    temp          (location, season, season_elapsed) float64 3MB 11.47 ... nan\n",
      "    precip        (location, season, season_elapsed) float64 3MB 4.697 ... nan\n",
      "    swe           (location, season, season_elapsed) float64 3MB 0.0 0.0 ... nan\n",
      "    time          (season, season_elapsed) datetime64[ns] 29kB 2008-10-01 ......\n",
      "    season_start  (location) float64 728B 1.944e+03 504.0 ... 624.0 336.0\n",
      "    precipc       (location, season, season_elapsed) float64 3MB 0.0 ... 427.3\n",
      "Forecast ------\n",
      "Data variables:\n",
      "    temp          (ensemble, location, season_elapsed) float64 16MB 15.71 ......\n",
      "    precip        (ensemble, location, season_elapsed) float64 16MB 22.06 ......\n",
      "    swe           (ensemble, location, season_elapsed) float64 16MB 0.0 ... 7...\n",
      "    time          (season_elapsed) datetime64[ns] 2kB 2024-10-01 ... 2025-04-30\n",
      "    season_start  (location) float64 728B 1.944e+03 504.0 ... 624.0 336.0\n",
      "    precipc       (ensemble, location, season_elapsed) float64 16MB 0.0 ... 9...\n"
     ]
    }
   ],
   "source": [
    "for ds in [hist, fcst]:\n",
    "    ds[\"precipc\"] = (\n",
    "        ds[\"precip\"]\n",
    "        .where(~(ds.season_elapsed < ds.season_start), 0)\n",
    "        .cumsum(dim=\"season_elapsed\", keep_attrs=True)\n",
    "        .assign_attrs({\"units\": \"mm\", \"long_name\": \"Cumulative precipitation\"})\n",
    "    )\n",
    "\n",
    "print(\"Historical ------\")\n",
    "print(hist.data_vars)\n",
    "print(\"Forecast ------\")\n",
    "print(fcst.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Anomalies\n",
    "\n",
    "Combine the historical and forecast datasets into a single dataset so that we can make sure they are aligned by `location`.\n",
    "\n",
    "We don't want to highlight resorts with below-average snowfall, so let's sort the dataset and cut out the bottom half.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim = hist.sel(season=slice(None, year - 1)).mean(\"season\", keep_attrs=True)\n",
    "\n",
    "\n",
    "def calc_anom(var, use_season_start=False, as_percent=False):\n",
    "    \"\"\"Calculate anomalies using resort-specific accumulation periods.\"\"\"\n",
    "    SPRING_EQUINOX = 170 * 24  # relative to October 1\n",
    "    start_time = fcst.season_start if use_season_start else WINTER_SOLSTICE\n",
    "    valid_fcst = (fcst.season_elapsed >= start_time) & (fcst.season_elapsed <= SPRING_EQUINOX)\n",
    "    valid_clim = (clim.season_elapsed >= start_time) & (clim.season_elapsed <= SPRING_EQUINOX)\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        f = fcst[var].where(valid_fcst).mean(\"season_elapsed\")\n",
    "        c = clim[var].where(valid_clim).mean(\"season_elapsed\")\n",
    "        if as_percent:\n",
    "            anom = 100 * (f / c)\n",
    "            anom.attrs[\"units\"] = \"%\"\n",
    "        else:\n",
    "            anom = f - c\n",
    "    anom.attrs[\"long_name\"] += \" anomaly\"\n",
    "    return anom\n",
    "\n",
    "\n",
    "anom = xr.Dataset(\n",
    "    {\n",
    "        \"swe\": calc_anom(\"swe\", use_season_start=False, as_percent=True),\n",
    "        \"precip\": calc_anom(\"precip\", use_season_start=True, as_percent=True),\n",
    "        \"precipc\": calc_anom(\"precipc\", use_season_start=True, as_percent=True),\n",
    "        \"temp\": calc_anom(\"temp\", use_season_start=True, as_percent=False),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Focus on above-median resorts by snowfall\n",
    "loc_avg = fcst.swe.mean(dim=[\"ensemble\", \"season_elapsed\"], keep_attrs=True)\n",
    "min_avg = loc_avg.median() + 10  # search just a bit above the median\n",
    "anom = anom.sel(location=loc_avg[loc_avg > min_avg].location.values)\n",
    "\n",
    "# Sort by SWE anomaly, highest first\n",
    "anom = anom.sel(\n",
    "    location=anom.swe.mean(dim=[\"ensemble\"]).sortby(anom.swe.mean(dim=[\"ensemble\"])).location\n",
    ")\n",
    "\n",
    "# Smooth out historical temperature for cleaner plotting\n",
    "clim[\"temp\"] = (\n",
    "    clim[\"temp\"]\n",
    "    .rolling(season_elapsed=len(clim.season_elapsed) // 10, center=True, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "htime = pd.to_datetime(hist[\"time\"].sel(season=year - 4).values) + pd.DateOffset(years=4)\n",
    "clim = clim.assign(time=(\"season_elapsed\", htime))\n",
    "anom = anom.assign(time=(\"season_elapsed\", htime))\n",
    "\n",
    "# Visualize the per-resort averages so we can understand the analysis cutoff\n",
    "loc_avg_plt = loc_avg.to_pandas().sort_values()\n",
    "ax = loc_avg_plt.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(15, 5),\n",
    "    color=[loc_avg.sel(location=locname).color.item() for locname in loc_avg_plt.index],\n",
    ")\n",
    "plt.axhline(y=min_avg, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xticklabels(loc_avg_plt.index)\n",
    "ax.set_ylabel(f\"Mean {loc_avg.attrs['long_name']} [{loc_avg.attrs['units']}]\")\n",
    "ax.set_xlabel(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denominate back in elapsed time for better plotting\n",
    "fcst = fcst.swap_dims({\"season_elapsed\": \"time\"})\n",
    "anom = anom.swap_dims({\"season_elapsed\": \"time\"})\n",
    "clim = clim.swap_dims({\"season_elapsed\": \"time\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(\n",
    "    fcst: xr.DataArray,\n",
    "    title: str = \"\",\n",
    "    vline=None,\n",
    "    legend_loc: str = \"center right\",\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"Plot predictions and observed values as a box-and-whisker plot.\"\"\"\n",
    "    # extract a table of seasonal averages (across all days) per location\n",
    "    box_data = fcst.to_dataframe(dim_order=[\"location\", \"ensemble\"])\n",
    "    box_data = box_data[fcst.name].unstack(level=0).to_numpy()\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 10))  # Create a new figure if no axis is provided\n",
    "\n",
    "    plt_box = ax.boxplot(\n",
    "        box_data,\n",
    "        showfliers=False,\n",
    "        vert=False,\n",
    "        tick_labels=fcst.location.values,\n",
    "        patch_artist=True,\n",
    "        meanline=True,\n",
    "        showmeans=True,\n",
    "        meanprops=dict(linewidth=1, color=\"white\"),\n",
    "        medianprops=dict(linewidth=0, color=\"gray\", alpha=0),\n",
    "        whiskerprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "        capprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "        boxprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "    )\n",
    "\n",
    "    [patch.set_facecolor(color) for patch, color in zip(plt_box[\"boxes\"], fcst.color.values)]\n",
    "    ax.set_xlabel(f\"{fcst.long_name} ({fcst.units})\")\n",
    "    ax.set_title(title)\n",
    "    legend_names = [\"japan\", \"alps\", \"pnw\", \"rockies\", \"new_england\"]\n",
    "    legend_handles = plt_box[\"boxes\"][: len(legend_names)]\n",
    "\n",
    "    if vline is not None:\n",
    "        ax.axvline(vline, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    if legend_loc == \"none\":\n",
    "        ax.set_yticklabels([])\n",
    "    else:\n",
    "        leg = ax.legend(legend_handles, legend_names, loc=legend_loc)\n",
    "        for patch, reg in zip(leg.get_patches(), legend_names):\n",
    "            patch.set_facecolor(geo_colors[reg])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 15))\n",
    "plot_boxes(anom[\"swe\"], vline=100, legend_loc=\"upper left\", ax=ax1)\n",
    "plot_boxes(anom[\"precip\"], vline=100, legend_loc=\"none\", ax=ax2)\n",
    "plot_boxes(anom[\"temp\"], vline=0, legend_loc=\"none\", ax=ax3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Location Detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensembles(fcst, clim, var=\"temp\", title=False):\n",
    "    \"\"\"Show ensemble values for a given variable.\"\"\"\n",
    "    x_val = \"time\"\n",
    "\n",
    "    favg = fcst.mean(dim=\"ensemble\", keep_attrs=True)\n",
    "    clr = fcst[\"color\"].values.tolist()\n",
    "\n",
    "    ens_lines = fcst[var].plot.line(x=x_val, color=\"grey\", alpha=0.1, add_legend=False)\n",
    "    mean_line = favg[var].plot.line(x=x_val, color=\"black\", linewidth=2, add_legend=False)\n",
    "    hist_line = clim[var].plot.line(x=x_val, color=clr, alpha=0.8, linewidth=2, add_legend=False)\n",
    "\n",
    "    winter_start = np.datetime64(\"2024-12-21\")\n",
    "    winter_end = np.datetime64(\"2025-03-20\")\n",
    "    plt.axvspan(winter_start, winter_end, color=\"grey\", alpha=0.1)\n",
    "    plt.text(\n",
    "        winter_start + (winter_end - winter_start) / 2,\n",
    "        plt.ylim()[0],  # bottom of plot\n",
    "        \"winter\",\n",
    "        color=\"grey\",\n",
    "        alpha=0.5,\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"bottom\",\n",
    "    )\n",
    "    plt.title(fcst[\"location\"].values if title else \"\")\n",
    "    plt.xlabel(\"\")\n",
    "    if title:\n",
    "        plt.legend(\n",
    "            [ens_lines[0], mean_line[0], hist_line[0]],\n",
    "            [\"Forecast Ensembles\", \"Forecast Mean\", \"Historical Mean\"],\n",
    "            loc=\"upper left\",\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_locations(focus_names):\n",
    "    \"\"\"Plot swe/temp/precip timeseries for the named resort.\"\"\"\n",
    "    (fig, axs) = plt.subplots(\n",
    "        nrows=len(focus_names),\n",
    "        ncols=3,\n",
    "        sharex=True,\n",
    "        sharey=\"col\",\n",
    "        figsize=(5 * 3, 5 * len(focus_names)),\n",
    "    )\n",
    "\n",
    "    for idx in range(len(focus_names)):\n",
    "        fcst_loc = fcst.sel(location=focus_names[idx])\n",
    "        clim_loc = clim.sel(location=focus_names[idx])\n",
    "        plt.sca(axs[idx, 0])\n",
    "        plot_ensembles(fcst_loc, clim_loc, \"swe\", title=True)\n",
    "        plt.sca(axs[idx, 1])\n",
    "        plot_ensembles(fcst_loc, clim_loc, \"precipc\")\n",
    "        plt.sca(axs[idx, 2])\n",
    "        plot_ensembles(fcst_loc, clim_loc, \"temp\")\n",
    "        plt.axhline(0, color=\"k\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on key resorts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_locations([\"RED\", \"Attitash\", \"Eldora\", \"Hakuba\", \"Kitzbuhel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_locations([\"Aspen Mtn\", \"Whistler\", \"Zermatt\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
