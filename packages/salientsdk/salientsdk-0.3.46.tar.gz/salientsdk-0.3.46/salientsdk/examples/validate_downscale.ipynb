{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Downscale Skill\n",
    "\n",
    "This example shows how to evaluate Salient's daily downscaled forecasts and calculate meaningful metrics. It demonstrates [validation best practices](https://salientpredictions.notion.site/Validation-0220c48b9460429fa86f577914ea5248) such as:\n",
    "\n",
    "- Proper scoring using the Ensemble Continuous Ranked Probability Score (CRPS)\n",
    "  - Considers the full forecast distribution to reward both accuracy and precision\n",
    "  - Less sensitive to climatology decisions than metrics like Anomaly Correlation\n",
    "- A long backtesting period (2015-2022)\n",
    "  - Short evaluation periods are subject to noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7fc556a8a090>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "# Prevent wrapping on tables for readability\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "sk.set_file_destination(\"validate_ensemble_example\")\n",
    "sk.login(\"SALIENT_USERNAME\", \"SALIENT_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize The Validation\n",
    "\n",
    "This notebook is written flexibly so you have the option of validating Salient and other forecasts multiple ways. These variables will control what, when, and how the validation proceeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast dates to test:\n",
      "['2021-04-21', '2021-05-19', '2021-06-16', '2021-07-21']\n"
     ]
    }
   ],
   "source": [
    "# 1. The meteorological variable that we'll be evaluating:\n",
    "vars = [\"temp\", \"precip\"]  # wspd, tsi\n",
    "\n",
    "# 2. Number of days in the downscale:\n",
    "length = 35  # fast 35-day evaluation vs gefs (also 35 days)\n",
    "# length = 366  # comprehensive full-year evaluation\n",
    "\n",
    "# 3. Debias temp, precip, and wind\n",
    "debias = False  # Evaluate vs ERA5\n",
    "# debias = True  # Evaluate vs observations from GHCNd\n",
    "\n",
    "# 4. Set the date range to test over.\n",
    "(start_date, end_date) = (\"2021-04-01\", \"2021-07-31\")  # fast \"sample\" dataset\n",
    "# (start_date, end_date) = (\"2015-01-01\", \"2022-12-31\")  # out-of-sample \"test\" set\n",
    "# (start_date, end_date) = (\"2000-01-01\", \"2022-12-31\")  # comprehensive \"all-history\" set\n",
    "\n",
    "# 5. The reference model to compare Salient blend to\n",
    "# ref_model = \"none\"  # skip the reference model comparison\n",
    "ref_model = \"noaa_gefs\"  # good for daily-frequency 35-day comparisons\n",
    "# ref_model = \"noaa_gfs\"  # hourly-frequency comparisons\n",
    "\n",
    "# 6. Variable to focus on for plots\n",
    "plot_var = \"temp\"\n",
    "# plot_var = \"precip\"\n",
    "assert plot_var in vars\n",
    "\n",
    "\n",
    "# ===== Additional shared variables ==========================\n",
    "# Not recommended to change these.\n",
    "\n",
    "# For specialized use only, Salient can manually provide a \"bulk downscale\" zarr\n",
    "# bulk = os.path.join(sk.get_file_destination(), \"bulk_downscale\")  # zarr directory\n",
    "bulk = None  # don't use bulk downscale (default)\n",
    "\n",
    "# Temporal resolution of the downscaled & historical timeseries:\n",
    "freq = \"daily\"\n",
    "# freq = \"hourly\"\n",
    "\n",
    "# Caching strategy:\n",
    "force = False  # Cache data to save on repeat API calls\n",
    "# force = True  # Repeat API calls, even if data exists\n",
    "\n",
    "# Make all figures have a consistent size:\n",
    "figsize = (8, 5)\n",
    "\n",
    "# Determine which dates for which to request forecasts:\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "# GEFS is unavailable for 2020 January-September\n",
    "if \"gefs\" in ref_model:\n",
    "    date_range = date_range[~((date_range.year == 2020) & (date_range.month <= 9))]\n",
    "\n",
    "# Find first Wednesday on or after 16th of each month\n",
    "date_range = (\n",
    "    date_range[(date_range.dayofweek == 2) & (date_range.day >= 16)]\n",
    "    .to_series()\n",
    "    .groupby([lambda x: x.year, lambda x: x.month])\n",
    "    .first()\n",
    "    .dt.strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Forecast dates to test:\")\n",
    "print(date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Area of Interest\n",
    "\n",
    "The Salient SDK uses a \"Location\" object to specify the geographic bounds of a request. In this case, we will be validating against the vector of airport locations that are used to settle the Chicago Mercantile Exchange's Cooling and Heating Degree Day contracts. With `load_location_file` we can see that the file contains:\n",
    "\n",
    "- `lat` / `lon`: latitude and longitude of the met station, standard for a `location_file`\n",
    "- `name`: the 3-letter IATA airport code of the location, also `location_file` standard\n",
    "- `ghcnd`: the global climate network ID of the station, used to validate against observations. To customize this analysis for any set of observation stations, use the NCEI [stations list](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt).\n",
    "- `cme`: the CME code for the location used to create CDD/HDD strip codes.\n",
    "- `description`: full name of the airport\n",
    "\n",
    "If you have a list of locations already defined in a separate CSV file, you can use [`upload_file`](https://sdk.salientpredictions.com/api/#salientsdk.upload_file) to upload the file directly without building it in code via `upload_location_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lat        lon name        ghcnd cme                description                     geometry\n",
      "0   33.62972  -84.44224  ATL  USW00013874   1         Atlanta Hartsfield   POINT (-84.44224 33.62972)\n",
      "1   42.36057  -71.00975  BOS  USW00014739   W               Boston Logan   POINT (-71.00975 42.36057)\n",
      "2   34.19966 -118.36543  BUR  USW00023152   P  Burbank-Glendale-Pasadena  POINT (-118.36543 34.19966)\n",
      "3   41.96017  -87.93164  ORD  USW00094846   2             Chicago O'Hare   POINT (-87.93164 41.96017)\n",
      "4   39.04443  -84.67241  CVG  USW00093814   3     Cincinnati (Covington)   POINT (-84.67241 39.04443)\n",
      "5   32.89744  -97.02196  DFW  USW00003927   5          Dallas-Fort Worth   POINT (-97.02196 32.89744)\n",
      "6   29.98438  -95.36072  IAH  USW00012960   R        Houston-George Bush   POINT (-95.36072 29.98438)\n",
      "7   36.07190 -115.16343  LAS  USW00023169   0         Las Vegas McCarran   POINT (-115.16343 36.0719)\n",
      "8   44.88523  -93.23133  MSP  USW00014922   Q         Minneapolis-StPaul   POINT (-93.23133 44.88523)\n",
      "9   40.77945  -73.88027  LGA  USW00014732   4        New York La Guardia   POINT (-73.88027 40.77945)\n",
      "10  39.87326  -75.22681  PHL  USW00013739   6               Philadelphia   POINT (-75.22681 39.87326)\n",
      "11  45.59578 -122.60919  PDX  USW00024229   7                   Portland  POINT (-122.60919 45.59578)\n",
      "12  38.50659 -121.49604  SAC  USW00023232   S            Sacramento Exec  POINT (-121.49604 38.50659)\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "loc = sk.Location(location_file=sk.upload_location_file(\n",
    "    lats =[33.62972     ,      42.36057,      34.19966,      41.96017,      39.04443,      32.89744,      29.98438,      36.07190,      44.88523,      40.77945,      39.87326,      45.59578,      38.50659],\n",
    "    lons =[-84.44224    ,     -71.00975,    -118.36543,     -87.93164,     -84.67241,     -97.02196,     -95.36072,    -115.16343,     -93.23133,     -73.88027,     -75.22681,    -122.60919,    -121.49604],\n",
    "    names=[\"ATL\"        ,         \"BOS\",         \"BUR\",         \"ORD\",         \"CVG\",         \"DFW\",         \"IAH\",         \"LAS\",         \"MSP\",         \"LGA\",         \"PHL\",         \"PDX\",         \"SAC\"],\n",
    "    ghcnd=[\"USW00013874\", \"USW00014739\", \"USW00023152\", \"USW00094846\", \"USW00093814\", \"USW00003927\", \"USW00012960\", \"USW00023169\", \"USW00014922\", \"USW00014732\", \"USW00013739\", \"USW00024229\", \"USW00023232\"],\n",
    "    cme  =[\"1\"          ,           \"W\",           \"P\",           \"2\",           \"3\",           \"5\",           \"R\",           \"0\",           \"Q\",           \"4\",           \"6\",           \"7\",           \"S\"],\n",
    "    geoname=\"cmeus\",\n",
    "    force=force,\n",
    "    description=[\"Atlanta Hartsfield\", \"Boston Logan\", \"Burbank-Glendale-Pasadena\", \"Chicago O'Hare\", \"Cincinnati (Covington)\",\"Dallas-Fort Worth\", \"Houston-George Bush\", \"Las Vegas McCarran\", \"Minneapolis-StPaul\", \"New York La Guardia\",\"Philadelphia\", \"Portland\", \"Sacramento Exec\"],\n",
    "))\n",
    "# fmt: on\n",
    "stations = loc.load_location_file()\n",
    "print(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Data\n",
    "\n",
    "To calculate forecast skill, we will want to compare forecasts made in the past with actuals. There are two flavors of actual data: (1) The ERA5 reanalysis dataset and (2) point weather station observations.\n",
    "\n",
    "Salient's forecast natively predicts (1) ERA5, but contains a debiasing function to remove bias between ERA5 and (2) station observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical ERA5 or Observed Data\n",
    "\n",
    "Download daily or hourly historical values from [`data_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.data_timeseries) or `get_ghcnd` and then aggregate to match the forecasts, so that we can ensure that all forecasts use the same dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request Historical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 36kB\n",
      "Dimensions:   (time: 163, location: 13)\n",
      "Coordinates:\n",
      "  * time      (time) datetime64[ns] 1kB 2021-03-27 2021-03-28 ... 2021-09-05\n",
      "    lat       (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon       (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "  * location  (location) <U3 156B 'ATL' 'BOS' 'BUR' 'ORD' ... 'PHL' 'PDX' 'SAC'\n",
      "Data variables:\n",
      "    temp      (time, location) float64 17kB 21.77 10.66 14.5 ... 21.44 24.16\n",
      "    precip    (time, location) float64 17kB 4.033 0.0 0.0 ... 3.343 0.01152 0.0\n"
     ]
    }
   ],
   "source": [
    "if debias:\n",
    "    # Use historical observations (not ERA5) as the truth.\n",
    "    hist = xr.load_dataset(\n",
    "        sk.met_observations(\n",
    "            loc=loc,\n",
    "            variables=vars,\n",
    "            start=np.datetime64(start_date) - np.timedelta64(5, \"D\"),\n",
    "            end=np.datetime64(end_date) + np.timedelta64(length + 1, \"D\"),\n",
    "            verbose=False,\n",
    "            force=force,\n",
    "        )\n",
    "    )\n",
    "    # Make sure that we found the expected set of stations:\n",
    "    assert hist.station.values.tolist() == stations.ghcnd.to_list()\n",
    "    # Remove lat-lons to prevent merge conflicts later:\n",
    "    hist = hist.reset_coords(drop=True)\n",
    "elif bulk:\n",
    "    # Use a Salient-provided bulk downscale zarr\n",
    "    hist = xr.open_zarr(bulk)\n",
    "    truth_vars = [var for var in hist.data_vars if \"truth\" in var]\n",
    "    hist = hist[truth_vars].rename({var: var.replace(\"_truth\", \"\") for var in truth_vars})\n",
    "else:\n",
    "    # Validate vs ERA5\n",
    "    hist = sk.load_multihistory(\n",
    "        sk.data_timeseries(\n",
    "            loc=loc,\n",
    "            variable=vars,\n",
    "            field=\"vals\",\n",
    "            start=np.datetime64(start_date) - np.timedelta64(5, \"D\"),\n",
    "            end=np.datetime64(end_date) + np.timedelta64(length + 1, \"D\"),\n",
    "            frequency=freq,\n",
    "            verbose=False,\n",
    "            force=force,\n",
    "        )\n",
    "    )\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscale the Salient Forecast\n",
    "\n",
    "The [`downscale`](https://sdk.salientpredictions.com/api/#salientsdk.downscale) API endpoint and SDK function converts Salient's native weekly/monthly/quarterly probabilistic forecasts into a daily or hourly ensemble timeseries.\n",
    "\n",
    "This is the most heavyweight call in the notebook, since it's getting multiple historical forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name        date\n",
      "0  validate_ensemble_example/downscale_d1f38fc0e6...  2021-04-21\n",
      "1  validate_ensemble_example/downscale_47d128c2a7...  2021-05-19\n",
      "2  validate_ensemble_example/downscale_3156cfdf5f...  2021-06-16\n",
      "3  validate_ensemble_example/downscale_f83414a94d...  2021-07-21\n"
     ]
    }
   ],
   "source": [
    "if bulk:\n",
    "    # Use a Salient-provided bulk downscale zarr\n",
    "    fcst = xr.open_zarr(bulk)\n",
    "    fcst_vars = [var for var in hist.data_vars if \"truth\" not in var]\n",
    "    fcst = fcst[fcst_vars]\n",
    "else:\n",
    "    fcst = sk.downscale(\n",
    "        loc=loc,\n",
    "        variables=vars,\n",
    "        debias=debias,\n",
    "        date=date_range,\n",
    "        frequency=freq,\n",
    "        length=length,\n",
    "        verbose=False,\n",
    "        force=force,\n",
    "        strict=False,  # if one downscale call fails, proceed with others\n",
    "    )\n",
    "    # Check to see if there are any missing forecasts:\n",
    "    fcst_na = fcst[fcst[\"file_name\"].isna()]\n",
    "    if not fcst_na.empty:\n",
    "        print(\"Missing forecast dates:\")\n",
    "        print(fcst_na)\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_source = (\n",
    "    None\n",
    "    if ref_model == \"none\"  # skip reference comparisons\n",
    "    else sk.forecast_timeseries(\n",
    "        loc=loc,\n",
    "        variable=vars,\n",
    "        date=date_range,\n",
    "        field=\"vals_ens\",\n",
    "        model=ref_model,\n",
    "        timescale=freq,\n",
    "        strict=False,\n",
    "        force=force,\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name        date\n",
      "0  validate_ensemble_example/forecast_timeseries_...  2021-04-21\n",
      "1  validate_ensemble_example/forecast_timeseries_...  2021-05-19\n",
      "2  validate_ensemble_example/forecast_timeseries_...  2021-06-16\n",
      "3  validate_ensemble_example/forecast_timeseries_...  2021-07-21\n"
     ]
    }
   ],
   "source": [
    "def format_as_downscale(ref: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reformat single-variable forecast_timeseries to match downscale's format.\n",
    "\n",
    "    Changes:\n",
    "    - Renames 'lead' dimension to 'forecast_day'\n",
    "    - Reorders dimensions to (ensemble, forecast_day, location)\n",
    "    - Converts forecast_day values to datetime using forecast_date + lead\n",
    "    \"\"\"\n",
    "    if ref is None:\n",
    "        return None\n",
    "\n",
    "    def process_date(date, group):\n",
    "        out_file = os.path.join(sk.get_file_destination(), f\"forecast_timeseries_ref_{date}.nc\")\n",
    "\n",
    "        # Load first dataset to get forecast_date\n",
    "        first_ds = xr.load_dataset(group.iloc[0].file_name, decode_timedelta=True)\n",
    "        forecast_date = first_ds.forecast_date\n",
    "\n",
    "        # Create dataset with reordered dimensions\n",
    "        ds = xr.Dataset(\n",
    "            {\n",
    "                row.variable: xr.load_dataset(row.file_name, decode_timedelta=True)\n",
    "                .vals_ens.transpose(\"ensemble\", \"lead\", \"location\")\n",
    "                .assign_attrs(xr.load_dataset(row.file_name, decode_timedelta=True).attrs)\n",
    "                for _, row in group.iterrows()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Convert lead to forecast_day\n",
    "        ds = ds.rename({\"lead\": \"forecast_day\"})\n",
    "        ds[\"forecast_day\"] = forecast_date + ds.forecast_day\n",
    "\n",
    "        ds.to_netcdf(out_file, encoding={\"location\": {\"dtype\": str}})\n",
    "        return {\"file_name\": out_file, \"date\": date}\n",
    "\n",
    "    result_files = [process_date(date, group) for date, group in ref.groupby(\"date\")]\n",
    "    return pd.DataFrame(result_files)\n",
    "\n",
    "\n",
    "ref = format_as_downscale(ref_source)\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Skill Metrics\n",
    "\n",
    "Compare the forecast and ERA5 datasets to see how well they match. Here we will calculate the same \"Continuous Ranked Probability Score\" that resulted from the call to `hindcast_summary` earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 37kB\n",
      "Dimensions:          (forecast_date: 4, lead: 35, location: 13)\n",
      "Coordinates:\n",
      "  * location         (location) <U3 156B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat              (location) float64 104B 33.63 42.36 34.2 ... 45.6 38.51\n",
      "    lon              (location) float64 104B -84.44 -71.01 ... -122.6 -121.5\n",
      "  * lead             (lead) timedelta64[ns] 280B 0 days 1 days ... 34 days\n",
      "  * forecast_date    (forecast_date) datetime64[ns] 32B 2021-04-21 ... 2021-0...\n",
      "Data variables:\n",
      "    crps_temp_all    (forecast_date, lead, location) float64 15kB 0.7206 ... ...\n",
      "    crps_precip_all  (forecast_date, lead, location) float64 15kB 0.0002451 ....\n",
      "    crps_temp        (lead, location) float64 4kB 0.5861 0.2641 ... 1.534 1.364\n",
      "    crps_precip      (lead, location) float64 4kB 0.408 1.316 ... 1.646 0.007467\n",
      "Attributes:\n",
      "    short_name:  crps\n",
      "    long_name:   CRPS\n"
     ]
    }
   ],
   "source": [
    "skill = sk.skill.crps_ensemble(observations=hist, forecasts=fcst)\n",
    "print(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "skill[f\"crps_{plot_var}\"].plot.line(x=\"lead\", hue=\"location\", add_legend=True, alpha=0.5, ax=ax)\n",
    "skill[f\"crps_{plot_var}\"].mean(\"location\").plot(ax=ax, color=\"black\", linewidth=2, label=\"Mean\")\n",
    "ax.set_xlabel(\"Lead Time (days)\")\n",
    "plot_name = skill[f\"crps_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())\n",
    "plot_units = (\n",
    "    f'[{skill[f\"crps_{plot_var}\"].attrs[\"units\"]}]'\n",
    "    if \"units\" in skill[f\"crps_{plot_var}\"].attrs\n",
    "    else \"\"\n",
    ")\n",
    "ax.set_ylabel(f\"CRPS {plot_name} {plot_units}\".strip())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Skill Relative to Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 36kB\n",
      "Dimensions:           (location: 13, lead: 34, forecast_date: 4)\n",
      "Coordinates:\n",
      "  * location          (location) <U3 156B 'ATL' 'BOS' 'BUR' ... 'PDX' 'SAC'\n",
      "  * lead              (lead) timedelta64[ns] 272B 1 days 2 days ... 34 days\n",
      "  * forecast_date     (forecast_date) datetime64[ns] 32B 2021-04-21 ... 2021-...\n",
      "    lat               (location) float64 104B 33.63 42.36 34.2 ... 45.6 38.51\n",
      "    lon               (location) float64 104B -84.44 -71.01 ... -122.6 -121.5\n",
      "Data variables:\n",
      "    crpss_temp_all    (forecast_date, lead, location) float64 14kB 0.8007 ......\n",
      "    crpss_precip_all  (forecast_date, lead, location) float64 14kB 1.0 ... 0....\n",
      "    crpss_temp        (lead, location) float64 4kB 0.5471 0.8113 ... 0.2946\n",
      "    crpss_precip      (lead, location) float64 4kB 0.08583 0.9439 ... -0.09593\n",
      "Attributes:\n",
      "    short_name:  crpss\n",
      "    long_name:   CRPSS\n"
     ]
    }
   ],
   "source": [
    "if ref is None:\n",
    "    print(\"No reference model, skipping relative comparison\")\n",
    "    skill_ref = None\n",
    "    skill_rel = None\n",
    "else:\n",
    "    # Skill of the reference model:\n",
    "    skill_ref = sk.skill.crps_ensemble(observations=hist, forecasts=ref)\n",
    "\n",
    "    # Relative skills score of Salient downscale vs the reference model:\n",
    "    skill_rel = sk.skill.crpss(forecast=skill, reference=skill_ref)\n",
    "    print(skill_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skill_ref is None:\n",
    "    print(\"Skipping relative skill plotting\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    skill_ref[f\"crps_{plot_var}\"].mean(\"location\").plot(\n",
    "        ax=ax,\n",
    "        color=\"#FF7F00\",\n",
    "        linewidth=2,\n",
    "        label=ref_model.replace(\"_\", \" \").upper(),\n",
    "    )\n",
    "    skill[f\"crps_{plot_var}\"].mean(\"location\").plot(\n",
    "        ax=ax,\n",
    "        color=\"dodgerblue\",\n",
    "        linewidth=2,\n",
    "        label=\"Salient downscale\",\n",
    "    )\n",
    "\n",
    "    ax.xaxis.set_major_formatter(lambda x, pos: f\"{x/1e9/86400:.0f}\")\n",
    "    ax.set_xlabel(\"Lead Time (days)\")\n",
    "    ax.set_ylabel(\n",
    "        f'CRPS {skill[f\"crps_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())} [{skill[f\"crps_{plot_var}\"].attrs.get(\"units\", \"\")}]'\n",
    "    )\n",
    "    ax.set_title(\"All-locations Mean Error (lower is better)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skill_rel is None:\n",
    "    print(\"Skipping relative skill timeseries plot\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    skill_rel[f\"crpss_{plot_var}\"].plot.line(\n",
    "        x=\"lead\", hue=\"location\", add_legend=True, alpha=0.5, ax=ax\n",
    "    )\n",
    "    skill_rel[f\"crpss_{plot_var}\"].mean(\"location\").plot(\n",
    "        ax=ax, color=\"black\", linewidth=2, label=\"Mean\"\n",
    "    )\n",
    "    ax.xaxis.set_major_formatter(lambda x, pos: f\"{x/1e9/86400:.0f}\")\n",
    "    ax.axhline(y=0, color=\"grey\", linestyle=\":\", zorder=0)\n",
    "    ax.set_title(f\"Relative Skill Salient downscale vs {ref_model} (higher is better)\")\n",
    "    ax.set_xlabel(\"Lead Time (days)\")\n",
    "    ax.set_ylabel(\n",
    "        f'CRPSS ({skill_rel[f\"crpss_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())})'\n",
    "    )\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_ylim(max(-0.7, ymin), min(0.7, ymax))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skill_rel is None:\n",
    "    print(\"Skipping relative skill boxplot\")\n",
    "else:\n",
    "    medians = skill_rel[f\"crpss_{plot_var}\"].median(\"lead\")\n",
    "    sorted_locations = medians.sortby(medians, ascending=False).location.values\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    df = skill_rel[f\"crpss_{plot_var}\"].to_pandas().melt(ignore_index=False)\n",
    "    ax.boxplot(\n",
    "        [df[df[\"location\"] == loc][\"value\"] for loc in sorted_locations],\n",
    "        tick_labels=sorted_locations,  # Updated parameter name\n",
    "        patch_artist=True,\n",
    "        showfliers=False,\n",
    "        medianprops=dict(color=\"black\"),\n",
    "        boxprops=dict(facecolor=\"dodgerblue\"),\n",
    "    )\n",
    "    ax.axhline(y=0, color=\"grey\", linestyle=\":\", zorder=0)\n",
    "    ax.set_xlabel(\"Location\")\n",
    "    ax.set_ylabel(\n",
    "        f'CRPSS ({skill_rel[f\"crpss_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())})'\n",
    "    )\n",
    "    ax.set_title(f\"Relative Skill Salient downscale vs {ref_model} (higher is better)\")\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_ylim(max(-1, ymin), min(1, ymax))\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salientsdk-54A4kIpb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
