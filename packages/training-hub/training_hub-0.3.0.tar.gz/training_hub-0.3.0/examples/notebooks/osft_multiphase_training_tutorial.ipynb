{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OSFT Multi-Phase Training Tutorial\n\nThis notebook demonstrates how to perform multi-phase training using the OSFT (Orthogonal Subspace Fine-Tuning) algorithm. The key innovation is that **OSFT eliminates the need for replay buffers** while still preserving all capabilities across training phases.\n\n## The Two-Phase OSFT Process:\n\n1. **Phase 1 - Knowledge Tuning (Phase07)**: Training on knowledge-heavy data to build foundational understanding\n2. **Phase 2 - Skills Training (Phase10)**: Training on skills data with a **reduced unfreeze_rank_ratio** to preserve Phase 1 knowledge\n\n## Key Advantages of OSFT Multi-Phase Training:\n\n- ✅ **No replay buffers needed** - OSFT naturally preserves prior knowledge\n- ✅ **Simpler data pipeline** - Just use your knowledge and skills data directly\n- ✅ **Better preservation** - Reduce unfreeze_rank_ratio during skills training for optimal retention\n- ✅ **No catastrophic forgetting** - Built into the algorithm\n- ✅ **Replaces traditional LAB workflows** - More efficient than LAB multi-phase training\n\n## The Unfreeze Ratio Strategy:\n\n- **Phase 1**: Use standard ratio (e.g., 0.3) for knowledge acquisition\n- **Phase 2**: Reduce by ~10% (e.g., 0.2) to preserve knowledge while adding skills\n\nThis progressive reduction ensures each phase builds upon the previous without overwriting.\n\n## How OSFT Multi-Phase Replaces LAB Workflows:\n\nTraditional LAB (Large-scale Alignment for chatBots) multi-phase training requires complex replay buffers and data mixing. **OSFT Multi-Phase training provides the same benefits with a much simpler approach** - no replay data needed, just progressive unfreeze ratio reduction."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our training environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training_hub for OSFT training\n",
    "from training_hub import osft\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Logging Configuration\n\nSet up logging to track progress while preventing notebook crashes from excessive output.\n\n**Note:** For production workflows or long-running jobs, we recommend using the script version at `scripts/osft_multiphase_training.py` for better logging consistency and resumption capabilities.\n\n**Quick script usage:**\n```bash\npython scripts/osft_multiphase_training.py \\\n  --base-model-path /path/to/model \\\n  --phase07-data-path /path/to/knowledge.jsonl \\\n  --phase10-data-path /path/to/skills.jsonl \\\n  --ckpt-output-base-dir /path/to/checkpoints\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to show only essential information\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Suppress verbose logging from transformers and other libraries\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"✅ Logging configured for notebook environment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Let's define some helper functions for checkpoint management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"\n",
    "    Find the most recent checkpoint in the training output directory.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Training output directory containing hf_format/ subdirectory\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the most recent checkpoint\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If no checkpoints are found\n",
    "    \"\"\"\n",
    "    # Get all checkpoint directories under hf_format\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"hf_format\", \"samples_*.0\")\n",
    "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        raise ValueError(f\"No checkpoints found in {os.path.join(output_dir, 'hf_format')}\")\n",
    "    \n",
    "    # Find the most recently created checkpoint\n",
    "    most_recent_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    \n",
    "    return most_recent_checkpoint\n",
    "\n",
    "print(\"✅ Checkpoint utility functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding Multi-Phase Data Requirements\n\nOSFT Multi-Phase training requires carefully curated datasets for each phase:\n\n### Phase07 (Knowledge) Data\n- Focus on factual knowledge, domain expertise, and foundational understanding\n- Examples: technical documentation, educational content, reference materials\n- Format: Standard JSONL with messages\n\n### Phase10 (Skills) Data  \n- Focus on task completion, instruction following, and practical applications\n- Examples: coding tasks, problem-solving, conversational skills\n- Format: Standard JSONL with messages\n\n### The OSFT Advantage Over Traditional LAB Workflows\n**With traditional LAB SFT**, Phase10 would need:\n- Skills data\n- \\+ Phase07 knowledge data (replay)\n- \\+ Base model instruction data (replay)\n- = Complex data mixing and large datasets\n\n**With OSFT Multi-Phase**, Phase10 only needs:\n- Skills data\n- That's it! OSFT preserves prior knowledge automatically\n\nThis makes OSFT Multi-Phase training a superior replacement for LAB workflows, eliminating the complexity of replay buffer management while providing the same multi-phase training benefits."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Model and Data Paths\n",
    "\n",
    "Configure your base model and data paths for the two-phase training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MODEL AND DATA CONFIGURATION\n# =============================================================================\n\n# Base model configuration\nBASE_MODEL_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"  # Or your preferred base model\n# BASE_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct\"\n# BASE_MODEL_PATH = \"microsoft/Phi-4-mini-instruct\"\n\n# Data paths for each phase\nPHASE07_DATA_PATH = \"/path/to/your/phase07_knowledge_data.jsonl\"  # Knowledge data\nPHASE10_DATA_PATH = \"/path/to/your/phase10_skills_data.jsonl\"     # Skills data ONLY (no replay needed!)\n\n# Output configuration\nCHECKPOINT_BASE_DIR = \"/path/to/checkpoints\"\nEXPERIMENT_PREFIX = \"osft_multiphase_experiment\"\n\n# Create timestamped experiment directory\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nexperiment_name = f\"{EXPERIMENT_PREFIX}_{timestamp}\"\n\nprint(\"📋 OSFT Multi-Phase Configuration\")\nprint(\"=\" * 50)\nprint(f\"Base Model: {BASE_MODEL_PATH}\")\nprint(f\"Phase07 Data: {PHASE07_DATA_PATH}\")\nprint(f\"Phase10 Data: {PHASE10_DATA_PATH}\")\nprint(f\"Output Directory: {CHECKPOINT_BASE_DIR}/{experiment_name}\")\nprint()\nprint(\"✨ Key Difference from Traditional LAB SFT:\")\nprint(\"  Phase10 only needs skills data - no replay buffers!\")\nprint(\"  OSFT preserves Phase07 knowledge automatically.\")\nprint(\"  This workflow replaces complex LAB multi-phase training.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSFT-Specific Parameters: The Progressive Unfreeze Strategy\n",
    "\n",
    "The key to successful multi-phase training with OSFT is progressively reducing the `unfreeze_rank_ratio` in each phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OSFT PROGRESSIVE UNFREEZE STRATEGY\n",
    "# =============================================================================\n",
    "\n",
    "# Phase07: Initial knowledge acquisition\n",
    "PHASE07_UNFREEZE_RATIO = 0.3  # Standard ratio for knowledge learning\n",
    "\n",
    "# Phase10: Reduced ratio for better preservation\n",
    "UNFREEZE_REDUCTION = 0.1  # Reduce by 10% for each subsequent phase\n",
    "PHASE10_UNFREEZE_RATIO = max(0.1, PHASE07_UNFREEZE_RATIO - UNFREEZE_REDUCTION)\n",
    "\n",
    "print(\"🎯 OSFT Progressive Unfreeze Strategy\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Phase07 (Knowledge): unfreeze_rank_ratio = {PHASE07_UNFREEZE_RATIO}\")\n",
    "print(f\"Phase10 (Skills):    unfreeze_rank_ratio = {PHASE10_UNFREEZE_RATIO}\")\n",
    "print(f\"Reduction:           -{UNFREEZE_REDUCTION} per phase\")\n",
    "print()\n",
    "print(\"📊 Strategy Explanation:\")\n",
    "print(f\"  • Phase07 ({PHASE07_UNFREEZE_RATIO}): More freedom to acquire new knowledge\")\n",
    "print(f\"  • Phase10 ({PHASE10_UNFREEZE_RATIO}): Reduced to preserve Phase07 learning\")\n",
    "print()\n",
    "print(\"💡 Guidelines:\")\n",
    "print(\"  • Start with 0.25-0.35 for Phase07\")\n",
    "print(\"  • Reduce by 0.05-0.15 for each subsequent phase\")\n",
    "print(\"  • Never go below 0.1 (too restrictive)\")\n",
    "print(\"  • Adjust based on your preservation needs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters\n",
    "\n",
    "Configure training parameters for both phases. Note that we can use similar settings for both phases since OSFT handles preservation automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Common parameters for both phases\n",
    "MAX_SEQ_LEN = 8_192                 # Maximum sequence length\n",
    "MAX_TOKENS_PER_GPU = 10_000         # Memory limit per GPU\n",
    "NUM_EPOCHS = 2                      # Training epochs per phase\n",
    "WARMUP_STEPS = 0                    # Warmup for Phase07\n",
    "USE_LIGER = True                    # Enable Liger kernels for efficiency\n",
    "\n",
    "# Phase07 specific parameters\n",
    "PHASE07_BATCH_SIZE = 128            # Batch size for knowledge training\n",
    "PHASE07_LEARNING_RATE = 5e-6        # Use low learning rate for better learning quality\n",
    "\n",
    "# Phase10 specific parameters  \n",
    "PHASE10_BATCH_SIZE = 128            # Can use same batch size (no replay data!)\n",
    "PHASE10_LEARNING_RATE = 5e-6        # Use low learning rate for better learning quality\n",
    "PHASE10_WARMUP_STEPS = 0            # No warmup\n",
    "\n",
    "# Distributed training configuration\n",
    "NPROC_PER_NODE = 8                  # Number of GPUs per node\n",
    "NNODES = 1                          # Number of nodes\n",
    "NODE_RANK = 0                       # Rank of this node\n",
    "RDZV_ID = 47                        # Unique job ID\n",
    "RDZV_ENDPOINT = \"127.0.0.1:29500\"   # Rendezvous endpoint\n",
    "\n",
    "print(\"⚙️  Training Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LEN:,}\")\n",
    "print(f\"Max Tokens per GPU: {MAX_TOKENS_PER_GPU:,}\")\n",
    "print(f\"Epochs per Phase: {NUM_EPOCHS}\")\n",
    "print()\n",
    "print(\"Phase07 (Knowledge):\")\n",
    "print(f\"  • Batch Size: {PHASE07_BATCH_SIZE}\")\n",
    "print(f\"  • Learning Rate: {PHASE07_LEARNING_RATE}\")\n",
    "print(f\"  • Warmup Steps: {WARMUP_STEPS}\")\n",
    "print()\n",
    "print(\"Phase10 (Skills):\")\n",
    "print(f\"  • Batch Size: {PHASE10_BATCH_SIZE}\")\n",
    "print(f\"  • Learning Rate: {PHASE10_LEARNING_RATE} (reduced for preservation)\")\n",
    "print(f\"  • Warmup Steps: {PHASE10_WARMUP_STEPS}\")\n",
    "print()\n",
    "print(f\"Distributed: {NPROC_PER_NODE} GPUs × {NNODES} nodes = {NPROC_PER_NODE * NNODES} total GPUs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Knowledge Training (Phase07)\n",
    "\n",
    "First, we train on knowledge data to build foundational understanding. This phase uses the standard unfreeze_rank_ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PHASE 1 (PHASE07): KNOWLEDGE TRAINING\n# =============================================================================\n\nphase07_output_dir = os.path.join(CHECKPOINT_BASE_DIR, f\"{experiment_name}_phase07\")\n\nprint(\"📚 Phase 1: Knowledge Training with OSFT\")\nprint(\"=\" * 60)\nprint(f\"Starting from: {BASE_MODEL_PATH}\")\nprint(f\"Training data: {PHASE07_DATA_PATH}\")\nprint(f\"Output directory: {phase07_output_dir}\")\nprint(f\"Unfreeze ratio: {PHASE07_UNFREEZE_RATIO}\")\nprint()\n\n# Capture output to prevent notebook crashes\noutput_buffer = StringIO()\nerror_buffer = StringIO()\n\nphase07_start_time = time.time()\n\ntry:\n    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n        # Phase07 OSFT training\n        phase07_result = osft(\n            # Model and data\n            model_path=BASE_MODEL_PATH,\n            data_path=PHASE07_DATA_PATH,\n            ckpt_output_dir=phase07_output_dir,\n            \n            # OSFT-specific\n            unfreeze_rank_ratio=PHASE07_UNFREEZE_RATIO,\n            \n            # Training parameters\n            num_epochs=NUM_EPOCHS,\n            effective_batch_size=PHASE07_BATCH_SIZE,\n            learning_rate=PHASE07_LEARNING_RATE,\n            max_seq_len=MAX_SEQ_LEN,\n            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n            \n            # Data processing\n            data_output_dir=os.path.join(phase07_output_dir, \"data_processing\"),\n            warmup_steps=WARMUP_STEPS,\n            \n            # Optimization\n            use_liger=USE_LIGER,\n            seed=42,\n            lr_scheduler=\"cosine\",\n            \n            # Checkpointing\n            checkpoint_at_epoch=True,\n            save_final_checkpoint=True,\n            \n            # Distributed training\n            nproc_per_node=NPROC_PER_NODE,\n            nnodes=NNODES,\n            node_rank=NODE_RANK,\n            rdzv_id=RDZV_ID,\n            rdzv_endpoint=RDZV_ENDPOINT,\n        )\n    \n    phase07_duration = time.time() - phase07_start_time\n    \n    print(f\"✅ Phase07 completed successfully in {phase07_duration/3600:.2f} hours!\")\n    print(f\"📁 Checkpoint saved to: {phase07_output_dir}\")\n    print()\n    print(\"📊 Phase07 Achievements:\")\n    print(\"  • Base model capabilities: ✅ Preserved\")\n    print(\"  • New knowledge integrated: ✅ Complete\")\n    print(\"  • Ready for Phase10: ✅ Yes\")\n    \n    # Find the most recent checkpoint for Phase10\n    PHASE07_CHECKPOINT = find_most_recent_checkpoint(phase07_output_dir)\n    print(f\"📁 Found most recent Phase07 checkpoint: {PHASE07_CHECKPOINT}\")\n    print(f\"📁 Ready for Phase10 training!\")\n    \nexcept Exception as e:\n    print(f\"❌ Phase07 training failed: {e}\")\n    print(\"\\nError details:\")\n    print(error_buffer.getvalue())\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Skills Training (Phase10)\n",
    "\n",
    "Now we train on skills data with a **reduced unfreeze_rank_ratio** to preserve Phase07 knowledge while adding new capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2 (PHASE10): SKILLS TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "phase10_output_dir = os.path.join(CHECKPOINT_BASE_DIR, f\"{experiment_name}_phase10\")\n",
    "\n",
    "print(\"🎯 Phase 2: Skills Training with OSFT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting from: {PHASE07_CHECKPOINT} (Phase07 checkpoint)\")\n",
    "print(f\"Training data: {PHASE10_DATA_PATH}\")\n",
    "print(f\"Output directory: {phase10_output_dir}\")\n",
    "print(f\"Unfreeze ratio: {PHASE10_UNFREEZE_RATIO} (reduced from {PHASE07_UNFREEZE_RATIO})\")\n",
    "print()\n",
    "print(\"💡 Key Innovation:\")\n",
    "print(\"  NO replay buffer needed! OSFT preserves Phase07 knowledge automatically.\")\n",
    "print(\"  The reduced unfreeze_rank_ratio ensures better preservation.\")\n",
    "print()\n",
    "\n",
    "# Capture output to prevent notebook crashes\n",
    "output_buffer = StringIO()\n",
    "error_buffer = StringIO()\n",
    "\n",
    "phase10_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
    "        # Phase10 OSFT training\n",
    "        phase10_result = osft(\n",
    "            # Start from Phase07 checkpoint\n",
    "            model_path=PHASE07_CHECKPOINT,\n",
    "            data_path=PHASE10_DATA_PATH,\n",
    "            ckpt_output_dir=phase10_output_dir,\n",
    "            \n",
    "            # OSFT-specific: REDUCED ratio for preservation\n",
    "            unfreeze_rank_ratio=PHASE10_UNFREEZE_RATIO,\n",
    "            \n",
    "            # Training parameters\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            effective_batch_size=PHASE10_BATCH_SIZE,\n",
    "            learning_rate=PHASE10_LEARNING_RATE,  # Lower LR for preservation\n",
    "            max_seq_len=MAX_SEQ_LEN,\n",
    "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
    "            \n",
    "            # Data processing\n",
    "            data_output_dir=os.path.join(phase10_output_dir, \"data_processing\"),\n",
    "            warmup_steps=PHASE10_WARMUP_STEPS,  # Shorter warmup\n",
    "            \n",
    "            # Optimization\n",
    "            use_liger=USE_LIGER,\n",
    "            seed=42,  \n",
    "            lr_scheduler=\"cosine\",\n",
    "            \n",
    "            # Checkpointing\n",
    "            checkpoint_at_epoch=True,\n",
    "            save_final_checkpoint=True,\n",
    "            \n",
    "            # Distributed training\n",
    "            nproc_per_node=NPROC_PER_NODE,\n",
    "            nnodes=NNODES,\n",
    "            node_rank=NODE_RANK,\n",
    "            rdzv_id=RDZV_ID + 1,  # Different ID for Phase10\n",
    "            rdzv_endpoint=RDZV_ENDPOINT,\n",
    "        )\n",
    "    \n",
    "    phase10_duration = time.time() - phase10_start_time\n",
    "    \n",
    "    print(f\"✅ Phase10 completed successfully in {phase10_duration/3600:.2f} hours!\")\n",
    "    print(f\"📁 Final checkpoint saved to: {phase10_output_dir}\")\n",
    "    print()\n",
    "    print(\"📊 Phase10 Achievements:\")\n",
    "    print(\"  • Base model capabilities: ✅ Preserved\")\n",
    "    print(\"  • Phase07 knowledge: ✅ Retained\")  \n",
    "    print(\"  • New skills integrated: ✅ Complete\")\n",
    "    \n",
    "    # Find the most recent checkpoint from Phase10 training\n",
    "    FINAL_CHECKPOINT = find_most_recent_checkpoint(phase10_output_dir)\n",
    "    print(f\"📁 Final model checkpoint: {FINAL_CHECKPOINT}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Phase10 training failed: {e}\")\n",
    "    print(\"\\nError details:\")\n",
    "    print(error_buffer.getvalue())\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis and Summary\n",
    "\n",
    "Let's analyze the complete two-phase training results and understand what we've achieved with OSFT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# FINAL ANALYSIS AND SUMMARY\n# =============================================================================\n\ntotal_duration = (phase07_duration + phase10_duration) / 3600\n\nprint(\"🎉 OSFT Multi-Phase Training Complete!\")\nprint(\"=\" * 60)\nprint(f\"Total training time: {total_duration:.2f} hours\")\nprint(f\"Final model: {FINAL_CHECKPOINT}\")\nprint()\n\n# Training summary\nprint(\"📊 Training Summary:\")\nprint(\"-\" * 50)\nprint(\"Phase 1 (Knowledge - Phase07):\")\nprint(f\"  • Duration: {phase07_duration/3600:.2f} hours\")\nprint(f\"  • Unfreeze ratio: {PHASE07_UNFREEZE_RATIO}\")\nprint(f\"  • Batch size: {PHASE07_BATCH_SIZE}\")\nprint(f\"  • Learning rate: {PHASE07_LEARNING_RATE}\")\nprint(f\"  • Checkpoint: {PHASE07_CHECKPOINT}\")\nprint()\nprint(\"Phase 2 (Skills - Phase10):\")\nprint(f\"  • Duration: {phase10_duration/3600:.2f} hours\")\nprint(f\"  • Unfreeze ratio: {PHASE10_UNFREEZE_RATIO} (reduced by {UNFREEZE_REDUCTION})\")\nprint(f\"  • Batch size: {PHASE10_BATCH_SIZE}\")\nprint(f\"  • Learning rate: {PHASE10_LEARNING_RATE}\")\nprint(f\"  • Checkpoint: {FINAL_CHECKPOINT}\")\nprint()\n\n# Model capabilities\nprint(\"🚀 Your Model Now Has:\")\nprint(\"-\" * 50)\nprint(\"1. ✅ Original base model capabilities (preserved)\")\nprint(\"2. ✅ New knowledge from Phase07 (integrated)\")\nprint(\"3. ✅ Task-specific skills from Phase10 (acquired)\")\nprint(\"4. ❌ Catastrophic forgetting (none!)\")\nprint()\n\n# How to use the model\nprint(\"💻 Using Your Trained Model:\")\nprint(\"-\" * 50)\nprint(\"```python\")\nprint(\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\nprint(\"\")\nprint(\"# Load your OSFT Multi-Phase trained model\")\nprint(f\"model = AutoModelForCausalLM.from_pretrained('{FINAL_CHECKPOINT}')\")\nprint(f\"tokenizer = AutoTokenizer.from_pretrained('{FINAL_CHECKPOINT}')\")\nprint(\"\")\nprint(\"# The model now excels at:\")\nprint(\"# 1. General instruction following (preserved from base)\")\nprint(\"# 2. Domain knowledge (from Phase07)\")\nprint(\"# 3. Specific skills (from Phase10)\")\nprint(\"\")\nprint(\"# Test it:\")\nprint(\"prompt = 'Your domain-specific question here'\")\nprint(\"inputs = tokenizer(prompt, return_tensors='pt')\")\nprint(\"outputs = model.generate(**inputs, max_new_tokens=200)\")\nprint(\"response = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\nprint(\"print(response)\")\nprint(\"```\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## OSFT Multi-Phase vs LAB Multi-Phase: Key Differences\n\nUnderstanding how OSFT Multi-Phase training simplifies and improves upon traditional LAB multi-phase training workflows."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# OSFT MULTI-PHASE vs LAB MULTI-PHASE COMPARISON\n# =============================================================================\n\nprint(\"📊 OSFT Multi-Phase vs LAB Multi-Phase Comparison\")\nprint(\"=\" * 70)\nprint()\nprint(\"| Aspect                  | LAB Multi-Phase (SFT)      | OSFT Multi-Phase           |\")\nprint(\"|-------------------------|-----------------------------|-----------------------------|\")\nprint(\"| **Phase07 Data**        | Knowledge data only         | Knowledge data only         |\")\nprint(\"| **Phase10 Data**        | Skills + Phase07 replay     | Skills data ONLY ✨         |\")\nprint(\"|                         | + Base model replay         |                             |\")\nprint(\"| **Data Complexity**     | Complex mixing ratios       | Simple, direct              |\")\nprint(\"| **Data Storage**        | 3x larger (with replays)    | 1x (no replays needed)      |\")\nprint(\"| **Preservation Method** | Data replay buffers         | Algorithm (unfreeze ratio)  |\")\nprint(\"| **Configuration**       | Complex replay ratios       | Simple ratio reduction      |\")\nprint(\"| **Forgetting Risk**     | If replay ratios wrong      | Minimal by design           |\")\nprint(\"| **Training Time**       | Longer (more data)          | Shorter (less data)         |\")\nprint(\"| **Task Performance**    | Full model fitting          | Similar (slight trade-off)  |\")\nprint(\"| **Capability Preserv.** | Depends on replay quality   | Guaranteed by algorithm     |\")\nprint()\nprint(\"✨ Key OSFT Multi-Phase Advantages:\")\nprint(\"  1. No need to store or manage replay buffers\")\nprint(\"  2. Simpler data pipeline - just your new data\")\nprint(\"  3. Progressive unfreeze strategy ensures preservation\")\nprint(\"  4. Reduced training time (less data to process)\")\nprint(\"  5. Guaranteed preservation through algorithm design\")\nprint(\"  6. Direct replacement for LAB workflows with better efficiency\")\nprint()\nprint(\"⚖️ Trade-offs:\")\nprint(\"  • OSFT may achieve slightly lower task-specific performance\")\nprint(\"  • But preserves other capabilities that SFT would degrade\")\nprint(\"  • Overall: Better capability preservation vs. task fitting balance\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Practices and Recommendations\n\nKey guidelines for successful OSFT Multi-Phase training that replaces traditional LAB workflows."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# BEST PRACTICES AND RECOMMENDATIONS\n# =============================================================================\n\nprint(\"📚 Best Practices for OSFT Multi-Phase Training\")\nprint(\"=\" * 60)\nprint()\nprint(\"1️⃣  **Unfreeze Ratio Strategy:**\")\nprint(\"   • Start with 0.25-0.35 for Phase07\")\nprint(\"   • Reduce by 0.05-0.15 for Phase10\")\nprint(\"   • Never go below 0.1 (too restrictive)\")\nprint(\"   • If seeing forgetting, reduce the ratio further\")\nprint()\nprint(\"2️⃣  **Data Quality:**\")\nprint(\"   • Phase07: Focus on high-quality knowledge/facts\")\nprint(\"   • Phase10: Focus on diverse skills and tasks\")\nprint(\"   • No need for replay data - OSFT handles preservation!\")\nprint()\nprint(\"3️⃣  **Learning Rate Strategy:**\")\nprint(\"   • Phase07: Standard LR (e.g., 5e-6)\")\nprint(\"   • Phase10: Standard LR (e.g., 5e-6) for stability\")\nprint(\"   • Use cosine scheduler for smooth convergence\")\nprint(\"   • If model isn't learning enough, try increasing LR or number of epochs;\")\nprint(\"     the optimal settings will vary by model and data\")\nprint()\nprint(\"4️⃣  **Monitoring:**\")\nprint(\"   • Track loss curves for both phases\")\nprint(\"   • Test preservation after each phase\")\nprint(\"   • Evaluate on held-out sets for each capability\")\n\nprint(\"🎯 Next Steps:\")\nprint(\"-\" * 50)\nprint(\"1. Test your model on evaluation sets\")\nprint(\"2. Compare with base model to verify improvements\")\nprint(\"3. Fine-tune unfreeze ratios if needed\")\nprint(\"4. Deploy with confidence - no capability regression expected!\")\nprint()\nprint(\"📝 For production use, remember to use the script:\")\nprint(\"   scripts/osft_multiphase_training.py\")\nprint()\nprint(\"🔄 Replacing LAB Workflows:\")\nprint(\"   This OSFT Multi-Phase approach can directly replace\")\nprint(\"   traditional LAB multi-phase training with better efficiency!\")\nprint()\nprint(\"⚖️ Performance Expectations:\")\nprint(\"   • Task-specific performance: Similar to LAB (may be slightly lower)\")\nprint(\"   • Capability preservation: Superior to SFT (no degradation)\")\nprint(\"   • OSFT trades some task fitting for capability preservation\")\nprint()\nprint(\"Happy training! 🚀\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Concepts Explained\n\n### OSFT Multi-Phase Training Benefits:\n\n1. **Knowledge → Skills with Progressive Unfreeze**: Phase07 builds foundational knowledge, Phase10 adds task-specific capabilities using reduced unfreeze ratio\n2. **No Replay Buffers Needed**: OSFT's orthogonal subspace approach preserves prior learning algorithmically\n3. **Simple Data Pipeline**: Just your knowledge and skills data - no complex mixing or replay datasets\n4. **Progressive Preservation**: Reducing unfreeze_rank_ratio in each phase ensures cumulative learning\n5. **Guaranteed Non-Forgetting**: Mathematical guarantee through orthogonal weight updates\n6. **LAB Workflow Replacement**: Provides all benefits of LAB multi-phase training with superior efficiency\n\n### OSFT Algorithm Advantages:\n\n1. **Algorithmic Preservation**: Prior knowledge preserved through math, not data replay\n2. **Simplified Training**: No need to manage replay buffers or mixing ratios\n3. **Reduced Storage**: No need to store ~370k sample replay buffers\n4. **Streamlined Training**: No replay data processing required\n5. **Predictable Behavior**: Preservation is guaranteed by the algorithm\n6. **Superior to LAB**: Replaces complex LAB workflows with simpler, more efficient approach\n\n### OSFT Multi-Phase Training Strategy:\n\n- **Phase07 Focus**: Knowledge acquisition with standard unfreeze ratio (0.25-0.35)\n- **Phase10 Focus**: Skills training with reduced ratio (0.15-0.25) for preservation\n- **Progressive Reduction**: Each phase reduces unfreeze_rank_ratio by 0.05-0.15\n- **Memory Management**: Same `max_tokens_per_gpu` strategy as SFT\n- **Fast Data Loading**: Using `/dev/shm` for data processing\n- **Simple Checkpointing**: Standard checkpointing, no special requirements\n\n### Data Requirements Comparison:\n\n**Traditional LAB Multi-Phase SFT**:\n- Phase07: Knowledge data\n- Phase10: Skills data + Knowledge replay + ~370k sample replay buffer\n\n**OSFT Multi-Phase (Our Approach)**:\n- Phase07: Knowledge data only  \n- Phase10: Skills data only\n- Result: Eliminates the replay buffer requirement entirely!\n\n**Why OSFT Multi-Phase is Superior**: Provides the same multi-phase training benefits as LAB workflows but with dramatically reduced complexity, storage requirements, and training time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visual comparison of training approaches\nprint(\"📊 Training Approach Comparison\")\nprint(\"=\" * 80)\nprint()\nprint(\"Traditional LAB Multi-Phase SFT:\")\nprint(\"  Phase07: [Knowledge Data] → Model_v1\")\nprint(\"  Phase10: [Skills Data] + [Knowledge Replay] + [~370k Replay Buffer] → Model_v2\")\nprint(\"           ~~~~~~~~~~~~~   ^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^\")\nprint(\"                               Requires additional replay buffers\")\nprint()\nprint(\"OSFT Multi-Phase (This Notebook):\")\nprint(\"  Phase07: [Knowledge Data] → Model_v1 (unfreeze_ratio=0.3)\")\nprint(\"  Phase10: [Skills Data] → Model_v2 (unfreeze_ratio=0.2)\")\nprint(\"           ~~~~~~~~~~~~~\")\nprint(\"        No replay buffers needed!\")\nprint()\nprint(\"✨ Key Benefit: Eliminates the ~370k sample replay buffer requirement!\")\nprint(\"🔄 This OSFT Multi-Phase workflow directly replaces LAB multi-phase training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### Common Issues:\n\n1. **Out of Memory (OOM)**:\n   - Reduce `max_tokens_per_gpu`\n   - Set `use_liger` to True\n   - Reduce `unfreeze_rank_ratio`\n   - Check GPU memory usage with `nvidia-smi`\n\n2. **Model Not Learning Well**:\n   - Check if `unfreeze_rank_ratio` is too low (try increasing slightly)\n   - Verify data quality and format\n   - Consider increasing learning rate or epochs\n   - Ensure warmup steps are appropriate for your dataset size\n\n3. **Knowledge Forgetting in Phase10**:\n   - Reduce Phase10's `unfreeze_rank_ratio` further (e.g., from 0.2 to 0.15)\n   - Consider using a lower learning rate in Phase10\n   - Verify you're loading the correct Phase07 checkpoint\n\n4. **Checkpoint Not Found**:\n   - Verify Phase07 completed successfully\n   - Check `ckpt_output_dir` permissions\n   - Look for error messages in training logs\n   - Ensure sufficient disk space\n\n5. **Distributed Training Issues**:\n   - Verify network connectivity between nodes\n   - Check `rdzv_endpoint` accessibility\n   - Ensure consistent environment across nodes\n   - Try with single node first to isolate issues\n\n6. **Data Loading Errors**:\n   - Verify JSONL format (each line must be valid JSON with 'messages' field)\n   - Check file paths and permissions\n   - Ensure sufficient disk space in `/dev/shm`\n   - Validate no corrupted entries in data\n\n### OSFT Multi-Phase Specific Tips:\n\n- **Finding Optimal Unfreeze Ratios**: Start with 0.3 for Phase07, reduce by 0.1 for Phase10. Adjust based on your preservation needs.\n- **Balancing Learning vs Preservation**: Higher ratios = more learning, lower ratios = more preservation\n- **Multi-Phase Beyond Two**: For 3+ phases, continue reducing ratio by 0.05-0.1 per phase, but don't go below 0.1\n- **Comparing with LAB**: Expect similar or better results than LAB multi-phase training, but with much simpler setup"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}