# ChukLLM Configuration File
# ==========================
#
# Provider Configuration Options:
# -------------------------------
# Each provider can be configured with:
#
# Required:
#   client_class: The Python class path for the LLM client
#   api_key_env: Environment variable name for API key
#
# Optional:
#   api_base: Base URL for the API endpoint
#   api_base_env: Custom environment variable for base URL (in extra section)
#   default_model: Default model to use
#   models: List of available models
#   features: List of supported features
#   max_context_length: Maximum context window size
#   max_output_tokens: Maximum output token limit
#
# Environment Variable Support:
# -----------------------------
# Base URLs can be overridden using environment variables:
# 1. Custom env var (set api_base_env in extra config)
# 2. Standard patterns: {PROVIDER}_API_BASE, {PROVIDER}_BASE_URL, etc.
# 3. Configured api_base value (lowest priority)
#
# Example:
#   export OPENAI_API_BASE=https://proxy.company.com/v1
#   export CUSTOM_LLM_ENDPOINT=https://api.custom.com/v1
#
# Dynamic Provider Registration:
# ------------------------------
# Providers can also be registered at runtime using the API:
#   from chuk_llm import register_openai_compatible
#   register_openai_compatible(
#       name="custom_provider",
#       api_base_env="CUSTOM_ENDPOINT",  # Uses env var for URL
#       api_key_env="CUSTOM_KEY",
#       models=["model1", "model2"]
#   )

__discovery_defaults__:
  feature_dependencies:
    json_mode:
      requires:
      - text
    parallel_calls:
      requires:
      - tools
    vision:
      requires:
      - multimodal
  parameter_patterns:
  - (?:(\d+(?:\.\d+)?)b)
  - (?:(\d+(?:\.\d+)?)B)
  - (?:(\d+)\s*billion)
  universal_patterns:
    code_models:
      add_features:
      - system_messages
      patterns:
      - .*code.*
      - starcoder
      - codellama
      - deepseek-coder
    embedding_models:
      context_length: 512
      family: embedding
      patterns:
      - .*embed.*
      - .*embedding.*
      remove_features:
      - text
      - streaming
      - tools
      - vision
      - reasoning
    instruct_models:
      add_features:
      - system_messages
      patterns:
      - .*instruct.*
      - .*chat.*
    reasoning_models:
      add_features:
      - reasoning
      patterns:
      - .*reasoning.*
      - o[1-5].*
      - magistral.*
      - gpt-oss.*
      - gpt-5.*
    vision_models:
      add_features:
      - vision
      - multimodal
      patterns:
      - .*vision.*
      - .*multimodal.*
      - .*vlm.*
  universal_size_rules:
    huge_model:
      add_features:
      - parallel_calls
      - reasoning
      - json_mode
      min_size_bytes: 50000000000
    large_model:
      add_features:
      - reasoning
      min_size_bytes: 1000000000
    very_large_model:
      add_features:
      - parallel_calls
      min_size_bytes: 10000000000
__global__:
  active_model: gpt-5
  active_provider: openai
  default_timeout: 30
  dynamic_discovery:
    auto_update_on_startup: false
    cache_timeout: 300
    enabled: true
    preferred_discoverers:
    - ollama
    - openai
  max_retries: 3
__global_aliases__:
  claude: anthropic/claude-sonnet-4-20250514
  deepseek: deepseek/deepseek-chat
  deepseek_reasoning: deepseek/deepseek-reasoner
  gemini: gemini/gemini-2.0-flash
  gemini_pro: gemini/gemini-1.5-pro
  gpt: openai/gpt-5
  gpt_mini: openai/gpt-5-mini
  gpt_nano: openai/gpt-5-nano
  gptoss: ollama/gpt-oss
  granite: ollama/granite3.3
  llama: groq/llama-3.3-70b-versatile
  mistral: mistral/magistral-medium-2506
  mixtral: groq/mixtral-8x7b-32768
  opus: anthropic/claude-opus-4-1-20250805
  phi: ollama/phi3
  qwen: ollama/qwen3
  reasoning: openai/gpt-5
  reasoning_advanced: deepseek/deepseek-reasoner
  reasoning_free: mistral/magistral-small-2506
  sonnet: anthropic/claude-sonnet-4-20250514
  watsonx: watsonx/ibm/granite-3-3-8b-instruct
  azure_gpt: azure_openai/gpt-5
  azure_vision: azure_openai/gpt-5
  azure_reasoning: azure_openai/o3-mini
  azure_turbo: azure_openai/gpt-4-turbo
  azure_latest: azure_openai/gpt-5
anthropic:
  api_key_env: ANTHROPIC_API_KEY
  client_class: chuk_llm.llm.providers.anthropic_client.AnthropicLLMClient
  default_model: claude-opus-4-1-20250805
  # Base URL defaults to Anthropic's API, override with ANTHROPIC_API_BASE
  extra:
    # Environment variable for base URL
    api_base_env: ANTHROPIC_API_BASE
    dynamic_discovery:
      discoverer_type: anthropic
      enabled: true
      inference_config:
        default_context_length: 200000
        default_features:
        - text
        - streaming
        - system_messages
        - reasoning
        family_rules:
          claude3:
            base_context_length: 200000
            base_max_output_tokens: 4096
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - json_mode
            patterns:
            - claude-3-
          claude3_5:
            base_context_length: 200000
            base_max_output_tokens: 4096
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - json_mode
            patterns:
            - claude-3-5-
          claude3_7:
            base_context_length: 200000
            base_max_output_tokens: 8192
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - reasoning
            - json_mode
            patterns:
            - claude-3-7-
          claude4:
            base_context_length: 200000
            base_max_output_tokens: 32000
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - reasoning
            - json_mode
            patterns:
            - claude-.*-4-
          claude4_1:
            base_context_length: 200000
            base_max_output_tokens: 32000
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - reasoning
            - json_mode
            patterns:
            - claude-.*-4-1-
  features:
  - text
  - streaming
  - system_messages
  - reasoning
  max_context_length: 200000
  max_output_tokens: 32000
  model_aliases:
    haiku: claude-3-5-haiku-20241022
    haiku35: claude-3-5-sonnet-20241022
    haiku37: claude-3-5-haiku-20241022
    latest: claude-opus-4-1-20250805
    opus: claude-opus-4-1-20250805
    opus4: claude-opus-4-20250514
    opus41: claude-opus-4-1-20250805
    reasoning: claude-opus-4-1-20250805
    sonnet: claude-sonnet-4-20250514
    sonnet35: claude-3-5-sonnet-20241022
    sonnet37: claude-3-7-sonnet-20250219
    sonnet4: claude-sonnet-4-20250514
    thinking: claude-opus-4-1-20250805
  model_capabilities:
  - features:
    - text
    - streaming
    - tools
    - vision
    - parallel_calls
    - reasoning
    - json_mode
    max_context_length: 200000
    max_output_tokens: 32000
    pattern: claude-opus-4-1-.*
  - features:
    - text
    - streaming
    - tools
    - vision
    - parallel_calls
    - reasoning
    - json_mode
    max_context_length: 200000
    max_output_tokens: 32000
    pattern: claude-(opus|sonnet)-4-.*
  - features:
    - text
    - streaming
    - tools
    - vision
    - parallel_calls
    - reasoning
    - json_mode
    max_context_length: 200000
    max_output_tokens: 8192
    pattern: claude-3-7-.*
  - features:
    - text
    - streaming
    - vision
    - tools
    - parallel_calls
    - json_mode
    max_context_length: 200000
    max_output_tokens: 4096
    pattern: claude-3-5-.*
  - features:
    - text
    - streaming
    - vision
    - tools
    - parallel_calls
    - json_mode
    max_context_length: 200000
    max_output_tokens: 4096
    pattern: claude-3-.*
  models:
  - claude-opus-4-1-20250805
  - claude-opus-4-20250514
  - claude-sonnet-4-20250514
  - claude-3-7-sonnet-20250219
  - claude-3-5-sonnet-20241022
  - claude-3-5-haiku-20241022
  - claude-3-sonnet-20240229
  - claude-3-opus-20240229
  - claude-3-haiku-20240307
  rate_limits:
    default: 4000
deepseek:
  api_base: https://api.deepseek.com  # Default fallback
  api_key_env: DEEPSEEK_API_KEY
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  default_model: deepseek-reasoner
  extra:
    # Environment variable for base URL
    api_base_env: DEEPSEEK_API_BASE
    dynamic_discovery:
      discoverer_type: deepseek
      enabled: true
      inference_config:
        default_context_length: 65536
        default_features:
        - text
        - streaming
        - json_mode
        - system_messages
        family_rules:
          chat_models:
            features:
            - text
            - streaming
            - json_mode
            - system_messages
            patterns:
            - .*chat.*
          reasoning_models:
            features:
            - text
            - streaming
            - json_mode
            - reasoning
            - system_messages
            patterns:
            - .*reasoner.*
            - .*reasoning.*
  features:
  - text
  - streaming
  - json_mode
  - system_messages
  inherits: openai
  max_context_length: 65536
  max_output_tokens: 8192
  model_aliases:
    chat: deepseek-chat
    default: deepseek-reasoner
    reasoner: deepseek-reasoner
    reasoning: deepseek-reasoner
    thinking: deepseek-reasoner
  model_capabilities:
  - features:
    - text
    - streaming
    - json_mode
    max_context_length: 65536
    max_output_tokens: 8192
    pattern: deepseek-chat
  - features:
    - text
    - streaming
    - json_mode
    - reasoning
    max_context_length: 65536
    max_output_tokens: 8192
    pattern: deepseek-reasoner
  models:
  - deepseek-chat
  - deepseek-reasoner
  rate_limits:
    default: 3000
gemini:
  api_key_env: GEMINI_API_KEY
  client_class: chuk_llm.llm.providers.gemini_client.GeminiLLMClient
  default_model: gemini-2.5-flash
  # Base URL defaults to Google's API, override with GEMINI_API_BASE
  extra:
    # Environment variable for base URL
    api_base_env: GEMINI_API_BASE
    dynamic_discovery:
      discoverer_type: gemini
      enabled: true
      inference_config:
        default_context_length: 1000000
        default_features:
        - text
        - streaming
        - tools
        - parallel_calls
        - vision
        - json_mode
        - system_messages
        - reasoning
        family_rules:
          gemini1_5:
            base_context_length: 1000000
            base_max_output_tokens: 8192
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - json_mode
            - system_messages
            patterns:
            - gemini-1.5-
          gemini2_0:
            base_context_length: 1000000
            base_max_output_tokens: 8192
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - json_mode
            - system_messages
            - reasoning
            patterns:
            - gemini-2.0-
          gemini2_5:
            base_context_length: 2000000
            base_max_output_tokens: 64000
            features:
            - text
            - streaming
            - tools
            - vision
            - parallel_calls
            - json_mode
            - system_messages
            - reasoning
            patterns:
            - gemini-2.5-
  features:
  - text
  - streaming
  - tools
  - parallel_calls
  - vision
  - json_mode
  - system_messages
  - reasoning
  max_context_length: 2000000
  max_output_tokens: 64000
  model_aliases:
    flash: gemini-2.5-flash
    pro: gemini-2.5-pro
    latest: gemini-2.5-flash
    reasoning: gemini-2.5-pro
    thinking: gemini-2.5-pro
    vision: gemini-2.5-flash
    legacy_pro: gemini-1.5-pro
    legacy_flash: gemini-1.5-flash
    budget: gemini-1.5-flash-8b
    multimodal: gemini-2.5-flash
  model_capabilities:
  # Gemini 2.5 series - Enhanced reasoning and thinking capabilities
  - features:
    - text
    - streaming
    - tools
    - vision
    - parallel_calls
    - json_mode
    - system_messages
    - reasoning
    max_context_length: 2000000
    max_output_tokens: 64000
    pattern: gemini-2\.5-.*
  # Gemini 2.0 series - Next-gen features
  - features:
    - text
    - streaming
    - tools
    - vision
    - parallel_calls
    - json_mode
    - system_messages
    - reasoning
    max_context_length: 1000000
    max_output_tokens: 8192
    pattern: gemini-2\.0-.*
  # Gemini 1.5 series - Production ready
  - features:
    - text
    - streaming
    - vision
    - tools
    - parallel_calls
    - json_mode
    - system_messages
    max_context_length: 1000000
    max_output_tokens: 8192
    pattern: gemini-1\.5-pro.*
  - features:
    - text
    - streaming
    - vision
    - tools
    - parallel_calls
    - json_mode
    - system_messages
    max_context_length: 1000000
    max_output_tokens: 8192
    pattern: gemini-1\.5-flash$
  - features:
    - text
    - streaming
    - tools
    - parallel_calls
    - json_mode
    - system_messages
    max_context_length: 1000000
    max_output_tokens: 8192
    pattern: gemini-1\.5-flash-8b
  models:
  # Gemini 2.5 series (Latest - Enhanced reasoning)
  - gemini-2.5-pro
  - gemini-2.5-flash
  # Gemini 2.0 series (Next-gen features)
  - gemini-2.0-flash
  - gemini-2.0-flash-lite
  # Gemini 1.5 series (Production ready)
  - gemini-1.5-pro
  - gemini-1.5-flash
  - gemini-1.5-flash-8b
  rate_limits:
    default: 1500
    tier_1: 1000
    tier_2: 2000

groq:
  api_base: https://api.groq.com/openai/v1  # Default fallback
  api_key_env: GROQ_API_KEY
  client_class: chuk_llm.llm.providers.groq_client.GroqAILLMClient
  default_model: llama-3.3-70b-versatile
  
  # Enhanced features based on actual capabilities
  features:
    - text
    - streaming
    - tools
    - parallel_calls
    - system_messages
    - json_mode
  
  inherits: openai
  
  # Actual Groq limits (131k context!)
  max_context_length: 131072
  max_output_tokens: 32768
  
  # Model aliases for convenience
  model_aliases:
    # Primary aliases
    latest: llama-3.3-70b-versatile
    fast: llama-3.1-8b-instant
    powerful: llama-3.3-70b-versatile
    
    # Model family aliases
    llama: llama-3.3-70b-versatile
    llama_fast: llama-3.1-8b-instant
    llama_guard: meta-llama/llama-guard-4-12b
    
    # GPT-OSS aliases
    gpt_oss: openai/gpt-oss-120b
    gpt_oss_large: openai/gpt-oss-120b
    gpt_oss_small: openai/gpt-oss-20b
    reasoning: openai/gpt-oss-120b
    
    # Other models
    deepseek: deepseek-r1-distill-llama-70b
    qwen: qwen/qwen3-32b
    kimi: moonshotai/kimi-k2-instruct
    
    # System models
    compound: compound-beta
    compound_mini: compound-beta-mini
  
  # Production models (stable, recommended for production)
  models:
    - llama-3.3-70b-versatile
    - llama-3.1-8b-instant
  
  # Preview models (evaluation only, may be discontinued)
  preview_models:
    - deepseek-r1-distill-llama-70b
    - meta-llama/llama-4-maverick-17b-128e-instruct
    - meta-llama/llama-4-scout-17b-16e-instruct
    - moonshotai/kimi-k2-instruct
    - openai/gpt-oss-120b
    - openai/gpt-oss-20b
    - qwen/qwen3-32b
    - compound-beta
    - compound-beta-mini
  
  # Model-specific capabilities with actual Groq limits
  model_capabilities:
    # Llama 3.3 70B - Most capable
    - pattern: "llama-3\\.3-70b-versatile"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
        - parallel_calls
      max_context_length: 131072
      max_output_tokens: 32768
    
    # Llama 3.1 8B - Fast
    - pattern: "llama-3\\.1-8b-instant"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
      max_context_length: 131072
      max_output_tokens: 131072  # 8B model has huge output!
    
    # GPT-OSS models - Reasoning
    - pattern: "openai/gpt-oss-120b"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
        - reasoning
      max_context_length: 131072
      max_output_tokens: 32766
    
    - pattern: "openai/gpt-oss-20b"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
        - reasoning
      max_context_length: 131072
      max_output_tokens: 32768
    
    # DeepSeek reasoning
    - pattern: "deepseek-r1-distill-llama-70b"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
        - reasoning
      max_context_length: 131072
      max_output_tokens: 131072
    
    # Llama 4 models
    - pattern: "meta-llama/llama-4-.*"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
      max_context_length: 131072
      max_output_tokens: 8192
    
    # Qwen model
    - pattern: "qwen/qwen3-32b"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
        - reasoning
      max_context_length: 131072
      max_output_tokens: 40960  # Qwen has 40k output!
    
    # Kimi model
    - pattern: "moonshotai/kimi-.*"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - json_mode
      max_context_length: 131072
      max_output_tokens: 16384
    
    # Compound systems
    - pattern: "compound-.*"
      features:
        - text
        - streaming
        - tools
        - system_messages
        - reasoning
      max_context_length: 131072
      max_output_tokens: 8192
    
    # Default for unknown models (generous Groq defaults)
    - pattern: ".*"
      features:
        - text
        - streaming
        - tools
        - system_messages
      max_context_length: 131072  # Groq standard
      max_output_tokens: 32768
  
  # Rate limits
  rate_limits:
    default: 30
    tier_1: 14400  # Requests per day
    tier_2: 28800  # Requests per day
  
  # Extra configuration
  extra:
    # Environment variable for base URL
    api_base_env: GROQ_API_BASE
    # Performance characteristics
    ultra_fast_inference: true
    openai_compatible: true
    
    # Dynamic discovery
    dynamic_discovery:
      enabled: true
      discoverer_type: openai
      cache_timeout: 300
      
      # Inference configuration for discovery
      inference_config:
        default_context_length: 131072  # Groq standard
        default_features:
          - text
          - streaming
          - tools
          - system_messages
          - json_mode
        
        # Family rules for pattern matching
        family_rules:
          llama_series:
            base_context_length: 131072
            features:
              - text
              - streaming
              - tools
              - system_messages
              - json_mode
            patterns:
              - llama
          
          gpt_oss_series:
            base_context_length: 131072
            features:
              - text
              - streaming
              - tools
              - system_messages
              - json_mode
              - reasoning
            patterns:
              - gpt-oss
          
          deepseek_series:
            base_context_length: 131072
            features:
              - text
              - streaming
              - tools
              - system_messages
              - json_mode
              - reasoning
            patterns:
              - deepseek
          
          qwen_series:
            base_context_length: 131072
            features:
              - text
              - streaming
              - tools
              - system_messages
              - json_mode
              - reasoning
            patterns:
              - qwen
          
          whisper_series:
            features:
              - audio
              - transcription
            patterns:
              - whisper
          
          guard_series:
            base_context_length: 131072
            features:
              - text
              - streaming
              - system_messages
            patterns:
              - guard
          
          compound_series:
            base_context_length: 131072
            features:
              - text
              - streaming
              - tools
              - system_messages
              - reasoning
            patterns:
              - compound

mistral:
  api_key_env: MISTRAL_API_KEY
  client_class: chuk_llm.llm.providers.mistral_client.MistralLLMClient
  default_model: mistral-medium-2505
  # Base URL defaults to Mistral's API, override with MISTRAL_API_BASE
  extra:
    # Environment variable for base URL
    api_base_env: MISTRAL_API_BASE
    dynamic_discovery:
      discoverer_type: mistral
      enabled: false
      inference_config:
        default_context_length: 128000
        default_features:
        - text
        - streaming
        - system_messages
        - parallel_calls
        family_rules:
          codestral:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            patterns:
            - codestral
            - devstral
          magistral:
            base_context_length: 40960
            features:
            - text
            - streaming
            - reasoning
            patterns:
            - magistral
          mistral_large:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - parallel_calls
            - json_mode
            patterns:
            - mistral-large
            - mistral-medium
          pixtral:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - vision
            - multimodal
            - parallel_calls
            patterns:
            - pixtral
  features:
  - text
  - streaming
  - system_messages
  - parallel_calls
  max_context_length: 128000
  max_output_tokens: 8192
  model_aliases:
    code: codestral-2501
    code_open: devstral-small-2505
    codestral: codestral-2501
    coding: codestral-2501
    default: mistral-medium-2505
    devstral: devstral-small-2505
    large: mistral-large-2411
    latest: mistral-medium-2505
    magistral: magistral-medium-2506
    magistral_medium: magistral-medium-2506
    magistral_small: magistral-small-2506
    medium: mistral-medium-2505
    ministral: ministral-8b-2410
    multimodal: mistral-medium-2505
    pixtral: pixtral-large-2411
    pixtral_small: pixtral-12b-2409
    reasoning: magistral-medium-2506
    reasoning_small: magistral-small-2506
    saba: mistral-saba-2502
    small: mistral-small-2503
    thinking: magistral-medium-2506
    tools: mistral-medium-2505
    tools_vision: pixtral-large-2411
    vision: pixtral-large-2411
    vision_small: pixtral-12b-2409
  model_capabilities:
  - features:
    - text
    - streaming
    - reasoning
    max_context_length: 40960
    max_output_tokens: 8192
    pattern: magistral-.*
  - features:
    - text
    - streaming
    - tools
    - parallel_calls
    - json_mode
    max_context_length: 131072
    max_output_tokens: 8192
    pattern: mistral-large-2411
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    - parallel_calls
    - json_mode
    max_context_length: 131072
    max_output_tokens: 8192
    pattern: mistral-medium-2505
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    - json_mode
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: mistral-small-2503
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    - parallel_calls
    - reasoning
    max_context_length: 131072
    max_output_tokens: 8192
    pattern: pixtral-.*
  - features:
    - text
    - streaming
    - tools
    max_context_length: 262144
    max_output_tokens: 8192
    pattern: codestral-.*
  - features:
    - text
    - streaming
    - tools
    max_context_length: 131072
    max_output_tokens: 8192
    pattern: devstral-.*
  - features:
    - text
    - streaming
    - tools
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: ministral-.*
  - features:
    - text
    - streaming
    - tools
    max_context_length: 32768
    max_output_tokens: 4096
    pattern: mistral-saba-.*
  models:
  - magistral-medium-2506
  - magistral-small-2506
  - mistral-medium-2505
  - codestral-2501
  - mistral-saba-2502
  - mistral-large-2411
  - pixtral-large-2411
  - ministral-3b-2410
  - ministral-8b-2410
  - devstral-small-2505
  - mistral-small-2503
  - pixtral-12b-2409
  rate_limits:
    default: 1000
    premium: 5000
ollama:
  api_base: http://localhost:11434  # Default fallback
  client_class: chuk_llm.llm.providers.ollama_client.OllamaLLMClient
  default_model: granite3.3
  # Override with OLLAMA_API_BASE or OLLAMA_HOST environment variables
  extra:
    # Environment variable for base URL
    api_base_env: OLLAMA_API_BASE
    dynamic_discovery:
      auto_update_models: true
      cache_timeout: 300
      discoverer_type: ollama
      enabled: true
      inference_config:
        default_context_length: 8192
        default_features:
        - text
        - streaming
        - system_messages
        family_rules:
          # ADDED: GPT-OSS support - CRITICAL FIX
          gpt_oss:
            base_context_length: 32768
            features:
            - text
            - streaming
            - tools          # CRITICAL: Enable tools support
            - reasoning      # Mark as reasoning model
            - system_messages
            patterns:
            - gpt-oss
          
          code:
            base_context_length: 16384
            features:
            - text
            - streaming
            - tools
            - system_messages
            patterns:
            - codellama
            - starcoder
            - deepseek-coder
            - codegemma
            - devstral
          gemma:
            base_context_length: 8192
            features:
            - text
            - streaming
            - tools
            - system_messages
            patterns:
            - gemma
          granite:
            base_context_length: 8192
            features:
            - text
            - streaming
            - tools
            - reasoning
            - system_messages
            patterns:
            - granite
          llama:
            base_context_length: 8192
            context_rules:
              llama-?2: 4096
              llama-?3: 8192
              llama-?3\.1: 128000
              llama-?3\.[23]: 128000
            features:
            - text
            - streaming
            - system_messages
            - tools
            patterns:
            - llama
            - yi-coder
          mistral:
            base_context_length: 32768
            features:
            - text
            - streaming
            - tools
            - system_messages
            patterns:
            - mistral
            - mixtral
          phi:
            base_context_length: 4096
            context_rules:
              phi-?3: 128000
              phi4: 128000
            features:
            - text
            - streaming
            - system_messages
            - reasoning
            patterns:
            - phi
          qwen:
            base_context_length: 32768
            context_rules:
              qwen2: 32768
              qwen2\.5: 32768
              qwen3: 32768
            features:
            - text
            - streaming
            - tools
            - reasoning
            - system_messages
            patterns:
            - qwen
            - codeqwen
          reasoning:
            base_context_length: 32768
            features:
            - text
            - streaming
            - reasoning
            - system_messages
            patterns:
            - .*reasoning.*
            - .*phi4.*
            - .*qwq.*
            - .*marco-o1.*
          vision:
            base_context_length: 8192
            features:
            - text
            - streaming
            - vision
            - multimodal
            - system_messages
            patterns:
            - .*vision.*
            - .*llava.*
            - .*moondream.*
        pattern_rules:
          # ADDED: GPT-OSS pattern rule - CRITICAL FIX
          gpt_oss_capable:
            add_features:
            - tools
            - reasoning
            patterns:
            - .*gpt-oss.*
          
          instruct_tuned:
            add_features:
            - system_messages
            patterns:
            - .*instruct.*
            - .*chat.*
          large_context:
            context_length: 128000
            patterns:
            - .*phi4.*
            - .*llama.*3\.[23].*
          reasoning_capable:
            add_features:
            - reasoning
            patterns:
            - .*qwen.*
            - .*granite.*
            - .*llama-?3\.[12].*
            - .*phi4.*
            - .*reasoning.*
            - .*gpt-oss.*  # ADDED: GPT-OSS to reasoning capable
          tool_capable:
            add_features:
            - tools
            patterns:
            - .*qwen.*
            - .*gemma.*
            - .*mistral.*
            - .*granite.*
            - .*devstral.*
            - .*gpt-oss.*  # ADDED: GPT-OSS to tool capable patterns - CRITICAL FIX
          vision_capable:
            add_features:
            - vision
            - multimodal
            patterns:
            - .*vision.*
            - .*llava.*
        size_rules:
          large_model:
            add_features:
            - reasoning
            min_size_bytes: 10000000000
          very_large_model:
            add_features:
            - parallel_calls
            - reasoning
            min_size_bytes: 50000000000
  features:
  - text
  - streaming
  - system_messages
  max_context_length: 8192
  max_output_tokens: 4096
  model_aliases:
    code: codellama
    creative: llama3.3
    default: llama3.3
    fast: llama3.3
    granite: granite3.3
    gpt_oss: gpt-oss:latest  # ADDED: GPT-OSS alias
    latest: llama3.3
    llama: llama3.3
    mistral_local: mistral
    phi: phi3
    programming: codellama
    qwen: qwen3
    reasoning: llama3.3
    reasoning_local: gpt-oss:latest  # ADDED: Local reasoning alias
    smart: llama3.3
  model_capabilities:
    # ADDED: GPT-OSS model capabilities - CRITICAL FIX
    - features:
      - text
      - streaming
      - tools       # CRITICAL: Tools support enabled
      - reasoning   # Reasoning model
      max_context_length: 32768
      max_output_tokens: 8192
      pattern: gpt-oss.*
    
    - features:
      - text
      - streaming
      - tools
      max_context_length: 8192
      max_output_tokens: 4096
      pattern: gemma.*
    - features:
      - text
      - streaming
      - tools
      max_context_length: 32768
      max_output_tokens: 8192
      pattern: llama3\.[23].*
    - features:
      - text
      - streaming
      - tools
      max_context_length: 32768
      max_output_tokens: 8192
      pattern: mistral.*
    - features:
      - text
      - streaming
      - tools
      - reasoning
      max_context_length: 32768
      max_output_tokens: 8192
      pattern: qwen.*
    - features:
      - text
      - streaming
      max_context_length: 16384
      max_output_tokens: 8192
      pattern: .*codellama.*|.*code.*
    - features:
      - text
      - streaming
      max_context_length: 4096
      max_output_tokens: 2048
      pattern: phi.*
    - features:
      - text
      - streaming
      - tools
      - vision
      - multimodal
      max_context_length: 8192
      max_output_tokens: 4096
      pattern: llama3\.2.*vision.*
    - features: []
      max_context_length: 512
      pattern: .*embed.*|.*embedding.*
    - features:
      - text
      - streaming
      - tools
      - reasoning
      max_context_length: 8192
      max_output_tokens: 4096
      pattern: granite.*
  models:
  - llama3.3
  - qwen3
  - granite3.3
  - mistral
  - gemma3
  - phi3
  - codellama
  - gpt-oss:latest  # ADDED: GPT-OSS to available models
  - gpt-oss:20b     # ADDED: Other GPT-OSS variants
  - gpt-oss:120b
  rate_limits: {}
openai:
  # Updated configuration for GPT-5 and enhanced model support
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: OPENAI_API_KEY
  default_model: gpt-5
  
  # Base URL can be overridden by environment variables:
  # - OPENAI_API_BASE (highest priority)
  # - OPENAI_BASE_URL
  # - OPENAI_API_URL
  # - OPENAI_ENDPOINT
  # Or set a custom env var using api_base_env in extra config
  
  # Enhanced features including unified reasoning
  features:
    - text
    - streaming
    - system_messages
    - json_mode
    - reasoning
    - tools
    - vision
    - parallel_calls
  
  max_context_length: 272000  # Updated for GPT-5
  max_output_tokens: 128000   # Updated for GPT-5
  
  # Extra configuration
  extra:
    # Environment variable for base URL (standard patterns work automatically)
    api_base_env: OPENAI_API_BASE
  
  # Updated model list with GPT-5 family and GPT-OSS
  models:
    # GPT-5 series (Latest - Unified reasoning models)
    - gpt-5
    - gpt-5-mini
    - gpt-5-nano
    # gpt-5-chat  # Not yet available in API
    
    # GPT-OSS series (Open source models) - Currently available via Hugging Face/local only
    # - gpt-oss-120b  # Not yet in OpenAI API
    # - gpt-oss-20b   # Not yet in OpenAI API
    
    # Standard GPT models (confirmed available)
    - gpt-4.1
    - gpt-4.1-mini
    - gpt-4.1-nano
    - gpt-4o
    - gpt-4o-mini
    - gpt-4-turbo
    - gpt-4
    - gpt-3.5-turbo
    
    # Reasoning models (with tier requirements)
    - o1-mini          # Available to most API users (Tier 1+)
    - o1               # Requires Tier 5 ($1000+ spent, 30+ days)
    - o1-2024-12-17    # Requires Tier 5
    - o3-mini          # Requires Tier 3+ (moderate usage)

  # Enhanced model capabilities with GPT-5 and GPT-OSS support
  model_capabilities:
    # GPT-5 series - Unified reasoning models
    - pattern: "gpt-5(-chat|-mini|-nano)?$"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - reasoning
        - json_mode
        - system_messages
      max_context_length: 272000
      max_output_tokens: 128000
      special_handling:
        parameter_mapping:
          max_tokens: max_completion_tokens
        verbosity_parameter: true
        reasoning_effort_parameter: true
        unified_reasoning: true
    
    # GPT-OSS series - Open source reasoning models
    - pattern: "gpt-oss-(120b|20b)"
      features:
        - text
        - streaming
        - tools
        - reasoning
        - system_messages
      max_context_length: 272000
      max_output_tokens: 128000
      special_handling:
        open_source: true
        apache_license: true
        reasoning_effort_levels: ["low", "medium", "high"]
        chain_of_thought_access: true
    
    # O3 reasoning models
    - pattern: "o3.*"
      features:
        - text
        - streaming  # o3 supports streaming
        - tools
        - reasoning
        - system_messages  # o3 supports system messages
      max_context_length: 200000
      max_output_tokens: 100000
      special_handling:
        parameter_mapping:
          max_tokens: max_completion_tokens
        restrictions:
          unsupported_params: [temperature, top_p, frequency_penalty, presence_penalty]
    
    # O1 reasoning models (FIXED with proper parameters)
    - pattern: "o1.*"
      features:
        - text
        - tools
        - reasoning
        # Note: o1 models don't support streaming or system messages
      max_context_length: 200000
      max_output_tokens: 100000
      special_handling:
        parameter_mapping:
          max_tokens: max_completion_tokens
        restrictions:
          no_streaming: true
          no_system_messages: true
          unsupported_params: [temperature, top_p, frequency_penalty, presence_penalty]
    
    # GPT-4.1 series (existing, working)
    - pattern: "gpt-4\\.1.*"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - json_mode
        - system_messages
      max_context_length: 128000
      max_output_tokens: 8192
    
    # GPT-4o series (existing, working)
    - pattern: "gpt-4o.*"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - json_mode
        - system_messages
      max_context_length: 128000
      max_output_tokens: 8192
    
    # GPT-4 Turbo and regular (existing, working)
    - pattern: "gpt-4(-turbo|-preview)?$"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - json_mode
        - system_messages
      max_context_length: 128000
      max_output_tokens: 8192
    
    # GPT-3.5 Turbo (existing, working)
    - pattern: "gpt-3\\.5-turbo.*"
      features:
        - text
        - streaming
        - tools
        - json_mode
        - system_messages
      max_context_length: 16384
      max_output_tokens: 4096

  # Enhanced model aliases including GPT-5 and GPT-OSS
  model_aliases:
    # GPT-5 aliases (primary models)
    latest: gpt-5
    gpt5: gpt-5
    mini: gpt-5-mini
    nano: gpt-5-nano
    #chat: gpt-5-chat
    
    # Standard aliases
    turbo: gpt-4-turbo
    
    # Reasoning model aliases
    reasoning: gpt-5         # Primary unified reasoning
    reasoning_mini: gpt-5-mini # GPT-5 mini with unified reasoning
    reasoning_nano: gpt-5-nano # Ultra low latency reasoning
    reasoning_full: o1       # Requires Tier 5
    reasoning_latest: o3-mini # Requires Tier 3+
    
    # GPT-OSS aliases (when available)
    # oss: gpt-oss-120b
    # oss_large: gpt-oss-120b  
    # oss_small: gpt-oss-20b
    # open_source: gpt-oss-120b
    
    # Specific model aliases
    o1_mini: o1-mini
    o1_standard: o1
    o1_latest: o1-2024-12-17
    o3_mini: o3-mini

  # Enhanced rate limits
  rate_limits:
    default: 3500
    tier_1: 500
    tier_5: 10000  # Higher limits for reasoning model users
    gpt_5: 5000    # Special limits for GPT-5

  # Enhanced dynamic discovery for GPT-5 and reasoning models
  extra:
    dynamic_discovery:
      cache_timeout: 600
      discoverer_type: openai
      enabled: true
      inference_config:
        default_context_length: 272000  # Updated for GPT-5
        default_features:
          - text
          - streaming
          - system_messages
          - json_mode
          - reasoning
          - tools
          - vision
          - parallel_calls
        default_max_output_tokens: 128000  # Updated for GPT-5
        
        # Model-specific overrides for new models
        model_overrides:
          # GPT-5 series overrides
          gpt-5:
            features: [text, streaming, tools, vision, parallel_calls, reasoning, json_mode, system_messages]
            context_length: 272000
            max_output_tokens: 128000
            family: gpt5_unified
            unified_reasoning: true
            verbosity_parameter: true
            reasoning_effort_parameter: true
          
          gpt-5-mini:
            features: [text, streaming, tools, vision, parallel_calls, reasoning, json_mode, system_messages]
            context_length: 272000
            max_output_tokens: 128000
            family: gpt5_unified
            cost_optimized: true
          
          gpt-5-nano:
            features: [text, streaming, tools, reasoning, json_mode, system_messages]
            context_length: 272000
            max_output_tokens: 128000
            family: gpt5_unified
            ultra_low_latency: true
          
          gpt-5-chat:
            features: [text, streaming, tools, vision, parallel_calls, reasoning, json_mode, system_messages]
            context_length: 272000
            max_output_tokens: 128000
            family: gpt5_unified
            conversation_optimized: true
          
          # GPT-OSS series overrides
          gpt-oss-120b:
            features: [text, streaming, tools, reasoning, system_messages]
            context_length: 272000
            max_output_tokens: 128000
            family: gpt_oss
            open_source: true
            apache_license: true
            reasoning_effort_levels: ["low", "medium", "high"]
            chain_of_thought_access: true
            
          gpt-oss-20b:
            features: [text, streaming, tools, reasoning, system_messages]
            context_length: 272000
            max_output_tokens: 128000
            family: gpt_oss
            open_source: true
            apache_license: true
            edge_optimized: true
          
          # O1 series overrides (existing)
          o1-mini:
            features: [text, tools, reasoning]
            context_length: 128000
            max_output_tokens: 65536
            family: o1_reasoning
            parameter_mapping:
              max_tokens: max_completion_tokens
            restrictions:
              no_streaming: true
              no_system_messages: true
              unsupported_params: [temperature, top_p, frequency_penalty, presence_penalty]
          
          o1:
            features: [text, tools, reasoning] 
            context_length: 200000
            max_output_tokens: 100000
            family: o1_reasoning
            parameter_mapping:
              max_tokens: max_completion_tokens
            restrictions:
              no_streaming: true
              no_system_messages: true
              unsupported_params: [temperature, top_p, frequency_penalty, presence_penalty]
          
          "o1-2024-12-17":
            features: [text, tools, reasoning] 
            context_length: 200000
            max_output_tokens: 100000
            family: o1_reasoning
            parameter_mapping:
              max_tokens: max_completion_tokens
            restrictions:
              no_streaming: true
              no_system_messages: true
              unsupported_params: [temperature, top_p, frequency_penalty, presence_penalty]
          
          # O3 series overrides
          o3-mini:
            features: [text, streaming, tools, reasoning, system_messages]
            context_length: 200000
            max_output_tokens: 65536
            family: o3_reasoning
            parameter_mapping:
              max_tokens: max_completion_tokens
            restrictions:
              unsupported_params: [temperature, top_p, frequency_penalty, presence_penalty]
        
        # Enhanced family rules for new models
        family_rules:
          gpt5_unified:
            base_context_length: 272000
            base_max_output_tokens: 128000
            features:
              - text
              - streaming
              - tools
              - vision
              - parallel_calls
              - reasoning
              - json_mode
              - system_messages
            unified_reasoning: true
            patterns:
              - gpt-5
              - gpt-5-mini
              - gpt-5-nano
              - gpt-5-chat
          
          gpt_oss:
            base_context_length: 272000
            base_max_output_tokens: 128000
            features:
              - text
              - streaming
              - tools
              - reasoning
              - system_messages
            open_source: true
            apache_license: true
            patterns:
              - gpt-oss-120b
              - gpt-oss-20b
          
          o1_series:
            base_context_length: 200000
            base_max_output_tokens: 65536
            features:
              - text
              - tools
              - reasoning
            restrictions:
              no_streaming: true
              no_system_messages: true
              parameter_mapping:
                max_tokens: max_completion_tokens
            patterns:
              - o1
              - o1-mini
              - o1-.*
          
          o3_series:
            base_context_length: 200000
            base_max_output_tokens: 65536
            features:
              - text
              - streaming
              - tools
              - reasoning
              - system_messages
            restrictions:
              parameter_mapping:
                max_tokens: max_completion_tokens
            patterns:
              - o3
              - o3-mini
              - o3-.*

litellm:
  # LiteLLM Gateway - Universal LLM Gateway
  # https://github.com/BerriAI/litellm
  # 
  # LiteLLM provides a unified interface to 100+ LLM providers
  # Set up your gateway and configure:
  #   LITELLM_API_BASE=http://localhost:4000  # Or your gateway URL
  #   LITELLM_API_KEY=your-litellm-key
  #
  # Models are prefixed by provider: openai/gpt-4, anthropic/claude-3, etc.
  #
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: LITELLM_API_KEY
  api_key_fallback_env: OPENAI_API_KEY
  api_base: http://localhost:4000  # Default fallback
  
  # Common models accessible via LiteLLM
  models:
    - gpt-3.5-turbo
    - gpt-4
    - claude-3-opus
    - claude-3-sonnet
    - gemini-pro
    - mistral-medium
    - "*"  # Accept any model/provider combination
  
  default_model: gpt-3.5-turbo
  
  features:
    - text
    - streaming
    - system_messages
    - tools
    - json_mode
    - vision
    - parallel_calls
  
  max_context_length: 128000
  max_output_tokens: 4096
  
  extra:
    # Environment variable for base URL
    api_base_env: LITELLM_API_BASE
    # LiteLLM supports model fallbacks and load balancing
    supports_fallbacks: true
    supports_caching: true
    supports_load_balancing: true

openrouter:
  # OpenRouter - Unified API for LLMs
  # https://openrouter.ai/docs/quickstart
  #
  # Get your API key from https://openrouter.ai/keys
  # Set: OPENROUTER_API_KEY=sk-or-...
  #
  # Models use provider prefixes: openai/gpt-4, anthropic/claude-3-opus, etc.
  #
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_base: https://openrouter.ai/api/v1  # Default fallback
  api_key_env: OPENROUTER_API_KEY
  
  # Popular models on OpenRouter
  models:
    - openai/gpt-3.5-turbo
    - openai/gpt-4
    - openai/gpt-4-turbo
    - anthropic/claude-3-opus
    - anthropic/claude-3-sonnet
    - anthropic/claude-3-haiku
    - google/gemini-pro
    - google/gemini-pro-vision
    - meta-llama/llama-3-70b-instruct
    - mistralai/mistral-large
    - deepseek/deepseek-chat
    - "*"  # OpenRouter has 100+ models
  
  default_model: openai/gpt-3.5-turbo
  
  features:
    - text
    - streaming
    - system_messages
    - tools
    - json_mode
    - vision
    - parallel_calls
  
  max_context_length: 200000  # Varies by model
  max_output_tokens: 4096
  
  extra:
    # Environment variable for base URL
    api_base_env: OPENROUTER_API_BASE
    # OpenRouter specific headers
    # You can set these in your code:
    # extra_headers = {
    #   "HTTP-Referer": "https://yourapp.com",
    #   "X-Title": "Your App Name"
    # }
    supports_usage_tracking: true
    supports_cost_tracking: true

vllm:
  # vLLM - High-performance OpenAI-compatible inference
  # https://docs.vllm.ai/en/latest/getting_started/quickstart.html
  #
  # Start vLLM server:
  #   python -m vllm.entrypoints.openai.api_server \
  #     --model meta-llama/Llama-3-70b-hf \
  #     --port 8000
  #
  # Then set: VLLM_API_BASE=http://localhost:8000/v1
  #
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: VLLM_API_KEY
  # Most vLLM deployments don't require API keys
  api_base: http://localhost:8000/v1  # Default fallback
  
  # Models depend on what you're serving
  models:
    - meta-llama/Llama-3-8b-hf
    - meta-llama/Llama-3-70b-hf
    - mistralai/Mistral-7B-Instruct-v0.2
    - mistralai/Mixtral-8x7B-Instruct-v0.1
    - deepseek-ai/deepseek-llm-67b-base
    - "*"  # Accept any model served by vLLM
  
  default_model: meta-llama/Llama-3-8b-hf
  
  features:
    - text
    - streaming
    - system_messages
    - tools
    - json_mode
  
  # vLLM supports very long contexts
  max_context_length: 128000
  max_output_tokens: 4096
  
  extra:
    # Environment variable for base URL
    api_base_env: VLLM_API_BASE
    # vLLM specific features
    supports_tensor_parallel: true
    supports_pipeline_parallel: true
    supports_continuous_batching: true
    supports_prefix_caching: true

togetherai:
  # Together AI - Scalable AI inference
  # https://www.together.ai/models
  #
  # Get API key from https://api.together.xyz/settings/api-keys
  # Set: TOGETHER_API_KEY=...
  # Set: TOGETHERAI_API_BASE=https://api.together.xyz/v1 (optional override)
  #
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_base: https://api.together.xyz/v1  # Default fallback
  api_key_env: TOGETHER_API_KEY
  api_key_fallback_env: TOGETHERAI_API_KEY
  
  # Together AI's top models
  models:
    # DeepSeek models
    - deepseek-ai/deepseek-v3
    - deepseek-ai/DeepSeek-V2.5-1210
    - deepseek-ai/deepseek-coder-33b-instruct
    
    # Llama models
    - meta-llama/Llama-3.3-70B-Instruct-Turbo
    - meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo
    - meta-llama/Llama-3-70b-chat-hf
    - meta-llama/Llama-3-8b-chat-hf
    
    # Mixtral models
    - mistralai/Mixtral-8x7B-Instruct-v0.1
    - mistralai/Mixtral-8x22B-Instruct-v0.1
    
    # Qwen models
    - Qwen/Qwen2.5-72B-Instruct-Turbo
    - Qwen/QwQ-32B-Preview
    
    # Code models
    - deepseek-ai/deepseek-coder-33b-instruct
    - codellama/CodeLlama-70b-Instruct-hf
    
    # Vision models
    - meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo
    - meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo
    
    - "*"  # Together has 100+ models
  
  default_model: meta-llama/Llama-3.3-70B-Instruct-Turbo
  
  features:
    - text
    - streaming
    - system_messages
    - tools
    - json_mode
    - vision  # For vision models
  
  max_context_length: 128000  # Varies by model
  max_output_tokens: 4096
  
  extra:
    # Environment variable for base URL
    api_base_env: TOGETHERAI_API_BASE
    # Together AI offers very competitive pricing
    supports_function_calling: true
    supports_json_mode: true
    supports_streaming: true

openai_compatible:
  # Generic OpenAI-compatible provider configuration
  # Use this for any service that implements the OpenAI API
  # without modifying the main openai provider
  #
  # Configure via environment variables:
  #   OPENAI_COMPATIBLE_API_BASE=https://your-service.com/v1
  #   OPENAI_COMPATIBLE_API_KEY=your-api-key
  #   OPENAI_COMPATIBLE_MODEL=model-name (optional)
  #
  # Or use custom environment variables by setting in extra:
  #   extra:
  #     api_base_env: YOUR_CUSTOM_BASE_ENV
  #
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: OPENAI_COMPATIBLE_API_KEY
  # Base URL will be read from environment: OPENAI_COMPATIBLE_API_BASE
  # Falls back to api_key_fallback_env if primary not set
  api_key_fallback_env: OPENAI_API_KEY
  
  # Accept any model name - will be validated by the service
  models:
    - gpt-3.5-turbo     # Common models for convenience
    - gpt-4
    - gpt-4-turbo
    - text-embedding-ada-002
    - "*"               # Accept any model name
  
  default_model: gpt-3.5-turbo  # Safe default

  # Full OpenAI feature compatibility assumed

advantage:
  # Advantage API Provider
  # OpenAI-compatible API with enhanced function calling support
  #
  # Configure via environment variables:
  #   ADVANTAGE_API_KEY=your-api-key (required)
  #   ADVANTAGE_API_BASE=your-api-endpoint (required)
  #
  # You can also set in extra section for custom env var:
  #   extra:
  #     api_base_env: YOUR_CUSTOM_ENV_VAR
  #
  client_class: chuk_llm.llm.providers.advantage_client.AdvantageClient
  api_key_env: ADVANTAGE_API_KEY
  api_key_fallback_env: OPENAI_COMPATIBLE_API_KEY

  # API base URL must be set via environment variable ADVANTAGE_API_BASE
  # or can be set in extra section:
  extra:
    api_base_env: ADVANTAGE_API_BASE

  models:
    - global/gpt-5-chat
    - global/gpt-4o
    - global/gpt-4-turbo
    - "*"  # Accept any model name

  default_model: global/gpt-5-chat

  # Feature support
  features:
    - text
    - streaming
    - system_messages
    - tools  # Supported with workaround
    - json_mode
    - vision

  # Model limits
  max_context_length: 128000
  max_output_tokens: 4096
  
  rate_limits:
    requests_per_minute: 1000  # Default, adjust based on your endpoint
  
  # Extra configuration for environment variable support
  extra:
    # Environment variable for base URL (automatically checked)
    api_base_env: OPENAI_COMPATIBLE_API_BASE
    
    # Service examples that work with this config:
    # - LocalAI: http://localhost:8080/v1
    # - FastChat: http://localhost:8000/v1
    # - vLLM: http://gpu-server:8000/v1
    # - Perplexity: https://api.perplexity.ai
    # - Together AI: https://api.together.xyz/v1
    # - Anyscale: https://api.endpoints.anyscale.com/v1
    # - LM Studio: http://localhost:1234/v1
    # - Ollama (OpenAI mode): http://localhost:11434/v1
  
  # Model capabilities - assume full OpenAI compatibility
  model_capabilities:
    - pattern: ".*"  # Match any model
      features:
        - text
        - streaming
        - tools
        - vision
        - json_mode
        - system_messages
        - parallel_calls
        - multimodal
      max_context_length: 128000  # Adjust based on your endpoint
      max_output_tokens: 4096     # Adjust based on your endpoint
perplexity:
  api_base: https://api.perplexity.ai  # Default fallback
  api_key_env: PERPLEXITY_API_KEY
  client_class: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  default_model: sonar-pro
  extra:
    # Environment variable for base URL
    api_base_env: PERPLEXITY_API_BASE
    beta_features:
      tier_0:
      - related_questions
      - structured_outputs
      tier_1_plus:
      - images
      - related_questions
      - search_domain_filter
      - structured_outputs
    dynamic_discovery:
      discoverer_type: perplexity
      enabled: true
      inference_config:
        default_context_length: 127072
        default_features:
        - text
        - streaming
        - json_mode
        - system_messages
        family_rules:
          offline_models:
            base_context_length: 127072
            features:
            - text
            - streaming
            - json_mode
            - system_messages
            patterns:
            - r1-
          research_models:
            base_context_length: 127072
            features:
            - text
            - streaming
            - json_mode
            - system_messages
            - reasoning
            patterns:
            - .*research.*
            - .*reasoning.*
          sonar_models:
            base_context_length: 127072
            features:
            - text
            - streaming
            - json_mode
            - system_messages
            - vision
            patterns:
            - sonar
    model_notes:
      r1-1776: Offline chat model - no search subsystem, no image search, no related
        questions, no domain filter
      sonar: Standard search with full beta features
      sonar-deep-research: Research-focused with lowest rate limits but increases
        with tier
      sonar-pro: Professional search with full beta features
      sonar-reasoning: Standard reasoning with full beta features
      sonar-reasoning-pro: Premium reasoning with full beta features
  features:
  - text
  - streaming
  - json_mode
  - system_messages
  - vision
  inherits: openai
  max_context_length: 127072
  max_output_tokens: 4096
  model_aliases:
    chat: r1-1776
    deep: sonar-deep-research
    default: sonar-pro
    latest: sonar-reasoning-pro
    offline: r1-1776
    pro: sonar-pro
    reasoning: sonar-reasoning
    reasoning_pro: sonar-reasoning-pro
    research: sonar-deep-research
    search: sonar-pro
    tier0: sonar-deep-research
    tier1_plus: sonar-pro
  model_capabilities:
  - features:
    - text
    - streaming
    - json_mode
    - system_messages
    max_context_length: 127072
    max_output_tokens: 4096
    pattern: sonar-deep-research
    rate_limits:
      tier_0: 5
      tier_1: 10
      tier_2: 15
      tier_3: 20
      tier_4: 30
      tier_5: 50
  - features:
    - text
    - streaming
    - json_mode
    - system_messages
    - vision
    max_context_length: 127072
    max_output_tokens: 4096
    pattern: sonar-reasoning.*
  - features:
    - text
    - streaming
    - json_mode
    - system_messages
    - vision
    max_context_length: 127072
    max_output_tokens: 4096
    pattern: sonar-pro|sonar$
  - features:
    - text
    - streaming
    - json_mode
    - system_messages
    max_context_length: 127072
    max_output_tokens: 4096
    pattern: r1-1776
  models:
  - sonar-deep-research
  - sonar-reasoning-pro
  - sonar-reasoning
  - sonar-pro
  - sonar
  - r1-1776
  rate_limits:
    default: 50
    tier_0: 5
    tier_1: 50
    tier_2: 50
    tier_3: 50
    tier_4: 50
    tier_5: 50
watsonx:
  api_key_env: WATSONX_API_KEY
  api_key_fallback_env: IBM_CLOUD_API_KEY
  client_class: chuk_llm.llm.providers.watsonx_client.WatsonXLLMClient
  default_model: ibm/granite-3-3-8b-instruct
  # Base URL defaults to IBM Cloud, override with WATSONX_API_BASE
  extra:
    # Environment variable for base URL
    api_base_env: WATSONX_API_BASE
    dynamic_discovery:
      discoverer_type: watsonx
      enabled: false
      inference_config:
        default_context_length: 131072
        default_features:
        - text
        - streaming
        - system_messages
        - reasoning
        family_rules:
          granite:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - reasoning
            - system_messages
            patterns:
            - ibm/granite
          granite_vision:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - vision
            - multimodal
            - system_messages
            patterns:
            - ibm/granite-vision
          llama3_vision:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - vision
            - multimodal
            - system_messages
            patterns:
            - meta-llama/llama-3-.*vision
          llama4:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - multimodal
            - system_messages
            patterns:
            - meta-llama/llama-4
          mistral_watsonx:
            base_context_length: 131072
            features:
            - text
            - streaming
            - tools
            - parallel_calls
            - system_messages
            patterns:
            - mistralai/mistral
    project_id_env: WATSONX_PROJECT_ID
    space_id_env: WATSONX_SPACE_ID
  features:
  - text
  - streaming
  - system_messages
  - reasoning
  max_context_length: 131072
  max_output_tokens: 4096
  model_aliases:
    granite: ibm/granite-3-3-8b-instruct
    granite2b: ibm/granite-3-2b-instruct
    granite8b: ibm/granite-3-3-8b-instruct
    granite_vision: ibm/granite-vision-3-2-2b-instruct
    latest: ibm/granite-3-3-8b-instruct
    llama: meta-llama/llama-4-maverick-17b-128e-instruct-fp8
    llama3_2_11b_vision: meta-llama/llama-3-2-11b-vision-instruct
    llama3_2_1b: meta-llama/llama-3-2-1b-instruct
    llama3_2_3b: meta-llama/llama-3-2-3b-instruct
    llama3_2_90b_vision: meta-llama/llama-3-2-90b-vision-instruct
    llama3_3: meta-llama/llama-3-3-70b-instruct
    llama3_3_70b: meta-llama/llama-3-3-70b-instruct
    llama3_405b: meta-llama/llama-3-405b-instruct
    llama4: meta-llama/llama-4-maverick-17b-128e-instruct-fp8
    llama4_maverick: meta-llama/llama-4-maverick-17b-128e-instruct-fp8
    llama4_scout: meta-llama/llama-4-scout-17b-16e-instruct
    llama_vision: meta-llama/llama-4-maverick-17b-128e-instruct-fp8
    mistral: mistralai/mistral-large-2
    mistral_large: mistralai/mistral-large-2
    mistral_medium: mistralai/mistral-medium-2505
    mistral_small: mistralai/mistral-small-3-1-24b-instruct-2503
    mistral_vision: mistralai/pixtral-12b
    pixtral: mistralai/pixtral-12b
    reasoning: ibm/granite-3-3-8b-instruct
    vision: ibm/granite-vision-3-2-2b-instruct
  model_capabilities:
  - features:
    - text
    - streaming
    - tools
    - multimodal
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: meta-llama/llama-4-.*
  - features:
    - text
    - streaming
    - tools
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: meta-llama/llama-3-3-70b-instruct
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: meta-llama/llama-3-2-.*vision-instruct
  - features:
    - text
    - streaming
    - tools
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: meta-llama/llama-3-2-[13]b-instruct
  - features:
    - text
    - streaming
    - tools
    - parallel_calls
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: meta-llama/llama-3-405b-instruct
  - features:
    - text
    - streaming
    - tools
    - reasoning
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: ibm/granite-.*
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: ibm/granite-vision-.*
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    - parallel_calls
    max_context_length: 131072
    max_output_tokens: 8192
    pattern: mistralai/mistral-medium-2505
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: mistralai/mistral-small-3-1-24b-instruct-2503
  - features:
    - text
    - streaming
    - tools
    - vision
    - multimodal
    max_context_length: 131072
    max_output_tokens: 4096
    pattern: mistralai/pixtral-12b
  - features:
    - text
    - streaming
    - tools
    - parallel_calls
    max_context_length: 131072
    max_output_tokens: 8192
    pattern: mistralai/mistral-large-2
  models:
  - ibm/granite-3-3-8b-instruct
  - ibm/granite-3-2-8b-instruct
  - ibm/granite-3-8b-instruct
  - ibm/granite-3-2b-instruct
  - ibm/granite-vision-3-2-2b-instruct
  - meta-llama/llama-4-scout-17b-16e-instruct
  - meta-llama/llama-4-maverick-17b-128e-instruct-fp8
  - meta-llama/llama-3-3-70b-instruct
  - meta-llama/llama-3-2-90b-vision-instruct
  - meta-llama/llama-3-2-11b-vision-instruct
  - meta-llama/llama-3-2-1b-instruct
  - meta-llama/llama-3-2-3b-instruct
  - meta-llama/llama-3-405b-instruct
  - mistralai/mistral-medium-2505
  - mistralai/mistral-small-3-1-24b-instruct-2503
  - mistralai/pixtral-12b
  - mistralai/mistral-large-2
  rate_limits:
    default: 500
    enterprise: 2000
  watsonx_ai_url: https://us-south.ml.cloud.ibm.com
azure_openai:
  client_class: chuk_llm.llm.providers.azure_openai_client.AzureOpenAILLMClient
  api_key_env: AZURE_OPENAI_API_KEY
  api_key_fallback_env: AZURE_OPENAI_KEY
  default_model: gpt-5
  
  # Azure-specific configuration
  extra:
    azure_endpoint_env: AZURE_OPENAI_ENDPOINT
    api_version: "2024-02-01"
    default_deployment_mapping: true  # Map model names to deployment names
    
    # Dynamic discovery for Azure OpenAI
    dynamic_discovery:
      enabled: true
      discoverer_type: azure_openai
      cache_timeout: 600
      inference_config:
        default_context_length: 272000  # Updated for GPT-5
        default_features:
          - text
          - streaming
          - system_messages
          - json_mode
          - reasoning
          - tools
          - vision
          - parallel_calls
        default_max_output_tokens: 128000  # Updated for GPT-5
        
        # Azure deployment patterns
        deployment_rules:
          gpt5_deployments:
            base_context_length: 272000
            base_max_output_tokens: 128000
            features:
              - text
              - streaming
              - tools
              - vision
              - parallel_calls
              - reasoning
              - json_mode
              - system_messages
            patterns:
              - gpt-5
              
          gpt4_deployments:
            base_context_length: 128000
            base_max_output_tokens: 4096
            features:
              - text
              - streaming
              - tools
              - vision
              - parallel_calls
              - json_mode
              - system_messages
            patterns:
              - gpt-4
              - gpt-4o
              - gpt-4.1
          
          gpt35_deployments:
            base_context_length: 16384
            base_max_output_tokens: 4096
            features:
              - text
              - streaming
              - tools
              - json_mode
              - system_messages
            patterns:
              - gpt-3.5
          
          reasoning_deployments:
            base_context_length: 200000
            base_max_output_tokens: 32768
            features:
              - text
              - streaming
              - tools
              - vision
              - reasoning
              - parallel_calls
            patterns:
              - o1
              - o3
              - o4
  
  # Enhanced baseline features
  features:
    - text
    - streaming
    - system_messages
    - json_mode
    - reasoning
    - tools
    - vision
    - parallel_calls
  
  max_context_length: 272000  # Updated for GPT-5
  max_output_tokens: 128000   # Updated for GPT-5
  
  # Enhanced model aliases
  model_aliases:
    # GPT-5 aliases
    gpt5: gpt-5
    gpt5_mini: gpt-5-mini
    gpt5_nano: gpt-5-nano
    latest: gpt-5
    mini: gpt-5-mini
    nano: gpt-5-nano
    
    # Standard aliases
    gpt4: gpt-4
    gpt4o: gpt-4o
    gpt4o_mini: gpt-4o-mini
    gpt35: gpt-3.5-turbo
    turbo: gpt-4-turbo
    reasoning: gpt-5
    vision: gpt-5
    
    # Azure deployment examples (customize for your deployments)
    my_gpt5_deployment: gpt-5
    my_gpt4_deployment: gpt-4
    my_gpt4o_deployment: gpt-4o
    my_vision_deployment: gpt-5
  
  # Enhanced model-specific capabilities
  model_capabilities:
    # GPT-5 series
    - pattern: "gpt-5.*"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - reasoning
        - json_mode
        - system_messages
      max_context_length: 272000
      max_output_tokens: 128000
    
    # O-series reasoning models
    - pattern: "o[1-4].*"
      features:
        - text
        - streaming
        - tools
        - vision
        - reasoning
        - parallel_calls
      max_context_length: 200000
      max_output_tokens: 32768
    
    # GPT-4o series
    - pattern: "gpt-4o.*"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - json_mode
        - system_messages
      max_context_length: 128000
      max_output_tokens: 4096
    
    # GPT-4.1 series
    - pattern: "gpt-4\\.1.*"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - json_mode
        - system_messages
      max_context_length: 128000
      max_output_tokens: 4096
    
    # GPT-4 Turbo and regular
    - pattern: "gpt-4(-turbo|-preview)?$"
      features:
        - text
        - streaming
        - tools
        - vision
        - parallel_calls
        - json_mode
        - system_messages
      max_context_length: 128000
      max_output_tokens: 4096
    
    # GPT-3.5 Turbo
    - pattern: "gpt-3\\.5-turbo.*"
      features:
        - text
        - streaming
        - tools
        - json_mode
        - system_messages
      max_context_length: 16384
      max_output_tokens: 4096
  
  # Enhanced models list
  models:
    # GPT-5 series
    - gpt-5
    - gpt-5-mini
    - gpt-5-nano
    - gpt-5-chat
    
    # Standard models
    - gpt-4.1
    - gpt-4.1-mini
    - gpt-4o
    - gpt-4o-mini
    - gpt-4-turbo
    - gpt-4
    - gpt-3.5-turbo
    
    # Reasoning models
    - o1
    - o1-mini
    - o3
    - o3-mini
    - o4-mini
  
  # Enhanced rate limits
  rate_limits:
    default: 3000
    tier_1: 1000
    tier_2: 5000
    premium: 10000
    gpt_5: 5000