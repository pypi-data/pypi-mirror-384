#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Smart Web Agent MCP æœåŠ¡

åŸºäº SmartWebAgent æä¾›ç½‘é¡µç”Ÿæˆçš„è§„åˆ’ä¸æ‰§è¡Œæ¥å£ï¼Œå…¼å®¹ Model Context Protocolã€‚
"""

from __future__ import annotations

import asyncio
import json
import os
import sys
import time
import traceback
import zipfile
import tempfile
import aiohttp
from typing import Any, Dict, Optional

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨æ¨¡å—æœç´¢è·¯å¾„ä¸­
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# å½“å‰æ–‡ä»¶åœ¨ src/htmlgen_mcp/ ä¸‹ï¼Œæ‰€ä»¥éœ€è¦å‘ä¸Šä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(CURRENT_DIR)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from fastmcp import FastMCP  # type: ignore

import uuid
from pathlib import Path

from htmlgen_mcp.agents.smart_web_agent import SmartWebAgent
from htmlgen_mcp.nas_storage import get_nas_storage
from htmlgen_mcp.nas_log_manager import get_nas_log_manager, ensure_job_log, log_progress, query_progress
from datetime import datetime

# ä½¿ç”¨ NAS ä½œä¸ºé»˜è®¤å­˜å‚¨è·¯å¾„
NAS_PATH = os.environ.get("NAS_STORAGE_PATH", "/app/mcp-servers/mcp-servers/html_agent")
DEFAULT_PROJECT_ROOT = os.path.abspath(
    os.environ.get("WEB_AGENT_PROJECT_ROOT", f"{NAS_PATH}/projects")
)
DEFAULT_MODEL = os.environ.get("WEB_AGENT_MODEL", "qwen3-coder-plus-2025-09-23")
DEFAULT_BASE_URL = os.environ.get(
    "OPENAI_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"
)

mcp = FastMCP("smart-web-agent")

# MCP æœåŠ¡æŒä¹…åŒ–ç›®å½•ï¼šä½¿ç”¨ NAS è·¯å¾„ä»¥ä¾¿é›†ç¾¤å…±äº«
MCP_SERVICE_NAME = os.environ.get("MCP_SERVICE_NAME", "make_web")
MCP_DATA_ROOT = Path(
    os.environ.get("MCP_DATA_DIR", f"{NAS_PATH}/mcp_data/{MCP_SERVICE_NAME}")
)
MCP_DATA_ROOT.mkdir(parents=True, exist_ok=True)

# ç®€å•çš„ç¼“å­˜ï¼šè®°å½•æœ€è¿‘ä¸€æ¬¡ç”Ÿæˆçš„è®¡åˆ’ï¼Œé¿å…â€œcreate_simple_site â†’ execute_planâ€æ—¶éœ€æ‰‹åŠ¨ä¼ é€’
PLAN_CACHE_DIR = MCP_DATA_ROOT / "plan_cache"
PLAN_CACHE_DIR.mkdir(exist_ok=True)

# è¿›åº¦æ—¥å¿—ç›®å½•ï¼Œå­˜å‚¨æ¯ä¸ªä»»åŠ¡çš„å®æ—¶è¿›åº¦
PROGRESS_LOG_DIR = MCP_DATA_ROOT / "progress_logs"
PROGRESS_LOG_DIR.mkdir(exist_ok=True)

# ä»»åŠ¡çŠ¶æ€ç›®å½•ï¼Œæ¯ä¸ªä»»åŠ¡ä¸€ä¸ª JSON æ–‡ä»¶
JOB_STATE_DIR = MCP_DATA_ROOT / "jobs" / "state"
JOB_STATE_DIR.mkdir(parents=True, exist_ok=True)

# ä¸Šä¸‹æ–‡ç¼“å­˜ç›®å½•
CONTEXT_CACHE_DIR = MCP_DATA_ROOT / "context_cache"
CONTEXT_CACHE_DIR.mkdir(exist_ok=True)

_PLAN_CACHE: dict[tuple[str, str], Dict[str, Any]] = {}
_PLAN_CACHE_BY_ID: dict[str, Dict[str, Any]] = {}
_PROGRESS_LOG_BY_ID: dict[str, str] = {}
_PROGRESS_LOG_BY_JOB: dict[str, str] = {}
_JOB_REGISTRY: dict[str, Dict[str, Any]] = {}
_CONTEXT_CACHE_BY_ID: dict[str, Dict[str, Any]] = {}


def _job_state_path(job_id: str) -> Path:
    return JOB_STATE_DIR / f"{job_id}.json"


def _persist_job_state(job_id: str) -> None:
    job = _JOB_REGISTRY.get(job_id)
    if not job:
        return
    job_copy = {k: v for k, v in job.items() if k not in {"agent"}}
    job_copy["updated_at"] = time.time()
    
    # åŒæ—¶ä¿å­˜åˆ°æœ¬åœ°å’Œ NAS
    path = _job_state_path(job_id)
    try:
        path.write_text(
            json.dumps(job_copy, ensure_ascii=False, indent=2), encoding="utf-8"
        )
    except Exception:
        pass
    
    # ä¿å­˜åˆ° NAS æ—¥å¿—
    try:
        log_manager = get_nas_log_manager()
        plan_id = job.get("plan_id")
        log_manager.create_job_log(job_id, plan_id)
        log_progress(job_id, status="registered", job_info=job_copy)
    except Exception:
        pass


def _load_job_states() -> None:
    for path in JOB_STATE_DIR.glob("*.json"):
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            job_id = data.get("job_id") or path.stem
            if not job_id:
                continue
            if data.get("status") == "running":
                data["status"] = "stopped"
                data["message"] = "ä»»åŠ¡åœ¨æœåŠ¡å™¨é‡å¯æ—¶ä¸­æ–­ï¼Œè¯·é‡æ–°æ‰§è¡Œ"
            _JOB_REGISTRY[job_id] = data
            progress_log = data.get("progress_log")
            if progress_log:
                if not os.path.isabs(progress_log):
                    progress_log = os.path.join(PROJECT_ROOT, progress_log)
                _PROGRESS_LOG_BY_JOB[job_id] = progress_log
                plan_id = data.get("plan_id")
                if plan_id and plan_id not in _PROGRESS_LOG_BY_ID:
                    _PROGRESS_LOG_BY_ID[plan_id] = progress_log
        except Exception:
            continue


_load_job_states()


def _context_cache_path(context_id: str) -> Path:
    return CONTEXT_CACHE_DIR / f"{context_id}.json"


def _load_context_cache() -> None:
    for path in CONTEXT_CACHE_DIR.glob("*.json"):
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            context_id = data.get("context_id") or path.stem
            if not context_id:
                continue
            data.setdefault("path", str(path))
            _CONTEXT_CACHE_BY_ID[context_id] = data
        except Exception:
            continue


_load_context_cache()


def _resolve_cached_context(context_id: Optional[str]) -> Optional[Dict[str, Any]]:
    if not context_id:
        return None
    cached = _CONTEXT_CACHE_BY_ID.get(context_id)
    if cached:
        return cached
    path = _context_cache_path(context_id)
    if path.exists():
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            data.setdefault("path", str(path))
            _CONTEXT_CACHE_BY_ID[context_id] = data
            return data
        except Exception:
            return None
    return None


def _resolve_project_directory(project_root: Optional[str]) -> str:
    base = project_root or DEFAULT_PROJECT_ROOT
    abs_path = os.path.abspath(base)
    os.makedirs(abs_path, exist_ok=True)
    return abs_path


def _build_agent(
    project_directory: str,
    model: Optional[str] = None,
    *,
    show_code: bool = False,
    verbose: bool = False,
    save_output: bool = False,
    force_single_page: bool = True,
) -> SmartWebAgent:
    return SmartWebAgent(
        project_directory=project_directory,
        model=model or DEFAULT_MODEL,
        show_code=show_code,
        verbose=verbose,
        save_output=save_output,
        force_single_page=force_single_page,
    )


def _prepare_agent_run(agent: SmartWebAgent, description: str) -> None:
    agent.execution_start_time = time.time()
    agent.execution_history = []
    agent.created_files = []
    agent.latest_user_request = description
    agent.current_plan = None


def _decode_plan(agent: SmartWebAgent, plan: Any) -> Dict[str, Any]:
    if isinstance(plan, str):
        plan = json.loads(plan)
    if not isinstance(plan, dict):
        raise ValueError("plan åº”è¯¥æ˜¯ JSON å¯¹è±¡")
    source_description = plan.pop("__source_description", None)
    plan.pop("__plan_id", None)
    plan.pop("__plan_path", None)
    if source_description:
        agent.latest_user_request = source_description
    repaired = agent._repair_plan_tools_sequence(plan)
    if not agent._validate_plan(repaired):
        raise ValueError("æ‰§è¡Œè®¡åˆ’ä¸å®Œæ•´æˆ–æ ¼å¼é”™è¯¯")
    agent.current_plan = repaired
    return repaired


def _create_plan(agent: SmartWebAgent, description: str) -> Dict[str, Any]:
    _prepare_agent_run(agent, description)
    enhanced = agent._enhance_user_input(description)
    plan = agent._get_execution_plan_with_retry(enhanced)
    if not plan:
        raise RuntimeError("æœªèƒ½ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œè¯·æ£€æŸ¥æè¿°æˆ–æ¨¡å‹é…ç½®")
    if not isinstance(plan, dict):
        raise ValueError("æ¨¡å‹è¿”å›çš„è®¡åˆ’æ ¼å¼å¼‚å¸¸ï¼Œåº”ä¸º JSON å¯¹è±¡")
    plan_id = uuid.uuid4().hex
    plan_path = PLAN_CACHE_DIR / f"{plan_id}.json"

    plan_for_storage = plan.copy()
    plan_for_storage["__source_description"] = description
    plan_path.write_text(
        json.dumps(plan_for_storage, ensure_ascii=False, indent=2), encoding="utf-8"
    )

    plan["__source_description"] = description

    _PLAN_CACHE_BY_ID[plan_id] = {
        "plan": plan,
        "project_directory": agent.project_directory,
        "description": description,
        "source_description": description,
        "path": str(plan_path),
        "plan_id": plan_id,
    }

    cache_key = (agent.project_directory, description)
    _PLAN_CACHE[cache_key] = {
        "plan": plan,
        "plan_id": plan_id,
        "description": description,
        "source_description": description,
    }

    plan["__plan_id"] = plan_id
    plan["__plan_path"] = str(plan_path)
    return plan


def _execute_plan(
    agent: SmartWebAgent,
    plan: Dict[str, Any],
    *,
    progress_log_path: Optional[str] = None,
    job_id: Optional[str] = None,
) -> Dict[str, Any]:
    progress_events: list[Dict[str, Any]] = []

    def _collect(event: Dict[str, Any]) -> None:
        if isinstance(event, dict):
            progress_events.append(event)
            
            # å†™å…¥æœ¬åœ°æ—¥å¿—
            if progress_log_path:
                try:
                    log_record = dict(event)
                    log_record.setdefault("timestamp", time.time())
                    with open(progress_log_path, "a", encoding="utf-8") as log_file:
                        log_file.write(json.dumps(log_record, ensure_ascii=False))
                        log_file.write("\n")
                except Exception:
                    pass
            
            # åŒæ—¶å†™å…¥ NAS æ—¥å¿—
            if job_id:
                try:
                    log_progress(job_id, **event)
                except Exception:
                    pass

    results = agent._execute_plan_with_recovery(
        plan,
        confirm_each_step=False,  # åå°æ‰§è¡Œæ¨¡å¼ï¼Œä¸éœ€è¦ç¡®è®¤
        progress_callback=_collect,
    )

    if any(
        r.get("status") == "success"
        and r.get("tool") in {"create_html_file", "create_css_file"}
        for r in results
    ):
        agent._run_consistency_review(plan)

    report = agent._generate_execution_report(plan, results)

    return {
        "report": report,
        "progress": progress_events,
        "results": results,
        "created_files": list(agent.created_files),
    }


# @mcp.tool()
# async def plan_site(
#     description: str,
#     project_root: Optional[str] = None,
#     context_id: Optional[str] = None,
#     context_content: Optional[str] = None,
# ) -> Dict[str, Any]:
#     """æ ¹æ®éœ€æ±‚ä¸ä¸Šä¸‹æ–‡ç”Ÿæˆç½‘é¡µæ„å»ºè®¡åˆ’ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½ä¼šäº¤ç”± AI æ¨¡å‹ç»Ÿä¸€åˆ†æã€‚
#
#     âš ï¸ é‡è¦å‚æ•°è¯´æ˜ï¼š
#     - description: ç½‘ç«™å»ºè®¾éœ€æ±‚æˆ–ç›®æ ‡ï¼Œä¾§é‡æè¿°è¦å®ç°çš„ç»“æ„ã€åŠŸèƒ½ã€é£æ ¼
#     - context_content: ğŸ”¥ æ ¸å¿ƒå‚æ•°ï¼ç½‘é¡µåˆ¶ä½œæ‰€éœ€çš„å…¨éƒ¨åŸå§‹æ–‡æœ¬æˆ–æ•°æ®
#       * ä¾‹å¦‚ï¼šåœ°å›¾æŸ¥è¯¢ç»“æœã€å’–å•¡é¦†ä¿¡æ¯ã€äº§å“æ•°æ®ã€è¥ä¸šæ—¶é—´ã€åœ°å€ç­‰
#       * è¿™æ˜¯æ¨¡å‹è·å–ä¸Šä¸‹æ–‡å†…å®¹çš„å”¯ä¸€å…¥å£ï¼Œä¸ä¼šè‡ªåŠ¨ä» description æ¨æ–­
#       * è¯·æŠŠéœ€è¦å¼•ç”¨çš„å®Œæ•´ä¿¡æ¯ç›´æ¥æ”¾å…¥è¯¥å‚æ•°
#       * æ”¯æŒå„ç§æ ¼å¼ï¼šæ–‡æœ¬ã€JSONå­—ç¬¦ä¸²ã€ç»“æ„åŒ–æ•°æ®ç­‰
#
#     å…¶ä»–å‚æ•°ï¼š
#     - project_root: å¯é€‰ï¼Œè‡ªå®šä¹‰é¡¹ç›®æ ¹ç›®å½•ï¼›ç¼ºçœæ—¶ä½¿ç”¨é»˜è®¤ç›®å½•
#     - context_id: å¯é€‰ï¼Œå¼•ç”¨å·²ç¼“å­˜çš„ä¸Šä¸‹æ–‡å¿«ç…§ä»¥å¤ç”¨å†å²èµ„æ–™
#
#     è¿”å›å€¼è¯´æ˜ï¼š
#     - status: æ“ä½œçŠ¶æ€ ("success" æˆ– "error")
#     - plan_id: ç”Ÿæˆçš„è®¡åˆ’å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåç»­æ‰§è¡Œ
#     - plan_path: è®¡åˆ’ JSON æ–‡ä»¶çš„ä¿å­˜è·¯å¾„
#     - project_directory: è§£æåçš„é¡¹ç›®ç›®å½•è·¯å¾„
#     - model: ä½¿ç”¨çš„ AI æ¨¡å‹åç§°
#
#     ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
#     å¦‚æœä½ å…ˆç”¨å…¶ä»–å·¥å…·ï¼ˆå¦‚åœ°å›¾æŸ¥è¯¢ï¼‰è·å–äº†æ•°æ®ï¼Œè¯·å°†ç»“æœå®Œæ•´ä¼ é€’ç»™ context_contentï¼Œ
#     è¿™æ · AI å°±èƒ½åŸºäºçœŸå®æ•°æ®ç”Ÿæˆä¸ªæ€§åŒ–ç½‘ç«™ã€‚
#     """
#     try:
#         project_dir = _resolve_project_directory(project_root)
#         agent = _build_agent(project_dir)
#
#         # ç›´æ¥ä½¿ç”¨ descriptionï¼Œå¦‚æœæœ‰é¢å¤–çš„ä¸Šä¸‹æ–‡åˆ™é™„åŠ 
#         final_description = description
#
#         # å¦‚æœæä¾›äº† context_contentï¼Œé™„åŠ åˆ°æè¿°ä¸­
#         if context_content:
#             final_description = f"{description}\n\nã€é™„åŠ å†…å®¹ã€‘\n{context_content}"
#
#         # å¦‚æœæä¾›äº† context_idï¼Œå°è¯•è·å–ç¼“å­˜çš„å†…å®¹
#         elif context_id:
#             cached = _resolve_cached_context(context_id)
#             if cached:
#                 cached_content = cached.get("context")
#                 if cached_content:
#                     # å°è¯•è§£æJSONæ ¼å¼çš„å¢å¼ºæ•°æ®
#                     try:
#                         enhanced_data = json.loads(cached_content)
#                         if "original_content" in enhanced_data:
#                             cached_content = enhanced_data["original_content"]
#                     except (json.JSONDecodeError, TypeError):
#                         pass  # ä½¿ç”¨åŸå§‹å†…å®¹
#
#                     if cached_content:
#                         final_description = f"{description}\n\nã€ç¼“å­˜å†…å®¹ã€‘\n{cached_content}"
#
#         # è®© AI æ¨¡å‹ç›´æ¥å¤„ç†æ‰€æœ‰å†…å®¹
#         plan = await asyncio.to_thread(_create_plan, agent, final_description)
#         plan_id = plan.pop("__plan_id", None)
#         plan_path = plan.pop("__plan_path", None)
#
#         return {
#             "status": "success",
#             "plan_id": plan_id,
#             "plan_path": plan_path,
#             "project_directory": project_dir,
#             "model": agent.model,
#             "message": "è®¡åˆ’å·²ç”Ÿæˆï¼ŒAIæ¨¡å‹å·²åˆ†ææ‰€æä¾›çš„å…¨éƒ¨å†…å®¹"
#         }
#     except Exception as exc:
#         return {
#             "status": "error",
#             "message": str(exc),
#             "traceback": traceback.format_exc(),
#         }


@mcp.tool()
async def execute_plan(
    plan_id: str,
    *,
    project_root: str,
    # auto_plan: bool = False,  # å·²ç¦ç”¨ï¼Œæ²¡æœ‰å®é™…ä½œç”¨
    # confirm_each_step: bool = False,  # åå°æ‰§è¡Œæ¨¡å¼ä¸‹ç”¨æˆ·æ— æ³•äº¤äº’ç¡®è®¤
    # show_code: bool = False,  # åå°æ‰§è¡Œæ—¶ç”¨æˆ·çœ‹ä¸åˆ°è¾“å‡º
    # verbose: bool = False,  # åå°æ‰§è¡Œæ—¶è¯¦ç»†æ—¥å¿—æ„ä¹‰ä¸å¤§
    save_output: bool = False,
    progress_log: Optional[str] = None,
    auto_upload: bool = False,
    upload_url: str = "https://www.mcpcn.cc/api/fileUploadAndDownload/uploadMcpFile",
) -> Dict[str, Any]:
    """æ‰§è¡Œç½‘é¡µæ„å»ºè®¡åˆ’ï¼Œå§‹ç»ˆä»¥åå°æ¨¡å¼è¿è¡Œã€‚

    å‚æ•°è¯¦ç»†è¯´æ˜ï¼š
    - plan_id: è®¡åˆ’çš„å”¯ä¸€æ ‡è¯†ç¬¦
        ç”± plan_site å·¥å…·è¿”å›çš„è®¡åˆ’IDï¼Œç”¨äºä»ç¼“å­˜æˆ–æ–‡ä»¶ç³»ç»Ÿä¸­æŸ¥æ‰¾å¯¹åº”çš„æ‰§è¡Œè®¡åˆ’ã€‚
        ä¾‹å¦‚ï¼š"a1b2c3d4e5f6..." è¿™æ ·çš„32ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ã€‚

    - project_root: ç½‘ç«™æ–‡ä»¶ç”Ÿæˆçš„ç›®æ ‡ç›®å½•è·¯å¾„
        æŒ‡å®šé¡¹ç›®æ–‡ä»¶çš„è¾“å‡ºä½ç½®ï¼Œå¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ã€‚
        ä¾‹å¦‚ï¼š"/path/to/my/website" æˆ– "./my-project"
        å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºã€‚

    - save_output: æ˜¯å¦ä¿å­˜æ‰§è¡Œè¿‡ç¨‹çš„è¯¦ç»†è¾“å‡ºåˆ°æ–‡ä»¶
        True: ä¼šè‡ªåŠ¨åˆ›å»ºè¿›åº¦æ—¥å¿—æ–‡ä»¶ï¼Œè®°å½•æ‰€æœ‰æ‰§è¡Œæ­¥éª¤å’Œç»“æœ
        False: ä¸åˆ›å»ºè¿›åº¦æ—¥å¿—æ–‡ä»¶ï¼Œåªä¿ç•™åŸºæœ¬çš„ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        å»ºè®®åœ¨è°ƒè¯•æˆ–éœ€è¦è¯¦ç»†è¿½è¸ªæ—¶è®¾ç½®ä¸º True

    - progress_log: è‡ªå®šä¹‰è¿›åº¦æ—¥å¿—æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        å¦‚æœæŒ‡å®šï¼šä½¿ç”¨è¯¥è·¯å¾„ä¿å­˜è¿›åº¦æ—¥å¿—ï¼ˆJSONLæ ¼å¼ï¼‰
        å¦‚æœæœªæŒ‡å®šä½† save_output=Trueï¼šè‡ªåŠ¨åœ¨ ~/.mcp/make_web/progress_logs ç›®å½•åˆ›å»ºæ—¶é—´æˆ³å‘½åçš„æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        å¦‚æœéƒ½æœªæŒ‡å®šï¼šä¸è®°å½•è¿›åº¦æ—¥å¿—
        è·¯å¾„å¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äº project_root çš„ç›¸å¯¹è·¯å¾„

    - auto_upload: æ˜¯å¦åœ¨æ„å»ºå®Œæˆåè‡ªåŠ¨ä¸Šä¼ åˆ°MCPæœåŠ¡å™¨
        True: æ„å»ºå®Œæˆåè‡ªåŠ¨æ‰“åŒ…ä¸ºZIPå¹¶ä¸Šä¼ ï¼Œè¿”å›è®¿é—®URL
        False: ä»…æ„å»ºï¼Œä¸ä¸Šä¼ 
        é»˜è®¤ä¸º False

    - upload_url: æ–‡ä»¶ä¸Šä¼ APIåœ°å€
        é»˜è®¤ä¸º "https://www.mcpcn.cc/api/fileUploadAndDownload/uploadMcpFile"
        åªåœ¨ auto_upload=True æ—¶ç”Ÿæ•ˆ

    æ‰§è¡Œæµç¨‹ï¼š
    - ä»»åŠ¡å§‹ç»ˆåœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å› job_id å’Œ progress_log è·¯å¾„
    - ä½¿ç”¨ get_progress(job_id=..., limit=...) å¯ä»¥å®æ—¶æŸ¥è¯¢æ‰§è¡ŒçŠ¶æ€å’Œè¿›åº¦
    - æ”¯æŒé€šè¿‡è¿›åº¦æ—¥å¿—æ–‡ä»¶è¿½è¸ªè¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤å’Œç»“æœ
    - æ‰§è¡Œå®Œæˆåå¯ä»¥è·å–å®Œæ•´çš„æ‰§è¡ŒæŠ¥å‘Šå’Œç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    - ğŸ¯ æ–°å¢ï¼šauto_upload=Trueæ—¶ï¼Œæ„å»ºå®Œæˆ(100%)åè‡ªåŠ¨ä¸Šä¼ å¹¶è¿”å›è®¿é—®URL
    """
    try:
        project_dir = _resolve_project_directory(project_root)

        # ç§»é™¤ auto_plan æ£€æŸ¥ï¼Œå› ä¸ºå‚æ•°å·²è¢«ç§»é™¤

        if progress_log:
            progress_log_path = (
                progress_log
                if os.path.isabs(progress_log)
                else os.path.join(project_dir, progress_log)
            )
        elif save_output:
            progress_log_path = os.path.join(
                PROGRESS_LOG_DIR, f"agent_progress_{int(time.time())}.jsonl"
            )
        else:
            progress_log_path = None

        if progress_log_path:
            try:
                Path(progress_log_path).parent.mkdir(parents=True, exist_ok=True)
                Path(progress_log_path).write_text("", encoding="utf-8")
            except Exception:
                progress_log_path = None

        agent = _build_agent(
            project_dir,
            save_output=save_output,
        )

        # é€šè¿‡ plan_id æŸ¥è¯¢è®¡åˆ’
        cached_by_id = _PLAN_CACHE_BY_ID.get(plan_id)

        # å°è¯•å¤šç§æ–‡ä»¶å‘½åæ ¼å¼
        possible_paths = [
            PLAN_CACHE_DIR / f"{plan_id}.json",  # æ ‡å‡†æ ¼å¼
            PLAN_CACHE_DIR
            / f"simple_site_plan_{plan_id}.json",  # create_simple_siteæ ¼å¼
        ]

        plan_path = None
        for path in possible_paths:
            if path.exists():
                plan_path = path
                break

        if not cached_by_id and plan_path:
            try:
                cached_plan_file = json.loads(plan_path.read_text(encoding="utf-8"))
                source_description = None
                if isinstance(cached_plan_file, dict):
                    source_description = cached_plan_file.get("__source_description")
                    # æå–å®é™…çš„planéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡ä»¶å†…å®¹
                    actual_plan = cached_plan_file.get("plan")
                    if not actual_plan:
                        # å¦‚æœæ²¡æœ‰planå­—æ®µï¼Œå¯èƒ½æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶å†…å®¹
                        actual_plan = cached_plan_file

                cached_by_id = {
                    "plan": actual_plan,  # ä¼ é€’å®é™…çš„planå†…å®¹
                    "project_directory": project_dir,
                    "plan_id": plan_id,
                    "description": source_description
                    or cached_plan_file.get("description"),
                    "source_description": source_description,
                }
                _PLAN_CACHE_BY_ID[plan_id] = cached_by_id
            except Exception:
                cached_by_id = None

        if not cached_by_id:
            raise ValueError(
                f"æœªæ‰¾åˆ° plan_id '{plan_id}' å¯¹åº”çš„è®¡åˆ’ï¼Œè¯·å…ˆè°ƒç”¨ create_simple_site ç”Ÿæˆè®¡åˆ’"
            )

        plan_dict = _decode_plan(agent, cached_by_id.get("plan"))
        effective_description = (
            cached_by_id.get("source_description")
            or cached_by_id.get("description")
            or plan_dict.get("task_analysis")
            or plan_dict.get("project_name")
            or plan_dict.get("site_type")
            or "Web Project Execution"
        )

        _prepare_agent_run(agent, effective_description)
        agent.current_plan = plan_dict

        # å§‹ç»ˆä»¥åå°æ¨¡å¼æ‰§è¡Œ
        job_id = uuid.uuid4().hex
        job_info = {
            "job_id": job_id,
            "status": "running",
            "plan_id": plan_id,
            "description": effective_description,
            "project_directory": project_dir,
            "model": agent.model,
            "progress_log": progress_log_path,
            "started_at": time.time(),
            "updated_at": time.time(),
        }
        _JOB_REGISTRY[job_id] = job_info

        if plan_id and progress_log_path:
            _PROGRESS_LOG_BY_ID[plan_id] = progress_log_path
        if progress_log_path:
            _PROGRESS_LOG_BY_JOB[job_id] = progress_log_path

        _persist_job_state(job_id)

        asyncio.create_task(
            _run_execution_job(
                job_id,
                agent,
                plan_dict,
                progress_log_path=progress_log_path,
                auto_upload=auto_upload,
                upload_url=upload_url,
            )
        )

        if auto_upload:
            message = (
                "æ‰§è¡Œå·²åœ¨åå°å¯åŠ¨ï¼ˆå«è‡ªåŠ¨ä¸Šä¼ ï¼‰ï¼šè°ƒç”¨ get_progress(job_id='{}', limit=20) "
                "å¯è·å–æ„å»ºå’Œä¸Šä¼ è¿›åº¦ã€‚å®Œæˆåå°†è¿”å›ç½‘ç«™è®¿é—®URLã€‚"
            ).format(job_id)
        else:
            message = (
                "æ‰§è¡Œå·²åœ¨åå°å¯åŠ¨ï¼šè°ƒç”¨ get_progress(job_id='{}', limit=20) "
                "æˆ–ä¼ å…¥ progress_log='{}' å¯è·å–å®æ—¶è¿›åº¦"
            ).format(job_id, progress_log_path or "<æœªå¯ç”¨è¿›åº¦æ—¥å¿—>")

        return {
            "status": "started",
            "job_id": job_id,
            "plan_id": plan_id,
            "progress_log": progress_log_path,
            "auto_upload": auto_upload,
            "upload_url": upload_url if auto_upload else None,
            "message": message,
        }
    except Exception as exc:
        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


async def _run_execution_job(
    job_id: str,
    agent: SmartWebAgent,
    plan_dict: Dict[str, Any],
    *,
    progress_log_path: Optional[str],
    auto_upload: bool = False,
    upload_url: str = "https://www.mcpcn.cc/api/fileUploadAndDownload/uploadMcpFile",
) -> None:
    job_info = _JOB_REGISTRY.get(job_id)
    if not job_info:
        return

    try:
        result = await asyncio.to_thread(
            _execute_plan,
            agent,
            plan_dict,
            progress_log_path=progress_log_path,
        )
        job_info["status"] = "completed"
        job_info["result"] = result
        job_info["completed_at"] = time.time()
        _persist_job_state(job_id)

        # ğŸ¯ å…³é”®ï¼šä»»åŠ¡å®Œæˆåè‡ªåŠ¨ä¸Šä¼ 
        if auto_upload and job_info.get("project_directory"):
            try:
                # è®°å½•ä¸Šä¼ å¼€å§‹
                job_info["upload_status"] = "uploading"
                _persist_job_state(job_id)

                upload_result = await upload_project_to_mcp_server(
                    folder_path=job_info["project_directory"], upload_url=upload_url
                )

                # è®°å½•ä¸Šä¼ ç»“æœ
                job_info["upload_result"] = upload_result
                job_info["upload_status"] = upload_result["status"]
                if upload_result["status"] == "success":
                    job_info["website_url"] = upload_result["url"]
                    job_info["upload_completed_at"] = time.time()

                    # æ·»åŠ åˆ°è¿›åº¦æ—¥å¿—
                    if progress_log_path:
                        upload_event = {
                            "timestamp": time.time(),
                            "type": "upload_completed",
                            "status": "success",
                            "website_url": upload_result["url"],
                            "message": upload_result["message"],
                        }
                        try:
                            with open(
                                progress_log_path, "a", encoding="utf-8"
                            ) as log_file:
                                log_file.write(
                                    json.dumps(upload_event, ensure_ascii=False)
                                )
                                log_file.write("\n")
                        except Exception:
                            pass

                _persist_job_state(job_id)
            except Exception as upload_exc:
                job_info["upload_status"] = "failed"
                job_info["upload_error"] = str(upload_exc)
                _persist_job_state(job_id)

    except Exception as exc:
        job_info["status"] = "failed"
        job_info["error"] = {
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }
        _persist_job_state(job_id)
    finally:
        job_info["updated_at"] = time.time()
        _persist_job_state(job_id)


@mcp.tool()
async def create_simple_site(
    description: str,
    site_title: str = "æˆ‘çš„ç½‘ç«™",
    project_root: Optional[str] = None,
    model: Optional[str] = None,
    context_id: Optional[str] = None,
    context_content: Optional[str] = None,
) -> Dict[str, Any]:
    """ä½¿ç”¨AIåˆ†æéœ€æ±‚ï¼Œç”Ÿæˆç®€å•ä½†ç¾è§‚çš„ç½‘ç«™è®¡åˆ’ã€‚

    âš ï¸ é‡è¦å‚æ•°è¯´æ˜ï¼š
    - description: ç½‘ç«™éœ€æ±‚æè¿°ï¼Œä¾‹å¦‚"ä¸ªäººä½œå“å±•ç¤ºç½‘ç«™"ã€"å°é¤å…å®˜ç½‘"ã€"åšå®¢ç½‘ç«™"ç­‰
    - site_title: ç½‘ç«™æ ‡é¢˜
    - context_content: ğŸ”¥ æ ¸å¿ƒå‚æ•°ï¼ç”¨äºä¼ é€’ç½‘é¡µåˆ¶ä½œæ‰€éœ€çš„æ‰€æœ‰åŸå§‹æ•°æ®å†…å®¹
      * ä¾‹å¦‚ï¼šå’–å•¡é¦†åˆ—è¡¨ã€äº§å“ä»‹ç»ã€èœå•å†…å®¹ã€åœ°å€ä¿¡æ¯ã€è¥ä¸šæ—¶é—´ç­‰
      * è¿™æ˜¯AIè·å–å…·ä½“ä¸šåŠ¡ä¿¡æ¯çš„å”¯ä¸€æ¸ é“ï¼Œè¯·åŠ¡å¿…å°†æŸ¥è¯¢åˆ°çš„è¯¦ç»†ä¿¡æ¯å®Œæ•´ä¼ å…¥
      * å¦‚æœæœ‰åœ°å›¾æŸ¥è¯¢ç»“æœã€APIè¿”å›æ•°æ®ç­‰ï¼Œéƒ½åº”è¯¥æ”¾åœ¨è¿™ä¸ªå‚æ•°ä¸­
      * æ ¼å¼å¯ä»¥æ˜¯æ–‡æœ¬ã€JSONå­—ç¬¦ä¸²æˆ–ç»“æ„åŒ–æ•°æ®çš„å­—ç¬¦ä¸²è¡¨ç¤º

    å…¶ä»–å‚æ•°ï¼š
    - project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œç¼ºçœä½¿ç”¨é»˜è®¤ç›®å½•
    - model: ä½¿ç”¨çš„AIæ¨¡å‹ï¼Œç¼ºçœä½¿ç”¨é»˜è®¤æ¨¡å‹
    - context_id: å¯é€‰ï¼Œå¼•ç”¨å·²ç¼“å­˜çš„ä¸Šä¸‹æ–‡å¿«ç…§ä»¥å¤ç”¨å†å²èµ„æ–™

    å›¾ç‰‡é…ç½®å‚æ•°ï¼ˆå¯é€‰ï¼‰ï¼š
    - image_style: å›¾ç‰‡é£æ ¼ (professional|artistic|minimal|vibrant|luxury)
    - image_topics: ç”¨æˆ·è‡ªå®šä¹‰çš„å›¾ç‰‡ä¸»é¢˜åˆ—è¡¨ï¼Œå¦‚ ["modern office", "team collaboration"]
    - include_gallery: æ˜¯å¦åœ¨ç½‘ç«™ä¸­åŒ…å«å›¾ç‰‡ç”»å»ŠåŠŸèƒ½
    - image_provider: å›¾ç‰‡æä¾›å•† (pollinations|dicebear|robohash)

    è¿”å›å€¼è¯´æ˜ï¼š
    - status: æ“ä½œçŠ¶æ€ ("success" æˆ– "error")
    - plan_id: ç”Ÿæˆçš„è®¡åˆ’å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåç»­æ‰§è¡Œ
    - plan_path: è®¡åˆ’ JSON æ–‡ä»¶çš„ä¿å­˜è·¯å¾„
    - project_directory: è§£æåçš„é¡¹ç›®ç›®å½•è·¯å¾„
    - plan: ç”Ÿæˆçš„ç®€åŒ–æ‰§è¡Œè®¡åˆ’æ¦‚è§ˆ
    - context_id: ä¸Šä¸‹æ–‡ç¼“å­˜IDï¼ˆå¦‚æœä½¿ç”¨äº†ä¸Šä¸‹æ–‡ï¼‰

    ä½¿ç”¨æµç¨‹ï¼š
    1. è°ƒç”¨æ­¤å·¥å…·ç”Ÿæˆè®¡åˆ’ï¼Œè·å¾— plan_id
    2. ä½¿ç”¨ plan_id è°ƒç”¨ execute_plan æ‰§è¡Œæ„å»º
    3. ä½¿ç”¨ plan_id è°ƒç”¨ get_progress æŸ¥è¯¢è¿›åº¦

    ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
    å¦‚æœä½ æœ‰åœ°å›¾æŸ¥è¯¢ç»“æœã€APIæ•°æ®ç­‰ï¼Œè¯·å°†å®Œæ•´ä¿¡æ¯ä¼ é€’ç»™ context_content å‚æ•°ï¼Œ
    è¿™æ ·AIå°±èƒ½åŸºäºçœŸå®æ•°æ®æ¥ç”Ÿæˆä¸ªæ€§åŒ–çš„ç½‘ç«™å†…å®¹ã€‚
    """
    try:
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        used_model = model or DEFAULT_MODEL

        # ç¡®å®šé¡¹ç›®ç›®å½•
        if project_root:
            if not os.path.isabs(project_root):
                project_directory = os.path.join(DEFAULT_PROJECT_ROOT, project_root)
            else:
                project_directory = project_root
        else:
            # ä½¿ç”¨ç«™ç‚¹æ ‡é¢˜ä½œä¸ºç›®å½•å
            safe_title = "".join(
                c if c.isalnum() or c in "._-" else "_" for c in site_title
            )
            project_directory = os.path.join(DEFAULT_PROJECT_ROOT, safe_title)

        # å¤„ç†ä¸Šä¸‹æ–‡
        context_data = ""
        actual_context_id = context_id

        if context_id and context_id in _CONTEXT_CACHE_BY_ID:
            # ä½¿ç”¨ç¼“å­˜çš„ä¸Šä¸‹æ–‡
            cached_context = _CONTEXT_CACHE_BY_ID[context_id]
            context_data = cached_context.get("content", "")
        elif context_content:
            # ä½¿ç”¨æ–°æä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹
            context_data = context_content
            # ç”Ÿæˆæ–°çš„ä¸Šä¸‹æ–‡IDå¹¶ç¼“å­˜
            actual_context_id = str(uuid.uuid4())
            _CONTEXT_CACHE_BY_ID[actual_context_id] = {
                "content": context_content,
                "created_at": time.time(),
                "site_title": site_title,
                "description": description,
            }

        # åˆ›å»ºAIä»£ç†è¿›è¡Œåˆ†æ
        agent = SmartWebAgent(
            project_directory=project_directory,
            model=used_model,
            show_code=False,
            verbose=False,
            force_single_page=True,
        )

        # æ„å»ºé’ˆå¯¹ç®€å•ç½‘ç«™çš„æç¤ºï¼ŒåŒ…å«ä¸Šä¸‹æ–‡å’Œå›¾ç‰‡è¦æ±‚
        context_section = ""
        if context_data:
            context_section = f"""

**ä¸Šä¸‹æ–‡å†…å®¹**:
{context_data}

**é‡è¦è¯´æ˜**: è¯·æ ¹æ®ä¸Šè¿°ä¸Šä¸‹æ–‡å†…å®¹æ¥å®šåˆ¶ç½‘ç«™çš„å…·ä½“å†…å®¹ï¼Œç¡®ä¿ç”Ÿæˆçš„ç½‘é¡µä¸æä¾›çš„ä¿¡æ¯é«˜åº¦åŒ¹é…ã€‚"""

        simple_prompt = f"""è¯·ä¸ºä»¥ä¸‹éœ€æ±‚è®¾è®¡ä¸€ä¸ªç®€å•ä½†ç¾è§‚çš„ç½‘ç«™ï¼š

**ç½‘ç«™éœ€æ±‚**: {description}
**ç½‘ç«™æ ‡é¢˜**: {site_title}{context_section}

**è®¾è®¡è¦æ±‚**:
1. ä¿æŒç®€æ´ä½†è¦æœ‰è§†è§‰å¸å¼•åŠ›
2. ä½¿ç”¨è½»é‡çº§CSSï¼ˆä¸è¶…è¿‡300è¡Œï¼‰
3. åŒ…å«åŸºç¡€ä½†å®ç”¨çš„åŠŸèƒ½
4. å“åº”å¼è®¾è®¡ï¼Œé€‚é…ç§»åŠ¨ç«¯
5. è‰¯å¥½çš„é…è‰²å’Œæ’ç‰ˆ
6. æ™ºèƒ½å›¾ç‰‡é›†æˆï¼Œæ ¹æ®å†…å®¹ç±»å‹åŒ¹é…åˆé€‚ä¸»é¢˜
7. å¦‚æœæœ‰ä¸Šä¸‹æ–‡å†…å®¹ï¼Œè¯·å……åˆ†åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥ä¸°å¯Œç½‘é¡µå†…å®¹

è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«3-6ä¸ªæ­¥éª¤çš„ç®€åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œæ¯ä¸ªæ­¥éª¤è¦ç®€æ´æ˜ç¡®ã€‚
é‡ç‚¹å…³æ³¨ï¼šé¡µé¢ç»“æ„ã€æ ·å¼è®¾è®¡ã€å›¾ç‰‡é›†æˆã€å¿…è¦åŠŸèƒ½ã€‚é¿å…å¤æ‚ç‰¹æ•ˆã€‚

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- æ¯ä¸ªæ­¥éª¤éƒ½è¦å…·ä½“å¯æ‰§è¡Œ
- ä¼˜å…ˆä½¿ç”¨ç®€å•æ¨¡æ¿å‡½æ•°è€Œéå¤æ‚æ¨¡æ¿
- æ³¨é‡å®ç”¨æ€§å’Œç¾è§‚æ€§çš„å¹³è¡¡
- å……åˆ†åˆ©ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹
"""

        # ç”Ÿæˆç®€åŒ–è®¡åˆ’ï¼ˆä»…è§„åˆ’ï¼Œä¸æ‰§è¡Œï¼‰
        plan = agent._get_execution_plan(simple_prompt)

        # åœ¨è®¡åˆ’ä¸­æ ‡è®°ä¸ºç®€å•ç½‘ç«™ç±»å‹å’Œç›¸å…³ä¿¡æ¯
        plan["site_type"] = "simple"
        plan["complexity"] = "ç®€å•ä½†ç¾è§‚"
        plan["css_limit"] = "ä¸è¶…è¿‡300è¡Œ"
        plan["model_used"] = used_model
        plan["has_context"] = bool(context_data)
        if actual_context_id:
            plan["context_id"] = actual_context_id

        # ç”Ÿæˆå”¯ä¸€çš„è®¡åˆ’ID
        plan_id = str(uuid.uuid4())

        # æ„å»ºå®Œæ•´çš„æºæè¿°ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        source_description = description
        if context_data:
            source_description = f"{description}\n\nã€é™„åŠ å†…å®¹ã€‘\n{context_data}"

        # åœ¨è®¡åˆ’ä¸­æ·»åŠ æºæè¿°å­—æ®µ
        plan["__source_description"] = source_description
        plan["__plan_id"] = plan_id

        # ä¿å­˜è®¡åˆ’åˆ°ç¼“å­˜ï¼ˆç»“æ„ä¸ create_simple_site ä¿æŒä¸€è‡´ï¼Œä¾¿äº execute_plan å¤ç”¨é€»è¾‘ï¼‰
        cached_entry = {
            "plan": plan,
            "project_directory": project_directory,
            "description": description,
            "source_description": source_description,
            "site_title": site_title,
            "plan_id": plan_id,
        }

        _PLAN_CACHE_BY_ID[plan_id] = cached_entry
        cache_key = (project_directory, description)
        _PLAN_CACHE[cache_key] = cached_entry

        # å°†ä¸Šä¸‹æ–‡ä¿¡æ¯å…³è”åˆ°è®¡åˆ’
        if actual_context_id:
            _PROGRESS_LOG_BY_ID[plan_id] = actual_context_id

        # ä¿å­˜è®¡åˆ’åˆ°æ–‡ä»¶
        plan_filename = f"simple_site_plan_{plan_id}.json"
        plan_path = PLAN_CACHE_DIR / plan_filename

        try:
            # æ„å»ºå®Œæ•´çš„æºæè¿°ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            source_description = description
            if context_data:
                source_description = f"{description}\n\nã€é™„åŠ å†…å®¹ã€‘\n{context_data}"

            plan_data = {
                "plan_id": plan_id,
                "site_title": site_title,
                "description": description,
                "project_directory": project_directory,
                "model": used_model,
                "plan_type": "simple_site",
                "created_at": time.time(),
                "plan": plan,
                "__source_description": source_description,  # æ·»åŠ å®Œæ•´çš„æºæè¿°å­—æ®µ
            }
            if actual_context_id:
                plan_data["context_id"] = actual_context_id
                plan_data["has_context"] = True

            with open(plan_path, "w", encoding="utf-8") as f:
                json.dump(plan_data, f, ensure_ascii=False, indent=2)
        except Exception:
            # æ–‡ä»¶ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass

        # ç”Ÿæˆè®¡åˆ’æ¦‚è§ˆ
        tools_sequence = plan.get("tools_sequence", [])
        plan_overview = {
            "description": plan.get("description", "ç®€å•ç½‘ç«™æ„å»ºè®¡åˆ’"),
            "steps": len(tools_sequence),
            "step_list": [
                step.get("description", step.get("tool", "")) for step in tools_sequence
            ],
            "estimated_files": plan.get("estimated_files", "3-5ä¸ªæ–‡ä»¶"),
            "features": plan.get("features", ["å“åº”å¼è®¾è®¡", "è½»é‡çº§æ ·å¼", "åŸºç¡€äº¤äº’"]),
            "has_context": bool(context_data),
        }

        result = {
            "status": "success",
            "message": f"ç®€å•ç½‘ç«™è®¡åˆ’ç”ŸæˆæˆåŠŸï¼ŒåŒ…å«{len(tools_sequence)}ä¸ªæ‰§è¡Œæ­¥éª¤",
            "plan_id": plan_id,
            "plan_path": str(plan_path),
            "project_directory": project_directory,
            "plan": plan_overview,
            "model_used": used_model,
            "next_step": f"ä½¿ç”¨ execute_plan(plan_id='{plan_id}') å¼€å§‹æ„å»ºç½‘ç«™",
        }

        if actual_context_id:
            result["context_id"] = actual_context_id
            result["context_used"] = True

        return result

    except Exception as exc:
        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


@mcp.tool()
async def get_progress(
    plan_id: Optional[str] = None,
    job_id: Optional[str] = None,
    log_path: Optional[str] = None,
    limit: int = 20,
) -> Dict[str, Any]:
    """æŸ¥è¯¢ç½‘é¡µæ„å»ºä»»åŠ¡çš„æ‰§è¡Œè¿›åº¦å’ŒçŠ¶æ€ã€‚

    å‚æ•°è¯´æ˜ï¼š
    - plan_id: create_simple_site è¿”å›çš„ IDï¼Œå¯å®šä½é»˜è®¤çš„è¿›åº¦æ—¥å¿—ã€‚
    - job_id: execute_plan(background=true) è¿”å›çš„ä»»åŠ¡ IDï¼Œå¯ç›´æ¥æŸ¥è¯¢åå°ä»»åŠ¡çŠ¶æ€ã€‚
    - log_path: è¿›åº¦æ—¥å¿— JSONL æ–‡ä»¶çš„è·¯å¾„ï¼ˆç»å¯¹æˆ–ç›¸å¯¹ï¼‰ï¼Œä¼˜å…ˆçº§æœ€é«˜ã€‚
    - limit: è¿”å›çš„æœ€æ–°äº‹ä»¶æ•°é‡ï¼ˆé»˜è®¤ 20 æ¡ï¼‰ã€‚

    ä½¿ç”¨æç¤ºï¼š
    1. æ¨èç›´æ¥ä¼ å…¥ execute_plan è¿”å›çš„ job_id å’Œ progress_logã€‚
    2. è‹¥æœªæä¾› log_pathï¼Œæœ¬å·¥å…·ä¼šæŒ‰ job_id -> plan_id çš„é¡ºåºå°è¯•æŸ¥æ‰¾å·²ç¼“å­˜çš„æ—¥å¿—ã€‚
    3. è¿”å›å†…å®¹åŒ…æ‹¬æœ€æ–°äº‹ä»¶åˆ—è¡¨ã€æ—¥å¿—è·¯å¾„ä»¥åŠï¼ˆè‹¥æœ‰ï¼‰ä»»åŠ¡å¿«ç…§æˆ–ç»“æœæ‘˜è¦ï¼Œå¯ç”¨äºæŒç»­è¿½è¸ªæ„å»ºè¿›åº¦ã€‚
    """

    try:
        # SSE æ¨¡å¼ä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥ I/O
        loop = asyncio.get_event_loop()
        
        if limit <= 0:
            limit = 20

        job_info = None
        resolved_path = None

        if job_id:
            job_info = _JOB_REGISTRY.get(job_id)
            if job_info and not plan_id:
                plan_id = job_info.get("plan_id")
            if job_id in _PROGRESS_LOG_BY_JOB:
                resolved_path = _PROGRESS_LOG_BY_JOB[job_id]
            elif job_info and job_info.get("progress_log"):
                resolved_path = job_info.get("progress_log")

        if not resolved_path and plan_id and plan_id in _PROGRESS_LOG_BY_ID:
            resolved_path = _PROGRESS_LOG_BY_ID[plan_id]

        if log_path:
            resolved_path = log_path

        if resolved_path:
            if not os.path.isabs(resolved_path):
                candidate = os.path.join(PROJECT_ROOT, resolved_path)
                # å¼‚æ­¥æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
                exists = await loop.run_in_executor(None, os.path.exists, candidate)
                if exists:
                    resolved_path = candidate
                else:
                    alt = os.path.sep + resolved_path.lstrip(os.path.sep)
                    exists_alt = await loop.run_in_executor(None, os.path.exists, alt)
                    if exists_alt:
                        resolved_path = alt

        # å¼‚æ­¥æ£€æŸ¥æœ€ç»ˆè·¯å¾„
        path_exists = await loop.run_in_executor(
            None, lambda: resolved_path and os.path.exists(resolved_path)
        )
        
        if not path_exists:
            return {
                "status": "error",
                "message": "æœªæ‰¾åˆ°è¿›åº¦æ—¥å¿—ï¼Œè¯·ç¡®è®¤ job_id/plan_id æˆ–æä¾› log_pathï¼ˆæ³¨æ„ç»å¯¹è·¯å¾„éœ€ä»¥/å¼€å¤´ï¼Œæ‰©å±•åä¸º .jsonlï¼‰",
            }

        # å¼‚æ­¥è¯»å–æ–‡ä»¶
        def read_file():
            events = []
            total = 0
            try:
                with open(resolved_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()
                    total = len(lines)
                    for line in lines[-limit:]:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            events.append(json.loads(line))
                        except Exception:
                            continue
            except Exception:
                pass
            return events, total
        
        events, total_lines = await loop.run_in_executor(None, read_file)

        response: Dict[str, Any] = {
            "status": "success",
            "plan_id": plan_id,
            "job_id": job_id,
            "log_path": resolved_path,
            "events": events,
            "total_records": total_lines,
            "returned": len(events),
        }

        if job_info:
            snapshot_keys = [
                "job_id",
                "status",
                "plan_id",
                "progress_log",
                "started_at",
                "updated_at",
                "completed_at",
                "project_directory",
                "model",
                "upload_status",
                "website_url",
                "upload_completed_at",
            ]
            job_snapshot = {
                k: job_info.get(k) for k in snapshot_keys if job_info.get(k) is not None
            }

            if job_info.get("status") == "completed":
                job_snapshot["result_summary"] = {
                    "report": job_info.get("result", {}).get("report"),
                    "created_files": job_info.get("result", {}).get("created_files"),
                }
                # æ·»åŠ ä¸Šä¼ ç»“æœä¿¡æ¯
                if job_info.get("upload_result"):
                    job_snapshot["upload_result"] = job_info.get("upload_result")

            if job_info.get("status") == "failed":
                job_snapshot["error"] = job_info.get("error")

            # æ·»åŠ ä¸Šä¼ é”™è¯¯ä¿¡æ¯
            if job_info.get("upload_error"):
                job_snapshot["upload_error"] = job_info.get("upload_error")

            response["job"] = job_snapshot

        return response
    except Exception as exc:
        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


@mcp.tool()
async def upload_project_to_mcp_server(
    folder_path: str,
    upload_url: str = "https://www.mcpcn.cc/api/fileUploadAndDownload/uploadMcpFile",
) -> Dict[str, Any]:
    """å°†é¡¹ç›®æ–‡ä»¶å¤¹æ‰“åŒ…æˆZIPå¹¶ä¸Šä¼ åˆ°MCPæœåŠ¡å™¨ã€‚

    å‚æ•°è¯´æ˜ï¼š
    - folder_path: é¡¹ç›®æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„
    - upload_url: ä¸Šä¼ APIåœ°å€ï¼Œé»˜è®¤ä¸ºMCPæœåŠ¡å™¨åœ°å€

    è¿”å›å€¼ï¼š
    - status: ä¸Šä¼ çŠ¶æ€ ("success" æˆ– "error")
    - url: ä¸Šä¼ æˆåŠŸåè¿”å›çš„æ–‡ä»¶è®¿é—®URL
    - zip_path: ä¸´æ—¶ZIPæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    - message: çŠ¶æ€ä¿¡æ¯
    """
    try:
        # éªŒè¯æ–‡ä»¶å¤¹è·¯å¾„
        if not os.path.exists(folder_path):
            return {"status": "error", "message": f"é¡¹ç›®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}"}

        if not os.path.isdir(folder_path):
            return {"status": "error", "message": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}"}

        # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
        project_name = os.path.basename(folder_path.rstrip("/"))
        temp_dir = tempfile.gettempdir()
        zip_filename = f"{project_name}_{int(time.time())}.zip"
        zip_path = os.path.join(temp_dir, zip_filename)

        # æ‰“åŒ…é¡¹ç›®æ–‡ä»¶
        with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                    arcname = os.path.relpath(file_path, folder_path)
                    zipf.write(file_path, arcname)

        # æ£€æŸ¥ZIPæ–‡ä»¶å¤§å°
        zip_size = os.path.getsize(zip_path)
        if zip_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
            os.remove(zip_path)
            return {
                "status": "error",
                "message": f"ZIPæ–‡ä»¶è¿‡å¤§: {zip_size / 1024 / 1024:.1f}MBï¼Œè¶…è¿‡50MBé™åˆ¶",
            }

        # ä¸Šä¼ æ–‡ä»¶
        async with aiohttp.ClientSession() as session:
            with open(zip_path, "rb") as f:
                data = aiohttp.FormData()
                data.add_field(
                    "file", f, filename=zip_filename, content_type="application/zip"
                )

                async with session.post(upload_url, data=data) as response:
                    response_text = await response.text()

                    if response.status != 200:
                        return {
                            "status": "error",
                            "message": f"ä¸Šä¼ å¤±è´¥ï¼ŒHTTP {response.status}: {response_text}",
                        }

                    # è§£æå“åº”JSON
                    try:
                        result = json.loads(response_text)
                        if result.get("code") == 0 and result.get("data", {}).get(
                            "url"
                        ):
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            try:
                                os.remove(zip_path)
                            except:
                                pass

                            return {
                                "status": "success",
                                "url": result["data"]["url"],
                                "message": f"é¡¹ç›® '{project_name}' ä¸Šä¼ æˆåŠŸ",
                                "zip_size": f"{zip_size / 1024:.1f}KB",
                            }
                        else:
                            return {
                                "status": "error",
                                "message": f"ä¸Šä¼ å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}",
                                "response": response_text,
                            }
                    except json.JSONDecodeError:
                        return {
                            "status": "error",
                            "message": f"å“åº”è§£æå¤±è´¥: {response_text}",
                        }

    except Exception as exc:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if "zip_path" in locals() and os.path.exists(zip_path):
            try:
                os.remove(zip_path)
            except:
                pass

        return {
            "status": "error",
            "message": str(exc),
            "traceback": traceback.format_exc(),
        }


@mcp.tool()
async def deploy_folder_or_zip(
    folder_path: str, env: str = "Production"
) -> Dict[str, Any]:
    """å°†æ„å»ºå¥½çš„ç½‘ç«™æ–‡ä»¶å¤¹æˆ–ZIPæ–‡ä»¶éƒ¨ç½²åˆ°EdgeOne Pagesã€‚

    å‚æ•°è¯´æ˜ï¼š
    - folder_path: æœ¬åœ°æ–‡ä»¶å¤¹æˆ–ZIPæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        æŒ‡å®šè¦éƒ¨ç½²çš„å‰ç«¯æ„å»ºäº§ç‰©ä½ç½®ï¼Œå¯ä»¥æ˜¯ï¼š
        * æ„å»ºå¥½çš„é™æ€ç½‘ç«™æ–‡ä»¶å¤¹ï¼ˆå¦‚ ./dist, ./build ç­‰ï¼‰
        * åŒ…å«ç½‘ç«™æ–‡ä»¶çš„ZIPå‹ç¼©åŒ…
        ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹è·¯å¾„ç±»å‹å¹¶é‡‡ç”¨ç›¸åº”çš„ä¸Šä¼ ç­–ç•¥

    - env: éƒ¨ç½²ç¯å¢ƒï¼ˆå¯é€‰ï¼‰
        * "Production": ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œä½¿ç”¨è‡ªå®šä¹‰åŸŸåï¼ˆå¦‚å·²é…ç½®ï¼‰
        * "Preview": é¢„è§ˆç¯å¢ƒéƒ¨ç½²ï¼Œç”Ÿæˆä¸´æ—¶é¢„è§ˆé“¾æ¥
        é»˜è®¤ä¸º "Production"

    ç¯å¢ƒå˜é‡è¦æ±‚ï¼š
    - EDGEONE_PAGES_API_TOKEN: EdgeOne Pages APIè®¿é—®ä»¤ç‰Œï¼ˆå¿…éœ€ï¼‰
    - EDGEONE_PAGES_PROJECT_NAME: é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼ŒæœªæŒ‡å®šæ—¶è‡ªåŠ¨åˆ›å»ºä¸´æ—¶é¡¹ç›®ï¼‰

    è¿”å›å€¼è¯´æ˜ï¼š
    - status: éƒ¨ç½²çŠ¶æ€ ("success" æˆ– "error")
    - deployment_logs: è¯¦ç»†çš„éƒ¨ç½²è¿‡ç¨‹æ—¥å¿—
    - result: éƒ¨ç½²ç»“æœä¿¡æ¯
        * type: åŸŸåç±»å‹ ("custom" è‡ªå®šä¹‰åŸŸå æˆ– "temporary" ä¸´æ—¶åŸŸå)
        * url: ç½‘ç«™è®¿é—®URL
        * project_id: EdgeOneé¡¹ç›®ID
        * project_name: é¡¹ç›®åç§°
        * console_url: EdgeOneæ§åˆ¶å°ç®¡ç†é“¾æ¥

    ä½¿ç”¨åœºæ™¯ï¼š
    1. å°†æœ¬åœ°å¼€å‘çš„é™æ€ç½‘ç«™éƒ¨ç½²ä¸Šçº¿
    2. å°†æ„å»ºå·¥å…·ï¼ˆå¦‚Webpackã€Viteç­‰ï¼‰ç”Ÿæˆçš„distç›®å½•éƒ¨ç½²
    3. å°†æ‰“åŒ…å¥½çš„ç½‘ç«™ZIPæ–‡ä»¶å¿«é€Ÿéƒ¨ç½²
    4. åˆ›å»ºç½‘ç«™çš„é¢„è§ˆç‰ˆæœ¬è¿›è¡Œæµ‹è¯•

    éƒ¨ç½²æµç¨‹ï¼š
    1. éªŒè¯æœ¬åœ°è·¯å¾„å’Œæ–‡ä»¶
    2. æ£€æµ‹å¯ç”¨çš„APIç«¯ç‚¹
    3. è·å–æˆ–åˆ›å»ºEdgeOneé¡¹ç›®
    4. ä¸Šä¼ æ–‡ä»¶åˆ°è…¾è®¯äº‘COS
    5. åˆ›å»ºéƒ¨ç½²ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
    6. ç”Ÿæˆè®¿é—®é“¾æ¥å’Œç®¡ç†ä¿¡æ¯

    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - éœ€è¦æœ‰æ•ˆçš„EdgeOne Pages APIä»¤ç‰Œ
    - ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œä¸Šä¼ å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
    - å¤§æ–‡ä»¶æˆ–å¤§é‡æ–‡ä»¶çš„ä¸Šä¼ ä¼šç›¸åº”å¢åŠ éƒ¨ç½²æ—¶é—´
    - ä¸´æ—¶åŸŸåé“¾æ¥åŒ…å«æ—¶æ•ˆæ€§è®¿é—®ä»¤ç‰Œ
    """
    try:
        # å¯¼å…¥EdgeOneéƒ¨ç½²å·¥å…·
        from htmlgen_mcp.agents.web_tools.edgeone_deploy import deploy_folder_or_zip_to_edgeone

        # éªŒè¯ç¯å¢ƒå˜é‡
        api_token = os.getenv("EDGEONE_PAGES_API_TOKEN")
        if not api_token:
            return {
                "status": "error",
                "message": "Missing EDGEONE_PAGES_API_TOKEN environment variable. Please set your EdgeOne Pages API token.",
            }

        # éªŒè¯è·¯å¾„æ ¼å¼
        if not os.path.isabs(folder_path):
            return {
                "status": "error",
                "message": f"Path must be absolute: {folder_path}",
            }

        # éªŒè¯ç¯å¢ƒå‚æ•°
        if env not in ["Production", "Preview"]:
            return {
                "status": "error",
                "message": "env must be 'Production' or 'Preview'",
            }

        # æ‰§è¡Œéƒ¨ç½²
        result_json = await asyncio.to_thread(
            deploy_folder_or_zip_to_edgeone, folder_path, env
        )
        result = json.loads(result_json)

        return {
            "status": "success",
            "message": f"Deployment to {env} environment completed successfully",
            "deployment_logs": result.get("deployment_logs", ""),
            "result": result.get("result", {}),
        }

    except Exception as exc:
        error_message = str(exc)

        # å¦‚æœæ˜¯EdgeOneéƒ¨ç½²é”™è¯¯ï¼Œå°è¯•è§£æJSONæ ¼å¼çš„é”™è¯¯ä¿¡æ¯
        try:
            if error_message.startswith("{"):
                error_data = json.loads(error_message)
                return {
                    "status": "error",
                    "message": error_data.get("error", error_message),
                    "deployment_logs": error_data.get("deployment_logs", ""),
                    "traceback": traceback.format_exc(),
                }
        except:
            pass

        return {
            "status": "error",
            "message": error_message,
            "traceback": traceback.format_exc(),
        }


def main() -> None:
    transport = os.environ.get("MCP_TRANSPORT", "stdio")
    print("ğŸš€ Smart Web Agent MCP æœåŠ¡å™¨å·²å¯åŠ¨")
    print(f"ğŸ“ é»˜è®¤é¡¹ç›®æ ¹ç›®å½•: {DEFAULT_PROJECT_ROOT}")
    print(f"ğŸ¤– é»˜è®¤æ¨¡å‹: {DEFAULT_MODEL}")
    print(f"ğŸŒ é»˜è®¤APIåœ°å€: {DEFAULT_BASE_URL}")
    print("ğŸŒ EdgeOne Pages éƒ¨ç½²å·¥å…·å·²åŠ è½½")
    mcp.run(transport=transport)


if __name__ == "__main__":
    main()
