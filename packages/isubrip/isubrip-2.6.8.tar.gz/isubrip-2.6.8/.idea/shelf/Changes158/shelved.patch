Index: isubrip/__main__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport logging\r\nimport sys\r\nfrom typing import ClassVar\r\n\r\nimport httpx\r\nfrom pydantic import ValidationError\r\n\r\nfrom isubrip.commands.download import download\r\nfrom isubrip.config import Config\r\nfrom isubrip.constants import (\r\n    DATA_FOLDER_PATH,\r\n    EVENT_LOOP,\r\n    LOG_FILES_PATH,\r\n    PACKAGE_NAME,\r\n    PACKAGE_VERSION,\r\n)\r\nfrom isubrip.logger import logger, setup_loggers\r\nfrom isubrip.scrapers.scraper import Scraper, ScraperFactory\r\nfrom isubrip.subtitle_formats.webvtt import WebVTTCaptionBlock\r\nfrom isubrip.utils import (\r\n    TempDirGenerator,\r\n    format_config_validation_error,\r\n    raise_for_status,\r\n    single_string_to_list,\r\n)\r\n\r\n\r\nclass AppSettings:\r\n    log_rotation_size: ClassVar[int | None] = None\r\n    stdout_loglevel: ClassVar[int] = logging.INFO\r\n    file_loglevel: ClassVar[int] = logging.DEBUG\r\n\r\n\r\ndef main() -> None:\r\n    try:\r\n        # Assure at least one argument was passed\r\n        if len(sys.argv) < 2:\r\n            print_usage()\r\n            exit(0)\r\n\r\n        if not DATA_FOLDER_PATH.is_dir():\r\n            DATA_FOLDER_PATH.mkdir(parents=True)\r\n\r\n        setup_loggers(stdout_loglevel=AppSettings.stdout_loglevel,\r\n                      file_loglevel=AppSettings.file_loglevel)\r\n\r\n        cli_args = \" \".join(sys.argv[1:])\r\n        logger.debug(f\"CLI Command: {PACKAGE_NAME} {cli_args}\")\r\n        logger.debug(f\"Python version: {sys.version}\")\r\n        logger.debug(f\"Package version: {PACKAGE_VERSION}\")\r\n        logger.debug(f\"OS: {sys.platform}\")\r\n\r\n        try:\r\n            config = Config()\r\n\r\n        except ValidationError as e:\r\n            logger.error(\"Invalid configuration - the following errors were found in the configuration file:\\n\"\r\n                         \"---\\n\" +\r\n                         format_config_validation_error(exc=e) +\r\n                         \"---\\n\"\r\n                         \"Please update your configuration to resolve the issue.\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            exit(1)\r\n\r\n        update_settings(config=config)\r\n\r\n        if config.general.check_for_updates:\r\n            check_for_updates(current_package_version=PACKAGE_VERSION)\r\n\r\n        EVENT_LOOP.run_until_complete(download(urls=single_string_to_list(item=sys.argv[1:]),\r\n                                               config=config))\r\n\r\n    except Exception as ex:\r\n        logger.error(f\"Error: {ex}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        exit(1)\r\n\r\n    finally:\r\n        if AppSettings.log_rotation_size:\r\n            handle_log_rotation(log_rotation_size=AppSettings.log_rotation_size)\r\n\r\n        # NOTE: This will only close scrapers that were initialized using the ScraperFactory.\r\n        async_cleanup_coroutines = []\r\n        for scraper in ScraperFactory.get_initialized_scrapers():\r\n            logger.debug(f\"Requests count for '{scraper.name}' scraper: {scraper.requests_count}\")\r\n            scraper.close()\r\n            async_cleanup_coroutines.append(scraper.async_close())\r\n\r\n        EVENT_LOOP.run_until_complete(asyncio.gather(*async_cleanup_coroutines))\r\n        TempDirGenerator.cleanup()\r\n\r\n\r\ndef check_for_updates(current_package_version: str) -> None:\r\n    \"\"\"\r\n    Check and print if a newer version of the package is available, and log accordingly.\r\n\r\n    Args:\r\n        current_package_version (str): The current version of the package.\r\n    \"\"\"\r\n    api_url = f\"https://pypi.org/pypi/{PACKAGE_NAME}/json\"\r\n    logger.debug(\"Checking for package updates on PyPI...\")\r\n    try:\r\n        response = httpx.get(\r\n            url=api_url,\r\n            headers={\"Accept\": \"application/json\"},\r\n            timeout=5,\r\n        )\r\n        raise_for_status(response)\r\n        response_data = response.json()\r\n\r\n        pypi_latest_version = response_data[\"info\"][\"version\"]\r\n\r\n        if pypi_latest_version != current_package_version:\r\n            logger.warning(f\"You are currently using version '{current_package_version}' of '{PACKAGE_NAME}', \"\r\n                           f\"however version '{pypi_latest_version}' is available.\"\r\n                           f'\\nConsider upgrading by running \"pip install --upgrade {PACKAGE_NAME}\"\\n')\r\n\r\n        else:\r\n            logger.debug(f\"Latest version of '{PACKAGE_NAME}' ({current_package_version}) is currently installed.\")\r\n\r\n    except Exception as e:\r\n        logger.warning(f\"Update check failed: {e}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        return\r\n\r\n\r\ndef handle_log_rotation(log_rotation_size: int) -> None:\r\n    \"\"\"\r\n    Handle log rotation and remove old log files if needed.\r\n\r\n    Args:\r\n        log_rotation_size (int): Maximum amount of log files to keep.\r\n    \"\"\"\r\n    sorted_log_files = sorted(LOG_FILES_PATH.glob(\"*.log\"), key=lambda file: file.stat().st_mtime, reverse=True)\r\n\r\n    if len(sorted_log_files) > log_rotation_size:\r\n        for log_file in sorted_log_files[log_rotation_size:]:\r\n            log_file.unlink()\r\n\r\n\r\ndef update_settings(config: Config) -> None:\r\n    \"\"\"\r\n    Update settings according to config.\r\n\r\n    Args:\r\n        config (Config): An instance of a config to set settings according to.\r\n    \"\"\"\r\n    Scraper.subtitles_fix_rtl = config.subtitles.fix_rtl\r\n    Scraper.subtitles_remove_duplicates = config.subtitles.remove_duplicates\r\n    Scraper.default_timeout = config.scrapers.default.timeout\r\n    Scraper.default_user_agent = config.scrapers.default.user_agent\r\n    Scraper.default_proxy = config.scrapers.default.proxy\r\n    Scraper.default_verify_ssl = config.scrapers.default.verify_ssl\r\n\r\n    WebVTTCaptionBlock.subrip_alignment_conversion = (\r\n        config.subtitles.webvtt.subrip_alignment_conversion\r\n    )\r\n\r\n    if log_rotation := config.general.log_rotation_size:\r\n        AppSettings.log_rotation_size = log_rotation\r\n\r\n\r\ndef print_usage() -> None:\r\n    \"\"\"Print usage information.\"\"\"\r\n    logger.info(f\"Usage: {PACKAGE_NAME} <iTunes movie URL> [iTunes movie URL...]\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/__main__.py b/isubrip/__main__.py
--- a/isubrip/__main__.py	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/isubrip/__main__.py	(date 1730277090544)
@@ -2,20 +2,27 @@
 
 import asyncio
 import logging
+from dataclasses import dataclass
+from functools import wraps
+from pathlib import Path
 import sys
-from typing import ClassVar
+from tomllib import TOMLDecodeError
+from typing import Callable, Any, Type, TYPE_CHECKING
 
 import httpx
 from pydantic import ValidationError
+import tomli
+import typer
 
 from isubrip.commands.download import download
 from isubrip.config import Config
 from isubrip.constants import (
     DATA_FOLDER_PATH,
+    DEFAULT_LOGS_ROTATION_SIZE,
     EVENT_LOOP,
     LOG_FILES_PATH,
     PACKAGE_NAME,
-    PACKAGE_VERSION,
+    PACKAGE_VERSION, USER_CONFIG_FILE_PATH,
 )
 from isubrip.logger import logger, setup_loggers
 from isubrip.scrapers.scraper import Scraper, ScraperFactory
@@ -23,77 +30,305 @@
 from isubrip.utils import (
     TempDirGenerator,
     format_config_validation_error,
-    raise_for_status,
-    single_string_to_list,
+    raise_for_status, return_first_valid,
 )
+
+if TYPE_CHECKING:
+    from pydantic import BaseModel
+
+app: typer.Typer = typer.Typer(no_args_is_help=True)
+
+
+@dataclass
+class ConfigOption:
+    """Represents a configuration option with its CLI and config file details."""
+    cli_name: str  # The CLI argument name (e.g., --check-for-updates)
+    config_path: str  # The dot-notation path in config (e.g., general.check_for_updates)
+    help_text: str
+    type_: Type
+    default: Any = None
+    short_name: str | None = None  # Optional short form (e.g., -c)
+
+
+class ConfigMapping:
+    """
+    Define custom mappings between config paths and CLI arguments.
+    Keys are config paths in dot notation, values are tuples of (cli_name, short_name, help_text).
+    Short name and help text are optional.
+    """
+
+    def __init__(self):
+        self.mappings = {
+            # Examples:
+            "general.check_for_updates": ("check-updates", "c", "Check for updates before running"),
+            "general.log_rotation_size": ("log-rotate", None, "Maximum number of log files to keep"),
+            "downloads.folder": ("output", "o", "Download folder path"),
+            "downloads.languages": ("lang", "l", "Languages to download"),
+            "downloads.overwrite_existing": ("overwrite", None, "Overwrite existing files"),
+            "downloads.zip": ("zip", "z", "Save multiple subtitles in ZIP"),
+            "subtitles.fix_rtl": ("fix-rtl", None, "Fix RTL for Arabic & Hebrew"),
+            "subtitles.remove_duplicates": ("dedup", None, "Remove duplicate paragraphs"),
+            "subtitles.convert_to_srt": ("to-srt", None, "Convert to SRT format"),
+            # Add more mappings as needed
+        }
+
+
+class ConfigMapper:
+    """Maps between CLI arguments and configuration file settings."""
+
+    def __init__(self, config_class: Type[T], mapping: ConfigMapping):
+        self.config_class = config_class
+        self.custom_mapping = mapping
+        self.options: Dict[str, ConfigOption] = {}
+        self._initialize_mapping()
+
+    def _initialize_mapping(self):
+        """Initialize the mapping based on the config class structure and custom mappings."""
+
+        def process_model(model: Type[BaseModel], prefix: str = ""):
+            for field_name, field in model.model_fields.items():
+                full_path = f"{prefix}{field_name}" if prefix else field_name
+
+                if isinstance(field.annotation, type) and issubclass(field.annotation, BaseModel):
+                    process_model(field.annotation, f"{full_path}.")
+                else:
+                    # Use custom mapping if available, otherwise use default
+                    if full_path in self.custom_mapping.mappings:
+                        cli_name, short_name, help_text = self.custom_mapping.mappings[full_path]
+                    else:
+                        cli_name = full_path.replace("_", "-").replace(".", "-")
+                        short_name = None
+                        help_text = field.description or ""
+
+                    self.options[cli_name] = ConfigOption(
+                        cli_name=cli_name,
+                        config_path=full_path,
+                        help_text=help_text or field.description or "",
+                        type_=field.annotation,
+                        default=field.default,
+                        short_name=short_name
+                    )
+
+        process_model(self.config_class)
+
+    def create_cli_options(self) -> dict[str, Any]:
+        """Create Typer CLI options based on the configuration mapping."""
+        cli_options = {}
+
+        for option in self.options.values():
+            param_decls = [f"--{option.cli_name}"]
+            if option.short_name:
+                param_decls.append(f"-{option.short_name}")
+
+            if option.type_ == bool:
+                cli_options[option.cli_name.replace("-", "_")] = typer.Option(
+                    None,
+                    *param_decls,
+                    help=option.help_text
+                )
+            else:
+                cli_options[option.cli_name.replace("-", "_")] = typer.Option(
+                    None,
+                    *param_decls,
+                    help=option.help_text
+                )
+
+        return cli_options
+
+    def merge_config(self, config: Config, cli_args: dict[str, Any]) -> Config:
+        """Merge CLI arguments into the configuration."""
+        config_dict = config.model_dump()
+
+        for cli_name, value in cli_args.items():
+            if value is not None:  # Only override if CLI argument was provided
+                option = self.options.get(cli_name.replace("_", "-"))
+                if option:
+                    parts = option.config_path.split(".")
+                    current = config_dict
+                    for part in parts[:-1]:
+                        current = current[part]
+                    current[parts[-1]] = value
+
+        return self.config_class.model_validate(config_dict)
+
+
+def with_config(mapping: ConfigMapping):
+    """Decorator to inject configuration into CLI commands."""
+    config_mapper = ConfigMapper(Config, mapping)
+    cli_options = config_mapper.create_cli_options()
+
+    def decorator(f):
+        # Add CLI options to the function
+        for param_name, option in cli_options.items():
+            f = typer.Option(option.default, **option.param_decls, help=option.help)(f)
+
+        @wraps(f)
+        def wrapper(*args, **kwargs):
+            # Load config file
+            config_path = Path("config.toml")
+            if config_path.exists():
+                with open(config_path, "rb") as f:
+                    config_data = tomli.load(f)
+                config = Config.model_validate(config_data)
+            else:
+                config = Config()
+
+            # Merge CLI arguments with config
+            config = config_mapper.merge_config(config, kwargs)
+
+            # Call the original function with the merged config
+            return f(*args, config=config, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+
 
 
-class AppSettings:
-    log_rotation_size: ClassVar[int | None] = None
-    stdout_loglevel: ClassVar[int] = logging.INFO
-    file_loglevel: ClassVar[int] = logging.DEBUG
 
 
-def main() -> None:
+
+
+def version_check_callback(value: bool) -> None:
+    """If value is true, display the current version of this project and raise typer.Exit"""
+    if value:
+        typer.echo(f"{PACKAGE_NAME} v{PACKAGE_VERSION}")
+        raise typer.Exit()
+
+
+def main():
     try:
-        # Assure at least one argument was passed
-        if len(sys.argv) < 2:
-            print_usage()
-            exit(0)
+        app()
+
+    except typer.Exit as e:  # Already handled exceptions
+        pass
+
+    except Exception as e:
+        logger.error(f"Error: {e}")
+        logger.debug("Debug information:", exc_info=True)
+        typer.Exit(1)
+
+    finally:
+        handle_log_rotation(rotation_size=LOG_ROTATION_SIZE)
 
-        if not DATA_FOLDER_PATH.is_dir():
-            DATA_FOLDER_PATH.mkdir(parents=True)
+        async_cleanup_coroutines = []
 
-        setup_loggers(stdout_loglevel=AppSettings.stdout_loglevel,
-                      file_loglevel=AppSettings.file_loglevel)
+        # NOTE: This will only close scrapers that were initialized using the ScraperFactory.
+        for scraper in ScraperFactory.get_initialized_scrapers():
+            logger.debug(f"Requests count for '{scraper.name}' scraper: {scraper.requests_count}")
+            scraper.close()
+            async_cleanup_coroutines.append(scraper.async_close())
 
-        cli_args = " ".join(sys.argv[1:])
-        logger.debug(f"CLI Command: {PACKAGE_NAME} {cli_args}")
-        logger.debug(f"Python version: {sys.version}")
-        logger.debug(f"Package version: {PACKAGE_VERSION}")
-        logger.debug(f"OS: {sys.platform}")
+        EVENT_LOOP.run_until_complete(asyncio.gather(*async_cleanup_coroutines))
+        TempDirGenerator.cleanup()
 
-        try:
-            config = Config()
+# @app.callback()
+# def init(
+#         ctx: typer.Context,
+#         verbose: bool = typer.Option(
+#             False, "--verbose", "-v", help="Enable verbose mode."
+#         ),
+#         version: bool = typer.Option(
+#             False, "--version", "-V", help="Show the version and exit.",
+#             callback=version_check_callback, is_eager=True
+#         ),
+#         check_for_updates: bool = typer.Option(
+#             True, "--check-for-updates/--no-check-for-updates",
+#             help="Whether to automatically check for updates."
+#         ),
+#         log_rotation_size: int = typer.Option(
+#             15, "--log-rotation-size", help="The maximum amount of log files to keep."
+#         ),
+#          config_path: Path = typer.Option(
+#              USER_CONFIG_FILE_PATH, "--config-path", help="Path to a config file to be used")  # TODO: Assure that if the file doesn't exist, nothing breaks.
+# ) -> None:
+#     update_settings(log_rotation_size=log_rotation_size)
+#
+#     if not DATA_FOLDER_PATH.is_dir():
+#         DATA_FOLDER_PATH.mkdir(parents=True)
+#
+#     stdout_loglevel = logging.DEBUG if return_first_valid(verbose, config.general.verbose) else logging.INFO
+#     setup_loggers(stdout_loglevel=stdout_loglevel,
+#                   file_loglevel=logging.DEBUG)
+#
+#     cli_args = " ".join(sys.argv[1:])
+#     logger.debug(f"CLI Command: {PACKAGE_NAME} {cli_args}")
+#     logger.debug(f"Python version: {sys.version}")
+#     logger.debug(f"Package version: {PACKAGE_VERSION}")
+#     logger.debug(f"OS: {sys.platform}")
+#
+#
+#     if return_first_valid(check_for_updates, config.general.check_for_updates):
+#         _check_for_updates(current_package_version=PACKAGE_VERSION)
+#
+#     if ctx.invoked_subcommand is None:
+#         url = next(iter(ctx.args), None)
+#         if url:
+#             ctx.invoke(download, url=url)
+#         else:
+#             typer.echo("Please provide a URL to download.")
+#             raise typer.Exit(code=1)
+
+def parse_config(config_file_location: Path | None = None) -> Config:
+    try:
+        if config_file_location:
+            Config.config_file_location = config_file_location
+
+        return Config()
 
-        except ValidationError as e:
-            logger.error("Invalid configuration - the following errors were found in the configuration file:\n"
-                         "---\n" +
-                         format_config_validation_error(exc=e) +
-                         "---\n"
-                         "Please update your configuration to resolve the issue.")
-            logger.debug("Debug information:", exc_info=True)
-            exit(1)
-
-        update_settings(config=config)
+    except ValidationError as e:
+        logger.error("Invalid configuration - the following errors were found in the configuration file:\n"
+                     "---\n" +
+                     format_config_validation_error(exc=e) +
+                     "---\n"
+                     "Please update your configuration to resolve the issue.")
+        logger.debug("Debug information:", exc_info=True)
+        raise typer.Exit(1) from e
 
-        if config.general.check_for_updates:
-            check_for_updates(current_package_version=PACKAGE_VERSION)
-
-        EVENT_LOOP.run_until_complete(download(urls=single_string_to_list(item=sys.argv[1:]),
-                                               config=config))
-
-    except Exception as ex:
-        logger.error(f"Error: {ex}")
+    except TOMLDecodeError as e:
+        logger.error(f"Error parsing config file: {e}")
         logger.debug("Debug information:", exc_info=True)
-        exit(1)
+        raise typer.Exit(1) from e
 
-    finally:
-        if AppSettings.log_rotation_size:
-            handle_log_rotation(log_rotation_size=AppSettings.log_rotation_size)
+    except Exception as e:
+        logger.error(f"Error loading configuration: {e}")
+        logger.debug("Debug information:", exc_info=True)
+        raise typer.Exit(1) from e
 
-        # NOTE: This will only close scrapers that were initialized using the ScraperFactory.
-        async_cleanup_coroutines = []
-        for scraper in ScraperFactory.get_initialized_scrapers():
-            logger.debug(f"Requests count for '{scraper.name}' scraper: {scraper.requests_count}")
-            scraper.close()
-            async_cleanup_coroutines.append(scraper.async_close())
 
-        EVENT_LOOP.run_until_complete(asyncio.gather(*async_cleanup_coroutines))
-        TempDirGenerator.cleanup()
+@app.command(name="download")
+@with_config(Config)
+def download_command(
+        urls: list[str] = typer.Argument(...,
+                                         help="URLs of media to download."),
+        output_path: Path = typer.Option(Path.cwd(), "--output", "-o",
+                                           help="Path to download to."),
+        language_filter: list[str] | None = typer.Option(None, "--language", "-l",
+                                                         help="Specific languages to download."),
+        convert_to_srt: bool = typer.Option(False,
+                                            "--srt", "-s", help="Convert subtitles to SRT."),
+        fix_rtl: bool = typer.Option(False,
+                                     "--fix-rtl", help="Fix right-to-left alignment in subtitles."),
+        overwrite_existing: bool = typer.Option(False,
+                                                "--overwrite", "-w", help="Overwrite existing subtitles."),
+        archive: bool = typer.Option(False, "--archive", "-a",
+                                     help="Archive multiple subtitles into a single file."),
+) -> None:
+    EVENT_LOOP.run_until_complete(
+        download(
+            urls=urls,
+            output_path=output_path,
+            language_filter=language_filter,
+            convert_to_srt=convert_to_srt,
+            overwrite_existing=overwrite_existing,
+            fix_rtl=fix_rtl,
+            archive=archive,
+        )
+    )
 
-
-def check_for_updates(current_package_version: str) -> None:
+def _check_for_updates(current_package_version: str) -> None:
     """
     Check and print if a newer version of the package is available, and log accordingly.
 
@@ -127,26 +362,37 @@
         return
 
 
-def handle_log_rotation(log_rotation_size: int) -> None:
+def handle_log_rotation(rotation_size: int) -> None:
     """
     Handle log rotation and remove old log files if needed.
 
     Args:
-        log_rotation_size (int): Maximum amount of log files to keep.
+        rotation_size (int): Maximum amount of log files to keep.
     """
+    if rotation_size <= 0:
+        return
+
     sorted_log_files = sorted(LOG_FILES_PATH.glob("*.log"), key=lambda file: file.stat().st_mtime, reverse=True)
 
-    if len(sorted_log_files) > log_rotation_size:
-        for log_file in sorted_log_files[log_rotation_size:]:
+    if len(sorted_log_files) > rotation_size:
+        for log_file in sorted_log_files[rotation_size:]:
             log_file.unlink()
 
 
-def update_settings(config: Config) -> None:
+def update_settings(
+        config: Config,
+        verbose: bool | None = None,
+        check_for_updates: bool | None = None,
+
+        log_rotation_size: int | None = None,
+
+) -> None:
     """
     Update settings according to config.
 
     Args:
-        config (Config): An instance of a config to set settings according to.
+        config (Config): The configuration object.
+        log_rotation_size (int): Maximum amount of log files
     """
     Scraper.subtitles_fix_rtl = config.subtitles.fix_rtl
     Scraper.subtitles_remove_duplicates = config.subtitles.remove_duplicates
@@ -159,13 +405,22 @@
         config.subtitles.webvtt.subrip_alignment_conversion
     )
 
-    if log_rotation := config.general.log_rotation_size:
-        AppSettings.log_rotation_size = log_rotation
+    # TODO: Update scrapers using config
 
+    if _log_rotation_size := return_first_valid(log_rotation_size, config.general.log_rotation_size):
+        global LOG_ROTATION_SIZE
+        LOG_ROTATION_SIZE = _log_rotation_size
 
-def print_usage() -> None:
-    """Print usage information."""
-    logger.info(f"Usage: {PACKAGE_NAME} <iTunes movie URL> [iTunes movie URL...]")
+
+# def preprocess_args():
+#     """Preprocess the command line arguments."""
+#     # List of registered subcommands
+#     commands = {command.name for command in app.registered_commands}
+#     non_flag_args = [arg for arg in sys.argv[1:] if not arg.startswith("-")]
+#
+#     # Ensure there's at least one argument after the script name
+#     if (len(non_flag_args) > 1) and (sys.argv[1] not in commands):
+#         sys.argv.insert(1, 'download')
 
 
 if __name__ == "__main__":
Index: uv.lock
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>version = 1\r\nrequires-python = \">=3.9\"\r\nresolution-markers = [\r\n    \"python_full_version < '3.13'\",\r\n    \"python_full_version >= '3.13'\",\r\n]\r\n\r\n[[package]]\r\nname = \"annotated-types\"\r\nversion = \"0.7.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz\", hash = \"sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89\", size = 16081 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl\", hash = \"sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53\", size = 13643 },\r\n]\r\n\r\n[[package]]\r\nname = \"anyio\"\r\nversion = \"4.6.2.post1\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"exceptiongroup\", marker = \"python_full_version < '3.11'\" },\r\n    { name = \"idna\" },\r\n    { name = \"sniffio\" },\r\n    { name = \"typing-extensions\", marker = \"python_full_version < '3.11'\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/9f/09/45b9b7a6d4e45c6bcb5bf61d19e3ab87df68e0601fa8c5293de3542546cc/anyio-4.6.2.post1.tar.gz\", hash = \"sha256:4c8bc31ccdb51c7f7bd251f51c609e038d63e34219b44aa86e47576389880b4c\", size = 173422 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/e4/f5/f2b75d2fc6f1a260f340f0e7c6a060f4dd2961cc16884ed851b0d18da06a/anyio-4.6.2.post1-py3-none-any.whl\", hash = \"sha256:6d170c36fba3bdd840c73d3868c1e777e33676a69c3a72cf0a0d5d6d8009b61d\", size = 90377 },\r\n]\r\n\r\n[[package]]\r\nname = \"backports-datetime-fromisoformat\"\r\nversion = \"2.0.2\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/96/05/2f1088c4c48dce684e5204e6fac41fc6fe55f884de32c0ebbec52b11b423/backports_datetime_fromisoformat-2.0.2.tar.gz\", hash = \"sha256:142313bde1f93b0ea55f20f5a6ea034f84c79713daeb252dc47d40019db3812f\", size = 23354 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/1a/13/79800812d4a71fb506e47764046887c1e19e4c7ee3a50cf54d582cc8cee3/backports_datetime_fromisoformat-2.0.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:09e70210726a70f3dd02ab9725bf2fcf469bda6d7554ea955588202e43e45b7d\", size = 27040 },\r\n    { url = \"https://files.pythonhosted.org/packages/68/2b/d21aa90015b1ff27317fb7b879bfa0fa4b20a92895bcb2a29d15e879a5b3/backports_datetime_fromisoformat-2.0.2-cp310-cp310-macosx_11_0_universal2.whl\", hash = \"sha256:ec971f93353e0ee957b3bbb037d58371331eedb9bee1b6676a866f8be97289a4\", size = 33677 },\r\n    { url = \"https://files.pythonhosted.org/packages/a4/ad/138c11f7ca8716c6c8ab2ec9aa519ec504982bd3be65992b3238cfadcb0a/backports_datetime_fromisoformat-2.0.2-cp310-cp310-macosx_11_0_x86_64.whl\", hash = \"sha256:191b0d327838eb21818e94a66b89118c086ada8f77ac9e6161980ef486fe0cbb\", size = 26582 },\r\n    { url = \"https://files.pythonhosted.org/packages/78/c9/94482ed92071334d364a82a8b6a42a3d3e2a0953500a8aa34a738436f444/backports_datetime_fromisoformat-2.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:00441807d47dec7b89acafaa6570f561c43f5c7b7934d86f101b783a365a0f0c\", size = 51909 },\r\n    { url = \"https://files.pythonhosted.org/packages/ac/1a/a405ed57fa5fbaac5af748e077fcd7d14f3f3c19820651d9fd592e008f30/backports_datetime_fromisoformat-2.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ba0af8719e161ce2fa5f5e426cceef1ff04b611c69a61636c8a7bf25d687cfa0\", size = 51850 },\r\n    { url = \"https://files.pythonhosted.org/packages/a9/06/cf898406c59e85a6cc045c36d05bf2cbb42fab4a3a69c1a736aaa7fbed9f/backports_datetime_fromisoformat-2.0.2-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:5afc32e1cdac293b054af04187d4adafcaceca99e12e5ff7807aee08074d85cb\", size = 52055 },\r\n    { url = \"https://files.pythonhosted.org/packages/c9/a1/fc0f02298333e6750300fcd97ad0d68b119699a95dcdeb459b4787b59826/backports_datetime_fromisoformat-2.0.2-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:70b044fdd274e32ece726d30b1728b4a21bc78fed0be6294091c6f04228b39ec\", size = 52069 },\r\n    { url = \"https://files.pythonhosted.org/packages/cd/13/7bd577b4d9622659b2a22340c64051582ae8cdef99a045ecf68df30817cc/backports_datetime_fromisoformat-2.0.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:6f493622b06e23e10646df7ea23e0d8350e8b1caccb5509ea82f8c3e64db32c7\", size = 28813 },\r\n    { url = \"https://files.pythonhosted.org/packages/eb/35/6191ab4b0d20166556d16e27ff4de46e0abfad70ce8480f667e8e4a35be3/backports_datetime_fromisoformat-2.0.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:55f59c88511dd15dabccf7916cbf23f8610203ac026454588084ddabf46127ee\", size = 27037 },\r\n    { url = \"https://files.pythonhosted.org/packages/d9/e2/700a6b5a0f6d9dbf28ae09e4a1af69dedb155fea6ed5ed314cc5dd8535fe/backports_datetime_fromisoformat-2.0.2-cp311-cp311-macosx_11_0_universal2.whl\", hash = \"sha256:65ca1f21319d78145456a70301396483ceebf078353641233494ea548ccc47db\", size = 33676 },\r\n    { url = \"https://files.pythonhosted.org/packages/45/35/a9260414f143f7d98d5700f9ecd265e7986657218095e68da200dcec3317/backports_datetime_fromisoformat-2.0.2-cp311-cp311-macosx_11_0_x86_64.whl\", hash = \"sha256:79fc695afd66989f28e73de0ad91019abad789045577180dd482b6ede5bdca1e\", size = 26578 },\r\n    { url = \"https://files.pythonhosted.org/packages/3b/6c/fd6a590d7e372707f047e9c18c5e31fbcb350f0644b41050a338a64f0321/backports_datetime_fromisoformat-2.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:019a87bd234734c2badb4c3e1ce4e807c5f2081f398a45a320e0c4919e5cee13\", size = 52820 },\r\n    { url = \"https://files.pythonhosted.org/packages/b2/dc/2d084c3c38bcf4a29779dd0e18c5a5e973af43ff5a0f8bf1b02060d0ec13/backports_datetime_fromisoformat-2.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ea2b77e8810b691f1dd347d5c3d4ad829d18a9e81a04a0ebbc958d431967db31\", size = 52755 },\r\n    { url = \"https://files.pythonhosted.org/packages/e8/66/cb6884a8510b3b37bfc0c6efa03dcf40a56dfe043e0c148be547b19e937d/backports_datetime_fromisoformat-2.0.2-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:944c987b777d7a81d97c94cdee2a8597bf6bdc94090094689456d3b02760cb73\", size = 52747 },\r\n    { url = \"https://files.pythonhosted.org/packages/99/d5/1fea00c9af4c052f1c4ffd4e9da18a2feb4dc223fd201d745478bd580506/backports_datetime_fromisoformat-2.0.2-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:30a2ab8c1fe4eb0013e7fcca29906fbe54e89f9120731ea71032b048dcf2fa17\", size = 52662 },\r\n    { url = \"https://files.pythonhosted.org/packages/50/9d/0b14c58d4e1084abbac9aaa794cf372a6519d2e1a82659d379eecf336de0/backports_datetime_fromisoformat-2.0.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:e23b602892827e15b1b4f94c61d4872b03b5d13417344d9a8daec80277244a32\", size = 28813 },\r\n    { url = \"https://files.pythonhosted.org/packages/3c/5a/1b1e7203420f2f11bec25574ca790d30d7f091fe18a5d0b1089bca80095a/backports_datetime_fromisoformat-2.0.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:64ec1ee18bc839847b067ab21a34a27e0d2cc4c6d041e4b05218cf6fed787740\", size = 27077 },\r\n    { url = \"https://files.pythonhosted.org/packages/fe/a9/1eb994e4afcc30754656157eebc05bfdedc2957e83802cb305878b5b70dd/backports_datetime_fromisoformat-2.0.2-cp312-cp312-macosx_11_0_universal2.whl\", hash = \"sha256:54a3df9d6ae0e64b7677b9e3bba4fc7dce3ad56a3fa6bd66fb26796f8911de67\", size = 33826 },\r\n    { url = \"https://files.pythonhosted.org/packages/fb/e0/d234d1bec10d00202d134eaef00f34d8fee50dc6e48101c8152cc4f83410/backports_datetime_fromisoformat-2.0.2-cp312-cp312-macosx_11_0_x86_64.whl\", hash = \"sha256:e54fa5663efcba6122bca037fd49220b7311e94cf6cc72e2f2a6f5d05c700bef\", size = 26683 },\r\n    { url = \"https://files.pythonhosted.org/packages/0d/23/6f35c7958754cfb3c3561c9ade296d65cd9623e797f57d14f597fb049290/backports_datetime_fromisoformat-2.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:00ecff906ed4eb19808d8e4f0b141c14a1963d3688ba318c9e00aa7da7f71301\", size = 53442 },\r\n    { url = \"https://files.pythonhosted.org/packages/b7/03/c3cf8bee660de0e1083cdafadbffa42be367d81b092b0ede132593290f1b/backports_datetime_fromisoformat-2.0.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e85f1ad56e2bcb24408e420de5508be47e54b0912ebe1325134e71837ec23a08\", size = 53995 },\r\n    { url = \"https://files.pythonhosted.org/packages/e3/e6/f30475365910acb227ea6ebbb124741f11a6c751b435642a1a154aa76385/backports_datetime_fromisoformat-2.0.2-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:36d5cbece09dff2a3f8f517f3cda64f2ccec56db07808714b1f122326cd76fbd\", size = 53766 },\r\n    { url = \"https://files.pythonhosted.org/packages/99/d9/5ba26d34e4d9ed35ff8a8846e379480e87c37c0079d3b188f6d100fb5af9/backports_datetime_fromisoformat-2.0.2-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:d47e186dcc366e6063248730a137a90de0472b2aaa5047ef39104fcacbcbcdbe\", size = 53852 },\r\n    { url = \"https://files.pythonhosted.org/packages/fc/89/148970b6a272a67c5d4003c692070298f1cd21ed25d44b8c9dd73026cdd0/backports_datetime_fromisoformat-2.0.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:3e9c81c6acc21953ffa9a627f15c4afcdbce6e456ca1d03e0d6dbf131429bd56\", size = 28891 },\r\n    { url = \"https://files.pythonhosted.org/packages/47/6d/81c8f372e84d41b901c6807c1ce802d26bed6c060a75f87533030ec92fc3/backports_datetime_fromisoformat-2.0.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:2eb563509f19e803dbbef3e4901d9553c9c3ea2b73c8d8fb85219fc57f16787a\", size = 27033 },\r\n    { url = \"https://files.pythonhosted.org/packages/ab/da/e3d4bae5624cfc41130fc247db8c232233ad6472abd63ac9a766949144d6/backports_datetime_fromisoformat-2.0.2-cp39-cp39-macosx_11_0_universal2.whl\", hash = \"sha256:d37d2f4238e0f412e56fe2c41e8e60bda93be0230d0ee846823b54254ccb95e0\", size = 33682 },\r\n    { url = \"https://files.pythonhosted.org/packages/e2/25/ebcacd3c0019720fc8005b8ab117c7ba53516231fa6a923fd6a552677dcb/backports_datetime_fromisoformat-2.0.2-cp39-cp39-macosx_11_0_x86_64.whl\", hash = \"sha256:7dcefbba71194c73b3b26593c2ea4ad254b19084d0eb83e98e2541651a692703\", size = 26573 },\r\n    { url = \"https://files.pythonhosted.org/packages/ea/e1/78bb8da7c0f3f31ab9e0739e9a56b2f458564841dfe3caba4da6889099fa/backports_datetime_fromisoformat-2.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:352f6b793cb402cc62c5b60ceab13d30c06fad1372869c716d4d07927b5c7c43\", size = 51424 },\r\n    { url = \"https://files.pythonhosted.org/packages/e0/a2/5234adcf6f8ecd4a7bfb7d52e4cab29d91eadb34114506e2496e7ed040d4/backports_datetime_fromisoformat-2.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e7d6a21b482001a9ea44f277dc21d9fb6590e543146aaabe816407d1b87cf41b\", size = 51359 },\r\n    { url = \"https://files.pythonhosted.org/packages/92/60/4cc87d888d7edd800546bb642fee62799de9094ebeb225782ffc2b0c43b2/backports_datetime_fromisoformat-2.0.2-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:f97285e80ea192357380cfd2fb2dce056ec65672597172f3af549dcf5d019b1e\", size = 51627 },\r\n    { url = \"https://files.pythonhosted.org/packages/a6/ba/4cb2d1ec301ad8edd90c743af104e22cff902696c86a663e3490792cf173/backports_datetime_fromisoformat-2.0.2-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:5a5cfff34bf80f0cd2771da88bd898be1fa60250d6f2dd9e4a59885dbcb7aa7c\", size = 51623 },\r\n    { url = \"https://files.pythonhosted.org/packages/e5/4b/b16698da2a3297f393ce5887f11ea5fda0e4caf69082d28729c01f0e7ff4/backports_datetime_fromisoformat-2.0.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:ed392607d457b1ed50a88dcaf459e11d81c30a2f2d8dab818a1564de6897e76f\", size = 28839 },\r\n    { url = \"https://files.pythonhosted.org/packages/83/ca/488a5dbc432f5bc8ba60410ee5d5e98984a6f0f9f47e2114d6dbc56dc0c7/backports_datetime_fromisoformat-2.0.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:cb0f24d2c596991e39dfaa60c685b8c69bc9b1da77e9baf2c453882adeec483b\", size = 28573 },\r\n    { url = \"https://files.pythonhosted.org/packages/bf/9c/effca58f1f83cc11a0341c5edcea6d642f04f9212f50cc9ff930c585779d/backports_datetime_fromisoformat-2.0.2-pp310-pypy310_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0083552588270acfaa31ac8de81b29786a1515d7608ff11ccdfcdffc2486212e\", size = 28079 },\r\n    { url = \"https://files.pythonhosted.org/packages/88/7f/207736bd2fd5d525d20129d78e9f4315507a78432a9fe985a4cb2cc41b92/backports_datetime_fromisoformat-2.0.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a94d5a7cf9cdee221b7721544f424c69747a04091cbff53aa6ae8454644b59f9\", size = 28577 },\r\n    { url = \"https://files.pythonhosted.org/packages/61/5a/1b4135c2cf31da12dd8b46d3d9c6e9c9aa066c82f8c35ee5eba448d277bd/backports_datetime_fromisoformat-2.0.2-pp39-pypy39_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6a5e4c77a91db6f434c2eec46c0199d3617c19c812f0c74f7ed8e0f9779da9f0\", size = 28076 },\r\n]\r\n\r\n[[package]]\r\nname = \"certifi\"\r\nversion = \"2024.8.30\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/b0/ee/9b19140fe824b367c04c5e1b369942dd754c4c5462d5674002f75c4dedc1/certifi-2024.8.30.tar.gz\", hash = \"sha256:bec941d2aa8195e248a60b31ff9f0558284cf01a52591ceda73ea9afffd69fd9\", size = 168507 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl\", hash = \"sha256:922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8\", size = 167321 },\r\n]\r\n\r\n[[package]]\r\nname = \"exceptiongroup\"\r\nversion = \"1.2.2\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/09/35/2495c4ac46b980e4ca1f6ad6db102322ef3ad2410b79fdde159a4b0f3b92/exceptiongroup-1.2.2.tar.gz\", hash = \"sha256:47c2edf7c6738fafb49fd34290706d1a1a2f4d1c6df275526b62cbb4aa5393cc\", size = 28883 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl\", hash = \"sha256:3111b9d131c238bec2f8f516e123e14ba243563fb135d3fe885990585aa7795b\", size = 16453 },\r\n]\r\n\r\n[[package]]\r\nname = \"h11\"\r\nversion = \"0.14.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/f5/38/3af3d3633a34a3316095b39c8e8fb4853a28a536e55d347bd8d8e9a14b03/h11-0.14.0.tar.gz\", hash = \"sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d\", size = 100418 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl\", hash = \"sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761\", size = 58259 },\r\n]\r\n\r\n[[package]]\r\nname = \"h2\"\r\nversion = \"4.1.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"hpack\" },\r\n    { name = \"hyperframe\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/2a/32/fec683ddd10629ea4ea46d206752a95a2d8a48c22521edd70b142488efe1/h2-4.1.0.tar.gz\", hash = \"sha256:a83aca08fbe7aacb79fec788c9c0bac936343560ed9ec18b82a13a12c28d2abb\", size = 2145593 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/2a/e5/db6d438da759efbb488c4f3fbdab7764492ff3c3f953132efa6b9f0e9e53/h2-4.1.0-py3-none-any.whl\", hash = \"sha256:03a46bcf682256c95b5fd9e9a99c1323584c3eec6440d379b9903d709476bc6d\", size = 57488 },\r\n]\r\n\r\n[[package]]\r\nname = \"hpack\"\r\nversion = \"4.0.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/3e/9b/fda93fb4d957db19b0f6b370e79d586b3e8528b20252c729c476a2c02954/hpack-4.0.0.tar.gz\", hash = \"sha256:fc41de0c63e687ebffde81187a948221294896f6bdc0ae2312708df339430095\", size = 49117 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/d5/34/e8b383f35b77c402d28563d2b8f83159319b509bc5f760b15d60b0abf165/hpack-4.0.0-py3-none-any.whl\", hash = \"sha256:84a076fad3dc9a9f8063ccb8041ef100867b1878b25ef0ee63847a5d53818a6c\", size = 32611 },\r\n]\r\n\r\n[[package]]\r\nname = \"httpcore\"\r\nversion = \"1.0.6\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"certifi\" },\r\n    { name = \"h11\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/b6/44/ed0fa6a17845fb033bd885c03e842f08c1b9406c86a2e60ac1ae1b9206a6/httpcore-1.0.6.tar.gz\", hash = \"sha256:73f6dbd6eb8c21bbf7ef8efad555481853f5f6acdeaff1edb0694289269ee17f\", size = 85180 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/06/89/b161908e2f51be56568184aeb4a880fd287178d176fd1c860d2217f41106/httpcore-1.0.6-py3-none-any.whl\", hash = \"sha256:27b59625743b85577a8c0e10e55b50b5368a4f2cfe8cc7bcfa9cf00829c2682f\", size = 78011 },\r\n]\r\n\r\n[[package]]\r\nname = \"httpx\"\r\nversion = \"0.27.2\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"anyio\" },\r\n    { name = \"certifi\" },\r\n    { name = \"httpcore\" },\r\n    { name = \"idna\" },\r\n    { name = \"sniffio\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/78/82/08f8c936781f67d9e6b9eeb8a0c8b4e406136ea4c3d1f89a5db71d42e0e6/httpx-0.27.2.tar.gz\", hash = \"sha256:f7c2be1d2f3c3c3160d441802406b206c2b76f5947b11115e6df10c6c65e66c2\", size = 144189 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl\", hash = \"sha256:7bb2708e112d8fdd7829cd4243970f0c223274051cb35ee80c03301ee29a3df0\", size = 76395 },\r\n]\r\n\r\n[package.optional-dependencies]\r\nhttp2 = [\r\n    { name = \"h2\" },\r\n]\r\n\r\n[[package]]\r\nname = \"hyperframe\"\r\nversion = \"6.0.1\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/5a/2a/4747bff0a17f7281abe73e955d60d80aae537a5d203f417fa1c2e7578ebb/hyperframe-6.0.1.tar.gz\", hash = \"sha256:ae510046231dc8e9ecb1a6586f63d2347bf4c8905914aa84ba585ae85f28a914\", size = 25008 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/d7/de/85a784bcc4a3779d1753a7ec2dee5de90e18c7bcf402e71b51fcf150b129/hyperframe-6.0.1-py3-none-any.whl\", hash = \"sha256:0ec6bafd80d8ad2195c4f03aacba3a8265e57bc4cff261e802bf39970ed02a15\", size = 12389 },\r\n]\r\n\r\n[[package]]\r\nname = \"idna\"\r\nversion = \"3.10\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz\", hash = \"sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9\", size = 190490 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl\", hash = \"sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3\", size = 70442 },\r\n]\r\n\r\n[[package]]\r\nname = \"isubrip\"\r\nversion = \"2.5.6\"\r\nsource = { editable = \".\" }\r\ndependencies = [\r\n    { name = \"httpx\", extra = [\"http2\"] },\r\n    { name = \"m3u8\" },\r\n    { name = \"pydantic\" },\r\n    { name = \"pydantic-settings\" },\r\n    { name = \"tomli\" },\r\n]\r\n\r\n[package.dev-dependencies]\r\ndev = [\r\n    { name = \"mypy\" },\r\n    { name = \"ruff\" },\r\n]\r\n\r\n[package.metadata]\r\nrequires-dist = [\r\n    { name = \"httpx\", extras = [\"http2\"], specifier = \">=0.27.2\" },\r\n    { name = \"m3u8\", specifier = \">=6.0.0\" },\r\n    { name = \"pydantic\", specifier = \">=2.9.2\" },\r\n    { name = \"pydantic-settings\", specifier = \">=2.6.0\" },\r\n    { name = \"tomli\", specifier = \">=2.0.2\" },\r\n]\r\n\r\n[package.metadata.requires-dev]\r\ndev = [\r\n    { name = \"mypy\", specifier = \">=1.13.0\" },\r\n    { name = \"ruff\", specifier = \">=0.7.1\" },\r\n]\r\n\r\n[[package]]\r\nname = \"m3u8\"\r\nversion = \"6.0.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"backports-datetime-fromisoformat\", marker = \"python_full_version < '3.11'\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/9b/a5/73697aaa99bb32b610adc1f11d46a0c0c370351292e9b271755084a145e6/m3u8-6.0.0.tar.gz\", hash = \"sha256:7ade990a1667d7a653bcaf9413b16c3eb5cd618982ff46aaff57fe6d9fa9c0fd\", size = 42720 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/f8/31/50f3c38b38ff28635ff9c4a4afefddccc5f1b57457b539bdbdf75ce18669/m3u8-6.0.0-py3-none-any.whl\", hash = \"sha256:566d0748739c552dad10f8c87150078de6a0ec25071fa48e6968e96fc6dcba5d\", size = 24133 },\r\n]\r\n\r\n[[package]]\r\nname = \"mypy\"\r\nversion = \"1.13.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"mypy-extensions\" },\r\n    { name = \"tomli\", marker = \"python_full_version < '3.11'\" },\r\n    { name = \"typing-extensions\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/e8/21/7e9e523537991d145ab8a0a2fd98548d67646dc2aaaf6091c31ad883e7c1/mypy-1.13.0.tar.gz\", hash = \"sha256:0291a61b6fbf3e6673e3405cfcc0e7650bebc7939659fdca2702958038bd835e\", size = 3152532 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/5e/8c/206de95a27722b5b5a8c85ba3100467bd86299d92a4f71c6b9aa448bfa2f/mypy-1.13.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:6607e0f1dd1fb7f0aca14d936d13fd19eba5e17e1cd2a14f808fa5f8f6d8f60a\", size = 11020731 },\r\n    { url = \"https://files.pythonhosted.org/packages/ab/bb/b31695a29eea76b1569fd28b4ab141a1adc9842edde080d1e8e1776862c7/mypy-1.13.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:8a21be69bd26fa81b1f80a61ee7ab05b076c674d9b18fb56239d72e21d9f4c80\", size = 10184276 },\r\n    { url = \"https://files.pythonhosted.org/packages/a5/2d/4a23849729bb27934a0e079c9c1aad912167d875c7b070382a408d459651/mypy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:7b2353a44d2179846a096e25691d54d59904559f4232519d420d64da6828a3a7\", size = 12587706 },\r\n    { url = \"https://files.pythonhosted.org/packages/5c/c3/d318e38ada50255e22e23353a469c791379825240e71b0ad03e76ca07ae6/mypy-1.13.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:0730d1c6a2739d4511dc4253f8274cdd140c55c32dfb0a4cf8b7a43f40abfa6f\", size = 13105586 },\r\n    { url = \"https://files.pythonhosted.org/packages/4a/25/3918bc64952370c3dbdbd8c82c363804678127815febd2925b7273d9482c/mypy-1.13.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:c5fc54dbb712ff5e5a0fca797e6e0aa25726c7e72c6a5850cfd2adbc1eb0a372\", size = 9632318 },\r\n    { url = \"https://files.pythonhosted.org/packages/d0/19/de0822609e5b93d02579075248c7aa6ceaddcea92f00bf4ea8e4c22e3598/mypy-1.13.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:581665e6f3a8a9078f28d5502f4c334c0c8d802ef55ea0e7276a6e409bc0d82d\", size = 10939027 },\r\n    { url = \"https://files.pythonhosted.org/packages/c8/71/6950fcc6ca84179137e4cbf7cf41e6b68b4a339a1f5d3e954f8c34e02d66/mypy-1.13.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:3ddb5b9bf82e05cc9a627e84707b528e5c7caaa1c55c69e175abb15a761cec2d\", size = 10108699 },\r\n    { url = \"https://files.pythonhosted.org/packages/26/50/29d3e7dd166e74dc13d46050b23f7d6d7533acf48f5217663a3719db024e/mypy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:20c7ee0bc0d5a9595c46f38beb04201f2620065a93755704e141fcac9f59db2b\", size = 12506263 },\r\n    { url = \"https://files.pythonhosted.org/packages/3f/1d/676e76f07f7d5ddcd4227af3938a9c9640f293b7d8a44dd4ff41d4db25c1/mypy-1.13.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:3790ded76f0b34bc9c8ba4def8f919dd6a46db0f5a6610fb994fe8efdd447f73\", size = 12984688 },\r\n    { url = \"https://files.pythonhosted.org/packages/9c/03/5a85a30ae5407b1d28fab51bd3e2103e52ad0918d1e68f02a7778669a307/mypy-1.13.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:51f869f4b6b538229c1d1bcc1dd7d119817206e2bc54e8e374b3dfa202defcca\", size = 9626811 },\r\n    { url = \"https://files.pythonhosted.org/packages/fb/31/c526a7bd2e5c710ae47717c7a5f53f616db6d9097caf48ad650581e81748/mypy-1.13.0-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:5c7051a3461ae84dfb5dd15eff5094640c61c5f22257c8b766794e6dd85e72d5\", size = 11077900 },\r\n    { url = \"https://files.pythonhosted.org/packages/83/67/b7419c6b503679d10bd26fc67529bc6a1f7a5f220bbb9f292dc10d33352f/mypy-1.13.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:39bb21c69a5d6342f4ce526e4584bc5c197fd20a60d14a8624d8743fffb9472e\", size = 10074818 },\r\n    { url = \"https://files.pythonhosted.org/packages/ba/07/37d67048786ae84e6612575e173d713c9a05d0ae495dde1e68d972207d98/mypy-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:164f28cb9d6367439031f4c81e84d3ccaa1e19232d9d05d37cb0bd880d3f93c2\", size = 12589275 },\r\n    { url = \"https://files.pythonhosted.org/packages/1f/17/b1018c6bb3e9f1ce3956722b3bf91bff86c1cefccca71cec05eae49d6d41/mypy-1.13.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:a4c1bfcdbce96ff5d96fc9b08e3831acb30dc44ab02671eca5953eadad07d6d0\", size = 13037783 },\r\n    { url = \"https://files.pythonhosted.org/packages/cb/32/cd540755579e54a88099aee0287086d996f5a24281a673f78a0e14dba150/mypy-1.13.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:a0affb3a79a256b4183ba09811e3577c5163ed06685e4d4b46429a271ba174d2\", size = 9726197 },\r\n    { url = \"https://files.pythonhosted.org/packages/11/bb/ab4cfdc562cad80418f077d8be9b4491ee4fb257440da951b85cbb0a639e/mypy-1.13.0-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:a7b44178c9760ce1a43f544e595d35ed61ac2c3de306599fa59b38a6048e1aa7\", size = 11069721 },\r\n    { url = \"https://files.pythonhosted.org/packages/59/3b/a393b1607cb749ea2c621def5ba8c58308ff05e30d9dbdc7c15028bca111/mypy-1.13.0-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:5d5092efb8516d08440e36626f0153b5006d4088c1d663d88bf79625af3d1d62\", size = 10063996 },\r\n    { url = \"https://files.pythonhosted.org/packages/d1/1f/6b76be289a5a521bb1caedc1f08e76ff17ab59061007f201a8a18cc514d1/mypy-1.13.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:de2904956dac40ced10931ac967ae63c5089bd498542194b436eb097a9f77bc8\", size = 12584043 },\r\n    { url = \"https://files.pythonhosted.org/packages/a6/83/5a85c9a5976c6f96e3a5a7591aa28b4a6ca3a07e9e5ba0cec090c8b596d6/mypy-1.13.0-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:7bfd8836970d33c2105562650656b6846149374dc8ed77d98424b40b09340ba7\", size = 13036996 },\r\n    { url = \"https://files.pythonhosted.org/packages/b4/59/c39a6f752f1f893fccbcf1bdd2aca67c79c842402b5283563d006a67cf76/mypy-1.13.0-cp313-cp313-win_amd64.whl\", hash = \"sha256:9f73dba9ec77acb86457a8fc04b5239822df0c14a082564737833d2963677dbc\", size = 9737709 },\r\n    { url = \"https://files.pythonhosted.org/packages/5f/d4/b33ddd40dad230efb317898a2d1c267c04edba73bc5086bf77edeb410fb2/mypy-1.13.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:0246bcb1b5de7f08f2826451abd947bf656945209b140d16ed317f65a17dc7dc\", size = 11013906 },\r\n    { url = \"https://files.pythonhosted.org/packages/f4/e6/f414bca465b44d01cd5f4a82761e15044bedd1bf8025c5af3cc64518fac5/mypy-1.13.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:7f5b7deae912cf8b77e990b9280f170381fdfbddf61b4ef80927edd813163732\", size = 10180657 },\r\n    { url = \"https://files.pythonhosted.org/packages/38/e9/fc3865e417722f98d58409770be01afb961e2c1f99930659ff4ae7ca8b7e/mypy-1.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:7029881ec6ffb8bc233a4fa364736789582c738217b133f1b55967115288a2bc\", size = 12586394 },\r\n    { url = \"https://files.pythonhosted.org/packages/2e/35/f4d8b6d2cb0b3dad63e96caf159419dda023f45a358c6c9ac582ccaee354/mypy-1.13.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:3e38b980e5681f28f033f3be86b099a247b13c491f14bb8b1e1e134d23bb599d\", size = 13103591 },\r\n    { url = \"https://files.pythonhosted.org/packages/22/1d/80594aef135f921dd52e142fa0acd19df197690bd0cde42cea7b88cf5aa2/mypy-1.13.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:a6789be98a2017c912ae6ccb77ea553bbaf13d27605d2ca20a76dfbced631b24\", size = 9634690 },\r\n    { url = \"https://files.pythonhosted.org/packages/3b/86/72ce7f57431d87a7ff17d442f521146a6585019eb8f4f31b7c02801f78ad/mypy-1.13.0-py3-none-any.whl\", hash = \"sha256:9c250883f9fd81d212e0952c92dbfcc96fc237f4b7c92f56ac81fd48460b3e5a\", size = 2647043 },\r\n]\r\n\r\n[[package]]\r\nname = \"mypy-extensions\"\r\nversion = \"1.0.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/98/a4/1ab47638b92648243faf97a5aeb6ea83059cc3624972ab6b8d2316078d3f/mypy_extensions-1.0.0.tar.gz\", hash = \"sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782\", size = 4433 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/2a/e2/5d3f6ada4297caebe1a2add3b126fe800c96f56dbe5d1988a2cbe0b267aa/mypy_extensions-1.0.0-py3-none-any.whl\", hash = \"sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d\", size = 4695 },\r\n]\r\n\r\n[[package]]\r\nname = \"pydantic\"\r\nversion = \"2.9.2\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"annotated-types\" },\r\n    { name = \"pydantic-core\" },\r\n    { name = \"typing-extensions\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/a9/b7/d9e3f12af310e1120c21603644a1cd86f59060e040ec5c3a80b8f05fae30/pydantic-2.9.2.tar.gz\", hash = \"sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f\", size = 769917 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl\", hash = \"sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\", size = 434928 },\r\n]\r\n\r\n[[package]]\r\nname = \"pydantic-core\"\r\nversion = \"2.23.4\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"typing-extensions\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/e2/aa/6b6a9b9f8537b872f552ddd46dd3da230367754b6f707b8e1e963f515ea3/pydantic_core-2.23.4.tar.gz\", hash = \"sha256:2584f7cf844ac4d970fba483a717dbe10c1c1c96a969bf65d61ffe94df1b2863\", size = 402156 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/5c/8b/d3ae387f66277bd8104096d6ec0a145f4baa2966ebb2cad746c0920c9526/pydantic_core-2.23.4-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:b10bd51f823d891193d4717448fab065733958bdb6a6b351967bd349d48d5c9b\", size = 1867835 },\r\n    { url = \"https://files.pythonhosted.org/packages/46/76/f68272e4c3a7df8777798282c5e47d508274917f29992d84e1898f8908c7/pydantic_core-2.23.4-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:4fc714bdbfb534f94034efaa6eadd74e5b93c8fa6315565a222f7b6f42ca1166\", size = 1776689 },\r\n    { url = \"https://files.pythonhosted.org/packages/cc/69/5f945b4416f42ea3f3bc9d2aaec66c76084a6ff4ff27555bf9415ab43189/pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:63e46b3169866bd62849936de036f901a9356e36376079b05efa83caeaa02ceb\", size = 1800748 },\r\n    { url = \"https://files.pythonhosted.org/packages/50/ab/891a7b0054bcc297fb02d44d05c50e68154e31788f2d9d41d0b72c89fdf7/pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:ed1a53de42fbe34853ba90513cea21673481cd81ed1be739f7f2efb931b24916\", size = 1806469 },\r\n    { url = \"https://files.pythonhosted.org/packages/31/7c/6e3fa122075d78f277a8431c4c608f061881b76c2b7faca01d317ee39b5d/pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:cfdd16ab5e59fc31b5e906d1a3f666571abc367598e3e02c83403acabc092e07\", size = 2002246 },\r\n    { url = \"https://files.pythonhosted.org/packages/ad/6f/22d5692b7ab63fc4acbc74de6ff61d185804a83160adba5e6cc6068e1128/pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:255a8ef062cbf6674450e668482456abac99a5583bbafb73f9ad469540a3a232\", size = 2659404 },\r\n    { url = \"https://files.pythonhosted.org/packages/11/ac/1e647dc1121c028b691028fa61a4e7477e6aeb5132628fde41dd34c1671f/pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4a7cd62e831afe623fbb7aabbb4fe583212115b3ef38a9f6b71869ba644624a2\", size = 2053940 },\r\n    { url = \"https://files.pythonhosted.org/packages/91/75/984740c17f12c3ce18b5a2fcc4bdceb785cce7df1511a4ce89bca17c7e2d/pydantic_core-2.23.4-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:f09e2ff1f17c2b51f2bc76d1cc33da96298f0a036a137f5440ab3ec5360b624f\", size = 1921437 },\r\n    { url = \"https://files.pythonhosted.org/packages/a0/74/13c5f606b64d93f0721e7768cd3e8b2102164866c207b8cd6f90bb15d24f/pydantic_core-2.23.4-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:e38e63e6f3d1cec5a27e0afe90a085af8b6806ee208b33030e65b6516353f1a3\", size = 1966129 },\r\n    { url = \"https://files.pythonhosted.org/packages/18/03/9c4aa5919457c7b57a016c1ab513b1a926ed9b2bb7915bf8e506bf65c34b/pydantic_core-2.23.4-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:0dbd8dbed2085ed23b5c04afa29d8fd2771674223135dc9bc937f3c09284d071\", size = 2110908 },\r\n    { url = \"https://files.pythonhosted.org/packages/92/2c/053d33f029c5dc65e5cf44ff03ceeefb7cce908f8f3cca9265e7f9b540c8/pydantic_core-2.23.4-cp310-none-win32.whl\", hash = \"sha256:6531b7ca5f951d663c339002e91aaebda765ec7d61b7d1e3991051906ddde119\", size = 1735278 },\r\n    { url = \"https://files.pythonhosted.org/packages/de/81/7dfe464eca78d76d31dd661b04b5f2036ec72ea8848dd87ab7375e185c23/pydantic_core-2.23.4-cp310-none-win_amd64.whl\", hash = \"sha256:7c9129eb40958b3d4500fa2467e6a83356b3b61bfff1b414c7361d9220f9ae8f\", size = 1917453 },\r\n    { url = \"https://files.pythonhosted.org/packages/5d/30/890a583cd3f2be27ecf32b479d5d615710bb926d92da03e3f7838ff3e58b/pydantic_core-2.23.4-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:77733e3892bb0a7fa797826361ce8a9184d25c8dffaec60b7ffe928153680ba8\", size = 1865160 },\r\n    { url = \"https://files.pythonhosted.org/packages/1d/9a/b634442e1253bc6889c87afe8bb59447f106ee042140bd57680b3b113ec7/pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:1b84d168f6c48fabd1f2027a3d1bdfe62f92cade1fb273a5d68e621da0e44e6d\", size = 1776777 },\r\n    { url = \"https://files.pythonhosted.org/packages/75/9a/7816295124a6b08c24c96f9ce73085032d8bcbaf7e5a781cd41aa910c891/pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:df49e7a0861a8c36d089c1ed57d308623d60416dab2647a4a17fe050ba85de0e\", size = 1799244 },\r\n    { url = \"https://files.pythonhosted.org/packages/a9/8f/89c1405176903e567c5f99ec53387449e62f1121894aa9fc2c4fdc51a59b/pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:ff02b6d461a6de369f07ec15e465a88895f3223eb75073ffea56b84d9331f607\", size = 1805307 },\r\n    { url = \"https://files.pythonhosted.org/packages/d5/a5/1a194447d0da1ef492e3470680c66048fef56fc1f1a25cafbea4bc1d1c48/pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:996a38a83508c54c78a5f41456b0103c30508fed9abcad0a59b876d7398f25fd\", size = 2000663 },\r\n    { url = \"https://files.pythonhosted.org/packages/13/a5/1df8541651de4455e7d587cf556201b4f7997191e110bca3b589218745a5/pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d97683ddee4723ae8c95d1eddac7c192e8c552da0c73a925a89fa8649bf13eea\", size = 2655941 },\r\n    { url = \"https://files.pythonhosted.org/packages/44/31/a3899b5ce02c4316865e390107f145089876dff7e1dfc770a231d836aed8/pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:216f9b2d7713eb98cb83c80b9c794de1f6b7e3145eef40400c62e86cee5f4e1e\", size = 2052105 },\r\n    { url = \"https://files.pythonhosted.org/packages/1b/aa/98e190f8745d5ec831f6d5449344c48c0627ac5fed4e5340a44b74878f8e/pydantic_core-2.23.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:6f783e0ec4803c787bcea93e13e9932edab72068f68ecffdf86a99fd5918878b\", size = 1919967 },\r\n    { url = \"https://files.pythonhosted.org/packages/ae/35/b6e00b6abb2acfee3e8f85558c02a0822e9a8b2f2d812ea8b9079b118ba0/pydantic_core-2.23.4-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:d0776dea117cf5272382634bd2a5c1b6eb16767c223c6a5317cd3e2a757c61a0\", size = 1964291 },\r\n    { url = \"https://files.pythonhosted.org/packages/13/46/7bee6d32b69191cd649bbbd2361af79c472d72cb29bb2024f0b6e350ba06/pydantic_core-2.23.4-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:d5f7a395a8cf1621939692dba2a6b6a830efa6b3cee787d82c7de1ad2930de64\", size = 2109666 },\r\n    { url = \"https://files.pythonhosted.org/packages/39/ef/7b34f1b122a81b68ed0a7d0e564da9ccdc9a2924c8d6c6b5b11fa3a56970/pydantic_core-2.23.4-cp311-none-win32.whl\", hash = \"sha256:74b9127ffea03643e998e0c5ad9bd3811d3dac8c676e47db17b0ee7c3c3bf35f\", size = 1732940 },\r\n    { url = \"https://files.pythonhosted.org/packages/2f/76/37b7e76c645843ff46c1d73e046207311ef298d3f7b2f7d8f6ac60113071/pydantic_core-2.23.4-cp311-none-win_amd64.whl\", hash = \"sha256:98d134c954828488b153d88ba1f34e14259284f256180ce659e8d83e9c05eaa3\", size = 1916804 },\r\n    { url = \"https://files.pythonhosted.org/packages/74/7b/8e315f80666194b354966ec84b7d567da77ad927ed6323db4006cf915f3f/pydantic_core-2.23.4-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:f3e0da4ebaef65158d4dfd7d3678aad692f7666877df0002b8a522cdf088f231\", size = 1856459 },\r\n    { url = \"https://files.pythonhosted.org/packages/14/de/866bdce10ed808323d437612aca1ec9971b981e1c52e5e42ad9b8e17a6f6/pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:f69a8e0b033b747bb3e36a44e7732f0c99f7edd5cea723d45bc0d6e95377ffee\", size = 1770007 },\r\n    { url = \"https://files.pythonhosted.org/packages/dc/69/8edd5c3cd48bb833a3f7ef9b81d7666ccddd3c9a635225214e044b6e8281/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:723314c1d51722ab28bfcd5240d858512ffd3116449c557a1336cbe3919beb87\", size = 1790245 },\r\n    { url = \"https://files.pythonhosted.org/packages/80/33/9c24334e3af796ce80d2274940aae38dd4e5676298b4398eff103a79e02d/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:bb2802e667b7051a1bebbfe93684841cc9351004e2badbd6411bf357ab8d5ac8\", size = 1801260 },\r\n    { url = \"https://files.pythonhosted.org/packages/a5/6f/e9567fd90104b79b101ca9d120219644d3314962caa7948dd8b965e9f83e/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:d18ca8148bebe1b0a382a27a8ee60350091a6ddaf475fa05ef50dc35b5df6327\", size = 1996872 },\r\n    { url = \"https://files.pythonhosted.org/packages/2d/ad/b5f0fe9e6cfee915dd144edbd10b6e9c9c9c9d7a56b69256d124b8ac682e/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:33e3d65a85a2a4a0dc3b092b938a4062b1a05f3a9abde65ea93b233bca0e03f2\", size = 2661617 },\r\n    { url = \"https://files.pythonhosted.org/packages/06/c8/7d4b708f8d05a5cbfda3243aad468052c6e99de7d0937c9146c24d9f12e9/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:128585782e5bfa515c590ccee4b727fb76925dd04a98864182b22e89a4e6ed36\", size = 2071831 },\r\n    { url = \"https://files.pythonhosted.org/packages/89/4d/3079d00c47f22c9a9a8220db088b309ad6e600a73d7a69473e3a8e5e3ea3/pydantic_core-2.23.4-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:68665f4c17edcceecc112dfed5dbe6f92261fb9d6054b47d01bf6371a6196126\", size = 1917453 },\r\n    { url = \"https://files.pythonhosted.org/packages/e9/88/9df5b7ce880a4703fcc2d76c8c2d8eb9f861f79d0c56f4b8f5f2607ccec8/pydantic_core-2.23.4-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:20152074317d9bed6b7a95ade3b7d6054845d70584216160860425f4fbd5ee9e\", size = 1968793 },\r\n    { url = \"https://files.pythonhosted.org/packages/e3/b9/41f7efe80f6ce2ed3ee3c2dcfe10ab7adc1172f778cc9659509a79518c43/pydantic_core-2.23.4-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:9261d3ce84fa1d38ed649c3638feefeae23d32ba9182963e465d58d62203bd24\", size = 2116872 },\r\n    { url = \"https://files.pythonhosted.org/packages/63/08/b59b7a92e03dd25554b0436554bf23e7c29abae7cce4b1c459cd92746811/pydantic_core-2.23.4-cp312-none-win32.whl\", hash = \"sha256:4ba762ed58e8d68657fc1281e9bb72e1c3e79cc5d464be146e260c541ec12d84\", size = 1738535 },\r\n    { url = \"https://files.pythonhosted.org/packages/88/8d/479293e4d39ab409747926eec4329de5b7129beaedc3786eca070605d07f/pydantic_core-2.23.4-cp312-none-win_amd64.whl\", hash = \"sha256:97df63000f4fea395b2824da80e169731088656d1818a11b95f3b173747b6cd9\", size = 1917992 },\r\n    { url = \"https://files.pythonhosted.org/packages/ad/ef/16ee2df472bf0e419b6bc68c05bf0145c49247a1095e85cee1463c6a44a1/pydantic_core-2.23.4-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:7530e201d10d7d14abce4fb54cfe5b94a0aefc87da539d0346a484ead376c3cc\", size = 1856143 },\r\n    { url = \"https://files.pythonhosted.org/packages/da/fa/bc3dbb83605669a34a93308e297ab22be82dfb9dcf88c6cf4b4f264e0a42/pydantic_core-2.23.4-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:df933278128ea1cd77772673c73954e53a1c95a4fdf41eef97c2b779271bd0bd\", size = 1770063 },\r\n    { url = \"https://files.pythonhosted.org/packages/4e/48/e813f3bbd257a712303ebdf55c8dc46f9589ec74b384c9f652597df3288d/pydantic_core-2.23.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0cb3da3fd1b6a5d0279a01877713dbda118a2a4fc6f0d821a57da2e464793f05\", size = 1790013 },\r\n    { url = \"https://files.pythonhosted.org/packages/b4/e0/56eda3a37929a1d297fcab1966db8c339023bcca0b64c5a84896db3fcc5c/pydantic_core-2.23.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:42c6dcb030aefb668a2b7009c85b27f90e51e6a3b4d5c9bc4c57631292015b0d\", size = 1801077 },\r\n    { url = \"https://files.pythonhosted.org/packages/04/be/5e49376769bfbf82486da6c5c1683b891809365c20d7c7e52792ce4c71f3/pydantic_core-2.23.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:696dd8d674d6ce621ab9d45b205df149399e4bb9aa34102c970b721554828510\", size = 1996782 },\r\n    { url = \"https://files.pythonhosted.org/packages/bc/24/e3ee6c04f1d58cc15f37bcc62f32c7478ff55142b7b3e6d42ea374ea427c/pydantic_core-2.23.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2971bb5ffe72cc0f555c13e19b23c85b654dd2a8f7ab493c262071377bfce9f6\", size = 2661375 },\r\n    { url = \"https://files.pythonhosted.org/packages/c1/f8/11a9006de4e89d016b8de74ebb1db727dc100608bb1e6bbe9d56a3cbbcce/pydantic_core-2.23.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8394d940e5d400d04cad4f75c0598665cbb81aecefaca82ca85bd28264af7f9b\", size = 2071635 },\r\n    { url = \"https://files.pythonhosted.org/packages/7c/45/bdce5779b59f468bdf262a5bc9eecbae87f271c51aef628d8c073b4b4b4c/pydantic_core-2.23.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:0dff76e0602ca7d4cdaacc1ac4c005e0ce0dcfe095d5b5259163a80d3a10d327\", size = 1916994 },\r\n    { url = \"https://files.pythonhosted.org/packages/d8/fa/c648308fe711ee1f88192cad6026ab4f925396d1293e8356de7e55be89b5/pydantic_core-2.23.4-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:7d32706badfe136888bdea71c0def994644e09fff0bfe47441deaed8e96fdbc6\", size = 1968877 },\r\n    { url = \"https://files.pythonhosted.org/packages/16/16/b805c74b35607d24d37103007f899abc4880923b04929547ae68d478b7f4/pydantic_core-2.23.4-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:ed541d70698978a20eb63d8c5d72f2cc6d7079d9d90f6b50bad07826f1320f5f\", size = 2116814 },\r\n    { url = \"https://files.pythonhosted.org/packages/d1/58/5305e723d9fcdf1c5a655e6a4cc2a07128bf644ff4b1d98daf7a9dbf57da/pydantic_core-2.23.4-cp313-none-win32.whl\", hash = \"sha256:3d5639516376dce1940ea36edf408c554475369f5da2abd45d44621cb616f769\", size = 1738360 },\r\n    { url = \"https://files.pythonhosted.org/packages/a5/ae/e14b0ff8b3f48e02394d8acd911376b7b66e164535687ef7dc24ea03072f/pydantic_core-2.23.4-cp313-none-win_amd64.whl\", hash = \"sha256:5a1504ad17ba4210df3a045132a7baeeba5a200e930f57512ee02909fc5c4cb5\", size = 1919411 },\r\n    { url = \"https://files.pythonhosted.org/packages/7a/04/2580b2deaae37b3e30fc30c54298be938b973990b23612d6b61c7bdd01c7/pydantic_core-2.23.4-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:a4fa4fc04dff799089689f4fd502ce7d59de529fc2f40a2c8836886c03e0175a\", size = 1868200 },\r\n    { url = \"https://files.pythonhosted.org/packages/39/6e/e311bd0751505350f0cdcee3077841eb1f9253c5a1ddbad048cd9fbf7c6e/pydantic_core-2.23.4-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:0a7df63886be5e270da67e0966cf4afbae86069501d35c8c1b3b6c168f42cb36\", size = 1749316 },\r\n    { url = \"https://files.pythonhosted.org/packages/d0/b4/95b5eb47c6dc8692508c3ca04a1f8d6f0884c9dacb34cf3357595cbe73be/pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:dcedcd19a557e182628afa1d553c3895a9f825b936415d0dbd3cd0bbcfd29b4b\", size = 1800880 },\r\n    { url = \"https://files.pythonhosted.org/packages/da/79/41c4f817acd7f42d94cd1e16526c062a7b089f66faed4bd30852314d9a66/pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:5f54b118ce5de9ac21c363d9b3caa6c800341e8c47a508787e5868c6b79c9323\", size = 1807077 },\r\n    { url = \"https://files.pythonhosted.org/packages/fb/53/d13d1eb0a97d5c06cf7a225935d471e9c241afd389a333f40c703f214973/pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:86d2f57d3e1379a9525c5ab067b27dbb8a0642fb5d454e17a9ac434f9ce523e3\", size = 2002859 },\r\n    { url = \"https://files.pythonhosted.org/packages/53/7d/6b8a1eff453774b46cac8c849e99455b27167971a003212f668e94bc4c9c/pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:de6d1d1b9e5101508cb37ab0d972357cac5235f5c6533d1071964c47139257df\", size = 2661437 },\r\n    { url = \"https://files.pythonhosted.org/packages/6c/ea/8820f57f0b46e6148ee42d8216b15e8fe3b360944284bbc705bf34fac888/pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1278e0d324f6908e872730c9102b0112477a7f7cf88b308e4fc36ce1bdb6d58c\", size = 2054404 },\r\n    { url = \"https://files.pythonhosted.org/packages/0f/36/d4ae869e473c3c7868e1cd1e2a1b9e13bce5cd1a7d287f6ac755a0b1575e/pydantic_core-2.23.4-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:9a6b5099eeec78827553827f4c6b8615978bb4b6a88e5d9b93eddf8bb6790f55\", size = 1921680 },\r\n    { url = \"https://files.pythonhosted.org/packages/0d/f8/eed5c65b80c4ac4494117e2101973b45fc655774ef647d17dde40a70f7d2/pydantic_core-2.23.4-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:e55541f756f9b3ee346b840103f32779c695a19826a4c442b7954550a0972040\", size = 1966093 },\r\n    { url = \"https://files.pythonhosted.org/packages/e8/c8/1d42ce51d65e571ab53d466cae83434325a126811df7ce4861d9d97bee4b/pydantic_core-2.23.4-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:a5c7ba8ffb6d6f8f2ab08743be203654bb1aaa8c9dcb09f82ddd34eadb695605\", size = 2111437 },\r\n    { url = \"https://files.pythonhosted.org/packages/aa/c9/7fea9d13383c2ec6865919e09cffe44ab77e911eb281b53a4deaafd4c8e8/pydantic_core-2.23.4-cp39-none-win32.whl\", hash = \"sha256:37b0fe330e4a58d3c58b24d91d1eb102aeec675a3db4c292ec3928ecd892a9a6\", size = 1735049 },\r\n    { url = \"https://files.pythonhosted.org/packages/98/95/dd7045c4caa2b73d0bf3b989d66b23cfbb7a0ef14ce99db15677a000a953/pydantic_core-2.23.4-cp39-none-win_amd64.whl\", hash = \"sha256:1498bec4c05c9c787bde9125cfdcc63a41004ff167f495063191b863399b1a29\", size = 1920180 },\r\n    { url = \"https://files.pythonhosted.org/packages/13/a9/5d582eb3204464284611f636b55c0a7410d748ff338756323cb1ce721b96/pydantic_core-2.23.4-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:f455ee30a9d61d3e1a15abd5068827773d6e4dc513e795f380cdd59932c782d5\", size = 1857135 },\r\n    { url = \"https://files.pythonhosted.org/packages/2c/57/faf36290933fe16717f97829eabfb1868182ac495f99cf0eda9f59687c9d/pydantic_core-2.23.4-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:1e90d2e3bd2c3863d48525d297cd143fe541be8bbf6f579504b9712cb6b643ec\", size = 1740583 },\r\n    { url = \"https://files.pythonhosted.org/packages/91/7c/d99e3513dc191c4fec363aef1bf4c8af9125d8fa53af7cb97e8babef4e40/pydantic_core-2.23.4-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2e203fdf807ac7e12ab59ca2bfcabb38c7cf0b33c41efeb00f8e5da1d86af480\", size = 1793637 },\r\n    { url = \"https://files.pythonhosted.org/packages/29/18/812222b6d18c2d13eebbb0f7cdc170a408d9ced65794fdb86147c77e1982/pydantic_core-2.23.4-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e08277a400de01bc72436a0ccd02bdf596631411f592ad985dcee21445bd0068\", size = 1941963 },\r\n    { url = \"https://files.pythonhosted.org/packages/0f/36/c1f3642ac3f05e6bb4aec3ffc399fa3f84895d259cf5f0ce3054b7735c29/pydantic_core-2.23.4-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:f220b0eea5965dec25480b6333c788fb72ce5f9129e8759ef876a1d805d00801\", size = 1915332 },\r\n    { url = \"https://files.pythonhosted.org/packages/f7/ca/9c0854829311fb446020ebb540ee22509731abad886d2859c855dd29b904/pydantic_core-2.23.4-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:d06b0c8da4f16d1d1e352134427cb194a0a6e19ad5db9161bf32b2113409e728\", size = 1957926 },\r\n    { url = \"https://files.pythonhosted.org/packages/c0/1c/7836b67c42d0cd4441fcd9fafbf6a027ad4b79b6559f80cf11f89fd83648/pydantic_core-2.23.4-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:ba1a0996f6c2773bd83e63f18914c1de3c9dd26d55f4ac302a7efe93fb8e7433\", size = 2100342 },\r\n    { url = \"https://files.pythonhosted.org/packages/a9/f9/b6bcaf874f410564a78908739c80861a171788ef4d4f76f5009656672dfe/pydantic_core-2.23.4-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:9a5bce9d23aac8f0cf0836ecfc033896aa8443b501c58d0602dbfd5bd5b37753\", size = 1920344 },\r\n    { url = \"https://files.pythonhosted.org/packages/32/fd/ac9cdfaaa7cf2d32590b807d900612b39acb25e5527c3c7e482f0553025b/pydantic_core-2.23.4-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:78ddaaa81421a29574a682b3179d4cf9e6d405a09b99d93ddcf7e5239c742e21\", size = 1857850 },\r\n    { url = \"https://files.pythonhosted.org/packages/08/fe/038f4b2bcae325ea643c8ad353191187a4c92a9c3b913b139289a6f2ef04/pydantic_core-2.23.4-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:883a91b5dd7d26492ff2f04f40fbb652de40fcc0afe07e8129e8ae779c2110eb\", size = 1740265 },\r\n    { url = \"https://files.pythonhosted.org/packages/51/14/b215c9c3cbd1edaaea23014d4b3304260823f712d3fdee52549b19b25d62/pydantic_core-2.23.4-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:88ad334a15b32a791ea935af224b9de1bf99bcd62fabf745d5f3442199d86d59\", size = 1793912 },\r\n    { url = \"https://files.pythonhosted.org/packages/62/de/2c3ad79b63ba564878cbce325be725929ba50089cd5156f89ea5155cb9b3/pydantic_core-2.23.4-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:233710f069d251feb12a56da21e14cca67994eab08362207785cf8c598e74577\", size = 1942870 },\r\n    { url = \"https://files.pythonhosted.org/packages/cb/55/c222af19e4644c741b3f3fe4fd8bbb6b4cdca87d8a49258b61cf7826b19e/pydantic_core-2.23.4-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:19442362866a753485ba5e4be408964644dd6a09123d9416c54cd49171f50744\", size = 1915610 },\r\n    { url = \"https://files.pythonhosted.org/packages/c4/7a/9a8760692a6f76bb54bcd43f245ff3d8b603db695899bbc624099c00af80/pydantic_core-2.23.4-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:624e278a7d29b6445e4e813af92af37820fafb6dcc55c012c834f9e26f9aaaef\", size = 1958403 },\r\n    { url = \"https://files.pythonhosted.org/packages/4c/91/9b03166feb914bb5698e2f6499e07c2617e2eebf69f9374d0358d7eb2009/pydantic_core-2.23.4-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:f5ef8f42bec47f21d07668a043f077d507e5bf4e668d5c6dfe6aaba89de1a5b8\", size = 2101154 },\r\n    { url = \"https://files.pythonhosted.org/packages/1d/d9/1d7ecb98318da4cb96986daaf0e20d66f1651d0aeb9e2d4435b916ce031d/pydantic_core-2.23.4-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:aea443fffa9fbe3af1a9ba721a87f926fe548d32cab71d188a6ede77d0ff244e\", size = 1920855 },\r\n]\r\n\r\n[[package]]\r\nname = \"pydantic-settings\"\r\nversion = \"2.6.0\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\ndependencies = [\r\n    { name = \"pydantic\" },\r\n    { name = \"python-dotenv\" },\r\n]\r\nsdist = { url = \"https://files.pythonhosted.org/packages/6c/66/5f1a9da10675bfb3b9da52f5b689c77e0a5612263fcce510cfac3e99a168/pydantic_settings-2.6.0.tar.gz\", hash = \"sha256:44a1804abffac9e6a30372bb45f6cafab945ef5af25e66b1c634c01dd39e0188\", size = 75232 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/34/19/26bb6bdb9fdad5f0dfce538780814084fb667b4bc37fcb28459c14b8d3b5/pydantic_settings-2.6.0-py3-none-any.whl\", hash = \"sha256:4a819166f119b74d7f8c765196b165f95cc7487ce58ea27dec8a5a26be0970e0\", size = 28578 },\r\n]\r\n\r\n[[package]]\r\nname = \"python-dotenv\"\r\nversion = \"1.0.1\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/bc/57/e84d88dfe0aec03b7a2d4327012c1627ab5f03652216c63d49846d7a6c58/python-dotenv-1.0.1.tar.gz\", hash = \"sha256:e324ee90a023d808f1959c46bcbc04446a10ced277783dc6ee09987c37ec10ca\", size = 39115 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl\", hash = \"sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a\", size = 19863 },\r\n]\r\n\r\n[[package]]\r\nname = \"ruff\"\r\nversion = \"0.7.1\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/a6/21/5c6e05e0fd3fbb41be4fb92edbc9a04de70baf60adb61435ce0c6b8c3d55/ruff-0.7.1.tar.gz\", hash = \"sha256:9d8a41d4aa2dad1575adb98a82870cf5db5f76b2938cf2206c22c940034a36f4\", size = 3181670 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/65/45/8a20a9920175c9c4892b2420f80ff3cf14949cf3067118e212f9acd9c908/ruff-0.7.1-py3-none-linux_armv6l.whl\", hash = \"sha256:cb1bc5ed9403daa7da05475d615739cc0212e861b7306f314379d958592aaa89\", size = 10389268 },\r\n    { url = \"https://files.pythonhosted.org/packages/1b/d3/2f8382db2cf4f9488e938602e33e36287f9d26cb283aa31f11c31297ce79/ruff-0.7.1-py3-none-macosx_10_12_x86_64.whl\", hash = \"sha256:27c1c52a8d199a257ff1e5582d078eab7145129aa02721815ca8fa4f9612dc35\", size = 10188348 },\r\n    { url = \"https://files.pythonhosted.org/packages/a2/31/7d14e2a88da351200f844b7be889a0845d9e797162cf76b136d21b832a23/ruff-0.7.1-py3-none-macosx_11_0_arm64.whl\", hash = \"sha256:588a34e1ef2ea55b4ddfec26bbe76bc866e92523d8c6cdec5e8aceefeff02d99\", size = 9841448 },\r\n    { url = \"https://files.pythonhosted.org/packages/db/99/738cafdc768eceeca0bd26c6f03e213aa91203d2278e1d95b1c31c4ece41/ruff-0.7.1-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:94fc32f9cdf72dc75c451e5f072758b118ab8100727168a3df58502b43a599ca\", size = 10674864 },\r\n    { url = \"https://files.pythonhosted.org/packages/fe/12/bcf2836b50eab53c65008383e7d55201e490d75167c474f14a16e1af47d2/ruff-0.7.1-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:985818742b833bffa543a84d1cc11b5e6871de1b4e0ac3060a59a2bae3969250\", size = 10192105 },\r\n    { url = \"https://files.pythonhosted.org/packages/2b/71/261d5d668bf98b6c44e89bfb5dfa4cb8cb6c8b490a201a3d8030e136ea4f/ruff-0.7.1-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:32f1e8a192e261366c702c5fb2ece9f68d26625f198a25c408861c16dc2dea9c\", size = 11194144 },\r\n    { url = \"https://files.pythonhosted.org/packages/90/1f/0926d18a3b566fa6e7b3b36093088e4ffef6b6ba4ea85a462d9a93f7e35c/ruff-0.7.1-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl\", hash = \"sha256:699085bf05819588551b11751eff33e9ca58b1b86a6843e1b082a7de40da1565\", size = 11917066 },\r\n    { url = \"https://files.pythonhosted.org/packages/cd/a8/9fac41f128b6a44ab4409c1493430b4ee4b11521e8aeeca19bfe1ce851f9/ruff-0.7.1-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:344cc2b0814047dc8c3a8ff2cd1f3d808bb23c6658db830d25147339d9bf9ea7\", size = 11458821 },\r\n    { url = \"https://files.pythonhosted.org/packages/25/cd/59644168f086ab13fe4e02943b9489a0aa710171f66b178e179df5383554/ruff-0.7.1-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:4316bbf69d5a859cc937890c7ac7a6551252b6a01b1d2c97e8fc96e45a7c8b4a\", size = 12700379 },\r\n    { url = \"https://files.pythonhosted.org/packages/fb/30/3bac63619eb97174661829c07fc46b2055a053dee72da29d7c304c1cd2c0/ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:79d3af9dca4c56043e738a4d6dd1e9444b6d6c10598ac52d146e331eb155a8ad\", size = 11019813 },\r\n    { url = \"https://files.pythonhosted.org/packages/4b/af/f567b885b5cb3bcdbcca3458ebf210cc8c9c7a9f61c332d3c2a050c3b21e/ruff-0.7.1-py3-none-musllinux_1_2_aarch64.whl\", hash = \"sha256:c5c121b46abde94a505175524e51891f829414e093cd8326d6e741ecfc0a9112\", size = 10662146 },\r\n    { url = \"https://files.pythonhosted.org/packages/bc/ad/eb930d3ad117a9f2f7261969c21559ebd82bb13b6e8001c7caed0d44be5f/ruff-0.7.1-py3-none-musllinux_1_2_armv7l.whl\", hash = \"sha256:8422104078324ea250886954e48f1373a8fe7de59283d747c3a7eca050b4e378\", size = 10256911 },\r\n    { url = \"https://files.pythonhosted.org/packages/20/d5/af292ce70a016fcec792105ca67f768b403dd480a11888bc1f418fed0dd5/ruff-0.7.1-py3-none-musllinux_1_2_i686.whl\", hash = \"sha256:56aad830af8a9db644e80098fe4984a948e2b6fc2e73891538f43bbe478461b8\", size = 10767488 },\r\n    { url = \"https://files.pythonhosted.org/packages/24/85/cc04a3bd027f433bebd2a097e63b3167653c079f7f13d8f9a1178e693412/ruff-0.7.1-py3-none-musllinux_1_2_x86_64.whl\", hash = \"sha256:658304f02f68d3a83c998ad8bf91f9b4f53e93e5412b8f2388359d55869727fd\", size = 11093368 },\r\n    { url = \"https://files.pythonhosted.org/packages/0b/fb/c39cbf32d1f3e318674b8622f989417231794926b573f76dd4d0ca49f0f1/ruff-0.7.1-py3-none-win32.whl\", hash = \"sha256:b517a2011333eb7ce2d402652ecaa0ac1a30c114fbbd55c6b8ee466a7f600ee9\", size = 8594180 },\r\n    { url = \"https://files.pythonhosted.org/packages/5a/71/ec8cdea34ecb90c830ca60d54ac7b509a7b5eab50fae27e001d4470fe813/ruff-0.7.1-py3-none-win_amd64.whl\", hash = \"sha256:f38c41fcde1728736b4eb2b18850f6d1e3eedd9678c914dede554a70d5241307\", size = 9419751 },\r\n    { url = \"https://files.pythonhosted.org/packages/79/7b/884553415e9f0a9bf358ed52fb68b934e67ef6c5a62397ace924a1afdf9a/ruff-0.7.1-py3-none-win_arm64.whl\", hash = \"sha256:19aa200ec824c0f36d0c9114c8ec0087082021732979a359d6f3c390a6ff2a37\", size = 8717402 },\r\n]\r\n\r\n[[package]]\r\nname = \"sniffio\"\r\nversion = \"1.3.1\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz\", hash = \"sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc\", size = 20372 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl\", hash = \"sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2\", size = 10235 },\r\n]\r\n\r\n[[package]]\r\nname = \"tomli\"\r\nversion = \"2.0.2\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/35/b9/de2a5c0144d7d75a57ff355c0c24054f965b2dc3036456ae03a51ea6264b/tomli-2.0.2.tar.gz\", hash = \"sha256:d46d457a85337051c36524bc5349dd91b1877838e2979ac5ced3e710ed8a60ed\", size = 16096 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/cf/db/ce8eda256fa131af12e0a76d481711abe4681b6923c27efb9a255c9e4594/tomli-2.0.2-py3-none-any.whl\", hash = \"sha256:2ebe24485c53d303f690b0ec092806a085f07af5a5aa1464f3931eec36caaa38\", size = 13237 },\r\n]\r\n\r\n[[package]]\r\nname = \"typing-extensions\"\r\nversion = \"4.12.2\"\r\nsource = { registry = \"https://pypi.org/simple\" }\r\nsdist = { url = \"https://files.pythonhosted.org/packages/df/db/f35a00659bc03fec321ba8bce9420de607a1d37f8342eee1863174c69557/typing_extensions-4.12.2.tar.gz\", hash = \"sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8\", size = 85321 }\r\nwheels = [\r\n    { url = \"https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl\", hash = \"sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d\", size = 37438 },\r\n]\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/uv.lock b/uv.lock
--- a/uv.lock	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/uv.lock	(date 1730277111707)
@@ -82,6 +82,27 @@
     { url = "https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl", hash = "sha256:922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8", size = 167321 },
 ]
 
+[[package]]
+name = "click"
+version = "8.1.7"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "colorama", marker = "platform_system == 'Windows'" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/96/d3/f04c7bfcf5c1862a2a5b845c6b2b360488cf47af55dfa79c98f6a6bf98b5/click-8.1.7.tar.gz", hash = "sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de", size = 336121 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl", hash = "sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28", size = 97941 },
+]
+
+[[package]]
+name = "colorama"
+version = "0.4.6"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
+]
+
 [[package]]
 name = "exceptiongroup"
 version = "1.2.2"
@@ -184,6 +205,7 @@
     { name = "pydantic" },
     { name = "pydantic-settings" },
     { name = "tomli" },
+    { name = "typer" },
 ]
 
 [package.dev-dependencies]
@@ -199,6 +221,7 @@
     { name = "pydantic", specifier = ">=2.9.2" },
     { name = "pydantic-settings", specifier = ">=2.6.0" },
     { name = "tomli", specifier = ">=2.0.2" },
+    { name = "typer", specifier = ">=0.12.5" },
 ]
 
 [package.metadata.requires-dev]
@@ -219,6 +242,27 @@
     { url = "https://files.pythonhosted.org/packages/f8/31/50f3c38b38ff28635ff9c4a4afefddccc5f1b57457b539bdbdf75ce18669/m3u8-6.0.0-py3-none-any.whl", hash = "sha256:566d0748739c552dad10f8c87150078de6a0ec25071fa48e6968e96fc6dcba5d", size = 24133 },
 ]
 
+[[package]]
+name = "markdown-it-py"
+version = "3.0.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "mdurl" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
+]
+
+[[package]]
+name = "mdurl"
+version = "0.1.2"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
+]
+
 [[package]]
 name = "mypy"
 version = "1.13.0"
@@ -381,6 +425,15 @@
     { url = "https://files.pythonhosted.org/packages/34/19/26bb6bdb9fdad5f0dfce538780814084fb667b4bc37fcb28459c14b8d3b5/pydantic_settings-2.6.0-py3-none-any.whl", hash = "sha256:4a819166f119b74d7f8c765196b165f95cc7487ce58ea27dec8a5a26be0970e0", size = 28578 },
 ]
 
+[[package]]
+name = "pygments"
+version = "2.18.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/8e/62/8336eff65bcbc8e4cb5d05b55faf041285951b6e80f33e2bff2024788f31/pygments-2.18.0.tar.gz", hash = "sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199", size = 4891905 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/f7/3f/01c8b82017c199075f8f788d0d906b9ffbbc5a47dc9918a945e13d5a2bda/pygments-2.18.0-py3-none-any.whl", hash = "sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a", size = 1205513 },
+]
+
 [[package]]
 name = "python-dotenv"
 version = "1.0.1"
@@ -390,6 +443,20 @@
     { url = "https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl", hash = "sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a", size = 19863 },
 ]
 
+[[package]]
+name = "rich"
+version = "13.9.3"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "markdown-it-py" },
+    { name = "pygments" },
+    { name = "typing-extensions", marker = "python_full_version < '3.11'" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/d9/e9/cf9ef5245d835065e6673781dbd4b8911d352fb770d56cf0879cf11b7ee1/rich-13.9.3.tar.gz", hash = "sha256:bc1e01b899537598cf02579d2b9f4a415104d3fc439313a7a2c165d76557a08e", size = 222889 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/9a/e2/10e9819cf4a20bd8ea2f5dabafc2e6bf4a78d6a0965daeb60a4b34d1c11f/rich-13.9.3-py3-none-any.whl", hash = "sha256:9836f5096eb2172c9e77df411c1b009bace4193d6a481d534fea75ebba758283", size = 242157 },
+]
+
 [[package]]
 name = "ruff"
 version = "0.7.1"
@@ -415,6 +482,15 @@
     { url = "https://files.pythonhosted.org/packages/79/7b/884553415e9f0a9bf358ed52fb68b934e67ef6c5a62397ace924a1afdf9a/ruff-0.7.1-py3-none-win_arm64.whl", hash = "sha256:19aa200ec824c0f36d0c9114c8ec0087082021732979a359d6f3c390a6ff2a37", size = 8717402 },
 ]
 
+[[package]]
+name = "shellingham"
+version = "1.5.4"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
+]
+
 [[package]]
 name = "sniffio"
 version = "1.3.1"
@@ -433,6 +509,21 @@
     { url = "https://files.pythonhosted.org/packages/cf/db/ce8eda256fa131af12e0a76d481711abe4681b6923c27efb9a255c9e4594/tomli-2.0.2-py3-none-any.whl", hash = "sha256:2ebe24485c53d303f690b0ec092806a085f07af5a5aa1464f3931eec36caaa38", size = 13237 },
 ]
 
+[[package]]
+name = "typer"
+version = "0.12.5"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "click" },
+    { name = "rich" },
+    { name = "shellingham" },
+    { name = "typing-extensions" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/c5/58/a79003b91ac2c6890fc5d90145c662fd5771c6f11447f116b63300436bc9/typer-0.12.5.tar.gz", hash = "sha256:f592f089bedcc8ec1b974125d64851029c3b1af145f04aca64d69410f0c9b722", size = 98953 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/a8/2b/886d13e742e514f704c33c4caa7df0f3b89e5a25ef8db02aa9ca3d9535d5/typer-0.12.5-py3-none-any.whl", hash = "sha256:62fe4e471711b147e3365034133904df3e235698399bc4de2b36c8579298d52b", size = 47288 },
+]
+
 [[package]]
 name = "typing-extensions"
 version = "4.12.2"
Index: isubrip/constants.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport datetime as dt\r\nimport logging\r\nfrom pathlib import Path\r\nfrom tempfile import gettempdir\r\n\r\n# General\r\nPACKAGE_NAME = \"isubrip\"\r\nPACKAGE_VERSION = \"2.5.6\"\r\n\r\n# Async\r\nEVENT_LOOP = asyncio.get_event_loop()\r\n\r\n# Logging\r\nPREORDER_MESSAGE = (\"'{movie_name}' is currently unavailable on {scraper_name}, \"\r\n                    \"and will be available on {preorder_date}.\")\r\n\r\nANSI_COLORS = {\r\n    logging.DEBUG: \"\\x1b[37;20m\",  # Light Grey\r\n    logging.INFO: \"\\x1b[38;20m\",  # Grey\r\n    logging.WARNING: \"\\x1b[33;20m\",  # Yellow\r\n    logging.ERROR: \"\\x1b[31;20m\",  # Red\r\n    logging.CRITICAL: \"\\x1b[31;1m\",  # Bold Red\r\n    }\r\nRESET_COLOR = \"\\x1b[0m\"\r\n\r\nLOGGING_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\r\nLOGGING_FILE_METADATA = \"[%(asctime)s | %(levelname)s | %(threadName)s | %(filename)s::%(funcName)s::%(lineno)d] \"\r\n\r\n# Downloads\r\nARCHIVE_FORMAT = \"zip\"\r\n\r\n# Paths\r\nDEFAULT_CONFIG_PATH = Path(__file__).parent / \"resources\" / \"default_config.toml\"\r\nDATA_FOLDER_PATH = Path.home() / f\".{PACKAGE_NAME}\"\r\nSCRAPER_MODULES_SUFFIX = \"_scraper\"\r\nTEMP_FOLDER_PATH = Path(gettempdir()) / PACKAGE_NAME\r\n\r\n# Config Paths\r\nUSER_CONFIG_FILE_NAME = \"config.toml\"\r\nUSER_CONFIG_FILE_PATH = DATA_FOLDER_PATH / USER_CONFIG_FILE_NAME\r\n\r\n# Logging Paths\r\nLOG_FILES_PATH = DATA_FOLDER_PATH / \"logs\"\r\nLOG_FILE_NAME = f\"{PACKAGE_NAME}_{dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\r\n\r\n\r\n# Other\r\nTITLE_REPLACEMENT_STRINGS = {  # Replacements will be done by the order of the keys.\r\n    \": \": \".\", \":\": \".\", \" - \": \"-\", \", \": \".\", \". \": \".\", \" \": \".\", \"|\": \".\", \"/\": \".\", \"…\": \".\",\r\n    \"<\": \"\", \">\": \"\", \"(\": \"\", \")\": \"\", '\"': \"\", \"?\": \"\", \"*\": \"\",\r\n}\r\nWINDOWS_RESERVED_FILE_NAMES = (\"CON\", \"PRN\", \"AUX\", \"NUL\", \"COM1\", \"COM2\", \"COM3\", \"COM4\", \"COM5\", \"COM6\", \"COM7\",\r\n                               \"COM8\", \"COM9\", \"LPT1\", \"LPT2\", \"LPT3\", \"LPT4\", \"LPT5\", \"LPT6\", \"LPT7\", \"LPT8\", \"LPT9\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/constants.py b/isubrip/constants.py
--- a/isubrip/constants.py	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/isubrip/constants.py	(date 1730277090557)
@@ -29,6 +29,8 @@
 LOGGING_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"
 LOGGING_FILE_METADATA = "[%(asctime)s | %(levelname)s | %(threadName)s | %(filename)s::%(funcName)s::%(lineno)d] "
 
+DEFAULT_LOGS_ROTATION_SIZE = 15  # Amount of files
+
 # Downloads
 ARCHIVE_FORMAT = "zip"
 
Index: isubrip/scrapers/scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC, abstractmethod\r\nimport asyncio\r\nfrom enum import Enum\r\nimport importlib\r\nimport inspect\r\nfrom pathlib import Path\r\nimport re\r\nimport sys\r\nfrom typing import TYPE_CHECKING, Any, ClassVar, Literal, TypeVar, Union, overload\r\n\r\nimport httpx\r\nimport m3u8\r\nfrom pydantic import BaseModel, ConfigDict, Field, create_model\r\n\r\nfrom isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX\r\nfrom isubrip.data_structures import (\r\n    MainPlaylist,\r\n    PlaylistMediaItem,\r\n    ScrapedMediaResponse,\r\n    SubtitlesData,\r\n    SubtitlesFormatType,\r\n    SubtitlesType,\r\n)\r\nfrom isubrip.logger import logger\r\nfrom isubrip.utils import (\r\n    SingletonMeta,\r\n    get_model_field,\r\n    merge_dict_values,\r\n    normalize_config_name,\r\n    return_first_valid,\r\n    single_string_to_list,\r\n)\r\n\r\nif TYPE_CHECKING:\r\n    from types import TracebackType\r\n\r\n    from isubrip.subtitle_formats.subtitles import Subtitles\r\n\r\n\r\nScraperT = TypeVar(\"ScraperT\", bound=\"Scraper\")\r\n\r\n\r\nclass ScraperConfigBase(BaseModel, ABC):\r\n    \"\"\"\r\n    A Pydantic BaseModel for base class for scraper's configuration classes.\r\n    Also serves for setting default configuration settings for all scrapers.\r\n\r\n    Attributes:\r\n        timeout (int | float): Timeout to use when making requests.\r\n        user_agent (st): User agent to use when making requests.\r\n        proxy (str | None): Proxy to use when making requests.\r\n        verify_ssl (bool): Whether to verify SSL certificates.\r\n    \"\"\"\r\n    model_config = ConfigDict(\r\n        extra='forbid',\r\n        alias_generator=normalize_config_name,\r\n    )\r\n\r\n    timeout: int | float | None = Field(default=None)\r\n    user_agent: str | None = Field(default=None, alias=\"user-agent\")\r\n    proxy: str | None = Field(default=None)\r\n    verify_ssl: bool | None = Field(default=None, alias=\"verify-ssl\")\r\n\r\n\r\nclass DefaultScraperConfig(ScraperConfigBase):\r\n    \"\"\"\r\n    A Pydantic BaseModel for base class for scraper's configuration classes.\r\n    Also serves for setting default configuration settings for all scrapers.\r\n\r\n    Attributes:\r\n        timeout (int | float): Timeout to use when making requests.\r\n        user_agent (st): User agent to use when making requests.\r\n        proxy (str | None): Proxy to use when making requests.\r\n        verify_ssl (bool): Whether to verify SSL certificates.\r\n    \"\"\"\r\n    timeout: int | float = Field(default=10)\r\n    user_agent: str = Field(\r\n        default=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\",    # noqa: E501\r\n        alias=\"user-agent\",\r\n    )\r\n    proxy: str | None = Field(default=None)\r\n    verify_ssl: bool = Field(default=True, alias=\"verify-ssl\")\r\n\r\n\r\nclass ScraperConfigSubcategory(BaseModel, ABC):\r\n    \"\"\"A Pydantic BaseModel for a scraper's configuration subcategory (which can be set under 'ScraperConfig').\"\"\"\r\n    model_config = ConfigDict(\r\n        extra='forbid',\r\n        alias_generator=normalize_config_name,\r\n    )\r\n\r\n\r\nclass Scraper(ABC, metaclass=SingletonMeta):\r\n    \"\"\"\r\n    A base class for scrapers.\r\n\r\n    Attributes:\r\n        default_user_agent (str): [Class Attribute]\r\n            Default user agent to use if no other user agent is specified when making requests.\r\n        default_proxy (str | None): [Class Attribute] Default proxy to use when making requests.\r\n        default_verify_ssl (bool): [Class Attribute] Whether to verify SSL certificates by default.\r\n        subtitles_fix_rtl (bool): [Class Attribute] Whether to fix RTL from downloaded subtitles.\r\n            A list of languages to fix RTL on. If None, a default list will be used.\r\n        subtitles_remove_duplicates (bool): [Class Attribute]\r\n            Whether to remove duplicate lines from downloaded subtitles.\r\n\r\n        id (str): [Class Attribute] ID of the scraper (must be unique).\r\n        name (str): [Class Attribute] Name of the scraper.\r\n        abbreviation (str): [Class Attribute] Abbreviation of the scraper.\r\n        url_regex (re.Pattern | list[re.Pattern]): [Class Attribute] A RegEx pattern to find URLs matching the service.\r\n        subtitles_class (type[Subtitles]): [Class Attribute] Class of the subtitles format returned by the scraper.\r\n        is_movie_scraper (bool): [Class Attribute] Whether the scraper is for movies.\r\n        is_series_scraper (bool): [Class Attribute] Whether the scraper is for series.\r\n        uses_scrapers (list[str]): [Class Attribute] A list of IDs for other scraper classes that this scraper uses.\r\n            This assures that the config data for the other scrapers is passed as well.\r\n        _session (httpx.Client): A synchronous HTTP client session.\r\n        _async_session (httpx.AsyncClient): An asynchronous HTTP client session.\r\n\r\n    Notes:\r\n        Each scraper implements its own `ScraperConfig` class (which can be overridden and updated),\r\n         inheriting from `ScraperConfigBase`, which sets configurable options for the scraper.\r\n    \"\"\"\r\n    default_timeout: ClassVar[int | float] = 10\r\n    default_user_agent: ClassVar[str] = httpx._client.USER_AGENT  # noqa: SLF001\r\n    default_proxy: ClassVar[str | None] = None\r\n    default_verify_ssl: ClassVar[bool] = True\r\n    subtitles_fix_rtl: ClassVar[bool] = False\r\n    subtitles_remove_duplicates: ClassVar[bool] = True\r\n\r\n    id: ClassVar[str]\r\n    name: ClassVar[str]\r\n    abbreviation: ClassVar[str]\r\n    url_regex: ClassVar[re.Pattern | list[re.Pattern]]\r\n    subtitles_class: ClassVar[type[Subtitles]]\r\n    is_movie_scraper: ClassVar[bool] = False\r\n    is_series_scraper: ClassVar[bool] = False\r\n    uses_scrapers: ClassVar[list[str]] = []\r\n\r\n    class ScraperConfig(ScraperConfigBase):\r\n        \"\"\"Set a default configuration for all scrapers using the default fields.\"\"\"\r\n\r\n\r\n    def __init__(self, timeout: int | float | None = None, user_agent: str | None = None,\r\n                 proxy: str | None = None, verify_ssl: bool | None = None, config: ScraperConfig | None = None):\r\n        \"\"\"\r\n        Initialize a Scraper object.\r\n\r\n        Args:\r\n            timeout (int | float | None, optional): A timeout to use when making requests. Defaults to None.\r\n            user_agent (str | None, optional): A user agent to use when making requests. Defaults to None.\r\n            proxy (str | None, optional): A proxy to use when making requests. Defaults to None.\r\n            verify_ssl (bool | None, optional): Whether to verify SSL certificates. Defaults to None.\r\n            config (ScraperConfig | None, optional): An optional config object\r\n                containing scraper configuration settings. Defaults to None.\r\n        \"\"\"\r\n        self._config = config\r\n        self._timeout = return_first_valid(timeout,\r\n                                           get_model_field(model=config, field='timeout'),\r\n                                           self.default_timeout,\r\n                                           raise_error=True)\r\n        self._user_agent = return_first_valid(user_agent,\r\n                                              get_model_field(model=config, field='user_agent'),\r\n                                              self.default_user_agent,\r\n                                              raise_error=True)\r\n        self._proxy = return_first_valid(proxy,\r\n                                         get_model_field(model=config, field='proxy'),\r\n                                         self.default_proxy)\r\n        self._verify_ssl = return_first_valid(verify_ssl,\r\n                                              get_model_field(model=config, field='verify_ssl'),\r\n                                              self.default_verify_ssl,\r\n                                              raise_error=True)\r\n\r\n        if self._timeout != self.default_timeout:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with custom timeout: '{self._timeout}'.\")\r\n\r\n        if self._user_agent != self.default_user_agent:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with custom user-agent: '{self._user_agent}'.\")\r\n\r\n        if self._proxy != self.default_proxy:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with proxy: '{self._proxy}'.\")\r\n\r\n        if self._verify_ssl != self.default_verify_ssl:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with SSL verification set to: '{self._verify_ssl}'.\")\r\n\r\n        self._requests_counter = 0\r\n        clients_params: dict[str, Any] = {\r\n            \"headers\": {\"User-Agent\": self._user_agent},\r\n            \"verify\": self._verify_ssl,\r\n            \"proxy\": self._proxy,\r\n            \"timeout\": float(self._timeout),\r\n        }\r\n        self._session = httpx.Client(\r\n            **clients_params,\r\n            event_hooks={\r\n                \"request\": [self._increment_requests_counter],\r\n            },\r\n        )\r\n        self._async_session = httpx.AsyncClient(\r\n            **clients_params,\r\n            event_hooks={\r\n                \"request\": [self._async_increment_requests_counter],\r\n            },\r\n        )\r\n\r\n        # Update session settings according to configurations\r\n        self._session.headers.update({\"User-Agent\": self._user_agent})\r\n        self._async_session.headers.update({\"User-Agent\": self._user_agent})\r\n\r\n        if not self._verify_ssl:\r\n            import urllib3\r\n            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\n    def _increment_requests_counter(self, request: httpx.Request) -> None:  # noqa: ARG002\r\n        self._requests_counter += 1\r\n\r\n    async def _async_increment_requests_counter(self, request: httpx.Request) -> None:  # noqa: ARG002\r\n        self._requests_counter += 1\r\n\r\n    @property\r\n    def requests_count(self) -> int:\r\n        return self._requests_counter\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[True] = ...) -> re.Match:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[False] = ...) -> re.Match | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def match_url(cls, url: str, raise_error: bool = False) -> re.Match | None:\r\n        \"\"\"\r\n        Checks if a URL matches scraper's url regex.\r\n\r\n        Args:\r\n            url (str): A URL to check against the regex.\r\n            raise_error (bool, optional): Whether to raise an error instead of returning None if the URL doesn't match.\r\n\r\n        Returns:\r\n            re.Match | None: A Match object if the URL matches the regex, None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If the URL doesn't match the regex and raise_error is True.\r\n        \"\"\"\r\n        if isinstance(cls.url_regex, re.Pattern) and (match_result := re.fullmatch(pattern=cls.url_regex, string=url)):\r\n            return match_result\r\n\r\n        if isinstance(cls.url_regex, list):\r\n            for url_regex_item in cls.url_regex:\r\n                if result := re.fullmatch(pattern=url_regex_item, string=url):\r\n                    return result\r\n\r\n        if raise_error:\r\n            raise ValueError(f\"URL '{url}' doesn't match the URL regex of {cls.name}.\")\r\n\r\n        return None\r\n\r\n    def __enter__(self) -> Scraper:\r\n        return self\r\n\r\n    def __exit__(self, exc_type: type[BaseException] | None,\r\n                 exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:\r\n        self.close()\r\n\r\n    async def async_close(self) -> None:\r\n        await self._async_session.aclose()\r\n\r\n    def close(self) -> None:\r\n        self._session.close()\r\n\r\n    @abstractmethod\r\n    async def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        \"\"\"\r\n        Scrape media information about the media on a URL.\r\n\r\n        Args:\r\n            url (str): A URL to get media information about.\r\n\r\n        Returns:\r\n            ScrapedMediaResponse: A ScrapedMediaResponse object containing scraped media information.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    async def download_subtitles(self, media_data: PlaylistMediaItem, subrip_conversion: bool = False) -> SubtitlesData:\r\n        \"\"\"\r\n        Download subtitles from a media object.\r\n\r\n        Args:\r\n            media_data (PlaylistMediaItem): A media object to download subtitles from.\r\n            subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.\r\n\r\n        Returns:\r\n            SubtitlesData: A SubtitlesData object containing downloaded subtitles.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def find_matching_media(self, main_playlist: MainPlaylist,\r\n                            filters: dict[str, str | list[str]] | None = None) -> list:\r\n        \"\"\"\r\n        Find media items that match the given filters in the main playlist (or all media items if no filters are given).\r\n\r\n        Args:\r\n            main_playlist (MainPlaylist): Main playlist to search for media items in.\r\n            filters (dict[str, str | list[str]] | None, optional): A dictionary of filters to match media items against.\r\n                Defaults to None.\r\n\r\n        Returns:\r\n            list: A list of media items that match the given filters.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def find_matching_subtitles(self, main_playlist: MainPlaylist,\r\n                                language_filter: list[str] | None = None) -> list[PlaylistMediaItem]:\r\n        \"\"\"\r\n        Find subtitles that match the given language filter in the main playlist.\r\n\r\n        Args:\r\n            main_playlist (MainPlaylist): Main playlist to search for subtitles in.\r\n            language_filter (list[str] | None, optional): A list of language codes to filter subtitles by.\r\n                Defaults to None.\r\n\r\n        Returns:\r\n            list[PlaylistMediaItem]: A list of subtitles that match the given language filter.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    async def load_playlist(self, url: str | list[str], headers: dict | None = None) -> MainPlaylist | None:\r\n        \"\"\"\r\n        Load a playlist from a URL to a representing object.\r\n        Multiple URLs can be given, in which case the first one that loads successfully will be returned.\r\n\r\n        Args:\r\n            url (str | list[str]): URL of the M3U8 playlist to load. Can also be a list of URLs (for redundancy).\r\n            headers (dict | None, optional): A dictionary of headers to use when making the request.\r\n                Defaults to None (results in using session's configured headers).\r\n\r\n        Returns:\r\n            MainPlaylist | None: A playlist object (matching the type), or None if the playlist couldn't be loaded.\r\n        \"\"\"\r\n\r\n\r\nclass HLSScraper(Scraper, ABC):\r\n    \"\"\"A base class for HLS (m3u8) scrapers.\"\"\"\r\n    class M3U8Attribute(Enum):\r\n        \"\"\"\r\n        An enum representing all possible M3U8 attributes.\r\n        Names / Keys represent M3U8 Media object attributes (should be converted to lowercase),\r\n        and values represent the name of the key for config usage.\r\n        \"\"\"\r\n        ASSOC_LANGUAGE = \"assoc-language\"\r\n        AUTOSELECT = \"autoselect\"\r\n        CHARACTERISTICS = \"characteristics\"\r\n        CHANNELS = \"channels\"\r\n        DEFAULT = \"default\"\r\n        FORCED = \"forced\"\r\n        GROUP_ID = \"group-id\"\r\n        INSTREAM_ID = \"instream-id\"\r\n        LANGUAGE = \"language\"\r\n        NAME = \"name\"\r\n        STABLE_RENDITION_ID = \"stable-rendition-id\"\r\n        TYPE = \"type\"\r\n\r\n    default_playlist_filters: ClassVar[dict[str, str | list[str] | None] | None] = None\r\n\r\n    _subtitles_filters: dict[str, str | list[str]] = {\r\n        M3U8Attribute.TYPE.value: \"SUBTITLES\",\r\n    }\r\n\r\n    # Resolve mypy errors as mypy doesn't support dynamic models.\r\n    if TYPE_CHECKING:\r\n        PlaylistFiltersSubcategory = ScraperConfigSubcategory\r\n\r\n    else:\r\n        PlaylistFiltersSubcategory = create_model(\r\n            \"PlaylistFiltersSubcategory\",\r\n            __base__=ScraperConfigSubcategory,\r\n            **{\r\n                m3u8_attribute.value: (Union[str, list[str], None],\r\n                                       Field(alias=normalize_config_name(m3u8_attribute.value), default=None))\r\n                for m3u8_attribute in M3U8Attribute\r\n            },  # type: ignore[call-overload]\r\n        )\r\n\r\n\r\n    class ScraperConfig(Scraper.ScraperConfig):\r\n        playlist_filters: HLSScraper.PlaylistFiltersSubcategory = Field(  # type: ignore[valid-type]\r\n            default_factory=lambda: HLSScraper.PlaylistFiltersSubcategory(),\r\n        )\r\n\r\n\r\n    def __init__(self, playlist_filters: dict[str, str | list[str] | None] | None = None,\r\n                 *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self._playlist_filters = return_first_valid(playlist_filters,\r\n                                                    get_model_field(model=self._config,\r\n                                                                    field='playlist_filters',\r\n                                                                    convert_to_dict=True),\r\n                                                    self.default_playlist_filters)\r\n\r\n        if self._playlist_filters != self.default_playlist_filters:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with custom playlist filters: {self._playlist_filters}.\")\r\n\r\n    def parse_language_name(self, media_data: m3u8.Media) -> str | None:\r\n        \"\"\"\r\n        Parse the language name from an M3U8 Media object.\r\n        Can be overridden in subclasses for normalization.\r\n\r\n        Args:\r\n            media_data (m3u8.Media): Media object to parse the language name from.\r\n\r\n        Returns:\r\n            str | None: The language name if found, None otherwise.\r\n        \"\"\"\r\n        name: str | None = media_data.name\r\n        return name\r\n\r\n    async def load_playlist(self, url: str | list[str], headers: dict | None = None) -> m3u8.M3U8 | None:\r\n        _headers = headers or self._session.headers\r\n        result: m3u8.M3U8 | None = None\r\n\r\n        for url_item in single_string_to_list(item=url):\r\n            try:\r\n                response = await self._async_session.get(url=url_item, headers=_headers, timeout=5)\r\n\r\n            except Exception as e:\r\n                logger.debug(f\"Failed to load M3U8 playlist '{url_item}': {e}\")\r\n                continue\r\n\r\n            if not response.text:\r\n                raise PlaylistLoadError(\"Received empty response for playlist from server.\")\r\n\r\n            result = m3u8.loads(content=response.text, uri=url_item)\r\n            break\r\n\r\n        return result\r\n\r\n    @staticmethod\r\n    def detect_subtitles_type(subtitles_media: m3u8.Media) -> SubtitlesType | None:\r\n        \"\"\"\r\n        Detect the subtitles type (Closed Captions, Forced, etc.) from an M3U8 Media object.\r\n\r\n        Args:\r\n            subtitles_media (m3u8.Media): Subtitles Media object to detect the type of.\r\n\r\n        Returns:\r\n            SubtitlesType | None: The type of the subtitles, None for regular subtitles.\r\n        \"\"\"\r\n        if subtitles_media.forced == \"YES\":\r\n            return SubtitlesType.FORCED\r\n\r\n        if subtitles_media.characteristics is not None and \"public.accessibility\" in subtitles_media.characteristics:\r\n            return SubtitlesType.CC\r\n\r\n        return None\r\n\r\n    async def download_subtitles(self, media_data: m3u8.Media, subrip_conversion: bool = False) -> SubtitlesData:\r\n        playlist_m3u8 = await self.load_playlist(url=media_data.absolute_uri)\r\n\r\n        if playlist_m3u8 is None:\r\n            raise PlaylistLoadError(\"Could not load subtitles M3U8 playlist.\")\r\n\r\n        downloaded_segments = await self.download_segments(playlist=playlist_m3u8)\r\n        subtitles = self.subtitles_class(data=downloaded_segments[0], language_code=media_data.language)\r\n\r\n        if len(downloaded_segments) > 1:\r\n            for segment_data in downloaded_segments[1:]:\r\n                segment_subtitles_obj = self.subtitles_class(data=segment_data, language_code=media_data.language)\r\n                subtitles.append_subtitles(segment_subtitles_obj)\r\n\r\n        subtitles.polish(\r\n            fix_rtl=self.subtitles_fix_rtl,\r\n            remove_duplicates=self.subtitles_remove_duplicates,\r\n        )\r\n\r\n        if subrip_conversion:\r\n            subtitles_format = SubtitlesFormatType.SUBRIP\r\n            content = subtitles.to_srt().dump()\r\n\r\n        else:\r\n            subtitles_format = SubtitlesFormatType.WEBVTT\r\n            content = subtitles.dump()\r\n\r\n        return SubtitlesData(\r\n            language_code=media_data.language,\r\n            language_name=self.parse_language_name(media_data=media_data),\r\n            subtitles_format=subtitles_format,\r\n            content=content,\r\n            content_encoding=subtitles.encoding,\r\n            special_type=self.detect_subtitles_type(subtitles_media=media_data),\r\n        )\r\n\r\n    async def download_segments(self, playlist: m3u8.M3U8) -> list[bytes]:\r\n        responses = await asyncio.gather(\r\n            *[\r\n                self._async_session.get(url=segment.absolute_uri)\r\n                for segment in playlist.segments\r\n            ],\r\n        )\r\n\r\n        responses_data = []\r\n\r\n        for result in responses:\r\n            try:\r\n                result.raise_for_status()\r\n                responses_data.append(result.content)\r\n\r\n            except Exception as e:\r\n                raise DownloadError(\"One of the subtitles segments failed to download.\") from e\r\n\r\n        return responses_data\r\n\r\n    def find_matching_media(self, main_playlist: m3u8.M3U8,\r\n                            filters: dict[str, str | list[str]] | None = None) -> list[m3u8.Media]:\r\n        results: list[m3u8.Media] = []\r\n        playlist_filters: dict[str, Union[str, list[str]]] | None\r\n\r\n        if self._playlist_filters:\r\n            # Merge filtering dictionaries into a single dictionary\r\n            playlist_filters = merge_dict_values(\r\n                *[dict_item for dict_item in (filters, self._playlist_filters)\r\n                  if dict_item is not None],\r\n            )\r\n\r\n        else:\r\n            playlist_filters = filters\r\n\r\n        for media in main_playlist.media:\r\n            if not playlist_filters:\r\n                results.append(media)\r\n                continue\r\n\r\n            is_valid = True\r\n\r\n            for filter_name, filter_value in playlist_filters.items():\r\n                # Skip filter if its value is None\r\n                if filter_value is None:\r\n                    continue\r\n\r\n                try:\r\n                    filter_name_enum = HLSScraper.M3U8Attribute(filter_name)\r\n                    attribute_value = getattr(media, filter_name_enum.name.lower(), None)\r\n\r\n                    if (attribute_value is None) or (\r\n                            isinstance(filter_value, list) and\r\n                            attribute_value.casefold() not in (x.casefold() for x in filter_value)\r\n                    ) or (\r\n                            isinstance(filter_value, str) and filter_value.casefold() != attribute_value.casefold()\r\n                    ):\r\n                        is_valid = False\r\n                        break\r\n\r\n                except Exception:\r\n                    is_valid = False\r\n\r\n            if is_valid:\r\n                results.append(media)\r\n\r\n        return results\r\n\r\n    def find_matching_subtitles(self, main_playlist: m3u8.M3U8,\r\n                                language_filter: list[str] | None = None) -> list[m3u8.Media]:\r\n        _filters = self._subtitles_filters\r\n\r\n        if language_filter:\r\n            _filters[self.M3U8Attribute.LANGUAGE.value] = language_filter\r\n\r\n        return self.find_matching_media(main_playlist=main_playlist, filters=_filters)\r\n\r\n\r\nclass ScraperFactory:\r\n    _scraper_classes_cache: list[type[Scraper]] | None = None\r\n    _scraper_instances_cache: dict[type[Scraper], Scraper] = {}\r\n    _currently_initializing: list[type[Scraper]] = []  # Used to prevent infinite recursion\r\n\r\n    @classmethod\r\n    def get_initialized_scrapers(cls) -> list[Scraper]:\r\n        \"\"\"\r\n        Get a list of all previously initialized scrapers.\r\n\r\n        Returns:\r\n            list[Scraper]: A list of initialized scrapers.\r\n        \"\"\"\r\n        return list(cls._scraper_instances_cache.values())\r\n\r\n    @classmethod\r\n    def get_scraper_classes(cls) -> list[type[Scraper]]:\r\n        \"\"\"\r\n        Find all scraper classes in the scrapers directory.\r\n\r\n        Returns:\r\n            list[Scraper]: A Scraper subclass.\r\n        \"\"\"\r\n        if cls._scraper_classes_cache is not None:\r\n            return cls._scraper_classes_cache\r\n\r\n        cls._scraper_classes_cache = []\r\n        scraper_modules_paths = Path(__file__).parent.glob(f\"*{SCRAPER_MODULES_SUFFIX}.py\")\r\n\r\n        for scraper_module_path in scraper_modules_paths:\r\n            sys.path.append(str(scraper_module_path))\r\n\r\n            module = importlib.import_module(f\"{PACKAGE_NAME}.scrapers.{scraper_module_path.stem}\")\r\n\r\n            # Find all 'Scraper' subclasses\r\n            for _, obj in inspect.getmembers(module,\r\n                                             predicate=lambda x: inspect.isclass(x) and issubclass(x, Scraper)):\r\n                # Skip object if it's an abstract or imported from another module\r\n                if not inspect.isabstract(obj) and obj.__module__ == module.__name__:\r\n                    cls._scraper_classes_cache.append(obj)\r\n\r\n        return cls._scraper_classes_cache\r\n\r\n    @classmethod\r\n    def _get_scraper_instance(cls, scraper_class: type[ScraperT], kwargs: dict | None = None,\r\n                              scraper_config: Scraper.ScraperConfig | None = None) -> ScraperT:\r\n        \"\"\"\r\n        Initialize and return a scraper instance.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT]): A scraper class to initialize.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            scraper_config (ScraperT.ScraperConfig | None, optional):\r\n                A scraper configuration object to pass to the scraper. Defaults to None.\r\n\r\n        Returns:\r\n            Scraper: An instance of the given scraper class.\r\n        \"\"\"\r\n        logger.debug(f\"Initializing '{scraper_class.name}' scraper...\")\r\n        kwargs = kwargs or {}\r\n        kwargs.update({\"config\": scraper_config})\r\n\r\n        if scraper_class not in cls._scraper_instances_cache:\r\n            logger.debug(f\"'{scraper_class.name}' scraper not found in cache, creating a new instance...\")\r\n\r\n            if scraper_class in cls._currently_initializing:\r\n                raise ScraperError(f\"'{scraper_class.name}' scraper is already being initialized.\\n\"\r\n                                   f\"Make sure there are no circular dependencies between scrapers.\")\r\n\r\n            cls._currently_initializing.append(scraper_class)\r\n\r\n            cls._scraper_instances_cache[scraper_class] = scraper_class(**kwargs)\r\n            cls._currently_initializing.remove(scraper_class)\r\n\r\n        else:\r\n            logger.debug(f\"Cached '{scraper_class.name}' scraper instance found and will be used.\")\r\n\r\n        return cls._scraper_instances_cache[scraper_class]  # type: ignore[return-value]\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[True] = ...) -> ScraperT:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[False] = ...) -> ScraperT | None:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[True] = ...) -> Scraper:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[False] = ...) -> Scraper | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def get_scraper_instance(cls, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,\r\n                             url: str | None = None, kwargs: dict | None = None,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: bool = True) -> Scraper | None:\r\n        \"\"\"\r\n        Find, initialize and return a scraper that matches the given URL or ID.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT] | None, optional): A scraper class to initialize. Defaults to None.\r\n            scraper_id (str | None, optional): ID of a scraper to initialize. Defaults to None.\r\n            url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            scrapers_configs (dict[str, ScraperConfigBase] | None, optional): A dictionary containing configurations\r\n                for scrapers, mapping scraper IDs to their configurations. Defaults to None.\r\n            raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.\r\n\r\n        Returns:\r\n            ScraperT | Scraper | None: An instance of a scraper that matches the given URL or ID,\r\n                None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If no scraper was found and 'raise_error' is True.\r\n        \"\"\"\r\n        if not scrapers_configs:\r\n            scrapers_configs = {}  # Allow `.get()` usage without checking for None\r\n\r\n        if not any((scraper_class, scraper_id, url)):\r\n            raise ValueError(\"At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.\")\r\n\r\n        if scraper_class:\r\n            return cls._get_scraper_instance(\r\n                scraper_class=scraper_class,\r\n                kwargs=kwargs,\r\n                scraper_config=scrapers_configs.get(scraper_class.id),\r\n            )\r\n\r\n        if scraper_id:\r\n            logger.debug(f\"Searching for a scraper object with ID '{scraper_id}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.id == scraper_id:\r\n                    return cls._get_scraper_instance(\r\n                        scraper_class=scraper,\r\n                        kwargs=kwargs,\r\n                        scraper_config=scrapers_configs.get(scraper_id),\r\n                    )\r\n\r\n        elif url:\r\n            logger.debug(f\"Searching for a scraper object that matches URL '{url}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.match_url(url) is not None:\r\n                    return cls._get_scraper_instance(\r\n                        scraper_class=scraper,\r\n                        kwargs=kwargs,\r\n                        scraper_config=scrapers_configs.get(scraper.id),\r\n                    )\r\n\r\n        error_message = \"No matching scraper was found.\"\r\n\r\n        if raise_error:\r\n            raise ValueError(error_message)\r\n\r\n        logger.debug(error_message)\r\n        return None\r\n\r\n\r\nclass ScraperError(Exception):\r\n    pass\r\n\r\n\r\nclass DownloadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass PlaylistLoadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass SubtitlesDownloadError(ScraperError):\r\n    def __init__(self, language_code: str, language_name: str | None = None, special_type: SubtitlesType | None = None,\r\n                 original_exc: Exception | None = None, *args: Any, **kwargs: dict[str, Any]):\r\n        \"\"\"\r\n        Initialize a SubtitlesDownloadError instance.\r\n\r\n        Args:\r\n            language_code (str): Language code of the subtitles that failed to download.\r\n            language_name (str | None, optional): Language name of the subtitles that failed to download.\r\n            special_type (SubtitlesType | None, optional): Type of the subtitles that failed to download.\r\n            original_exc (Exception | None, optional): The original exception that caused the error.\r\n        \"\"\"\r\n        super().__init__(*args, **kwargs)\r\n        self.language_code = language_code\r\n        self.language_name = language_name\r\n        self.special_type = special_type\r\n        self.original_exc = original_exc\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/scraper.py b/isubrip/scrapers/scraper.py
--- a/isubrip/scrapers/scraper.py	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/isubrip/scrapers/scraper.py	(date 1730277090530)
@@ -403,6 +403,8 @@
                                                     self.default_playlist_filters)
 
         if self._playlist_filters != self.default_playlist_filters:
+            # TODO: Currently seem to always print:
+            # Initializing 'iTunes' scraper with custom playlist filters: {'assoc-language': None, 'autoselect': None, 'characteristics': None, 'channels': None, 'default': None, 'forced': None, 'group-id': None, 'instream-id': None, 'language': None, 'name': None, 'stable-rendition-id': None, 'type': None}.
             logger.debug(f"Initializing '{self.name}' scraper with custom playlist filters: {self._playlist_filters}.")
 
     def parse_language_name(self, media_data: m3u8.Media) -> str | None:
@@ -616,24 +618,19 @@
         return cls._scraper_classes_cache
 
     @classmethod
-    def _get_scraper_instance(cls, scraper_class: type[ScraperT], kwargs: dict | None = None,
-                              scraper_config: Scraper.ScraperConfig | None = None) -> ScraperT:
+    def _get_scraper_instance(cls, scraper_class: type[ScraperT], **kwargs: dict) -> ScraperT:
         """
         Initialize and return a scraper instance.
 
         Args:
             scraper_class (type[ScraperT]): A scraper class to initialize.
-            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.
+            kwargs (dict ): Parameters to pass to the scraper's constructor.
                 Defaults to None.
-            scraper_config (ScraperT.ScraperConfig | None, optional):
-                A scraper configuration object to pass to the scraper. Defaults to None.
 
         Returns:
             Scraper: An instance of the given scraper class.
         """
         logger.debug(f"Initializing '{scraper_class.name}' scraper...")
-        kwargs = kwargs or {}
-        kwargs.update({"config": scraper_config})
 
         if scraper_class not in cls._scraper_instances_cache:
             logger.debug(f"'{scraper_class.name}' scraper not found in cache, creating a new instance...")
@@ -656,7 +653,6 @@
     @overload
     def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,
                              url: str | None = ..., kwargs: dict | None = ...,
-                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,
                              raise_error: Literal[True] = ...) -> ScraperT:
         ...
 
@@ -664,7 +660,6 @@
     @overload
     def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,
                              url: str | None = ..., kwargs: dict | None = ...,
-                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,
                              raise_error: Literal[False] = ...) -> ScraperT | None:
         ...
 
@@ -672,7 +667,6 @@
     @overload
     def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,
                              url: str | None = ..., kwargs: dict | None = ...,
-                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,
                              raise_error: Literal[True] = ...) -> Scraper:
         ...
 
@@ -680,14 +674,12 @@
     @overload
     def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,
                              url: str | None = ..., kwargs: dict | None = ...,
-                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,
                              raise_error: Literal[False] = ...) -> Scraper | None:
         ...
 
     @classmethod
     def get_scraper_instance(cls, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,
                              url: str | None = None, kwargs: dict | None = None,
-                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,
                              raise_error: bool = True) -> Scraper | None:
         """
         Find, initialize and return a scraper that matches the given URL or ID.
@@ -698,8 +690,6 @@
             url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.
             kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.
                 Defaults to None.
-            scrapers_configs (dict[str, ScraperConfigBase] | None, optional): A dictionary containing configurations
-                for scrapers, mapping scraper IDs to their configurations. Defaults to None.
             raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.
 
         Returns:
@@ -709,9 +699,6 @@
         Raises:
             ValueError: If no scraper was found and 'raise_error' is True.
         """
-        if not scrapers_configs:
-            scrapers_configs = {}  # Allow `.get()` usage without checking for None
-
         if not any((scraper_class, scraper_id, url)):
             raise ValueError("At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.")
 
@@ -719,7 +706,6 @@
             return cls._get_scraper_instance(
                 scraper_class=scraper_class,
                 kwargs=kwargs,
-                scraper_config=scrapers_configs.get(scraper_class.id),
             )
 
         if scraper_id:
@@ -729,7 +715,6 @@
                     return cls._get_scraper_instance(
                         scraper_class=scraper,
                         kwargs=kwargs,
-                        scraper_config=scrapers_configs.get(scraper_id),
                     )
 
         elif url:
@@ -739,7 +724,6 @@
                     return cls._get_scraper_instance(
                         scraper_class=scraper,
                         kwargs=kwargs,
-                        scraper_config=scrapers_configs.get(scraper.id),
                     )
 
         error_message = "No matching scraper was found."
Index: pyproject.toml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>[project]\r\nname = \"isubrip\"\r\nversion = \"2.5.6\"\r\ndescription = \"A Python package for scraping and downloading subtitles from AppleTV / iTunes movie pages.\"\r\nauthors = [\r\n    {name = \"Michael Yochpaz\"}\r\n]\r\nreadme = \"README.md\"\r\nkeywords = [\r\n    \"iTunes\",\r\n    \"AppleTV\",\r\n    \"movies\",\r\n    \"subtitles\",\r\n    \"scrape\",\r\n    \"scraper\",\r\n    \"download\",\r\n    \"m3u8\"\r\n]\r\nclassifiers = [\r\n    \"Development Status :: 5 - Production/Stable\",\r\n    \"Intended Audience :: End Users/Desktop\",\r\n    \"Intended Audience :: Developers\",\r\n    \"Operating System :: Microsoft :: Windows\",\r\n    \"Operating System :: MacOS\",\r\n    \"Operating System :: POSIX :: Linux\",\r\n    \"Topic :: Utilities\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Programming Language :: Python :: 3.9\",\r\n    \"Programming Language :: Python :: 3.10\",\r\n    \"Programming Language :: Python :: 3.11\",\r\n    \"Programming Language :: Python :: 3.12\",\r\n]\r\nrequires-python = \">= 3.9\"\r\ndependencies = [\r\n    \"httpx[http2]>=0.27.2\",\r\n    \"m3u8>=6.0.0\",\r\n    \"pydantic-settings>=2.6.0\",\r\n    \"pydantic>=2.9.2\",\r\n    \"tomli>=2.0.2\",\r\n]\r\n\r\n[project.urls]\r\nHomepage = \"https://github.com/MichaelYochpaz/iSubRip\"\r\nRepository = \"https://github.com/MichaelYochpaz/iSubRip\"\r\nIssues = \"https://github.com/MichaelYochpaz/iSubRip/issues\"\r\nChangelog = \"https://github.com/MichaelYochpaz/iSubRip/blob/main/CHANGELOG.md\"\r\n\r\n[project.scripts]\r\nisubrip = \"isubrip.__main__:main\"\r\n\r\n[build-system]\r\nrequires = [\"hatchling\"]\r\nbuild-backend = \"hatchling.build\"\r\n\r\n[tool.uv]\r\ndev-dependencies = [\r\n    \"mypy>=1.13.0\",\r\n    \"ruff>=0.7.1\",\r\n]\r\n\r\n[tool.mypy]\r\ncheck_untyped_defs = true\r\ndisallow_untyped_defs = true\r\nexplicit_package_bases = true\r\nignore_missing_imports = true\r\npython_version = \"3.9\"\r\nwarn_return_any = true\r\nplugins = [\r\n    \"pydantic.mypy\"\r\n]\r\n\r\n[tool.ruff]\r\nline-length = 120\r\ntarget-version = \"py39\"\r\n\r\n[tool.ruff.lint]\r\nselect = [\r\n    \"ARG\",\r\n    \"ASYNC\",\r\n    \"B\",\r\n    \"C4\",\r\n    \"COM\",\r\n    \"E\",\r\n    \"F\",\r\n    \"FA\",\r\n    \"I\",\r\n    \"INP\",\r\n    \"ISC\",\r\n    \"N\",\r\n    \"PIE\",\r\n    \"PGH\",\r\n    \"PT\",\r\n    \"PTH\",\r\n    \"Q\",\r\n    \"RSE\",\r\n    \"RET\",\r\n    \"RUF\",\r\n    \"S\",\r\n    \"SIM\",\r\n    \"SLF\",\r\n    \"T20\",\r\n    \"TCH\",\r\n    \"TID\",\r\n    \"TRY\",\r\n    \"UP\",\r\n]\r\nignore = [\r\n    \"C416\",\r\n    \"Q000\",\r\n    \"RUF010\",\r\n    \"RUF012\",\r\n    \"SIM108\",\r\n    \"TD002\",\r\n    \"TD003\",\r\n    \"TRY003\",\r\n]\r\nunfixable = [\"ARG\"]\r\n\r\n[tool.ruff.lint.flake8-tidy-imports]\r\nban-relative-imports = \"all\"\r\n\r\n[tool.ruff.lint.flake8-quotes]\r\ndocstring-quotes = \"double\"\r\n\r\n[tool.ruff.lint.isort]\r\nforce-sort-within-sections = true\r\n\r\n[tool.ruff.lint.pyupgrade]\r\nkeep-runtime-typing = true\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pyproject.toml b/pyproject.toml
--- a/pyproject.toml	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/pyproject.toml	(date 1730277111669)
@@ -37,6 +37,7 @@
     "pydantic-settings>=2.6.0",
     "pydantic>=2.9.2",
     "tomli>=2.0.2",
+    "typer>=0.12.5",
 ]
 
 [project.urls]
Index: isubrip/commands/download.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom pathlib import Path\r\nimport shutil\r\nfrom typing import TYPE_CHECKING\r\n\r\nfrom isubrip.constants import (\r\n    ARCHIVE_FORMAT,\r\n    PREORDER_MESSAGE,\r\n)\r\nfrom isubrip.data_structures import (\r\n    Episode,\r\n    MediaData,\r\n    Movie,\r\n    ScrapedMediaResponse,\r\n    Season,\r\n    Series,\r\n    SubtitlesData,\r\n    SubtitlesDownloadResults,\r\n)\r\nfrom isubrip.logger import logger\r\nfrom isubrip.scrapers.scraper import PlaylistLoadError, Scraper, ScraperError, ScraperFactory, SubtitlesDownloadError\r\nfrom isubrip.utils import (\r\n    TempDirGenerator,\r\n    download_subtitles_to_file,\r\n    format_media_description,\r\n    format_subtitles_description,\r\n    generate_media_folder_name,\r\n    generate_non_conflicting_path,\r\n    get_model_field,\r\n)\r\n\r\nif TYPE_CHECKING:\r\n    from isubrip.config import Config\r\n\r\n\r\nasync def download(urls: list[str], config: Config) -> None:\r\n    \"\"\"\r\n    Download subtitles from a given URL.\r\n\r\n    Args:\r\n        urls (list[str]): A list of URLs to download subtitles from.\r\n        config (Config): A config to use for downloading subtitles.\r\n    \"\"\"\r\n    scrapers_configs = {\r\n        scraper_id: get_model_field(config.scrapers, scraper_id) for scraper_id in config.scrapers.model_fields\r\n    }\r\n\r\n    for url in urls:\r\n        try:\r\n            logger.info(f\"Scraping '{url}'...\")\r\n\r\n            scraper = ScraperFactory.get_scraper_instance(url=url, scrapers_configs=scrapers_configs)\r\n\r\n            try:\r\n                logger.debug(f\"Fetching '{url}'...\")\r\n                scraper_response: ScrapedMediaResponse = await scraper.get_data(url=url)\r\n\r\n            except ScraperError as e:\r\n                logger.error(f\"Error: {e}\")\r\n                logger.debug(\"Debug information:\", exc_info=True)\r\n                continue\r\n\r\n            media_data = scraper_response.media_data\r\n            playlist_scraper = ScraperFactory.get_scraper_instance(scraper_id=scraper_response.playlist_scraper,\r\n                                                                   scrapers_configs=scrapers_configs)\r\n\r\n            if not media_data:\r\n                logger.error(f\"Error: No supported media was found for {url}.\")\r\n                continue\r\n\r\n            for media_item in media_data:\r\n                try:\r\n                    logger.info(f\"Found {media_item.media_type}: {format_media_description(media_data=media_item)}\")\r\n                    await download_media(scraper=playlist_scraper,\r\n                                         media_item=media_item,\r\n                                         download_path=config.downloads.folder,\r\n                                         language_filter=config.downloads.languages,\r\n                                         convert_to_srt=config.subtitles.convert_to_srt,\r\n                                         overwrite_existing=config.downloads.overwrite_existing,\r\n                                         archive=config.downloads.zip)\r\n\r\n                except Exception as e:\r\n                    if len(media_data) > 1:\r\n                        logger.warning(f\"Error scraping media item \"\r\n                                       f\"'{format_media_description(media_data=media_item)}': {e}\\n\"\r\n                                       f\"Skipping to next media item...\")\r\n                        logger.debug(\"Debug information:\", exc_info=True)\r\n                        continue\r\n\r\n                    raise\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error while scraping '{url}': {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            continue\r\n\r\n\r\nasync def download_media(scraper: Scraper, media_item: MediaData, download_path: Path,\r\n                              language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                              overwrite_existing: bool = True, archive: bool = False) -> None:\r\n    \"\"\"\r\n    Download a media item.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_item (MediaData): A media data item to download subtitles for.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        archive (bool, optional): Whether to archive the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n    \"\"\"\r\n    if isinstance(media_item, Series):\r\n        for season in media_item.seasons:\r\n            await download_media(media_item=season, scraper=scraper, download_path=download_path,\r\n                                 language_filter=language_filter, convert_to_srt=convert_to_srt,\r\n                                 overwrite_existing=overwrite_existing, archive=archive)\r\n\r\n    elif isinstance(media_item, Season):\r\n        for episode in media_item.episodes:\r\n            logger.info(f\"{format_media_description(media_data=episode, shortened=True)}:\")\r\n            await download_media_item(media_item=episode, scraper=scraper, download_path=download_path,\r\n                                 language_filter=language_filter, convert_to_srt=convert_to_srt,\r\n                                 overwrite_existing=overwrite_existing, archive=archive)\r\n\r\n    elif isinstance(media_item, (Movie, Episode)):\r\n        await download_media_item(media_item=media_item, scraper=scraper, download_path=download_path,\r\n                                 language_filter=language_filter, convert_to_srt=convert_to_srt,\r\n                                 overwrite_existing=overwrite_existing, archive=archive)\r\n\r\n\r\nasync def download_media_item(scraper: Scraper, media_item: Movie | Episode, download_path: Path,\r\n                              language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                              overwrite_existing: bool = True, archive: bool = False) -> None:\r\n    \"\"\"\r\n    Download subtitles for a single media item.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_item (Movie | Episode): A movie or episode data object.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        archive (bool, optional): Whether to archive the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n    \"\"\"\r\n    ex: Exception | None = None\r\n\r\n    if media_item.playlist:\r\n        try:\r\n            results = await download_subtitles(\r\n                scraper=scraper,\r\n                media_data=media_item,\r\n                download_path=download_path,\r\n                language_filter=language_filter,\r\n                convert_to_srt=convert_to_srt,\r\n                overwrite_existing=overwrite_existing,\r\n                archive=archive,\r\n            )\r\n\r\n            success_count = len(results.successful_subtitles)\r\n            failed_count = len(results.failed_subtitles)\r\n\r\n            if success_count or failed_count:\r\n                logger.info(f\"{success_count}/{success_count + failed_count} matching subtitles \"\r\n                            f\"were successfully downloaded.\")\r\n\r\n            else:\r\n                logger.info(\"No matching subtitles were found.\")\r\n\r\n            return  # noqa: TRY300\r\n\r\n        except PlaylistLoadError as e:\r\n            ex = e\r\n\r\n    # We get here if there is no playlist, or there is one, but it failed to load\r\n    if isinstance(media_item, Movie) and media_item.preorder_availability_date:\r\n        preorder_date_str = media_item.preorder_availability_date.strftime(\"%Y-%m-%d\")\r\n        logger.info(PREORDER_MESSAGE.format(movie_name=media_item.name, scraper_name=scraper.name,\r\n                                            preorder_date=preorder_date_str))\r\n\r\n    else:\r\n        if ex:\r\n            logger.error(f\"Error: {ex}\")\r\n\r\n        else:\r\n            logger.error(\"Error: No valid playlist was found.\")\r\n\r\n\r\nasync def download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,\r\n                             language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                             overwrite_existing: bool = True, archive: bool = False) -> SubtitlesDownloadResults:\r\n    \"\"\"\r\n    Download subtitles for the given media data.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        archive (bool, optional): Whether to archive the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n\r\n    Returns:\r\n        SubtitlesDownloadResults: A SubtitlesDownloadResults object containing the results of the download.\r\n    \"\"\"\r\n    temp_dir_name = generate_media_folder_name(media_data=media_data, source=scraper.abbreviation)\r\n    temp_download_path = TempDirGenerator.generate(directory_name=temp_dir_name)\r\n\r\n    successful_downloads: list[SubtitlesData] = []\r\n    failed_downloads: list[SubtitlesDownloadError] = []\r\n    temp_downloads: list[Path] = []\r\n\r\n    if not media_data.playlist:\r\n        raise PlaylistLoadError(\"No playlist was found for provided media data.\")\r\n\r\n    main_playlist = await scraper.load_playlist(url=media_data.playlist)  # type: ignore[func-returns-value]\r\n\r\n    if not main_playlist:\r\n        raise PlaylistLoadError(\"Failed to load the main playlist.\")\r\n\r\n    matching_subtitles = scraper.find_matching_subtitles(main_playlist=main_playlist,  # type: ignore[var-annotated]\r\n                                                         language_filter=language_filter)\r\n\r\n    logger.debug(f\"{len(matching_subtitles)} matching subtitles were found.\")\r\n\r\n    for matching_subtitles_item in matching_subtitles:\r\n        subtitles_data = await scraper.download_subtitles(media_data=matching_subtitles_item,\r\n                                                          subrip_conversion=convert_to_srt)\r\n        language_info = format_subtitles_description(language_code=subtitles_data.language_code,\r\n                                                     language_name=subtitles_data.language_name,\r\n                                                     special_type=subtitles_data.special_type)\r\n\r\n        if isinstance(subtitles_data, SubtitlesDownloadError):\r\n            logger.warning(f\"Failed to download '{language_info}' subtitles. Skipping...\")\r\n            logger.debug(\"Debug information:\", exc_info=subtitles_data.original_exc)\r\n            failed_downloads.append(subtitles_data)\r\n            continue\r\n\r\n        try:\r\n            temp_downloads.append(download_subtitles_to_file(\r\n                media_data=media_data,\r\n                subtitles_data=subtitles_data,\r\n                output_path=temp_download_path,\r\n                source_abbreviation=scraper.abbreviation,\r\n                overwrite=overwrite_existing,\r\n            ))\r\n\r\n            logger.info(f\"'{language_info}' subtitles were successfully downloaded.\")\r\n            successful_downloads.append(subtitles_data)\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error: Failed to save '{language_info}' subtitles: {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            failed_downloads.append(\r\n                SubtitlesDownloadError(\r\n                    language_code=subtitles_data.language_code,\r\n                    language_name=subtitles_data.language_name,\r\n                    special_type=subtitles_data.special_type,\r\n                    original_exc=e,\r\n                ),\r\n            )\r\n\r\n    if not archive or len(temp_downloads) == 1:\r\n        for file_path in temp_downloads:\r\n            if overwrite_existing:\r\n                new_path = download_path / file_path.name\r\n\r\n            else:\r\n                new_path = generate_non_conflicting_path(file_path=download_path / file_path.name)\r\n\r\n            # str conversion needed only for Python <= 3.8 - https://github.com/python/cpython/issues/76870\r\n            shutil.move(src=str(file_path), dst=new_path)\r\n\r\n    elif len(temp_downloads) > 0:\r\n        archive_path = Path(shutil.make_archive(\r\n            base_name=str(temp_download_path.parent / temp_download_path.name),\r\n            format=ARCHIVE_FORMAT,\r\n            root_dir=temp_download_path,\r\n        ))\r\n\r\n        file_name = generate_media_folder_name(media_data=media_data,\r\n                                               source=scraper.abbreviation) + f\".{ARCHIVE_FORMAT}\"\r\n\r\n        if overwrite_existing:\r\n            destination_path = download_path / file_name\r\n\r\n        else:\r\n            destination_path = generate_non_conflicting_path(file_path=download_path / file_name)\r\n\r\n        shutil.move(src=str(archive_path), dst=destination_path)\r\n\r\n    return SubtitlesDownloadResults(\r\n        media_data=media_data,\r\n        successful_subtitles=successful_downloads,\r\n        failed_subtitles=failed_downloads,\r\n        is_archive=archive,\r\n    )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/commands/download.py b/isubrip/commands/download.py
--- a/isubrip/commands/download.py	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/isubrip/commands/download.py	(date 1730277090509)
@@ -4,8 +4,11 @@
 import shutil
 from typing import TYPE_CHECKING
 
+import typer
+
 from isubrip.constants import (
     ARCHIVE_FORMAT,
+    EVENT_LOOP,
     PREORDER_MESSAGE,
 )
 from isubrip.data_structures import (
@@ -27,30 +30,62 @@
     format_subtitles_description,
     generate_media_folder_name,
     generate_non_conflicting_path,
-    get_model_field,
 )
 
-if TYPE_CHECKING:
-    from isubrip.config import Config
+app = typer.Typer()
 
+@app.command(name="download")
+def download_command(
+        urls: list[str] = typer.Argument(...,
+                                         help="URLs of media to download."),
+        output_path: Path = typer.Option(Path.cwd(), "--output", "-o",
+                                           help="Path to download to."),
+        language_filter: list[str] | None = typer.Option(None, "--language", "-l",
+                                                         help="Specific languages to download."),
+        convert_to_srt: bool = typer.Option(False,
+                                            "--srt", "-s", help="Convert subtitles to SRT."),
+        overwrite_existing: bool = typer.Option(False,
+                                                "--overwrite", "-w", help="Overwrite existing subtitles."),
+        archive: bool = typer.Option(False, "--archive", "-a",
+                                     help="Archive multiple subtitles into a single file."),
+) -> None:
+    EVENT_LOOP.run_until_complete(
+        download(
+            urls=urls,
+            output_path=output_path,
+            language_filter=language_filter,
+            convert_to_srt=convert_to_srt,
+            overwrite_existing=overwrite_existing,
+            archive=archive,
+        )
+    )
 
-async def download(urls: list[str], config: Config) -> None:
+async def download(urls: list[str],
+                   output_path: Path,
+                   language_filter: list[str] | None = None,
+                   convert_to_srt: bool = False,
+                   fix_rtl: bool = False,
+                   overwrite_existing: bool = True,
+                   archive: bool = False,
+                   ) -> None:  # TODO: Accept parameters instead of "config"
     """
     Download subtitles from a given URL.
 
     Args:
         urls (list[str]): A list of URLs to download subtitles from.
-        config (Config): A config to use for downloading subtitles.
+        output_path (Path): Path to a folder where the subtitles will be downloaded to.
+        language_filter (list[str] | None): List of specific languages to download subtitles for.
+            None for all languages (no filter). Defaults to None.
+        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.
+        fix_rtl (bool, optional): Whether to fix right-to-left (RTL) text direction in subtitles. Defaults to False.
+        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.
+        archive (bool, optional): Whether to archive the subtitles into a single zip file.
     """
-    scrapers_configs = {
-        scraper_id: get_model_field(config.scrapers, scraper_id) for scraper_id in config.scrapers.model_fields
-    }
-
     for url in urls:
         try:
             logger.info(f"Scraping '{url}'...")
 
-            scraper = ScraperFactory.get_scraper_instance(url=url, scrapers_configs=scrapers_configs)
+            scraper = ScraperFactory.get_scraper_instance(url=url)
 
             try:
                 logger.debug(f"Fetching '{url}'...")
@@ -62,8 +97,7 @@
                 continue
 
             media_data = scraper_response.media_data
-            playlist_scraper = ScraperFactory.get_scraper_instance(scraper_id=scraper_response.playlist_scraper,
-                                                                   scrapers_configs=scrapers_configs)
+            playlist_scraper = ScraperFactory.get_scraper_instance(scraper_id=scraper_response.playlist_scraper)
 
             if not media_data:
                 logger.error(f"Error: No supported media was found for {url}.")
@@ -74,11 +108,11 @@
                     logger.info(f"Found {media_item.media_type}: {format_media_description(media_data=media_item)}")
                     await download_media(scraper=playlist_scraper,
                                          media_item=media_item,
-                                         download_path=config.downloads.folder,
-                                         language_filter=config.downloads.languages,
-                                         convert_to_srt=config.subtitles.convert_to_srt,
-                                         overwrite_existing=config.downloads.overwrite_existing,
-                                         archive=config.downloads.zip)
+                                         output_path=output_path,
+                                         language_filter=language_filter,
+                                         convert_to_srt=convert_to_srt,
+                                         overwrite_existing=overwrite_existing,
+                                         archive=archive)
 
                 except Exception as e:
                     if len(media_data) > 1:
@@ -96,16 +130,21 @@
             continue
 
 
-async def download_media(scraper: Scraper, media_item: MediaData, download_path: Path,
-                              language_filter: list[str] | None = None, convert_to_srt: bool = False,
-                              overwrite_existing: bool = True, archive: bool = False) -> None:
+async def download_media(scraper: Scraper,
+                         media_item: MediaData,
+                         output_path: Path,
+                         language_filter: list[str] | None = None,
+                         convert_to_srt: bool = False,
+                         fix_rtl: bool = False,
+                         overwrite_existing: bool = True,
+                         archive: bool = False) -> None:
     """
     Download a media item.
 
     Args:
         scraper (Scraper): A Scraper object to use for downloading subtitles.
         media_item (MediaData): A media data item to download subtitles for.
-        download_path (Path): Path to a folder where the subtitles will be downloaded to.
+        output_path (Path): Path to a folder where the subtitles will be downloaded to.
         language_filter (list[str] | None): List of specific languages to download subtitles for.
             None for all languages (no filter). Defaults to None.
         convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.
@@ -113,98 +152,81 @@
         archive (bool, optional): Whether to archive the subtitles into a single zip file
             (only if there are multiple subtitles).
     """
-    if isinstance(media_item, Series):
-        for season in media_item.seasons:
-            await download_media(media_item=season, scraper=scraper, download_path=download_path,
-                                 language_filter=language_filter, convert_to_srt=convert_to_srt,
-                                 overwrite_existing=overwrite_existing, archive=archive)
-
-    elif isinstance(media_item, Season):
-        for episode in media_item.episodes:
-            logger.info(f"{format_media_description(media_data=episode, shortened=True)}:")
-            await download_media_item(media_item=episode, scraper=scraper, download_path=download_path,
-                                 language_filter=language_filter, convert_to_srt=convert_to_srt,
-                                 overwrite_existing=overwrite_existing, archive=archive)
-
-    elif isinstance(media_item, (Movie, Episode)):
-        await download_media_item(media_item=media_item, scraper=scraper, download_path=download_path,
-                                 language_filter=language_filter, convert_to_srt=convert_to_srt,
-                                 overwrite_existing=overwrite_existing, archive=archive)
-
-
-async def download_media_item(scraper: Scraper, media_item: Movie | Episode, download_path: Path,
-                              language_filter: list[str] | None = None, convert_to_srt: bool = False,
-                              overwrite_existing: bool = True, archive: bool = False) -> None:
-    """
-    Download subtitles for a single media item.
-
-    Args:
-        scraper (Scraper): A Scraper object to use for downloading subtitles.
-        media_item (Movie | Episode): A movie or episode data object.
-        download_path (Path): Path to a folder where the subtitles will be downloaded to.
-        language_filter (list[str] | None): List of specific languages to download subtitles for.
-            None for all languages (no filter). Defaults to None.
-        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.
-        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.
-        archive (bool, optional): Whether to archive the subtitles into a single zip file
-            (only if there are multiple subtitles).
-    """
-    ex: Exception | None = None
+    if isinstance(media_item, (Movie, Episode)):
+        ex: Exception | None = None
 
-    if media_item.playlist:
-        try:
-            results = await download_subtitles(
-                scraper=scraper,
-                media_data=media_item,
-                download_path=download_path,
-                language_filter=language_filter,
-                convert_to_srt=convert_to_srt,
-                overwrite_existing=overwrite_existing,
-                archive=archive,
-            )
+        if media_item.playlist:
+            try:
+                results = await download_subtitles(
+                    scraper=scraper,
+                    media_data=media_item,
+                    output_path=output_path,
+                    language_filter=language_filter,
+                    convert_to_srt=convert_to_srt,
+                    overwrite_existing=overwrite_existing,
+                    archive=archive,
+                )
 
-            success_count = len(results.successful_subtitles)
-            failed_count = len(results.failed_subtitles)
+                success_count = len(results.successful_subtitles)
+                failed_count = len(results.failed_subtitles)
 
-            if success_count or failed_count:
-                logger.info(f"{success_count}/{success_count + failed_count} matching subtitles "
-                            f"were successfully downloaded.")
+                if success_count or failed_count:
+                    logger.info(f"{success_count}/{success_count + failed_count} matching subtitles "
+                                f"were successfully downloaded.")
 
-            else:
-                logger.info("No matching subtitles were found.")
+                else:
+                    logger.info("No matching subtitles were found.")
 
-            return  # noqa: TRY300
+                return  # noqa: TRY300
 
-        except PlaylistLoadError as e:
-            ex = e
+            except PlaylistLoadError as e:
+                ex = e
 
-    # We get here if there is no playlist, or there is one, but it failed to load
-    if isinstance(media_item, Movie) and media_item.preorder_availability_date:
-        preorder_date_str = media_item.preorder_availability_date.strftime("%Y-%m-%d")
-        logger.info(PREORDER_MESSAGE.format(movie_name=media_item.name, scraper_name=scraper.name,
-                                            preorder_date=preorder_date_str))
+        # We get here if there is no playlist, or there is one, but it failed to load
+        if isinstance(media_item, Movie) and media_item.preorder_availability_date:
+            preorder_date_str = media_item.preorder_availability_date.strftime("%Y-%m-%d")
+            logger.info(PREORDER_MESSAGE.format(movie_name=media_item.name, scraper_name=scraper.name,
+                                                preorder_date=preorder_date_str))
 
-    else:
-        if ex:
-            logger.error(f"Error: {ex}")
+        else:
+            if ex:
+                logger.error(f"Error: {ex}")
 
-        else:
-            logger.error("Error: No valid playlist was found.")
+            else:
+                logger.error("Error: No valid playlist was found.")
 
+    elif isinstance(media_item, Season):
+        for episode in media_item.episodes:
+            logger.info(f"{format_media_description(media_data=episode, shortened=True)}:")
+            await download_media(media_item=episode, scraper=scraper, output_path=output_path,
+                                 language_filter=language_filter, convert_to_srt=convert_to_srt,
+                                 overwrite_existing=overwrite_existing, archive=archive)
+
+    elif isinstance(media_item, Series):
+        for season in media_item.seasons:
+            await download_media(media_item=season, scraper=scraper, output_path=output_path,
+                                 language_filter=language_filter, convert_to_srt=convert_to_srt,
+                                 overwrite_existing=overwrite_existing, archive=archive)
 
-async def download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,
-                             language_filter: list[str] | None = None, convert_to_srt: bool = False,
-                             overwrite_existing: bool = True, archive: bool = False) -> SubtitlesDownloadResults:
+
+async def download_subtitles(scraper: Scraper, media_data: Movie | Episode,
+                             output_path: Path,
+                             language_filter: list[str] | None = None,
+                             convert_to_srt: bool = False,
+                             fix_rtl: bool = False,
+                             overwrite_existing: bool = True,
+                             archive: bool = False) -> SubtitlesDownloadResults:
     """
     Download subtitles for the given media data.
 
     Args:
         scraper (Scraper): A Scraper object to use for downloading subtitles.
         media_data (Movie | Episode): A movie or episode data object.
-        download_path (Path): Path to a folder where the subtitles will be downloaded to.
+        output_path (Path): Path to a folder where the subtitles will be downloaded to.
         language_filter (list[str] | None): List of specific languages to download subtitles for.
             None for all languages (no filter). Defaults to None.
         convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.
+        fix_rtl (bool, optional): Whether to fix right-to-left (RTL) text direction in subtitles. Defaults to False.
         overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.
         archive (bool, optional): Whether to archive the subtitles into a single zip file
             (only if there are multiple subtitles).
@@ -272,10 +294,10 @@
     if not archive or len(temp_downloads) == 1:
         for file_path in temp_downloads:
             if overwrite_existing:
-                new_path = download_path / file_path.name
+                new_path = output_path / file_path.name
 
             else:
-                new_path = generate_non_conflicting_path(file_path=download_path / file_path.name)
+                new_path = generate_non_conflicting_path(file_path=output_path / file_path.name)
 
             # str conversion needed only for Python <= 3.8 - https://github.com/python/cpython/issues/76870
             shutil.move(src=str(file_path), dst=new_path)
@@ -291,10 +313,10 @@
                                                source=scraper.abbreviation) + f".{ARCHIVE_FORMAT}"
 
         if overwrite_existing:
-            destination_path = download_path / file_name
+            destination_path = output_path / file_name
 
         else:
-            destination_path = generate_non_conflicting_path(file_path=download_path / file_name)
+            destination_path = generate_non_conflicting_path(file_path=output_path / file_name)
 
         shutil.move(src=str(archive_path), dst=destination_path)
 
Index: isubrip/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC\r\nfrom pathlib import Path\r\nfrom typing import TYPE_CHECKING\r\n\r\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator\r\nfrom pydantic_core import PydanticCustomError\r\nfrom pydantic_settings import BaseSettings, PydanticBaseSettingsSource, SettingsConfigDict, TomlConfigSettingsSource\r\n\r\nfrom isubrip.constants import USER_CONFIG_FILE_PATH\r\nfrom isubrip.scrapers.scraper import DefaultScraperConfig, ScraperFactory\r\nfrom isubrip.utils import normalize_config_name\r\n\r\n\r\nclass ConfigCategory(BaseModel, ABC):\r\n    \"\"\"A base class for settings categories.\"\"\"\r\n    model_config = ConfigDict(\r\n        extra='allow',\r\n        alias_generator=normalize_config_name,\r\n    )\r\n\r\n\r\nclass GeneralCategory(ConfigCategory):\r\n    check_for_updates: bool = Field(default=True)\r\n    log_rotation_size: int = Field(default=15)\r\n\r\n\r\nclass DownloadsCategory(ConfigCategory):\r\n    folder: Path = Field(default=Path.cwd().resolve())\r\n    languages: list[str] = Field(default=[])\r\n    overwrite_existing: bool = Field(default=False)\r\n    zip: bool = Field(default=False)\r\n\r\n    @field_validator('folder')\r\n    @classmethod\r\n    def assure_path_exists(cls, value: Path) -> Path:\r\n        if value.exists():\r\n            if not value.is_dir():\r\n                raise PydanticCustomError(\r\n                    \"invalid_path\",\r\n                    \"Path is not a directory.\",\r\n                )\r\n\r\n        else:\r\n            raise PydanticCustomError(\r\n                \"invalid_path\",\r\n                \"Path does not exist.\")\r\n\r\n        return value\r\n\r\n\r\nclass WebVTTSubcategory(ConfigCategory):\r\n    subrip_alignment_conversion: bool = Field(default=False)\r\n\r\n\r\nclass SubtitlesCategory(ConfigCategory):\r\n    fix_rtl: bool = Field(default=False)\r\n    remove_duplicates: bool = Field(default=True)\r\n    convert_to_srt: bool = Field(default=False)\r\n    webvtt: WebVTTSubcategory = WebVTTSubcategory()\r\n\r\n\r\nclass ScrapersCategory(ConfigCategory):\r\n    default: DefaultScraperConfig = Field(default_factory=DefaultScraperConfig)\r\n\r\n\r\n# Resolve mypy errors as mypy doesn't support dynamic models.\r\nif TYPE_CHECKING:\r\n    DynamicScrapersCategory = ScrapersCategory\r\n\r\nelse:\r\n    # A config model that's dynamically created based on the available scrapers and their configurations.\r\n    DynamicScrapersCategory = create_model(\r\n        'DynamicScrapersCategory',\r\n        __base__=ScrapersCategory,\r\n        **{\r\n            scraper.id: (scraper.ScraperConfig, Field(default_factory=scraper.ScraperConfig))\r\n            for scraper in ScraperFactory.get_scraper_classes()\r\n        },  # type: ignore[call-overload]\r\n    )\r\n\r\n\r\nclass Config(BaseSettings):\r\n    model_config = SettingsConfigDict(\r\n        extra='forbid',\r\n        alias_generator=normalize_config_name,\r\n        toml_file=USER_CONFIG_FILE_PATH,\r\n    )\r\n\r\n    general: GeneralCategory = Field(default_factory=GeneralCategory)\r\n    downloads: DownloadsCategory = Field(default_factory=DownloadsCategory)\r\n    subtitles: SubtitlesCategory = Field(default_factory=SubtitlesCategory)\r\n    scrapers: DynamicScrapersCategory = Field(default_factory=DynamicScrapersCategory)\r\n\r\n    @classmethod\r\n    def settings_customise_sources(\r\n        cls,\r\n        settings_cls: type[BaseSettings],\r\n        init_settings: PydanticBaseSettingsSource,\r\n        env_settings: PydanticBaseSettingsSource,\r\n        dotenv_settings: PydanticBaseSettingsSource,\r\n        file_secret_settings: PydanticBaseSettingsSource,\r\n    ) -> tuple[PydanticBaseSettingsSource, ...]:\r\n        return (\r\n            init_settings,\r\n            TomlConfigSettingsSource(settings_cls),\r\n            env_settings,\r\n            dotenv_settings,\r\n            file_secret_settings,\r\n        )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/config.py b/isubrip/config.py
--- a/isubrip/config.py	(revision 87790b8ad14765157818cebafec3f181a13f3208)
+++ b/isubrip/config.py	(date 1730277090550)
@@ -2,13 +2,12 @@
 
 from abc import ABC
 from pathlib import Path
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, ClassVar
 
 from pydantic import BaseModel, ConfigDict, Field, create_model, field_validator
 from pydantic_core import PydanticCustomError
 from pydantic_settings import BaseSettings, PydanticBaseSettingsSource, SettingsConfigDict, TomlConfigSettingsSource
 
-from isubrip.constants import USER_CONFIG_FILE_PATH
 from isubrip.scrapers.scraper import DefaultScraperConfig, ScraperFactory
 from isubrip.utils import normalize_config_name
 
@@ -24,16 +23,17 @@
 class GeneralCategory(ConfigCategory):
     check_for_updates: bool = Field(default=True)
     log_rotation_size: int = Field(default=15)
+    verbose: bool = Field(default=False)
 
 
 class DownloadsCategory(ConfigCategory):
     folder: Path = Field(default=Path.cwd().resolve())
-    languages: list[str] = Field(default=[])
+    languages: list[str] = Field(default=[])    # TODO: Use Pydantic's Language type.
     overwrite_existing: bool = Field(default=False)
     zip: bool = Field(default=False)
 
-    @field_validator('folder')
     @classmethod
+    @field_validator('folder')  # TODO: Assure this works
     def assure_path_exists(cls, value: Path) -> Path:
         if value.exists():
             if not value.is_dir():
@@ -82,10 +82,13 @@
 
 
 class Config(BaseSettings):
+    config_file_location: ClassVar[Path | None] = None
+
     model_config = SettingsConfigDict(
         extra='forbid',
         alias_generator=normalize_config_name,
-        toml_file=USER_CONFIG_FILE_PATH,
+        cli_parse_args=False,  # TODO
+        toml_file=config_file_location,
     )
 
     general: GeneralCategory = Field(default_factory=GeneralCategory)
