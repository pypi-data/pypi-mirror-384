Index: isubrip/scrapers/scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport importlib\r\nimport inspect\r\nimport os\r\nimport re\r\nimport sys\r\nfrom abc import abstractmethod, ABC\r\nfrom enum import Enum\r\nfrom glob import glob\r\nfrom pathlib import Path\r\nfrom typing import Any, ClassVar, Iterator, List, Literal, overload, Union, TypeVar\r\n\r\nimport aiohttp\r\nimport m3u8\r\nimport requests\r\nimport requests.utils\r\nfrom m3u8 import M3U8, Media, Segment, SegmentList\r\n\r\nfrom isubrip.config import Config, ConfigSetting\r\nfrom isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX\r\nfrom isubrip.data_structures import SubtitlesData, SubtitlesFormatType, SubtitlesType, ScrapedMediaResponse\r\nfrom isubrip.logger import logger\r\nfrom isubrip.subtitle_formats.subtitles import Subtitles\r\nfrom isubrip.utils import merge_dict_values, single_to_list, SingletonMeta\r\n\r\n\r\nScraperT = TypeVar(\"ScraperT\", bound=\"Scraper\")\r\n\r\n\r\nclass Scraper(ABC, metaclass=SingletonMeta):\r\n    \"\"\"\r\n    A base class for scrapers.\r\n\r\n    Attributes:\r\n        default_user_agent (str): [Class Attribute]\r\n            Default user agent to use if no other user agent is specified when making requests.\r\n        subtitles_fix_rtl (bool): [Class Attribute] Whether to fix RTL from downloaded subtitles.\r\n        subtitles_fix_rtl_languages (list[str] | None): [Class Attribute]\r\n            A list of languages to fix RTL on. If None, a default list will be used.\r\n        subtitles_remove_duplicates (bool): [Class Attribute]\r\n            Whether to remove duplicate lines from downloaded subtitles.\r\n\r\n        id (str): [Class Attribute] ID of the scraper.\r\n        name (str): [Class Attribute] Name of the scraper.\r\n        abbreviation (str): [Class Attribute] Abbreviation of the scraper.\r\n        url_regex (str): [Class Attribute] A RegEx pattern to find URLs matching the service.\r\n        subtitles_class (type[Subtitles]): [Class Attribute] Class of the subtitles format returned by the scraper.\r\n        is_movie_scraper (bool): [Class Attribute] Whether the scraper is for movies.\r\n        is_series_scraper (bool): [Class Attribute] Whether the scraper is for series.\r\n        uses_scrapers (list[str]): [Class Attribute] A list of IDs for other scraper classes that this scraper uses.\r\n            This assures that the config data for the other scrapers is passed as well.\r\n        _session (requests.Session): A requests session to use for making requests.\r\n        config (Config): A Config object containing the scraper's configuration.\r\n    \"\"\"\r\n    default_user_agent: ClassVar[str] = requests.utils.default_user_agent()\r\n    subtitles_fix_rtl: ClassVar[bool] = False\r\n    subtitles_fix_rtl_languages: ClassVar[list | None] = [\"ar\", \"he\"]\r\n    subtitles_remove_duplicates: ClassVar[bool] = True\r\n\r\n    id: ClassVar[str]\r\n    name: ClassVar[str]\r\n    abbreviation: ClassVar[str]\r\n    url_regex: ClassVar[str | list[str]]\r\n    subtitles_class: ClassVar[type[Subtitles]]\r\n    is_movie_scraper: ClassVar[bool] = False\r\n    is_series_scraper: ClassVar[bool] = False\r\n    uses_scrapers: ClassVar[list[str]] = []\r\n\r\n    def __init__(self, config_data: dict | None = None):\r\n        \"\"\"\r\n        Initialize a Scraper object.\r\n\r\n        Args:\r\n            config_data (dict | None, optional): A dictionary containing scraper's configuration data. Defaults to None.\r\n        \"\"\"\r\n        self._session = requests.Session()\r\n        self._config_data = config_data\r\n        self.config = Config(config_data=config_data.get(self.id) if config_data else None)\r\n\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                key=\"user-agent\",\r\n                type=str,\r\n                required=False,\r\n            )],\r\n            check_config=False)\r\n\r\n        self._session.headers.update({\"User-Agent\": self.config.get(\"user-agent\") or self.default_user_agent})\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[True] = ...) -> re.Match:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[False] = ...) -> re.Match | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def match_url(cls, url: str, raise_error: bool = False) -> re.Match | None:\r\n        \"\"\"\r\n        Checks if a URL matches scraper's url regex.\r\n\r\n        Args:\r\n            url (str): A URL to check against the regex.\r\n            raise_error (bool, optional): Whether to raise an error instead of returning None if the URL doesn't match.\r\n\r\n        Returns:\r\n            re.Match | None: A Match object if the URL matches the regex, None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If the URL doesn't match the regex and raise_error is True.\r\n        \"\"\"\r\n        if isinstance(cls.url_regex, str):\r\n            return re.fullmatch(pattern=cls.url_regex, string=url, flags=re.IGNORECASE)\r\n\r\n        else:  # isinstance(cls.url_regex, (list, tuple)):\r\n            for url_regex_item in cls.url_regex:\r\n                if result := re.fullmatch(pattern=url_regex_item, string=url, flags=re.IGNORECASE):\r\n                    return result\r\n\r\n        if raise_error:\r\n            raise ValueError(f\"URL '{url}' doesn't match the URL regex of {cls.name}.\")\r\n\r\n        return None\r\n\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        self.close()\r\n\r\n    def close(self):\r\n        self._session.close()\r\n\r\n    @abstractmethod\r\n    def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        \"\"\"\r\n        Scrape media information about the media on a URL.\r\n\r\n        Args:\r\n            url (str): A URL to get media information about.\r\n\r\n        Returns:\r\n            ScrapedMediaResponse: A ScrapedMediaResponse object containing scraped media information.\r\n        \"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData]:\r\n        \"\"\"\r\n        Find and yield subtitles data from a main_playlist.\r\n\r\n        Args:\r\n            main_playlist (M3U8): Main playlist of the media to search for subtitles in.\r\n            language_filter (list[str], optional): A list of languages to filter for.\r\n            subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.\r\n\r\n        Yields:\r\n            SubtitlesData: A SubtitlesData object for each subtitle found\r\n                in the main playlist (matching the filters, if given).\r\n        \"\"\"\r\n        pass\r\n\r\n\r\nclass MovieScraper(Scraper, ABC):\r\n    \"\"\"A base class for movie scrapers.\"\"\"\r\n    is_movie_scraper = True\r\n\r\n\r\nclass SeriesScraper(Scraper, ABC):\r\n    \"\"\"A base class for series scrapers.\"\"\"\r\n    is_series_scraper = True\r\n\r\n\r\nclass AsyncScraper(Scraper, ABC):\r\n    \"\"\"A base class for scrapers that utilize async requests.\"\"\"\r\n    def __init__(self, config_data: dict | None = None):\r\n        super().__init__(config_data)\r\n        self.async_session = aiohttp.ClientSession()\r\n        self.async_session.headers.update(self._session.headers)\r\n\r\n    def close(self):\r\n        asyncio.get_event_loop().run_until_complete(self._async_close())\r\n        super().close()\r\n\r\n    async def _async_close(self):\r\n        await self.async_session.close()\r\n\r\n\r\nclass M3U8Scraper(AsyncScraper, ABC):\r\n    \"\"\"A base class for M3U8 scrapers.\"\"\"\r\n    playlist_filters_config_category = \"playlist-filters\"\r\n\r\n    class M3U8Attribute(Enum):\r\n        \"\"\"\r\n        An enum representing all possible M3U8 attributes.\r\n        Names / Keys represent M3U8 Media object attributes (should be converted to lowercase),\r\n        and values represent the name of the key for config usage.\r\n        \"\"\"\r\n        ASSOC_LANGUAGE = \"assoc-language\"\r\n        AUTOSELECT = \"autoselect\"\r\n        CHARACTERISTICS = \"characteristics\"\r\n        CHANNELS = \"channels\"\r\n        DEFAULT = \"default\"\r\n        FORCED = \"forced\"\r\n        GROUP_ID = \"group-id\"\r\n        INSTREAM_ID = \"instream-id\"\r\n        LANGUAGE = \"language\"\r\n        NAME = \"name\"\r\n        STABLE_RENDITION_ID = \"stable-rendition-id\"\r\n        TYPE = \"type\"\r\n\r\n    def __init__(self, config_data: dict | None = None):\r\n        super().__init__(config_data)\r\n\r\n        if self.config is None:\r\n            self.config = Config()\r\n\r\n        # Add M3U8 filters settings\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                category=self.playlist_filters_config_category,\r\n                key=m3u8_attribute.value,\r\n                type=Union[str, List[str]],\r\n                required=False,\r\n            ) for m3u8_attribute in self.M3U8Attribute],\r\n            check_config=False)\r\n\r\n        self._m3u8_cache: dict[str, M3U8] = {}\r\n\r\n    def _download_segments_async(self, segments: SegmentList[Segment]) -> list[bytes]:\r\n        \"\"\"\r\n        Download M3U8 segments asynchronously.\r\n\r\n        Args:\r\n            segments (m3u8.SegmentList[m3u8.Segment]): List of segments to download.\r\n\r\n        Returns:\r\n            list[bytes]: List of downloaded segments.\r\n        \"\"\"\r\n        loop = asyncio.get_event_loop()\r\n        async_tasks = [loop.create_task(self._download_segment_async(segment.absolute_uri)) for segment in segments]\r\n        segments_bytes = loop.run_until_complete(asyncio.gather(*async_tasks))\r\n\r\n        return list(segments_bytes)\r\n\r\n    async def _download_segment_async(self, url: str) -> bytes:\r\n        \"\"\"\r\n        Download an M3U8 segment asynchronously.\r\n\r\n        Args:\r\n            url (str): URL of the segment to download.\r\n\r\n        Returns:\r\n            bytes: Downloaded segment.\r\n        \"\"\"\r\n        async with self.async_session.get(url) as response:\r\n            return await response.read()\r\n\r\n    @overload\r\n    def load_m3u8(self, url: str | list[str], raise_error: Literal[True] = ...) -> M3U8:\r\n        ...\r\n\r\n    @overload\r\n    def load_m3u8(self, url: str | list[str], raise_error: Literal[False] = ...) -> M3U8 | None:\r\n        ...\r\n\r\n    def load_m3u8(self, url: str | list[str], raise_error: bool = False) -> M3U8 | None:\r\n        \"\"\"\r\n        Load an M3U8 playlist from a URL to an M3U8 object.\r\n        Multiple URLs can be given, in which case the first one that loads successfully will be returned.\r\n        The method uses caching to avoid loading the same playlist multiple times.\r\n\r\n        Args:\r\n            url (str | list[str]: URL of the M3U8 playlist to load.\r\n            raise_error (bool, optional): Whether to raise an error if none of the playlists loaded successfully.\r\n                If set to false, a None value will be returned instead. Defaults to False.\r\n\r\n        Returns:\r\n            m3u8.M3U8: An M3U8 object representing the playlist.\r\n        \"\"\"\r\n        errors = {}\r\n\r\n        for _url in single_to_list(url):\r\n            if _url in self._m3u8_cache:\r\n                return self._m3u8_cache[_url]\r\n\r\n            else:\r\n                try:\r\n                    self._m3u8_cache[_url] = m3u8.load(uri=_url, timeout=5)\r\n                    return self._m3u8_cache[_url]\r\n\r\n                except Exception as e:\r\n                    errors[_url] = e\r\n                    continue\r\n\r\n        if raise_error:\r\n            errors_str = \"\\n\".join([f\"{url}: {error}\" for url, error in errors.items()])\r\n            raise ScraperException(f\"Failed to load M3U8 playlist: {url}:\\n{errors_str}\")\r\n\r\n        return None\r\n\r\n    def _map_session_data(self, playlist_data: M3U8) -> dict[str, Any]:\r\n        \"\"\"\r\n        Create and return a dictionary of session data from an M3U8 playlist.\r\n\r\n        Args:\r\n            playlist_data (m3u8.M3U8): M3U8 playlist to map session data from.\r\n\r\n        Returns:\r\n            dict[str, Any]: Dictionary of session data.\r\n        \"\"\"\r\n        session_data = {}\r\n\r\n        if playlist_data.session_data:\r\n            for session_data_item in playlist_data.session_data:\r\n                session_data[session_data_item.data_id] = session_data_item.value\r\n\r\n        return session_data\r\n\r\n    @staticmethod\r\n    def detect_subtitles_type(subtitles_media: Media) -> SubtitlesType | None:\r\n        \"\"\"\r\n        Detect the subtitles type (Closed Captions, Forced, etc.) from an M3U8 Media object.\r\n\r\n        Args:\r\n            subtitles_media (m3u8.Media): Subtitles Media object to detect the type of.\r\n\r\n        Returns:\r\n            SubtitlesType | None: The type of the subtitles, None for regular subtitles.\r\n        \"\"\"\r\n        if subtitles_media.forced == \"YES\":\r\n            return SubtitlesType.FORCED\r\n\r\n        elif subtitles_media.characteristics is not None and \"public.accessibility\" in subtitles_media.characteristics:\r\n            return SubtitlesType.CC\r\n\r\n        return None\r\n\r\n    def find_valid_playlist(self, playlists: list[str] | str) -> M3U8 | None:\r\n        \"\"\"\r\n        Find and return a valid M3U8 playlist from a list of playlists.\r\n\r\n        Args:\r\n            playlists (list[str] | str): List of playlists to check (list[str]). Can also be a single playlist (str).\r\n\r\n        Returns:\r\n            m3u8.M3U8 | None: A successfully loaded M3U8 playlist, or None if none of the playlists loaded successfully.\r\n        \"\"\"\r\n        for playlist in single_to_list(playlists):  # type: str\r\n            try:\r\n                logger.debug(f\"Loading playlist M3U8 playlist: '{playlist}'\")\r\n                return self.load_m3u8(playlist)\r\n\r\n            except Exception:\r\n                logger.debug(f\"Failed to load playlist: '{playlist}'\")\r\n                continue\r\n\r\n        return None\r\n\r\n    def get_media_playlists(self, main_playlist: M3U8,\r\n                            playlist_filters: dict[str, str | list[str]] | None = None,\r\n                            include_default_filters: bool = True) -> list[Media]:\r\n        \"\"\"\r\n        Find and yield playlists of media within an M3U8 main_playlist using optional filters.\r\n\r\n        Args:\r\n            main_playlist (m3u8.M3U8): An M3U8 object of the main main_playlist.\r\n            playlist_filters (dict[str, str | list[str], optional):\r\n                A dictionary of filters to use when searching for subtitles.\r\n                Will be added to filters set by the config (unless `include_default_filters` is set to false).\r\n                Defaults to None.\r\n            include_default_filters (bool, optional): Whether to include the default filters set by the config or not.\r\n                Defaults to True.\r\n\r\n        Returns:\r\n            list[Media]: A list of  matching Media objects.\r\n        \"\"\"\r\n        results = []\r\n        default_filters: dict | None = self.config.get(M3U8Scraper.playlist_filters_config_category)\r\n\r\n        if include_default_filters and default_filters:\r\n            if not playlist_filters:\r\n                playlist_filters = default_filters\r\n\r\n            else:\r\n                playlist_filters = merge_dict_values(default_filters, playlist_filters)\r\n\r\n        for media in main_playlist.media:\r\n            if not playlist_filters:\r\n                results.append(media)\r\n                continue\r\n\r\n            is_valid = True\r\n\r\n            for filter_name, filter_value in playlist_filters.items():\r\n                try:\r\n                    filter_name_enum = M3U8Scraper.M3U8Attribute(filter_name)\r\n                    attribute_value = getattr(media, filter_name_enum.name.lower(), None)\r\n\r\n                    if (attribute_value is None) or (\r\n                            isinstance(filter_value, list) and\r\n                            attribute_value.casefold() not in (x.casefold() for x in filter_value)\r\n                    ) or (\r\n                            isinstance(filter_value, str) and filter_value.casefold() != attribute_value.casefold()\r\n                    ):\r\n                        is_valid = False\r\n                        break\r\n\r\n                except Exception:\r\n                    is_valid = False\r\n\r\n            if is_valid:\r\n                results.append(media)\r\n\r\n        return results\r\n\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData]:\r\n        \"\"\"\r\n        Find and yield subtitles for a movie using optional filters.\r\n\r\n        Args:\r\n            main_playlist(str | list[str]): A URL or a list of URLs (for redundancy) of the main playlist.\r\n            language_filter (list[str] | str | None, optional):\r\n                A language or a list of languages to filter for. Defaults to None.\r\n            subrip_conversion (bool, optional): Whether to convert and return the subtitles as an SRT file or not.\r\n                Defaults to False.\r\n\r\n        Yields:\r\n            SubtitlesData: A SubtitlesData NamedTuple with a matching playlist, and it's metadata.\r\n        \"\"\"\r\n        playlist_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None\r\n        main_playlist_m3u8 = self.find_valid_playlist(main_playlist)\r\n\r\n        if main_playlist_m3u8 is None:\r\n            raise PlaylistLoadError\r\n\r\n        matched_media_items = self.get_media_playlists(main_playlist=main_playlist_m3u8,\r\n                                                       playlist_filters=playlist_filters)\r\n\r\n        for matched_media in matched_media_items:\r\n            try:\r\n                matched_media_playlist = m3u8.load(matched_media.absolute_uri)\r\n                subtitles = self.subtitles_class(language_code=matched_media.language)\r\n                for segment in self._download_segments_async(matched_media_playlist.segments):\r\n                    subtitles.append_subtitles(subtitles.loads(segment.decode(\"utf-8\")))\r\n\r\n                subtitles.polish(\r\n                    fix_rtl=self.subtitles_fix_rtl,\r\n                    rtl_languages=self.subtitles_fix_rtl_languages,\r\n                    remove_duplicates=self.subtitles_remove_duplicates,\r\n                )\r\n\r\n                yield SubtitlesData(\r\n                    language_code=matched_media.language,\r\n                    language_name=matched_media.name,\r\n                    subtitles_format=SubtitlesFormatType.SUBRIP if subrip_conversion else SubtitlesFormatType.WEBVTT,\r\n                    content=subtitles.to_srt().dump() if subrip_conversion else subtitles.dump(),\r\n                    special_type=self.detect_subtitles_type(matched_media),\r\n                )\r\n\r\n            except Exception:\r\n                continue\r\n\r\n\r\nclass ScraperFactory(metaclass=SingletonMeta):\r\n    def __init__(self):\r\n        self._scraper_classes_cache: list[type[Scraper]] | None = None\r\n        self._scraper_instances_cache: dict[type[Scraper], Scraper] = {}\r\n        self._currently_initializing: list[type[Scraper]] = []  # Used to prevent infinite recursion\r\n\r\n    def get_initialized_scrapers(self) -> list[Scraper]:\r\n        \"\"\"\r\n        Get a list of all previously initialized scrapers.\r\n\r\n        Returns:\r\n            list[Scraper]: A list of initialized scrapers.\r\n        \"\"\"\r\n        return list(self._scraper_instances_cache.values())\r\n\r\n    def get_scraper_classes(self) -> Iterator[type[Scraper]]:\r\n        \"\"\"\r\n        Iterate over all scraper classes.\r\n\r\n        Yields:\r\n            type[Scraper]: A Scraper subclass.\r\n        \"\"\"\r\n        if self._scraper_classes_cache is not None:\r\n            return self._scraper_classes_cache\r\n\r\n        else:\r\n            scraper_modules_paths = glob(os.path.dirname(__file__) + f\"/*{SCRAPER_MODULES_SUFFIX}.py\")\r\n\r\n            for scraper_module_path in scraper_modules_paths:\r\n                sys.path.append(scraper_module_path)\r\n\r\n                module = importlib.import_module(f\"{PACKAGE_NAME}.scrapers.{Path(scraper_module_path).stem}\")\r\n\r\n                # Find all 'Scraper' subclasses\r\n                for _, obj in inspect.getmembers(module,\r\n                                                 predicate=lambda x: inspect.isclass(x) and issubclass(x, Scraper)):\r\n                    # Skip object if it's an abstract or imported from another module\r\n                    if not inspect.isabstract(obj) and obj.__module__ == module.__name__:\r\n                        if any((obj.is_movie_scraper, obj.is_series_scraper)):\r\n                            yield obj\r\n\r\n            return\r\n\r\n    def _get_scraper_instance(self, scraper_class: type[ScraperT],\r\n                              scrapers_config_data: dict | None = None) -> ScraperT:\r\n        \"\"\"\r\n        Initialize and return a scraper instance.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT]): A scraper class to initialize.\r\n            scrapers_config_data (dict, optional): A dictionary containing scrapers config data to use\r\n                when creating a new scraper. Defaults to None.\r\n\r\n        Returns:\r\n            Scraper: An instance of the given scraper class.\r\n        \"\"\"\r\n        logger.debug(f\"Initializing '{scraper_class.name}' scraper...\")\r\n\r\n        if scraper_class not in self._scraper_instances_cache:\r\n            logger.debug(f\"'{scraper_class.name}' scraper not found in cache, creating a new instance...\")\r\n\r\n            if scraper_class in self._currently_initializing:\r\n                raise ScraperException(f\"'{scraper_class.name}' scraper is already being initialized.\\n\"\r\n                                       f\"Make sure there are no circular dependencies between scrapers.\")\r\n\r\n            self._currently_initializing.append(scraper_class)\r\n\r\n            # Set config data for the scraper and its dependencies, if any\r\n            if not scrapers_config_data:\r\n                config_data = None\r\n\r\n            else:\r\n                required_scrapers_ids = [scraper_class.id] + scraper_class.uses_scrapers\r\n                config_data = \\\r\n                    {scraper_id: scrapers_config_data[scraper_id] for scraper_id in required_scrapers_ids\r\n                     if scrapers_config_data.get(scraper_id)}\r\n\r\n            self._scraper_instances_cache[scraper_class] = scraper_class(config_data=config_data)\r\n            self._currently_initializing.remove(scraper_class)\r\n\r\n        else:\r\n            logger.debug(f\"'{scraper_class.id}' scraper found in cache. Cached instance will be used.\")\r\n\r\n        return self._scraper_instances_cache[scraper_class]  # type: ignore[return-value]\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[True] = ...) -> ScraperT:\r\n        ...\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[False] = ...) -> ScraperT | None:\r\n        ...\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[True] = ...) -> Scraper:\r\n        ...\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[False] = ...) -> Scraper | None:\r\n        ...\r\n\r\n    def get_scraper_instance(self, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,\r\n                             url: str | None = None, config_data: dict | None = None,\r\n                             raise_error: bool = True) -> Scraper | None:\r\n        \"\"\"\r\n        Find, initialize and return a scraper that matches the given URL or ID.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT] | None, optional): A scraper class to initialize. Defaults to None.\r\n            scraper_id (str | None, optional): ID of a scraper to initialize. Defaults to None.\r\n            url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.\r\n            config_data (dict, optional): A dictionary containing scrapers config data to use\r\n                when creating a new scraper. Defaults to None.\r\n            raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.\r\n\r\n        Returns:\r\n            ScraperT | Scraper | None: An instance of a scraper that matches the given URL or ID,\r\n                None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If no scraper was found and raise_error is True.\r\n        \"\"\"\r\n        if scraper_class:\r\n            return self._get_scraper_instance(scraper_class=scraper_class,\r\n                                              scrapers_config_data=config_data)\r\n\r\n        elif scraper_id or url:\r\n            if scraper_id:\r\n                logger.debug(f\"Searching for a scraper object with ID '{scraper_id}'...\")\r\n                for scraper in self.get_scraper_classes():\r\n                    if scraper.id == scraper_id:\r\n                        return self._get_scraper_instance(scraper_class=scraper, scrapers_config_data=config_data)\r\n\r\n            elif url:\r\n                logger.debug(f\"Searching for a scraper object that matches URL '{url}'...\")\r\n                for scraper in self.get_scraper_classes():\r\n                    if scraper.match_url(url) is not None:\r\n                        return self._get_scraper_instance(scraper_class=scraper, scrapers_config_data=config_data)\r\n\r\n            if raise_error:\r\n                raise ValueError(f\"No matching scraper was found for URL '{url}'\")\r\n\r\n            else:\r\n                logger.debug(\"No matching scraper was found.\")\r\n                return None\r\n\r\n        else:\r\n            raise ValueError(\"At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.\")\r\n\r\n\r\nclass ScraperException(Exception):\r\n    pass\r\n\r\n\r\nclass PlaylistLoadError(ScraperException):\r\n    pass\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/scraper.py b/isubrip/scrapers/scraper.py
--- a/isubrip/scrapers/scraper.py	(revision 61e841425acc39cef381f61c82ed759ff83764df)
+++ b/isubrip/scrapers/scraper.py	(date 1690536278720)
@@ -156,8 +156,9 @@
         Find and yield subtitles data from a main_playlist.
 
         Args:
-            main_playlist (M3U8): Main playlist of the media to search for subtitles in.
-            language_filter (list[str], optional): A list of languages to filter for.
+            main_playlist(str | list[str]): A URL or a list of URLs (for redundancy) of the main playlist.
+            language_filter (list[str] | str | None, optional):
+                A language or a list of languages to filter for. Defaults to None.
             subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.
 
         Yields:
@@ -422,19 +423,6 @@
 
     def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,
                       subrip_conversion: bool = False) -> Iterator[SubtitlesData]:
-        """
-        Find and yield subtitles for a movie using optional filters.
-
-        Args:
-            main_playlist(str | list[str]): A URL or a list of URLs (for redundancy) of the main playlist.
-            language_filter (list[str] | str | None, optional):
-                A language or a list of languages to filter for. Defaults to None.
-            subrip_conversion (bool, optional): Whether to convert and return the subtitles as an SRT file or not.
-                Defaults to False.
-
-        Yields:
-            SubtitlesData: A SubtitlesData NamedTuple with a matching playlist, and it's metadata.
-        """
         playlist_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None
         main_playlist_m3u8 = self.find_valid_playlist(main_playlist)
 
Index: isubrip/__main__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport atexit\r\nimport datetime as dt\r\nimport logging\r\nimport os\r\nimport shutil\r\nimport sys\r\nfrom pathlib import Path\r\nfrom typing import List\r\n\r\nimport requests\r\nfrom requests.utils import default_user_agent\r\n\r\nfrom isubrip.config import Config, ConfigException, ConfigSetting, SpecialConfigType\r\nfrom isubrip.constants import ARCHIVE_FORMAT, DATA_FOLDER_PATH, DEFAULT_CONFIG_PATH, PACKAGE_NAME, TEMP_FOLDER_PATH, \\\r\n    USER_CONFIG_FILE, LOG_FILES_PATH, LOG_FILE_NAME\r\nfrom isubrip.data_structures import Movie, ScrapedMediaResponse, SubtitlesDownloadResults, SubtitlesData\r\nfrom isubrip.logger import CustomLogFileFormatter, CustomStdoutFormatter, logger\r\nfrom isubrip.scrapers.scraper import Scraper, ScraperFactory, PlaylistLoadError\r\nfrom isubrip.utils import download_subtitles_to_file, generate_non_conflicting_path, generate_release_name, \\\r\n    single_to_list\r\n\r\nLOG_ROTATION_SIZE: int | None = None\r\nPREORDER_MESSAGE = \"'{movie_name}' will be available on {scraper_name} on {preorder_date}.\"\r\n\r\nBASE_CONFIG_SETTINGS = [\r\n    ConfigSetting(\r\n        key=\"check-for-updates\",\r\n        type=bool,\r\n        category=\"general\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"log_rotation_size\",\r\n        type=str,\r\n        category=\"general\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"add-release-year-to-series\",\r\n        type=bool,\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"folder\",\r\n        type=str,\r\n        category=\"downloads\",\r\n        required=True,\r\n        special_type=SpecialConfigType.EXISTING_FOLDER_PATH,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"languages\",\r\n        type=List[str],\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"overwrite-existing\",\r\n        type=bool,\r\n        category=\"downloads\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"zip\",\r\n        type=bool,\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"fix-rtl\",\r\n        type=bool,\r\n        category=\"subtitles\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"rtl-languages\",\r\n        type=List[str],\r\n        category=\"subtitles\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"remove-duplicates\",\r\n        type=bool,\r\n        category=\"subtitles\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"convert-to-srt\",\r\n        type=bool,\r\n        category=\"subtitles\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"user-agent\",\r\n        type=str,\r\n        category=\"scrapers\",\r\n        required=True,\r\n    ),\r\n]\r\n\r\n\r\ndef main():\r\n    # Assure at least one argument was passed\r\n    if len(sys.argv) < 2:\r\n        print_usage()\r\n        exit(0)\r\n\r\n    create_required_folders()\r\n    setup_loggers(stdout_loglevel=logging.INFO, file_loglevel=logging.DEBUG)\r\n\r\n    cli_args = \" \".join(sys.argv[1:])\r\n\r\n    if sys.modules.get(PACKAGE_NAME):\r\n        package_version = sys.modules[PACKAGE_NAME].__version__\r\n\r\n    else:\r\n        package_version = \"Unknown\"\r\n        logger.debug(\"Could not find pack's version.\")\r\n\r\n    logger.debug(f'Used CLI Command: {PACKAGE_NAME} {cli_args}')\r\n    logger.debug(f'Python version: {sys.version}')\r\n    logger.debug(f'Package version: {package_version}')\r\n    logger.debug(f'OS: {sys.platform}')\r\n\r\n    config = generate_config()\r\n    update_settings(config)\r\n\r\n    if config.general.get(\"check-for-updates\", True):\r\n        check_for_updates()\r\n\r\n    scraper_factory = ScraperFactory()\r\n\r\n    for idx, url in enumerate(sys.argv[1:]):\r\n        logger.info(f\"Scraping '{url}'...\")\r\n\r\n        scraper = scraper_factory.get_scraper_instance(url=url, config_data=config.data.get(\"scrapers\"))\r\n        atexit.register(scraper.close)\r\n        scraper.config.check()  # Recheck config after scraper settings were loaded\r\n\r\n        scraper_response: ScrapedMediaResponse[Movie] = scraper.get_data(url=url)\r\n        movie_data: list[Movie] = single_to_list(scraper_response.media_data)\r\n\r\n        if not movie_data:\r\n            logger.error(f\"Error: No supported media was found for {url}.\")\r\n            continue\r\n\r\n        download_media_subtitles_args = {\r\n            \"download_path\": Path(config.downloads[\"folder\"]),\r\n            \"language_filter\": config.downloads.get(\"languages\"),\r\n            \"convert_to_srt\": config.subtitles.get(\"convert-to-srt\", False),\r\n            \"overwrite_existing\": config.downloads.get(\"overwrite-existing\", False),\r\n            \"zip_files\": config.downloads.get(\"zip\", False),\r\n        }\r\n\r\n        for movie_item in movie_data:\r\n            id_str = f\" (ID: {movie_item.id})\" if movie_item.id else ''\r\n            logger.info(f\"Found movie: {movie_item.name} [{movie_item.release_date.year}]\" + id_str)\r\n\r\n\r\n            try:\r\n                if not movie_item.playlist:\r\n                    if movie_item.preorder_availability_date:\r\n                        raise PlaylistLoadError\r\n                    else:\r\n                        logger.error(f\"No valid playlist was found for '{movie_item.name}' ({scraper.name}).\")\r\n                        continue\r\n\r\n                results = download_subtitles(movie_data=movie_item,\r\n                                             scraper=scraper,\r\n                                             **download_media_subtitles_args)\r\n\r\n                success_count = len(results.successful_subtitles)\r\n                failed_count = len(results.failed_subtitles)\r\n\r\n                if success_count:\r\n                    logger.info(f\"{success_count}/{success_count + failed_count} matching subtitles \"\r\n                                f\"have been successfully downloaded.\")\r\n\r\n                elif failed_count:\r\n                    logger.info(f\"{failed_count} subtitles were matched, but failed to download.\")\r\n\r\n                else:\r\n                    logger.info(\"No matching subtitles were found.\")\r\n\r\n            except Exception as e:\r\n                if isinstance(e, PlaylistLoadError) and movie_item.preorder_availability_date:\r\n                    preorder_date_str = movie_item.preorder_availability_date.strftime(\"%Y-%m-%d\")\r\n                    logger.info(PREORDER_MESSAGE.format(movie_name=movie_item.name, scraper_name=scraper.name,\r\n                                                        preorder_date=preorder_date_str))\r\n\r\n                else:\r\n                    logger.error(f\"Error: Encountered an error while scraping '{url}'{id_str}: {e}\")\r\n                    logger.debug(f\"Error details: {e}\", exc_info=True)\r\n\r\n                continue\r\n\r\n\r\ndef check_for_updates() -> None:\r\n    \"\"\"Check and print if a newer version of the package is available.\"\"\"\r\n    api_url = f\"https://pypi.org/pypi/{PACKAGE_NAME}/json\"\r\n    logger.debug(\"Checking for package updates on PyPI...\")\r\n    try:\r\n        current_version = sys.modules[PACKAGE_NAME].__version__\r\n\r\n        response = requests.get(\r\n            url=api_url,\r\n            headers={\"Accept\": \"application/json\"},\r\n            timeout=10,\r\n        )\r\n        response.raise_for_status()\r\n        response_data = response.json()\r\n\r\n        pypi_latest_version = response_data[\"info\"][\"version\"]\r\n\r\n        if pypi_latest_version != current_version:\r\n            logger.info(f\"Found a newer version of {PACKAGE_NAME} - {pypi_latest_version}\")\r\n\r\n            logger.warning(f\"Note: You are currently using version '{current_version}' of '{PACKAGE_NAME}', \"\r\n                           f\"however version '{pypi_latest_version}' is available.\",\r\n                           f\"\\nConsider upgrading by running \\\"python3 -m pip install --upgrade {PACKAGE_NAME}\\\"\\n\")\r\n\r\n        else:\r\n            logger.debug(f\"Latest version of {PACKAGE_NAME} ({current_version}) is currently installed.\")\r\n\r\n    except Exception as e:\r\n        logger.warning(f\"Update check failed: {e}\")\r\n        logger.debug(f\"Stack trace: {e}\", exc_info=True)\r\n        return\r\n\r\n\r\ndef create_required_folders():\r\n    if not DATA_FOLDER_PATH.is_dir():\r\n        logger.debug(f\"'{DATA_FOLDER_PATH}' directory could not be found and will be created.\")\r\n        LOG_FILES_PATH.mkdir(parents=True, exist_ok=True)\r\n\r\n    else:\r\n        if not LOG_FILES_PATH.is_dir():\r\n            logger.debug(f\"'{LOG_FILES_PATH}' directory could not be found and will be created.\")\r\n            LOG_FILES_PATH.mkdir()\r\n\r\n\r\ndef download_subtitles(movie_data: Movie, scraper: Scraper, download_path: Path,\r\n                       language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                       overwrite_existing: bool = True, zip_files: bool = False) -> SubtitlesDownloadResults:\r\n    \"\"\"\r\n    Download subtitles for the given media data.\r\n\r\n    Args:\r\n        movie_data (Movie | Episode): A Movie object.\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        zip_files (bool, optional): Whether to unite the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n\r\n    Returns:\r\n        Path: Path to the parent folder of the downloaded subtitles files / zip file.\r\n    \"\"\"\r\n    temp_download_path = generate_media_path(base_path=TEMP_FOLDER_PATH, movie_data=movie_data)\r\n    atexit.register(shutil.rmtree, TEMP_FOLDER_PATH, ignore_errors=False, onerror=None)\r\n\r\n    successful_downloads: list[SubtitlesData] = []\r\n    failed_downloads: list[SubtitlesData] = []\r\n    temp_downloads: list[Path] = []\r\n\r\n    for subtitles_data in scraper.get_subtitles(main_playlist=movie_data.playlist,\r\n                                                language_filter=language_filter,\r\n                                                subrip_conversion=convert_to_srt):\r\n        language_data = f\"{subtitles_data.language_name} ({subtitles_data.language_code})\"\r\n\r\n        try:\r\n            temp_downloads.append(download_subtitles_to_file(\r\n                media_data=movie_data,\r\n                subtitles_data=subtitles_data,\r\n                output_path=temp_download_path,\r\n                overwrite=overwrite_existing,\r\n            ))\r\n\r\n            logger.info(f\"{language_data} subtitles were successfully downloaded.\")\r\n            successful_downloads.append(subtitles_data)\r\n\r\n        except Exception as e:\r\n            print(f\"Error: Failed to download '{language_data}' subtitles: {e}\")\r\n            logger.debug(\"Stack trace:\", exc_info=True)\r\n            failed_downloads.append(subtitles_data)\r\n            continue\r\n\r\n    if not zip_files or len(temp_downloads) == 1:\r\n        for file_path in temp_downloads:\r\n            if overwrite_existing:\r\n                new_path = download_path / file_path.name\r\n\r\n            else:\r\n                new_path = generate_non_conflicting_path(download_path / file_path.name)\r\n\r\n            # str conversion needed only for Python <= 3.8 - https://github.com/python/cpython/issues/76870\r\n            shutil.move(src=str(file_path), dst=new_path)\r\n\r\n    elif len(temp_downloads) > 0:\r\n        archive_path = Path(shutil.make_archive(\r\n            base_name=str(temp_download_path.parent / temp_download_path.name),\r\n            format=ARCHIVE_FORMAT,\r\n            root_dir=temp_download_path,\r\n        ))\r\n\r\n        file_name = generate_media_folder_name(movie_data=movie_data,\r\n                                               source=scraper.abbreviation) + f\".{ARCHIVE_FORMAT}\"\r\n\r\n        if overwrite_existing:\r\n            destination_path = download_path / file_name\r\n\r\n        else:\r\n            destination_path = generate_non_conflicting_path(download_path / file_name)\r\n\r\n        shutil.move(src=str(archive_path), dst=destination_path)\r\n\r\n    shutil.rmtree(temp_download_path)\r\n    atexit.unregister(shutil.rmtree)\r\n\r\n    return SubtitlesDownloadResults(\r\n        movie_data=movie_data,\r\n        successful_subtitles=successful_downloads,\r\n        failed_subtitles=failed_downloads,\r\n        is_zip=zip_files,\r\n    )\r\n\r\ndef handle_log_rotation(log_rotation_size: int):\r\n    \"\"\"\r\n    Handle log rotation and remove old log files if needed.\r\n\r\n    Args:\r\n        log_rotation_size (int): Maximum amount of log files to keep.\r\n    \"\"\"\r\n    log_files: list[Path] = sorted(LOG_FILES_PATH.glob(\"*.log\"), key=os.path.getctime, reverse=True)\r\n\r\n    if len(log_files) > log_rotation_size:\r\n        for log_file in log_files[log_rotation_size:]:\r\n            log_file.unlink()\r\n\r\n\r\ndef generate_config() -> Config:\r\n    \"\"\"\r\n    Generate a config object using config files, and validate it.\r\n\r\n    Returns:\r\n        Config: A config object.\r\n\r\n    Raises:\r\n        ConfigException: If there is a general config error.\r\n        MissingConfigValue: If a required config value is missing.\r\n        InvalidConfigValue: If a config value is invalid.\r\n    \"\"\"\r\n    if not DEFAULT_CONFIG_PATH.is_file():\r\n        raise ConfigException(\"Default config file could not be found.\")\r\n\r\n    config = Config(config_settings=BASE_CONFIG_SETTINGS)\r\n\r\n    logger.debug(f\"Loading default config data...\")\r\n\r\n    with open(DEFAULT_CONFIG_PATH, 'r') as data:\r\n        config.loads(config_data=data.read(), check_config=True)\r\n\r\n    logger.debug(f\"Default config data loaded and validated successfully.\")\r\n\r\n    # If logs folder doesn't exist, create it (also handles data folder)\r\n    if not DATA_FOLDER_PATH.is_dir():\r\n        logger.debug(f\"'{DATA_FOLDER_PATH}' directory could not be found and will be created.\")\r\n        DATA_FOLDER_PATH.mkdir(parents=True, exist_ok=True)\r\n        LOG_FILES_PATH.mkdir()\r\n\r\n    else:\r\n        if not LOG_FILES_PATH.is_dir():\r\n            logger.debug(f\"'{LOG_FILES_PATH}' directory could not be found and will be created.\")\r\n            LOG_FILES_PATH.mkdir()\r\n\r\n        # If a user config file exists, add it to config_files\r\n        if USER_CONFIG_FILE.is_file():\r\n            logger.info(f\"User config file detected at '{USER_CONFIG_FILE}' and will be used.\")\r\n            with open(USER_CONFIG_FILE, 'r') as data:\r\n                config.loads(config_data=data.read(), check_config=True)\r\n            logger.debug(f\"User config file loaded and validated successfully.\")\r\n\r\n    return config\r\n\r\n\r\ndef generate_media_folder_name(movie_data: Movie, source: str | None = None) -> str:\r\n    \"\"\"\r\n    Generate a folder name for media data.\r\n\r\n    Args:\r\n        movie_data (MediaData): A movie data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        str: A folder name for the media data.\r\n    \"\"\"\r\n    return generate_release_name(\r\n        title=movie_data.name,\r\n        release_date=movie_data.release_date,\r\n        media_source=source,\r\n    )\r\n\r\n\r\ndef generate_media_path(base_path: Path, movie_data: Movie) -> Path:\r\n    \"\"\"\r\n    Generate a temporary folder for downloading media data.\r\n\r\n    Args:\r\n        base_path (Path): A base path to generate the folder in.\r\n        movie_data (MediaData): A movie data object.\r\n\r\n    Returns:\r\n        Path: A path to the temporary folder.\r\n    \"\"\"\r\n    temp_folder_name = generate_media_folder_name(movie_data=movie_data)\r\n    path = generate_non_conflicting_path(base_path / temp_folder_name, has_extension=False)\r\n    path.mkdir(parents=True, exist_ok=True)\r\n\r\n    return path\r\n\r\n\r\ndef update_settings(config: Config) -> None:\r\n    \"\"\"\r\n    Update settings according to config.\r\n\r\n    Args:\r\n        config (Config): An instance of a config to set settings according to.\r\n    \"\"\"\r\n    Scraper.subtitles_fix_rtl = config.subtitles[\"fix-rtl\"]\r\n    Scraper.subtitles_fix_rtl_languages = config.subtitles.get(\"rtl-languages\")\r\n    Scraper.subtitles_remove_duplicates = config.subtitles[\"remove-duplicates\"]\r\n    Scraper.default_user_agent = config.scrapers.get(\"user-agent\", default_user_agent())\r\n\r\n    if log_rotation := config.general.get(\"log-rotation-size\"):\r\n        global LOG_ROTATION_SIZE\r\n        LOG_ROTATION_SIZE = log_rotation\r\n\r\n\r\ndef print_usage() -> None:\r\n    \"\"\"Print usage information.\"\"\"\r\n    print(f\"Usage: {PACKAGE_NAME} <iTunes movie URL> [iTunes movie URL...]\")\r\n\r\n\r\ndef setup_loggers(stdout_loglevel: int, file_loglevel: int) -> None:\r\n    \"\"\"\r\n    Configure loggers.\r\n\r\n    Args:\r\n        stdout_loglevel (int): Log level for STDOUT logger.\r\n        file_loglevel (int): Log level for logfile logger.\r\n    \"\"\"\r\n    logger.setLevel(logging.DEBUG)\r\n\r\n    # Setup STDOUT logger\r\n    stdout_handler = logging.StreamHandler(sys.stdout)\r\n    stdout_handler.setLevel(stdout_loglevel)\r\n    stdout_handler.setFormatter(CustomStdoutFormatter())\r\n    logger.addHandler(stdout_handler)\r\n\r\n    # Setup logfile logger\r\n    logfile_path = generate_non_conflicting_path(LOG_FILES_PATH / LOG_FILE_NAME)\r\n    logfile_handler = logging.FileHandler(filename=logfile_path, encoding=\"utf-8\")\r\n    logfile_handler.setLevel(file_loglevel)\r\n    logfile_handler.setFormatter(CustomLogFileFormatter())\r\n    logger.addHandler(logfile_handler)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        main()\r\n\r\n    except Exception as ex:\r\n        logger.error(f\"Error: {ex}\")\r\n        logger.debug(f\"Stack trace: {ex}\", exc_info=True)\r\n        exit(1)\r\n\r\n    finally:\r\n        if _log_rotation_size := LOG_ROTATION_SIZE:\r\n            handle_log_rotation(log_rotation_size=_log_rotation_size)\r\n\r\n        _scraper_factory = ScraperFactory()\r\n\r\n        # Note: This will only close scrapers that were initialized using the ScraperFactory.\r\n        for _scraper in _scraper_factory.get_initialized_scrapers():\r\n            _scraper.close()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/__main__.py b/isubrip/__main__.py
--- a/isubrip/__main__.py	(revision 61e841425acc39cef381f61c82ed759ff83764df)
+++ b/isubrip/__main__.py	(date 1690548488518)
@@ -132,7 +132,7 @@
 
     scraper_factory = ScraperFactory()
 
-    for idx, url in enumerate(sys.argv[1:]):
+    for url in sys.argv[1:]:
         logger.info(f"Scraping '{url}'...")
 
         scraper = scraper_factory.get_scraper_instance(url=url, config_data=config.data.get("scrapers"))
@@ -156,8 +156,13 @@
 
         for movie_item in movie_data:
             id_str = f" (ID: {movie_item.id})" if movie_item.id else ''
-            logger.info(f"Found movie: {movie_item.name} [{movie_item.release_date.year}]" + id_str)
 
+            if isinstance(movie_item.release_date, dt.datetime):
+                year_str = movie_item.release_date.year
+            else:
+                year_str = movie_item.release_date
+
+            logger.info(f"Found movie: {movie_item.name} [{year_str}]" + id_str)
 
             try:
                 if not movie_item.playlist:
@@ -259,7 +264,7 @@
             (only if there are multiple subtitles).
 
     Returns:
-        Path: Path to the parent folder of the downloaded subtitles files / zip file.
+        SubtitlesDownloadResults: A SubtitlesDownloadResults object containing the results of the download.
     """
     temp_download_path = generate_media_path(base_path=TEMP_FOLDER_PATH, movie_data=movie_data)
     atexit.register(shutil.rmtree, TEMP_FOLDER_PATH, ignore_errors=False, onerror=None)
@@ -268,7 +273,7 @@
     failed_downloads: list[SubtitlesData] = []
     temp_downloads: list[Path] = []
 
-    for subtitles_data in scraper.get_subtitles(main_playlist=movie_data.playlist,
+    for subtitles_data in scraper.get_subtitles(main_playlist=movie_data.playlist,  # type: ignore[arg-type]
                                                 language_filter=language_filter,
                                                 subrip_conversion=convert_to_srt):
         language_data = f"{subtitles_data.language_name} ({subtitles_data.language_code})"
@@ -336,7 +341,7 @@
     Args:
         log_rotation_size (int): Maximum amount of log files to keep.
     """
-    log_files: list[Path] = sorted(LOG_FILES_PATH.glob("*.log"), key=os.path.getctime, reverse=True)
+    log_files: list[Path] = sorted(LOG_FILES_PATH.glob("*.log"), key=os.path.getctime, reverse=True)  # type: ignore
 
     if len(log_files) > log_rotation_size:
         for log_file in log_files[log_rotation_size:]:
@@ -360,12 +365,12 @@
 
     config = Config(config_settings=BASE_CONFIG_SETTINGS)
 
-    logger.debug(f"Loading default config data...")
+    logger.debug("Loading default config data...")
 
     with open(DEFAULT_CONFIG_PATH, 'r') as data:
         config.loads(config_data=data.read(), check_config=True)
 
-    logger.debug(f"Default config data loaded and validated successfully.")
+    logger.debug("Default config data loaded and validated successfully.")
 
     # If logs folder doesn't exist, create it (also handles data folder)
     if not DATA_FOLDER_PATH.is_dir():
@@ -383,7 +388,7 @@
             logger.info(f"User config file detected at '{USER_CONFIG_FILE}' and will be used.")
             with open(USER_CONFIG_FILE, 'r') as data:
                 config.loads(config_data=data.read(), check_config=True)
-            logger.debug(f"User config file loaded and validated successfully.")
+            logger.debug("User config file loaded and validated successfully.")
 
     return config
 
