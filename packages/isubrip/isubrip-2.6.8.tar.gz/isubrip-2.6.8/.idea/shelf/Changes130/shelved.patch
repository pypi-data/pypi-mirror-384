Index: isubrip/__main__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport logging\r\nfrom pathlib import Path\r\nimport shutil\r\nimport sys\r\nfrom typing import List, Union\r\n\r\nimport httpx\r\n\r\nfrom isubrip.config import Config, ConfigError, ConfigSetting, SpecialConfigType\r\nfrom isubrip.constants import (\r\n    ARCHIVE_FORMAT,\r\n    DATA_FOLDER_PATH,\r\n    DEFAULT_CONFIG_PATH,\r\n    EVENT_LOOP,\r\n    LOG_FILE_NAME,\r\n    LOG_FILES_PATH,\r\n    PACKAGE_NAME,\r\n    PACKAGE_VERSION,\r\n    PREORDER_MESSAGE,\r\n    TEMP_FOLDER_PATH,\r\n    USER_CONFIG_FILE,\r\n)\r\nfrom isubrip.data_structures import (\r\n    Episode,\r\n    MediaData,\r\n    Movie,\r\n    ScrapedMediaResponse,\r\n    Season,\r\n    Series,\r\n    SubtitlesData,\r\n    SubtitlesDownloadResults,\r\n)\r\nfrom isubrip.logger import CustomLogFileFormatter, CustomStdoutFormatter, logger\r\nfrom isubrip.scrapers.scraper import PlaylistLoadError, Scraper, ScraperError, ScraperFactory, SubtitlesDownloadError\r\nfrom isubrip.subtitle_formats.webvtt import WebVTTCaptionBlock\r\nfrom isubrip.utils import (\r\n    TempDirGenerator,\r\n    download_subtitles_to_file,\r\n    format_media_description,\r\n    format_release_name,\r\n    format_subtitles_description,\r\n    generate_non_conflicting_path,\r\n    raise_for_status,\r\n    single_to_list,\r\n)\r\n\r\nLOG_ROTATION_SIZE: int | None = None\r\n\r\nBASE_CONFIG_SETTINGS = [\r\n    ConfigSetting(\r\n        key=\"check-for-updates\",\r\n        value_type=bool,\r\n        category=\"general\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"log_rotation_size\",\r\n        value_type=str,\r\n        category=\"general\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"add-release-year-to-series\",\r\n        value_type=bool,\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"folder\",\r\n        value_type=str,\r\n        category=\"downloads\",\r\n        required=True,\r\n        special_type=SpecialConfigType.EXISTING_FOLDER_PATH,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"languages\",\r\n        value_type=List[str],\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"overwrite-existing\",\r\n        value_type=bool,\r\n        category=\"downloads\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"zip\",\r\n        value_type=bool,\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"fix-rtl\",\r\n        value_type=bool,\r\n        category=\"subtitles\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"rtl-languages\",\r\n        value_type=List[str],\r\n        category=\"subtitles\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"remove-duplicates\",\r\n        value_type=bool,\r\n        category=\"subtitles\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"convert-to-srt\",\r\n        value_type=bool,\r\n        category=\"subtitles\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"subrip-alignment-conversion\",\r\n        value_type=bool,\r\n        category=(\"subtitles\", \"webvtt\"),\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"timeout\",\r\n        value_type=Union[int, float],\r\n        category=\"scrapers\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"user-agent\",\r\n        value_type=str,\r\n        category=\"scrapers\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"proxy\",\r\n        value_type=str,\r\n        category=\"scrapers\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"verify-ssl\",\r\n        value_type=bool,\r\n        category=\"scrapers\",\r\n        required=False,\r\n    ),\r\n]\r\n\r\n\r\ndef main() -> None:\r\n    try:\r\n        # Assure at least one argument was passed\r\n        if len(sys.argv) < 2:\r\n            print_usage()\r\n            exit(0)\r\n\r\n        if not DATA_FOLDER_PATH.is_dir():\r\n            DATA_FOLDER_PATH.mkdir(parents=True)\r\n\r\n        setup_loggers(stdout_loglevel=logging.INFO,\r\n                      file_loglevel=logging.DEBUG)\r\n\r\n        cli_args = \" \".join(sys.argv[1:])\r\n        logger.debug(f\"CLI Command: {PACKAGE_NAME} {cli_args}\")\r\n        logger.debug(f\"Python version: {sys.version}\")\r\n        logger.debug(f\"Package version: {PACKAGE_VERSION}\")\r\n        logger.debug(f\"OS: {sys.platform}\")\r\n\r\n        config = generate_config()\r\n        update_settings(config)\r\n\r\n        if config.general.get(\"check-for-updates\", True):\r\n            check_for_updates(current_package_version=PACKAGE_VERSION)\r\n\r\n        urls = single_to_list(sys.argv[1:])\r\n        EVENT_LOOP.run_until_complete(download(urls=urls, config=config))\r\n\r\n    except Exception as ex:\r\n        logger.error(f\"Error: {ex}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        exit(1)\r\n\r\n    finally:\r\n        if log_rotation_size := LOG_ROTATION_SIZE:\r\n            handle_log_rotation(log_rotation_size=log_rotation_size)\r\n\r\n        # NOTE: This will only close scrapers that were initialized using the ScraperFactory.\r\n        async_cleanup_coroutines = []\r\n        for scraper in ScraperFactory.get_initialized_scrapers():\r\n            # Log scraper.requests_count\r\n            logger.debug(f\"Requests count for '{scraper.name}' scraper: {scraper.requests_count}\")\r\n            scraper.close()\r\n            async_cleanup_coroutines.append(scraper.async_close())\r\n\r\n        EVENT_LOOP.run_until_complete(asyncio.gather(*async_cleanup_coroutines))\r\n        TempDirGenerator.cleanup()\r\n\r\n\r\nasync def download(urls: list[str], config: Config) -> None:\r\n    \"\"\"\r\n    Download subtitles from a given URL.\r\n\r\n    Args:\r\n        urls (list[str]): A list of URLs to download subtitles from.\r\n        config (Config): A config to use for downloading subtitles.\r\n    \"\"\"\r\n    for url in urls:\r\n        try:\r\n            logger.info(f\"Scraping '{url}'...\")\r\n\r\n            scraper = ScraperFactory.get_scraper_instance(url=url,\r\n                                                          kwargs={\"config_data\": config.data.get(\"scrapers\")},\r\n                                                          extract_scraper_config=True)\r\n            scraper.config.check()  # Recheck config after scraper settings were loaded\r\n\r\n            try:\r\n                logger.debug(f\"Fetching '{url}'...\")\r\n                scraper_response: ScrapedMediaResponse = await scraper.get_data(url=url)\r\n\r\n            except ScraperError as e:\r\n                logger.error(f\"Error: {e}\")\r\n                logger.debug(\"Debug information:\", exc_info=True)\r\n                continue\r\n\r\n            media_data = scraper_response.media_data\r\n            playlist_scraper = ScraperFactory.get_scraper_instance(scraper_id=scraper_response.playlist_scraper,\r\n                                                                   kwargs={\"config_data\": config.data.get(\"scrapers\")},\r\n                                                                   extract_scraper_config=True)\r\n\r\n            if not media_data:\r\n                logger.error(f\"Error: No supported media was found for {url}.\")\r\n                continue\r\n\r\n            for media_item in media_data:\r\n                try:\r\n                    logger.info(f\"Found {media_item.media_type}: {format_media_description(media_data=media_item)}\")\r\n                    await download_media(scraper=playlist_scraper, media_item=media_item, config=config)\r\n\r\n                except Exception as e:\r\n                    if len(media_data) > 1:\r\n                        logger.warning(f\"Error scraping media item \"\r\n                                       f\"'{format_media_description(media_data=media_item)}': {e}\\n\"\r\n                                       f\"Skipping to next media item...\")\r\n                        logger.debug(\"Debug information:\", exc_info=True)\r\n                        continue\r\n\r\n                    raise\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error while scraping '{url}': {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            continue\r\n\r\n\r\nasync def download_media(scraper: Scraper, media_item: MediaData, config: Config) -> None:\r\n    \"\"\"\r\n    Download a media item.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_item (MediaData): A media data item to download subtitles for.\r\n        config (Config): A config to use for downloading subtitles.\r\n    \"\"\"\r\n    if isinstance(media_item, Series):\r\n        for season in media_item.seasons:\r\n            await download_media(scraper=scraper, media_item=season, config=config)\r\n\r\n    elif isinstance(media_item, Season):\r\n        for episode in media_item.episodes:\r\n            logger.info(f\"{format_media_description(media_data=episode, shortened=True)}:\")\r\n            await download_media_item(scraper=scraper, media_item=episode, config=config)\r\n\r\n    elif isinstance(media_item, (Movie, Episode)):\r\n        await download_media_item(scraper=scraper, media_item=media_item, config=config)\r\n\r\n\r\nasync def download_media_item(scraper: Scraper, media_item: Movie | Episode, config: Config) -> None:\r\n    if media_item.playlist:\r\n        download_subtitles_kwargs = {\r\n            \"download_path\": Path(config.downloads[\"folder\"]),\r\n            \"language_filter\": config.downloads.get(\"languages\"),\r\n            \"convert_to_srt\": config.subtitles.get(\"convert-to-srt\", False),\r\n            \"overwrite_existing\": config.downloads.get(\"overwrite-existing\", False),\r\n            \"zip_files\": config.downloads.get(\"zip\", False),\r\n        }\r\n\r\n        try:\r\n            results = await download_subtitles(scraper=scraper,\r\n                                               media_data=media_item,\r\n                                               **download_subtitles_kwargs)\r\n\r\n            success_count = len(results.successful_subtitles)\r\n            failed_count = len(results.failed_subtitles)\r\n\r\n            if success_count or failed_count:\r\n                logger.info(f\"{success_count}/{success_count + failed_count} matching subtitles \"\r\n                            f\"were successfully downloaded.\")\r\n\r\n            else:\r\n                logger.info(\"No matching subtitles were found.\")\r\n\r\n            return  # noqa: TRY300\r\n\r\n        except PlaylistLoadError:\r\n            pass\r\n\r\n    # We get here if there is no playlist, or there is one, but it failed to load\r\n    if isinstance(media_item, Movie) and media_item.preorder_availability_date:\r\n        preorder_date_str = media_item.preorder_availability_date.strftime(\"%Y-%m-%d\")\r\n        logger.info(PREORDER_MESSAGE.format(movie_name=media_item.name, scraper_name=scraper.name,\r\n                                            preorder_date=preorder_date_str))\r\n\r\n    else:\r\n        logger.error(\"No valid playlist was found.\")\r\n\r\n\r\ndef check_for_updates(current_package_version: str) -> None:\r\n    \"\"\"\r\n    Check and print if a newer version of the package is available, and log accordingly.\r\n\r\n    Args:\r\n        current_package_version (str): The current version of the package.\r\n    \"\"\"\r\n    api_url = f\"https://pypi.org/pypi/{PACKAGE_NAME}/json\"\r\n    logger.debug(\"Checking for package updates on PyPI...\")\r\n    try:\r\n        response = httpx.get(\r\n            url=api_url,\r\n            headers={\"Accept\": \"application/json\"},\r\n            timeout=5,\r\n        )\r\n        raise_for_status(response)\r\n        response_data = response.json()\r\n\r\n        pypi_latest_version = response_data[\"info\"][\"version\"]\r\n\r\n        if pypi_latest_version != current_package_version:\r\n            logger.warning(f\"You are currently using version '{current_package_version}' of '{PACKAGE_NAME}', \"\r\n                           f\"however version '{pypi_latest_version}' is available.\"\r\n                           f'\\nConsider upgrading by running \"pip install --upgrade {PACKAGE_NAME}\"\\n')\r\n\r\n        else:\r\n            logger.debug(f\"Latest version of '{PACKAGE_NAME}' ({current_package_version}) is currently installed.\")\r\n\r\n    except Exception as e:\r\n        logger.warning(f\"Update check failed: {e}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        return\r\n\r\n\r\nasync def download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,\r\n                             language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                             overwrite_existing: bool = True, zip_files: bool = False) -> SubtitlesDownloadResults:\r\n    \"\"\"\r\n    Download subtitles for the given media data.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        zip_files (bool, optional): Whether to unite the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n\r\n    Returns:\r\n        SubtitlesDownloadResults: A SubtitlesDownloadResults object containing the results of the download.\r\n    \"\"\"\r\n    temp_dir_name = generate_media_folder_name(media_data=media_data, source=scraper.abbreviation)\r\n    temp_download_path = TempDirGenerator.generate(directory_name=temp_dir_name)\r\n\r\n    successful_downloads: list[SubtitlesData] = []\r\n    failed_downloads: list[SubtitlesDownloadError] = []\r\n    temp_downloads: list[Path] = []\r\n\r\n    if not media_data.playlist:\r\n        raise PlaylistLoadError(\"No playlist was found for provided media data.\")\r\n\r\n    main_playlist = await scraper.load_playlist(url=media_data.playlist)\r\n    matching_subtitles = scraper.find_matching_subtitles(main_playlist=main_playlist,  # type: ignore[var-annotated]\r\n                                                         language_filter=language_filter)\r\n\r\n    logger.debug(f\"{len(matching_subtitles)} matching subtitles were found.\")\r\n\r\n    for matching_subtitles_item in matching_subtitles:\r\n        subtitles_data = await scraper.download_subtitles(media_data=matching_subtitles_item,\r\n                                                          subrip_conversion=convert_to_srt)\r\n        language_info = format_subtitles_description(language_code=subtitles_data.language_code,\r\n                                                     language_name=subtitles_data.language_name,\r\n                                                     special_type=subtitles_data.special_type)\r\n\r\n        if isinstance(subtitles_data, SubtitlesDownloadError):\r\n            logger.warning(f\"Failed to download '{language_info}' subtitles. Skipping...\")\r\n            logger.debug(\"Debug information:\", exc_info=subtitles_data.original_exc)\r\n            failed_downloads.append(subtitles_data)\r\n            continue\r\n\r\n        try:\r\n            temp_downloads.append(download_subtitles_to_file(\r\n                media_data=media_data,\r\n                subtitles_data=subtitles_data,\r\n                output_path=temp_download_path,\r\n                source_abbreviation=scraper.abbreviation,\r\n                overwrite=overwrite_existing,\r\n            ))\r\n\r\n            logger.info(f\"'{language_info}' subtitles were successfully downloaded.\")\r\n            successful_downloads.append(subtitles_data)\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error: Failed to save '{language_info}' subtitles: {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            failed_downloads.append(\r\n                SubtitlesDownloadError(\r\n                    language_code=subtitles_data.language_code,\r\n                    language_name=subtitles_data.language_name,\r\n                    special_type=subtitles_data.special_type,\r\n                    original_exc=e,\r\n                ),\r\n            )\r\n\r\n    if not zip_files or len(temp_downloads) == 1:\r\n        for file_path in temp_downloads:\r\n            if overwrite_existing:\r\n                new_path = download_path / file_path.name\r\n\r\n            else:\r\n                new_path = generate_non_conflicting_path(file_path=download_path / file_path.name)\r\n\r\n            # str conversion needed only for Python <= 3.8 - https://github.com/python/cpython/issues/76870\r\n            shutil.move(src=str(file_path), dst=new_path)\r\n\r\n    elif len(temp_downloads) > 0:\r\n        archive_path = Path(shutil.make_archive(\r\n            base_name=str(temp_download_path.parent / temp_download_path.name),\r\n            format=ARCHIVE_FORMAT,\r\n            root_dir=temp_download_path,\r\n        ))\r\n\r\n        file_name = generate_media_folder_name(media_data=media_data,\r\n                                               source=scraper.abbreviation) + f\".{ARCHIVE_FORMAT}\"\r\n\r\n        if overwrite_existing:\r\n            destination_path = download_path / file_name\r\n\r\n        else:\r\n            destination_path = generate_non_conflicting_path(file_path=download_path / file_name)\r\n\r\n        shutil.move(src=str(archive_path), dst=destination_path)\r\n\r\n    return SubtitlesDownloadResults(\r\n        media_data=media_data,\r\n        successful_subtitles=successful_downloads,\r\n        failed_subtitles=failed_downloads,\r\n        is_zip=zip_files,\r\n    )\r\n\r\n\r\ndef handle_log_rotation(log_rotation_size: int) -> None:\r\n    \"\"\"\r\n    Handle log rotation and remove old log files if needed.\r\n\r\n    Args:\r\n        log_rotation_size (int): Maximum amount of log files to keep.\r\n    \"\"\"\r\n    sorted_log_files = sorted(LOG_FILES_PATH.glob(\"*.log\"), key=lambda file: file.stat().st_mtime, reverse=True)\r\n\r\n    if len(sorted_log_files) > log_rotation_size:\r\n        for log_file in sorted_log_files[log_rotation_size:]:\r\n            log_file.unlink()\r\n\r\n\r\ndef generate_config() -> Config:\r\n    \"\"\"\r\n    Generate a config object using config files, and validate it.\r\n\r\n    Returns:\r\n        Config: A config object.\r\n\r\n    Raises:\r\n        ConfigException: If there is a general config error.\r\n        MissingConfigValue: If a required config value is missing.\r\n        InvalidConfigValue: If a config value is invalid.\r\n    \"\"\"\r\n    if not DEFAULT_CONFIG_PATH.is_file():\r\n        raise ConfigError(\"Default config file could not be found.\")\r\n\r\n    config = Config(config_settings=BASE_CONFIG_SETTINGS)\r\n\r\n    logger.debug(\"Loading default config data...\")\r\n\r\n    with DEFAULT_CONFIG_PATH.open('r') as data:\r\n        config.loads(config_data=data.read(), check_config=True)\r\n\r\n    logger.debug(\"Default config data loaded and validated successfully.\")\r\n\r\n    # If logs folder doesn't exist, create it (also handles data folder)\r\n    if not DATA_FOLDER_PATH.is_dir():\r\n        logger.debug(f\"'{DATA_FOLDER_PATH}' directory could not be found and will be created.\")\r\n        DATA_FOLDER_PATH.mkdir(parents=True, exist_ok=True)\r\n        LOG_FILES_PATH.mkdir()\r\n\r\n    else:\r\n        if not LOG_FILES_PATH.is_dir():\r\n            logger.debug(f\"'{LOG_FILES_PATH}' directory could not be found and will be created.\")\r\n            LOG_FILES_PATH.mkdir()\r\n\r\n        # If a user config file exists, add it to config_files\r\n        if USER_CONFIG_FILE.is_file():\r\n            logger.info(f\"User config file detected at '{USER_CONFIG_FILE}' and will be used.\")\r\n\r\n            with USER_CONFIG_FILE.open('r') as data:\r\n                config.loads(config_data=data.read(), check_config=True)\r\n\r\n            logger.debug(\"User config file loaded and validated successfully.\")\r\n\r\n    return config\r\n\r\n\r\ndef generate_media_folder_name(media_data: Movie | Episode, source: str | None = None) -> str:\r\n    \"\"\"\r\n    Generate a folder name for media data.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        str: A folder name for the media data.\r\n    \"\"\"\r\n    if isinstance(media_data, Movie):\r\n        return format_release_name(\r\n            title=media_data.name,\r\n            release_date=media_data.release_date,\r\n            media_source=source,\r\n        )\r\n\r\n    # elif isinstance(media_data, Episode):\r\n    return format_release_name(\r\n        title=media_data.series_name,\r\n        season_number=media_data.season_number,\r\n        episode_number=media_data.episode_number,\r\n        media_source=source,\r\n    )\r\n\r\n\r\ndef generate_temp_media_path(media_data: Movie | Episode, source: str | None = None) -> Path:\r\n    \"\"\"\r\n    Generate a temporary directory for downloading media data.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        Path: A path to the temporary folder.\r\n    \"\"\"\r\n    temp_folder_name = generate_media_folder_name(media_data=media_data, source=source)\r\n    path = generate_non_conflicting_path(file_path=TEMP_FOLDER_PATH / temp_folder_name, has_extension=False)\r\n\r\n    return TempDirGenerator.generate(directory_name=path.name)\r\n\r\n\r\ndef update_settings(config: Config) -> None:\r\n    \"\"\"\r\n    Update settings according to config.\r\n\r\n    Args:\r\n        config (Config): An instance of a config to set settings according to.\r\n    \"\"\"\r\n    Scraper.subtitles_fix_rtl = config.subtitles[\"fix-rtl\"]\r\n    Scraper.subtitles_remove_duplicates = config.subtitles[\"remove-duplicates\"]\r\n    Scraper.default_timeout = config.scrapers.get(\"timeout\", 10)\r\n    Scraper.default_user_agent = config.scrapers.get(\"user-agent\", httpx._client.USER_AGENT)  # noqa: SLF001\r\n    Scraper.default_proxy = config.scrapers.get(\"proxy\")\r\n    Scraper.default_verify_ssl = config.scrapers.get(\"verify-ssl\", True)\r\n\r\n    if not Scraper.default_verify_ssl:\r\n        import urllib3\r\n        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\n    WebVTTCaptionBlock.subrip_alignment_conversion = (\r\n        config.subtitles.get(\"webvtt\", {}).get(\"subrip-alignment-conversion\", False)\r\n    )\r\n\r\n    if log_rotation := config.general.get(\"log-rotation-size\"):\r\n        global LOG_ROTATION_SIZE\r\n        LOG_ROTATION_SIZE = log_rotation\r\n\r\n\r\ndef print_usage() -> None:\r\n    \"\"\"Print usage information.\"\"\"\r\n    logger.info(f\"Usage: {PACKAGE_NAME} <iTunes movie URL> [iTunes movie URL...]\")\r\n\r\n\r\ndef setup_loggers(stdout_loglevel: int, file_loglevel: int) -> None:\r\n    \"\"\"\r\n    Configure loggers.\r\n\r\n    Args:\r\n        stdout_loglevel (int): Log level for STDOUT logger.\r\n        file_loglevel (int): Log level for logfile logger.\r\n    \"\"\"\r\n    logger.setLevel(logging.DEBUG)\r\n\r\n    # Setup STDOUT logger\r\n    stdout_handler = logging.StreamHandler(sys.stdout)\r\n    stdout_handler.setLevel(stdout_loglevel)\r\n    stdout_handler.setFormatter(CustomStdoutFormatter())\r\n    logger.addHandler(stdout_handler)\r\n\r\n    # Setup logfile logger\r\n    if not LOG_FILES_PATH.is_dir():\r\n        logger.debug(\"Logs directory could not be found and will be created.\")\r\n        LOG_FILES_PATH.mkdir()\r\n\r\n    logfile_path = generate_non_conflicting_path(file_path=LOG_FILES_PATH / LOG_FILE_NAME)\r\n    logfile_handler = logging.FileHandler(filename=logfile_path, encoding=\"utf-8\")\r\n    logfile_handler.setLevel(file_loglevel)\r\n    logfile_handler.setFormatter(CustomLogFileFormatter())\r\n    logger.debug(f\"Log file location: '{logfile_path}'\")\r\n    logger.addHandler(logfile_handler)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/__main__.py b/isubrip/__main__.py
--- a/isubrip/__main__.py	(revision f040b9612ca4349900c8d4bb1178838dc8a97be1)
+++ b/isubrip/__main__.py	(date 1723918513218)
@@ -9,11 +9,10 @@
 
 import httpx
 
-from isubrip.config import Config, ConfigError, ConfigSetting, SpecialConfigType
+from isubrip.config import Config, ConfigSetting, NewConfig, SpecialConfigType
 from isubrip.constants import (
     ARCHIVE_FORMAT,
     DATA_FOLDER_PATH,
-    DEFAULT_CONFIG_PATH,
     EVENT_LOOP,
     LOG_FILE_NAME,
     LOG_FILES_PATH,
@@ -21,7 +20,6 @@
     PACKAGE_VERSION,
     PREORDER_MESSAGE,
     TEMP_FOLDER_PATH,
-    USER_CONFIG_FILE,
 )
 from isubrip.data_structures import (
     Episode,
@@ -49,106 +47,6 @@
 
 LOG_ROTATION_SIZE: int | None = None
 
-BASE_CONFIG_SETTINGS = [
-    ConfigSetting(
-        key="check-for-updates",
-        value_type=bool,
-        category="general",
-        required=False,
-    ),
-    ConfigSetting(
-        key="log_rotation_size",
-        value_type=str,
-        category="general",
-        required=False,
-    ),
-    ConfigSetting(
-        key="add-release-year-to-series",
-        value_type=bool,
-        category="downloads",
-        required=False,
-    ),
-    ConfigSetting(
-        key="folder",
-        value_type=str,
-        category="downloads",
-        required=True,
-        special_type=SpecialConfigType.EXISTING_FOLDER_PATH,
-    ),
-    ConfigSetting(
-        key="languages",
-        value_type=List[str],
-        category="downloads",
-        required=False,
-    ),
-    ConfigSetting(
-        key="overwrite-existing",
-        value_type=bool,
-        category="downloads",
-        required=True,
-    ),
-    ConfigSetting(
-        key="zip",
-        value_type=bool,
-        category="downloads",
-        required=False,
-    ),
-    ConfigSetting(
-        key="fix-rtl",
-        value_type=bool,
-        category="subtitles",
-        required=True,
-    ),
-    ConfigSetting(
-        key="rtl-languages",
-        value_type=List[str],
-        category="subtitles",
-        required=False,
-    ),
-    ConfigSetting(
-        key="remove-duplicates",
-        value_type=bool,
-        category="subtitles",
-        required=True,
-    ),
-    ConfigSetting(
-        key="convert-to-srt",
-        value_type=bool,
-        category="subtitles",
-        required=False,
-    ),
-    ConfigSetting(
-        key="subrip-alignment-conversion",
-        value_type=bool,
-        category=("subtitles", "webvtt"),
-        required=False,
-    ),
-    ConfigSetting(
-        key="timeout",
-        value_type=Union[int, float],
-        category="scrapers",
-        required=False,
-    ),
-    ConfigSetting(
-        key="user-agent",
-        value_type=str,
-        category="scrapers",
-        required=False,
-    ),
-    ConfigSetting(
-        key="proxy",
-        value_type=str,
-        category="scrapers",
-        required=False,
-    ),
-    ConfigSetting(
-        key="verify-ssl",
-        value_type=bool,
-        category="scrapers",
-        required=False,
-    ),
-]
-
 
 def main() -> None:
     try:
@@ -169,10 +67,12 @@
         logger.debug(f"Package version: {PACKAGE_VERSION}")
         logger.debug(f"OS: {sys.platform}")
 
-        config = generate_config()
-        update_settings(config)
+        config = NewConfig()
+        # add config category
 
-        if config.general.get("check-for-updates", True):
+        update_settings(config=config)
+
+        if config.general.check_for_updates:
             check_for_updates(current_package_version=PACKAGE_VERSION)
 
         urls = single_to_list(sys.argv[1:])
@@ -199,22 +99,21 @@
         TempDirGenerator.cleanup()
 
 
-async def download(urls: list[str], config: Config) -> None:
+async def download(urls: list[str], config: NewConfig) -> None:
     """
     Download subtitles from a given URL.
 
     Args:
         urls (list[str]): A list of URLs to download subtitles from.
-        config (Config): A config to use for downloading subtitles.
+        config (NewConfig): A config to use for downloading subtitles.
     """
     for url in urls:
         try:
             logger.info(f"Scraping '{url}'...")
 
             scraper = ScraperFactory.get_scraper_instance(url=url,
-                                                          kwargs={"config_data": config.data.get("scrapers")},
+                                                          kwargs={"config_data": config.scrapers},
                                                           extract_scraper_config=True)
-            scraper.config.check()  # Recheck config after scraper settings were loaded
 
             try:
                 logger.debug(f"Fetching '{url}'...")
@@ -255,14 +154,14 @@
             continue
 
 
-async def download_media(scraper: Scraper, media_item: MediaData, config: Config) -> None:
+async def download_media(scraper: Scraper, media_item: MediaData, config: NewConfig) -> None:
     """
     Download a media item.
 
     Args:
         scraper (Scraper): A Scraper object to use for downloading subtitles.
         media_item (MediaData): A media data item to download subtitles for.
-        config (Config): A config to use for downloading subtitles.
+        config (NewConfig): A config to use for downloading subtitles.
     """
     if isinstance(media_item, Series):
         for season in media_item.seasons:
@@ -277,14 +176,14 @@
         await download_media_item(scraper=scraper, media_item=media_item, config=config)
 
 
-async def download_media_item(scraper: Scraper, media_item: Movie | Episode, config: Config) -> None:
+async def download_media_item(scraper: Scraper, media_item: Movie | Episode, config: NewConfig) -> None:
     if media_item.playlist:
-        download_subtitles_kwargs = {
-            "download_path": Path(config.downloads["folder"]),
-            "language_filter": config.downloads.get("languages"),
-            "convert_to_srt": config.subtitles.get("convert-to-srt", False),
-            "overwrite_existing": config.downloads.get("overwrite-existing", False),
-            "zip_files": config.downloads.get("zip", False),
+        download_subtitles_kwargs = {  # TODO: Accept parameter instead of passing config
+            "download_path": config.downloads.folder,
+            "language_filter": config.downloads.languages,
+            "convert_to_srt": config.subtitles.convert_to_srt,
+            "overwrite_existing": config.downloads.overwrite_existing,
+            "zip_files": config.downloads.zip,
         }
 
         try:
@@ -475,53 +374,6 @@
             log_file.unlink()
 
 
-def generate_config() -> Config:
-    """
-    Generate a config object using config files, and validate it.
-
-    Returns:
-        Config: A config object.
-
-    Raises:
-        ConfigException: If there is a general config error.
-        MissingConfigValue: If a required config value is missing.
-        InvalidConfigValue: If a config value is invalid.
-    """
-    if not DEFAULT_CONFIG_PATH.is_file():
-        raise ConfigError("Default config file could not be found.")
-
-    config = Config(config_settings=BASE_CONFIG_SETTINGS)
-
-    logger.debug("Loading default config data...")
-
-    with DEFAULT_CONFIG_PATH.open('r') as data:
-        config.loads(config_data=data.read(), check_config=True)
-
-    logger.debug("Default config data loaded and validated successfully.")
-
-    # If logs folder doesn't exist, create it (also handles data folder)
-    if not DATA_FOLDER_PATH.is_dir():
-        logger.debug(f"'{DATA_FOLDER_PATH}' directory could not be found and will be created.")
-        DATA_FOLDER_PATH.mkdir(parents=True, exist_ok=True)
-        LOG_FILES_PATH.mkdir()
-
-    else:
-        if not LOG_FILES_PATH.is_dir():
-            logger.debug(f"'{LOG_FILES_PATH}' directory could not be found and will be created.")
-            LOG_FILES_PATH.mkdir()
-
-        # If a user config file exists, add it to config_files
-        if USER_CONFIG_FILE.is_file():
-            logger.info(f"User config file detected at '{USER_CONFIG_FILE}' and will be used.")
-
-            with USER_CONFIG_FILE.open('r') as data:
-                config.loads(config_data=data.read(), check_config=True)
-
-            logger.debug("User config file loaded and validated successfully.")
-
-    return config
-
-
 def generate_media_folder_name(media_data: Movie | Episode, source: str | None = None) -> str:
     """
     Generate a folder name for media data.
@@ -566,29 +418,30 @@
     return TempDirGenerator.generate(directory_name=path.name)
 
 
-def update_settings(config: Config) -> None:
+def update_settings(config: NewConfig) -> None:
     """
     Update settings according to config.
 
     Args:
-        config (Config): An instance of a config to set settings according to.
+        config (NewConfig): An instance of a config to set settings according to.
     """
-    Scraper.subtitles_fix_rtl = config.subtitles["fix-rtl"]
-    Scraper.subtitles_remove_duplicates = config.subtitles["remove-duplicates"]
-    Scraper.default_timeout = config.scrapers.get("timeout", 10)
-    Scraper.default_user_agent = config.scrapers.get("user-agent", httpx._client.USER_AGENT)  # noqa: SLF001
-    Scraper.default_proxy = config.scrapers.get("proxy")
-    Scraper.default_verify_ssl = config.scrapers.get("verify-ssl", True)
+    Scraper.subtitles_fix_rtl = config.subtitles.fix_rtl
+    Scraper.subtitles_remove_duplicates = config.subtitles.remove_duplicates
+    Scraper.default_timeout = config.scrapers.timeout
+    Scraper.default_user_agent = config.scrapers.user_agent
+    Scraper.default_proxy = config.scrapers.proxy
+    Scraper.default_verify_ssl = config.scrapers.verify_ssl
 
     if not Scraper.default_verify_ssl:
-        import urllib3
+        import urllib3  # TODO: Replace code, as urllib3 is not a dependency of isubrip
         urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
 
     WebVTTCaptionBlock.subrip_alignment_conversion = (
-        config.subtitles.get("webvtt", {}).get("subrip-alignment-conversion", False)
+        config.subtitles.webvtt.subrip_alignment_conversion
     )
 
-    if log_rotation := config.general.get("log-rotation-size"):
+    # TODO: Not use global variables
+    if log_rotation := config.general.log_rotation_size:
         global LOG_ROTATION_SIZE
         LOG_ROTATION_SIZE = log_rotation
 
Index: isubrip/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom copy import deepcopy\r\nfrom enum import Enum\r\nfrom pathlib import Path\r\nimport typing\r\nfrom typing import Any, NamedTuple, Type\r\n\r\nfrom mergedeep import merge\r\nimport tomli\r\n\r\nfrom isubrip.utils import check_type, single_to_list\r\n\r\n\r\nclass DuplicateBehavior(Enum):\r\n    \"\"\"\r\n    An Enum representing optional behaviors for when a duplicate config key is found.\r\n\r\n    Attributes:\r\n        OVERWRITE: Overwrite the existing value with the new value.\r\n        IGNORE: Ignore the new value and keep the existing value.\r\n        RAISE_ERROR: Raise an error.\r\n    \"\"\"\r\n    OVERWRITE = 1\r\n    IGNORE = 2\r\n    RAISE_ERROR = 3\r\n\r\n\r\nclass SpecialConfigType(Enum):\r\n    \"\"\"\r\n    An Enum representing special config value properties to validate.\r\n\r\n    Attributes:\r\n        EXISTING_FILE_PATH: The value must be of a path to an existing file.\r\n        EXISTING_FOLDER_PATH: The value must be of a path to an existing folder.\r\n    \"\"\"\r\n    EXISTING_FILE_PATH = 1\r\n    EXISTING_FOLDER_PATH = 2\r\n\r\n\r\nclass ConfigSetting(NamedTuple):\r\n    \"\"\"\r\n    A NamedTuple representing a config setting.\r\n\r\n    Attributes:\r\n        key (str): Dictionary key used to access the setting.\r\n        value_type (type): Variable type of the value of the setting. Used for validation.\r\n        category (str | tuple[str, ...], optional): A category that the setting is under.\r\n            Categories are used to group related settings' keys together in a sub-dictionary.\r\n            A tuple can be used to nest categories (first item is the top-level category). Defaults to None.\r\n        required (bool, optional): Whether the setting is required. Defaults to False.\r\n        enum_type (type[Enum], optional): An Enum that the settings values will be converted to. Defaults to None.\r\n        special_type (SpecialConfigType | list[SpecialConfigType], optional): A special property of the setting's value\r\n            to validate, represented by a SpecialConfigType value. Defaults to None.\r\n    \"\"\"\r\n    key: str\r\n    # TODO: Use `types.UnionType` instead of `typing._UnionGenericAlias`, once minimum Python version >= 3.10.\r\n    # TODO: Update 'InvalidConfigType' exception as well.\r\n    value_type: type | typing._UnionGenericAlias  # type: ignore[name-defined]\r\n    category: str | tuple[str, ...] | None = None\r\n    required: bool = False\r\n    enum_type: Type[Enum] | None = None\r\n    special_type: SpecialConfigType | list[SpecialConfigType] | None = None\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        if isinstance(other, ConfigSetting):\r\n            return self.key == other.key and self.category == other.category\r\n        return False\r\n\r\n\r\nclass Config:\r\n    \"\"\"A class for managing iSubRip config files.\"\"\"\r\n    def __init__(self, config_settings: list[ConfigSetting] | None = None, config_data: dict | None = None):\r\n        \"\"\"\r\n        Create a new Config instance.\r\n\r\n        Args:\r\n            config_settings (list[ConfigSetting], optional): A list of ConfigSettings objects\r\n                that will be used for validations. Defaults to None.\r\n            config_data (dict, optional): A dict of config data to add to the config. Defaults to None.\r\n        \"\"\"\r\n        self._config_settings: list = []\r\n        self._config_data: dict = {}\r\n\r\n        if config_settings:\r\n            self.add_settings(config_settings, check_config=False)\r\n\r\n        if config_data:\r\n            self._config_data = deepcopy(config_data)\r\n\r\n    def __contains__(self, item: Any) -> bool:\r\n        \"\"\"\r\n        Allow checking if a key exists in the config using the 'in' operator.\r\n\r\n        Args:\r\n            item (Any): The key to check for.\r\n\r\n        Returns:\r\n            bool: True if the key exists in the config, False otherwise.\r\n        \"\"\"\r\n        return item in self._config_data\r\n\r\n    def __getattr__(self, key: str) -> Any:\r\n        \"\"\"\r\n        Allow access to config settings using attributes.\r\n\r\n        Args:\r\n            key (str): Config key to get.\r\n\r\n        Returns:\r\n            Any: The corresponding value for the key in the config.\r\n        \"\"\"\r\n        if self._config_data and key in self._config_data:\r\n            return self._config_data[key]\r\n\r\n        raise AttributeError(f\"Attribute '{key}' does not exist.\")\r\n\r\n    def __getitem__(self, key: str) -> Any:\r\n        \"\"\"\r\n        Allow access to config settings using dict-like syntax.\r\n\r\n        Args:\r\n            key (str): Config key to get.\r\n\r\n        Returns:\r\n            Any: The corresponding value for the key in the config.\r\n        \"\"\"\r\n        return self._config_data[key]\r\n\r\n    def get(self, key: str, default: Any = None) -> Any:\r\n        \"\"\"\r\n        Get a config value.\r\n\r\n        Args:\r\n            key (str): Config key to get.\r\n            default (Any, optional): Default value to return if the key does not exist. Defaults to None.\r\n\r\n        Returns:\r\n            Any: The corresponding value for the key in the config or the default value if the key does not exist.\r\n        \"\"\"\r\n        return self._config_data.get(key, default)\r\n\r\n    @property\r\n    def data(self) -> dict:\r\n        return self._config_data\r\n\r\n    def add_settings(self, config_settings: ConfigSetting | list[ConfigSetting],\r\n                     duplicate_behavior: DuplicateBehavior = DuplicateBehavior.OVERWRITE,\r\n                     check_config: bool = True) -> None:\r\n        \"\"\"\r\n        Add new config settings to the config.\r\n\r\n        Args:\r\n            config_settings (ConfigSetting | list[ConfigSetting]): A ConfigSetting object or a list of ConfigSetting\r\n                objects to add to the config.\r\n            duplicate_behavior (DuplicateBehavior, optional): Behaviour to apply if a duplicate is found.\r\n                Defaults to DuplicateBehavior.OVERWRITE.\r\n            check_config (bool, optional): Whether to check the config after loading it. Defaults to True.\r\n        \"\"\"\r\n        config_settings_copy = deepcopy(single_to_list(config_settings))\r\n\r\n        for config_setting in config_settings_copy:\r\n            if config_setting in self._config_settings:\r\n                if duplicate_behavior == DuplicateBehavior.OVERWRITE:\r\n                    self._config_settings.remove(config_setting)\r\n                    self._config_settings.append(config_setting)\r\n\r\n                elif duplicate_behavior == DuplicateBehavior.RAISE_ERROR:\r\n                    raise ValueError(f\"Duplicate config setting: {config_setting}\")\r\n\r\n            else:\r\n                self._config_settings.append(config_setting)\r\n\r\n        if check_config:\r\n            self.check()\r\n\r\n    def loads(self, config_data: str, check_config: bool = True) -> None:\r\n        \"\"\"\r\n        Parse a tomli config from a string.\r\n\r\n        Args:\r\n            config_data (str): Config file data as a string.\r\n            check_config (bool, optional): Whether to check the config after loading it. Defaults to True.\r\n\r\n        Raises:\r\n            FileNotFoundError: Config file could not be found in the specified path.\r\n            TOMLDecodeError: Config file is not a valid TOML file.\r\n            ConfigValueMissing: A required config value is missing.\r\n            InvalidConfigValue: An invalid value was used in the config file.\r\n        \"\"\"\r\n        # Load settings from default config file\r\n        loaded_data: dict = tomli.loads(config_data)\r\n\r\n        if self._config_data:\r\n            temp_config = dict(merge(self._config_data, loaded_data))\r\n\r\n        else:\r\n            temp_config = loaded_data\r\n\r\n        self._config_data = temp_config\r\n\r\n        if check_config and self._config_settings:\r\n            self.check()\r\n\r\n    def _map_config_settings(self, settings: list[ConfigSetting], data: dict) -> dict[ConfigSetting, Any]:\r\n        \"\"\"\r\n        Map config settings to their values.\r\n        This function wil also unflatten data.\r\n\r\n        Args:\r\n            settings (list[ConfigSetting]): A list or tuple of ConfigSettings objects.\r\n            data (dict): A dictionary containing the config data.\r\n\r\n        Returns:\r\n            dict[ConfigSetting, Any]: A dictionary mapping config settings to their values.\r\n        \"\"\"\r\n        mapped_settings: dict = {}\r\n\r\n        for setting in settings:\r\n            if setting.category:\r\n                setting_categories = single_to_list(setting.category)\r\n                config_dict_iter: dict = data\r\n\r\n                for setting_category in setting_categories:\r\n                    if setting_category not in config_dict_iter:\r\n                        mapped_settings[setting] = None\r\n                        break\r\n\r\n                    config_dict_iter = config_dict_iter[setting_category]\r\n\r\n            else:\r\n                config_dict_iter = data\r\n\r\n            if setting.key not in config_dict_iter:\r\n                mapped_settings[setting] = None\r\n\r\n            else:\r\n                value = config_dict_iter[setting.key]\r\n                enum_type = setting.enum_type\r\n\r\n                if enum_type is not None:\r\n                    try:\r\n                        value = enum_type(value)\r\n\r\n                    except ValueError as e:\r\n                        setting_path = '.'.join(single_to_list(setting.category))\r\n                        raise InvalidEnumConfigValueError(setting_path=setting_path,\r\n                                                          value=value, enum_type=enum_type) from e\r\n\r\n                if type(value) in (list, tuple) and len(value) == 0:\r\n                    value = None\r\n\r\n                special_types = single_to_list(setting.special_type)\r\n\r\n                if SpecialConfigType.EXISTING_FILE_PATH in special_types:\r\n                    value = value.rstrip(r\"\\/\")\r\n\r\n                mapped_settings[setting] = value\r\n\r\n        return mapped_settings\r\n\r\n    def check(self) -> None:\r\n        \"\"\"\r\n        Check whether the config is valid by comparing config's data to the config settings.\r\n        Raises an error if an invalid value is found.\r\n\r\n        Raises:\r\n            MissingConfigValue: A required config value is missing.\r\n            InvalidConfigValue: An invalid value was used in the config file.\r\n        \"\"\"\r\n        if not (self._config_data and self._config_settings):\r\n            return\r\n\r\n        mapped_config = self._map_config_settings(self._config_settings, self._config_data)\r\n\r\n        for setting, value in mapped_config.items():\r\n            if isinstance(setting.category, (list, tuple)):\r\n                setting_path = '.'.join(setting.category) + f\".{setting.key}\"\r\n\r\n            elif isinstance(setting.category, str):\r\n                setting_path = setting.category + f\".{setting.key}\"\r\n\r\n            else:\r\n                setting_path = setting.key\r\n\r\n            if value is None:\r\n                if setting.required:\r\n                    raise MissingRequiredConfigSettingError(setting_path=setting_path)\r\n\r\n                continue\r\n\r\n            if setting.enum_type is None and not check_type(value, setting.value_type):\r\n                raise InvalidConfigTypeError(setting_path=setting_path, value=value, expected_type=setting.value_type)\r\n\r\n            special_types = single_to_list(setting.special_type)\r\n\r\n            if SpecialConfigType.EXISTING_FILE_PATH in special_types and not Path(value).is_file():\r\n                raise InvalidConfigFilePathError(setting_path=setting_path, value=value)\r\n\r\n            if SpecialConfigType.EXISTING_FOLDER_PATH in special_types and not Path(value).is_dir():\r\n                raise InvalidConfigFolderPathError(setting_path=setting_path, value=value)\r\n\r\n\r\nclass ConfigError(Exception):\r\n    pass\r\n\r\n\r\nclass MissingRequiredConfigSettingError(ConfigError):\r\n    \"\"\"A required config value is missing.\"\"\"\r\n    def __init__(self, setting_path: str):\r\n        super().__init__(f\"Missing required config value: '{setting_path}'.\")\r\n\r\n\r\nclass InvalidConfigValueError(ConfigError):\r\n    \"\"\"An invalid config setting has been set.\"\"\"\r\n    def __init__(self, setting_path: str, value: Any, additional_note: str | None = None):\r\n        message = f\"Invalid config value for '{setting_path}': '{value}'.\"\r\n\r\n        if additional_note:\r\n            message += f\"\\n{additional_note}\"\r\n\r\n        super().__init__(message)\r\n\r\n\r\nclass InvalidEnumConfigValueError(InvalidConfigValueError):\r\n    \"\"\"An invalid config value of an enum type setting has been set.\"\"\"\r\n    def __init__(self, setting_path: str, value: Any, enum_type: type[Enum]):\r\n        enum_options = ', '.join([f\"'{option.name}'\" for option in enum_type])\r\n\r\n        super().__init__(\r\n            setting_path=setting_path,\r\n            value=value,\r\n            additional_note=f\"Value can only be one of: {enum_options}.\",\r\n        )\r\n\r\n\r\nclass InvalidConfigTypeError(InvalidConfigValueError):\r\n    \"\"\"An invalid config value type has been set.\"\"\"\r\n    def __init__(self, setting_path: str,\r\n                 expected_type: type | typing._UnionGenericAlias,  # type: ignore[name-defined]\r\n                 value: Any):\r\n        expected_type_str = expected_type.__name__ if hasattr(expected_type, '__name__') else str(expected_type)\r\n        value_type_str = type(value).__name__ if hasattr(type(value), '__name__') else str(type(value))\r\n\r\n        super().__init__(\r\n            setting_path=setting_path,\r\n            value=value,\r\n            additional_note=f\"Expected type: '{expected_type_str}'. Received: '{value_type_str}'.\",\r\n        )\r\n\r\n\r\nclass InvalidConfigFilePathError(InvalidConfigValueError):\r\n    \"\"\"An invalid config value of a file path has been set.\"\"\"\r\n    def __init__(self, setting_path: str, value: str):\r\n        super().__init__(\r\n            setting_path=setting_path,\r\n            value=value,\r\n            additional_note=f\"File '{value}' not found.\",\r\n        )\r\n\r\n\r\nclass InvalidConfigFolderPathError(InvalidConfigValueError):\r\n    \"\"\"An invalid config value of a folder path has been set.\"\"\"\r\n    def __init__(self, setting_path: str, value: str):\r\n        super().__init__(\r\n            setting_path=setting_path,\r\n            value=value,\r\n            additional_note=f\"Folder '{value}' not found.\",\r\n        )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/config.py b/isubrip/config.py
--- a/isubrip/config.py	(revision f040b9612ca4349900c8d4bb1178838dc8a97be1)
+++ b/isubrip/config.py	(date 1723918906906)
@@ -1,15 +1,88 @@
 from __future__ import annotations
 
+from abc import ABC
 from copy import deepcopy
 from enum import Enum
 from pathlib import Path
 import typing
-from typing import Any, NamedTuple, Type
+from typing import Any, List, NamedTuple, Tuple, Type
 
 from mergedeep import merge
+from pydantic import BaseModel, ConfigDict
+from pydantic_settings import BaseSettings, PydanticBaseSettingsSource, SettingsConfigDict, TomlConfigSettingsSource
 import tomli
 
-from isubrip.utils import check_type, single_to_list
+from isubrip.constants import USER_CONFIG_FILE_PATH
+from isubrip.utils import check_type, single_to_list  # TODO: Remove
+
+
+class ConfigCategory(BaseModel, ABC):
+    """A base class for settings categories."""
+    @staticmethod
+    def _normalize_config_setting_name(setting_name: str):
+        return setting_name.replace("_", "-")
+
+    model_config = ConfigDict(
+        alias_generator=_normalize_config_setting_name,
+        extra='ignore',
+    )
+
+
+class GeneralConfig(ConfigCategory):
+    check_for_updates: bool = True
+    log_rotation_size: int = 15
+
+
+class DownloadsConfig(ConfigCategory):
+    folder: Path = Path.cwd().resolve()
+    languages: List[str] = []
+    overwrite_existing: bool = False
+    zip: bool = False
+
+
+class WebVTTConfig(ConfigCategory):
+    # Change name to 'WebVTT'
+    subrip_alignment_conversion: bool = False
+
+
+class SubtitlesConfig(ConfigCategory):
+    fix_rtl: bool = False
+    remove_duplicates: bool = True
+    convert_to_srt: bool = False
+    webvtt: WebVTTConfig = WebVTTConfig()
+
+
+class ScrapersConfig(ConfigCategory):
+    timeout: float = 10.0
+    user_agent: str = \
+        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36"  # noqa: E501
+    proxy: typing.Optional[str] = None
+    verify_ssl: bool = True
+
+
+class NewConfig(BaseSettings):
+    model_config = SettingsConfigDict(
+        extra='allow',
+        toml_file=USER_CONFIG_FILE_PATH,
+    )
+
+    general: GeneralConfig = GeneralConfig()
+    downloads: DownloadsConfig = DownloadsConfig()
+    subtitles: SubtitlesConfig = SubtitlesConfig()
+    scrapers: dict[str, ScrapersConfig] = {
+        "default": ScrapersConfig(),
+    }
+
+    @classmethod
+    def settings_customise_sources(
+        cls,
+        settings_cls: Type[BaseSettings],
+        init_settings: PydanticBaseSettingsSource,
+        env_settings: PydanticBaseSettingsSource,
+        dotenv_settings: PydanticBaseSettingsSource,
+        file_secret_settings: PydanticBaseSettingsSource,
+    ) -> Tuple[PydanticBaseSettingsSource, ...]:
+        return (TomlConfigSettingsSource(settings_cls),)
 
 
 class DuplicateBehavior(Enum):
Index: isubrip/scrapers/scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC, abstractmethod\r\nimport asyncio\r\nfrom enum import Enum\r\nimport importlib\r\nimport inspect\r\nfrom pathlib import Path\r\nimport re\r\nimport sys\r\nfrom typing import TYPE_CHECKING, Any, ClassVar, List, Literal, Type, TypeVar, Union, overload\r\n\r\nimport httpx\r\nimport m3u8\r\n\r\nfrom isubrip.config import Config, ConfigSetting\r\nfrom isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX\r\nfrom isubrip.data_structures import (\r\n    MainPlaylist,\r\n    PlaylistMediaItem,\r\n    ScrapedMediaResponse,\r\n    SubtitlesData,\r\n    SubtitlesFormatType,\r\n    SubtitlesType,\r\n)\r\nfrom isubrip.logger import logger\r\nfrom isubrip.utils import SingletonMeta, merge_dict_values, single_to_list\r\n\r\nif TYPE_CHECKING:\r\n    from types import TracebackType\r\n\r\n    from isubrip.subtitle_formats.subtitles import Subtitles\r\n\r\nScraperT = TypeVar(\"ScraperT\", bound=\"Scraper\")\r\n\r\n\r\nclass Scraper(ABC, metaclass=SingletonMeta):\r\n    \"\"\"\r\n    A base class for scrapers.\r\n\r\n    Attributes:\r\n        _playlist_filters_config_category (ClassVar[str]): Config category to look for playlist filters.\r\n        default_user_agent (str): [Class Attribute]\r\n            Default user agent to use if no other user agent is specified when making requests.\r\n        default_proxy (str | None): [Class Attribute] Default proxy to use when making requests.\r\n        default_verify_ssl (bool): [Class Attribute] Whether to verify SSL certificates by default.\r\n        subtitles_fix_rtl (bool): [Class Attribute] Whether to fix RTL from downloaded subtitles.\r\n            A list of languages to fix RTL on. If None, a default list will be used.\r\n        subtitles_remove_duplicates (bool): [Class Attribute]\r\n            Whether to remove duplicate lines from downloaded subtitles.\r\n\r\n        id (str): [Class Attribute] ID of the scraper.\r\n        name (str): [Class Attribute] Name of the scraper.\r\n        abbreviation (str): [Class Attribute] Abbreviation of the scraper.\r\n        url_regex (re.Pattern | list[re.Pattern]): [Class Attribute] A RegEx pattern to find URLs matching the service.\r\n        subtitles_class (type[Subtitles]): [Class Attribute] Class of the subtitles format returned by the scraper.\r\n        is_movie_scraper (bool): [Class Attribute] Whether the scraper is for movies.\r\n        is_series_scraper (bool): [Class Attribute] Whether the scraper is for series.\r\n        uses_scrapers (list[str]): [Class Attribute] A list of IDs for other scraper classes that this scraper uses.\r\n            This assures that the config data for the other scrapers is passed as well.\r\n        _session (httpx.Client): A synchronous HTTP client session.\r\n        _async_session (httpx.AsyncClient): An asynchronous HTTP client session.\r\n        config (Config): A Config object containing scraper's configuration.\r\n    \"\"\"\r\n    _playlist_filters_config_category: ClassVar[str] = \"playlist-filters\"\r\n    default_timeout: ClassVar[int] = 10\r\n    default_user_agent: ClassVar[str] = httpx._client.USER_AGENT  # noqa: SLF001\r\n    default_proxy: ClassVar[str | None] = None\r\n    default_verify_ssl: ClassVar[bool] = True\r\n    subtitles_fix_rtl: ClassVar[bool] = False\r\n    subtitles_remove_duplicates: ClassVar[bool] = True\r\n\r\n    id: ClassVar[str]\r\n    name: ClassVar[str]\r\n    abbreviation: ClassVar[str]\r\n    url_regex: ClassVar[re.Pattern | list[re.Pattern]]\r\n    subtitles_class: ClassVar[type[Subtitles]]\r\n    is_movie_scraper: ClassVar[bool] = False\r\n    is_series_scraper: ClassVar[bool] = False\r\n    uses_scrapers: ClassVar[list[str]] = []\r\n\r\n    def __init__(self, timeout: int | float | None = None,\r\n                 user_agent: str | None = None, proxy: str | None = None,\r\n                 verify_ssl: bool | None = None, config_data: dict | None = None):\r\n        \"\"\"\r\n        Initialize a Scraper object.\r\n\r\n        Args:\r\n            timeout (int | float | None, optional): A timeout to use when making requests. Defaults to None.\r\n            user_agent (str | None, optional): A user agent to use when making requests. Defaults to None.\r\n            proxy (str | None, optional): A proxy to use when making requests. Defaults to None.\r\n            verify_ssl (bool | None, optional): Whether to verify SSL certificates. Defaults to None.\r\n            config_data (dict | None, optional): A dictionary containing scraper's configuration data. Defaults to None.\r\n        \"\"\"\r\n        self.config = Config(config_data=config_data.get(self.id) if config_data else None)\r\n\r\n        # Add a \"user-agent\" setting by default to all scrapers\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                key=\"timeout\",\r\n                value_type=Union[int, float],\r\n                required=False,\r\n            ),\r\n            ConfigSetting(\r\n                key=\"user-agent\",\r\n                value_type=str,\r\n                required=False,\r\n            ),\r\n            ConfigSetting(\r\n                key=\"proxy\",\r\n                value_type=str,\r\n                required=False,\r\n            ),\r\n            ConfigSetting(\r\n                key=\"verify-ssl\",\r\n                value_type=bool,\r\n                required=False,\r\n            ),\r\n        ],\r\n            check_config=False)\r\n\r\n        self._user_agent: str\r\n        self._proxy: str | None\r\n        self._verify_ssl: bool\r\n\r\n        # Timeout Configuration\r\n        if timeout is not None:\r\n            self._timeout = timeout\r\n\r\n        elif \"timeout\" in self.config:\r\n            self._timeout = self.config[\"timeout\"]\r\n\r\n        else:\r\n            self._timeout = self.default_timeout\r\n\r\n        # User-Agent Configuration\r\n        if user_agent is not None:\r\n            self._user_agent = user_agent\r\n\r\n        elif \"user-agent\" in self.config:\r\n            self._user_agent = self.config[\"user-agent\"]\r\n\r\n        else:\r\n            self._user_agent = self.default_user_agent\r\n\r\n        if self._user_agent != self.default_user_agent:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with user-agent: '{user_agent}'.\")\r\n\r\n        # Proxy Configuration\r\n        if proxy is not None:\r\n            self._proxy = proxy or self.config.get(\"proxy\") or self.default_proxy\r\n\r\n        elif \"proxy\" in self.config:\r\n            self._proxy = self.config[\"proxy\"]\r\n\r\n        else:\r\n            self._proxy = self.default_proxy\r\n\r\n        if self._proxy != self.default_proxy:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with proxy: '{proxy}'.\")\r\n\r\n        # SSL Verification Configuration\r\n        if verify_ssl is not None:\r\n            self._verify_ssl = verify_ssl\r\n\r\n        elif \"verify-ssl\" in self.config:\r\n            self._verify_ssl = self.config[\"verify-ssl\"]\r\n\r\n        else:\r\n            self._verify_ssl = self.default_verify_ssl\r\n\r\n        if self._verify_ssl != self.default_verify_ssl:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with SSL verification set to: '{verify_ssl}'.\")\r\n\r\n        self._requests_counter = 0\r\n        clients_params: dict[str, Any] = {\r\n            \"headers\": {\"User-Agent\": self._user_agent},\r\n            \"verify\": self._verify_ssl,\r\n            \"proxy\": self._proxy,\r\n            \"timeout\": float(self._timeout),\r\n        }\r\n        self._session = httpx.Client(\r\n            **clients_params,\r\n            event_hooks={\r\n                \"request\": [self._increment_requests_counter],\r\n            },\r\n        )\r\n        self._async_session = httpx.AsyncClient(\r\n            **clients_params,\r\n            event_hooks={\r\n                \"request\": [self._async_increment_requests_counter],\r\n            },\r\n        )\r\n\r\n        # Update session settings according to configurations\r\n        self._session.headers.update({\"User-Agent\": self._user_agent})\r\n        self._async_session.headers.update({\"User-Agent\": self._user_agent})\r\n\r\n        if not self._verify_ssl:\r\n            import urllib3\r\n            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\n    def _increment_requests_counter(self, request: httpx.Request) -> None:  # noqa: ARG002\r\n        self._requests_counter += 1\r\n\r\n    async def _async_increment_requests_counter(self, request: httpx.Request) -> None:  # noqa: ARG002\r\n        self._requests_counter += 1\r\n\r\n    @property\r\n    def requests_count(self) -> int:\r\n        return self._requests_counter\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[True] = ...) -> re.Match:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[False] = ...) -> re.Match | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def match_url(cls, url: str, raise_error: bool = False) -> re.Match | None:\r\n        \"\"\"\r\n        Checks if a URL matches scraper's url regex.\r\n\r\n        Args:\r\n            url (str): A URL to check against the regex.\r\n            raise_error (bool, optional): Whether to raise an error instead of returning None if the URL doesn't match.\r\n\r\n        Returns:\r\n            re.Match | None: A Match object if the URL matches the regex, None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If the URL doesn't match the regex and raise_error is True.\r\n        \"\"\"\r\n        if isinstance(cls.url_regex, re.Pattern) and (match_result := re.fullmatch(pattern=cls.url_regex, string=url)):\r\n            return match_result\r\n\r\n        if isinstance(cls.url_regex, list):\r\n            for url_regex_item in cls.url_regex:\r\n                if result := re.fullmatch(pattern=url_regex_item, string=url):\r\n                    return result\r\n\r\n        if raise_error:\r\n            raise ValueError(f\"URL '{url}' doesn't match the URL regex of {cls.name}.\")\r\n\r\n        return None\r\n\r\n    def __enter__(self) -> Scraper:\r\n        return self\r\n\r\n    def __exit__(self, exc_type: Type[BaseException] | None,\r\n                 exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:\r\n        self.close()\r\n\r\n    async def async_close(self) -> None:\r\n        await self._async_session.aclose()\r\n\r\n    def close(self) -> None:\r\n        self._session.close()\r\n\r\n    @abstractmethod\r\n    async def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        \"\"\"\r\n        Scrape media information about the media on a URL.\r\n\r\n        Args:\r\n            url (str): A URL to get media information about.\r\n\r\n        Returns:\r\n            ScrapedMediaResponse: A ScrapedMediaResponse object containing scraped media information.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    async def download_subtitles(self, media_data: PlaylistMediaItem, subrip_conversion: bool = False) -> SubtitlesData:\r\n        \"\"\"\r\n        Download subtitles from a media object.\r\n\r\n        Args:\r\n            media_data (PlaylistMediaItem): A media object to download subtitles from.\r\n            subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.\r\n\r\n        Returns:\r\n            SubtitlesData: A SubtitlesData object containing downloaded subtitles.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def find_matching_media(self, main_playlist: MainPlaylist,\r\n                            filters: dict[str, str | list[str]] | None = None) -> list:\r\n        \"\"\"\r\n        Find media items that match the given filters in the main playlist (or all media items if no filters are given).\r\n\r\n        Args:\r\n            main_playlist (MainPlaylist): Main playlist to search for media items in.\r\n            filters (dict[str, str | list[str]] | None, optional): A dictionary of filters to match media items against.\r\n                Defaults to None.\r\n\r\n        Returns:\r\n            list: A list of media items that match the given filters.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def find_matching_subtitles(self, main_playlist: MainPlaylist,\r\n                                language_filter: list[str] | None = None) -> list[PlaylistMediaItem]:\r\n        \"\"\"\r\n        Find subtitles that match the given language filter in the main playlist.\r\n\r\n        Args:\r\n            main_playlist (MainPlaylist): Main playlist to search for subtitles in.\r\n            language_filter (list[str] | None, optional): A list of language codes to filter subtitles by.\r\n                Defaults to None.\r\n\r\n        Returns:\r\n            list[PlaylistMediaItem]: A list of subtitles that match the given language filter.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    async def load_playlist(self, url: str | list[str], headers: dict | None = None) -> MainPlaylist | None:\r\n        \"\"\"\r\n        Load a playlist from a URL to a representing object.\r\n        Multiple URLs can be given, in which case the first one that loads successfully will be returned.\r\n\r\n        Args:\r\n            url (str | list[str]): URL of the M3U8 playlist to load. Can also be a list of URLs (for redundancy).\r\n            headers (dict | None, optional): A dictionary of headers to use when making the request.\r\n                Defaults to None (results in using session's configured headers).\r\n\r\n        Returns:\r\n            MainPlaylist | None: A playlist object (matching the type), or None if the playlist couldn't be loaded.\r\n        \"\"\"\r\n\r\n\r\nclass HLSScraper(Scraper, ABC):\r\n    \"\"\"A base class for HLS (m3u8) scrapers.\"\"\"\r\n    class M3U8Attribute(Enum):\r\n        \"\"\"\r\n        An enum representing all possible M3U8 attributes.\r\n        Names / Keys represent M3U8 Media object attributes (should be converted to lowercase),\r\n        and values represent the name of the key for config usage.\r\n        \"\"\"\r\n        ASSOC_LANGUAGE = \"assoc-language\"\r\n        AUTOSELECT = \"autoselect\"\r\n        CHARACTERISTICS = \"characteristics\"\r\n        CHANNELS = \"channels\"\r\n        DEFAULT = \"default\"\r\n        FORCED = \"forced\"\r\n        GROUP_ID = \"group-id\"\r\n        INSTREAM_ID = \"instream-id\"\r\n        LANGUAGE = \"language\"\r\n        NAME = \"name\"\r\n        STABLE_RENDITION_ID = \"stable-rendition-id\"\r\n        TYPE = \"type\"\r\n\r\n    _subtitles_filters: dict[str, str | list[str]] = {\r\n        M3U8Attribute.TYPE.value: \"SUBTITLES\",\r\n    }\r\n\r\n    def __init__(self,  user_agent: str | None = None, config_data: dict | None = None):\r\n        super().__init__(user_agent=user_agent, config_data=config_data)\r\n\r\n        # Add M3U8 filters settings\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                category=self._playlist_filters_config_category,\r\n                key=m3u8_attribute.value,\r\n                value_type=Union[str, List[str]],\r\n                required=False,\r\n            ) for m3u8_attribute in self.M3U8Attribute],\r\n            check_config=False)\r\n\r\n    def parse_language_name(self, media_data: m3u8.Media) -> str | None:\r\n        \"\"\"\r\n        Parse the language name from an M3U8 Media object.\r\n        Can be overridden in subclasses for normalization.\r\n\r\n        Args:\r\n            media_data (m3u8.Media): Media object to parse the language name from.\r\n\r\n        Returns:\r\n            str | None: The language name if found, None otherwise.\r\n        \"\"\"\r\n        name: str | None = media_data.name\r\n        return name\r\n\r\n    async def load_playlist(self, url: str | list[str], headers: dict | None = None) -> m3u8.M3U8 | None:\r\n        _headers = headers or self._session.headers\r\n        result: m3u8.M3U8 | None = None\r\n\r\n        for url_item in single_to_list(url):\r\n            try:\r\n                response = await self._async_session.get(url=url_item, headers=_headers, timeout=5)\r\n\r\n            except Exception as e:\r\n                logger.debug(f\"Failed to load M3U8 playlist '{url_item}': {e}\")\r\n                continue\r\n\r\n            if not response.text:\r\n                raise PlaylistLoadError(\"Received empty response for playlist from server.\")\r\n\r\n            result = m3u8.loads(content=response.text, uri=url_item)\r\n            break\r\n\r\n        return result\r\n\r\n    @staticmethod\r\n    def detect_subtitles_type(subtitles_media: m3u8.Media) -> SubtitlesType | None:\r\n        \"\"\"\r\n        Detect the subtitles type (Closed Captions, Forced, etc.) from an M3U8 Media object.\r\n\r\n        Args:\r\n            subtitles_media (m3u8.Media): Subtitles Media object to detect the type of.\r\n\r\n        Returns:\r\n            SubtitlesType | None: The type of the subtitles, None for regular subtitles.\r\n        \"\"\"\r\n        if subtitles_media.forced == \"YES\":\r\n            return SubtitlesType.FORCED\r\n\r\n        if subtitles_media.characteristics is not None and \"public.accessibility\" in subtitles_media.characteristics:\r\n            return SubtitlesType.CC\r\n\r\n        return None\r\n\r\n    async def download_subtitles(self, media_data: m3u8.Media, subrip_conversion: bool = False) -> SubtitlesData:\r\n        playlist_m3u8 = await self.load_playlist(url=media_data.absolute_uri)\r\n\r\n        if playlist_m3u8 is None:\r\n            raise PlaylistLoadError(\"Could not load subtitles M3U8 playlist.\")\r\n\r\n        downloaded_segments = await self.download_segments(playlist=playlist_m3u8)\r\n        subtitles = self.subtitles_class(data=downloaded_segments[0], language_code=media_data.language)\r\n\r\n        if len(downloaded_segments) > 1:\r\n            for segment_data in downloaded_segments[1:]:\r\n                segment_subtitles_obj = self.subtitles_class(data=segment_data, language_code=media_data.language)\r\n                subtitles.append_subtitles(segment_subtitles_obj)\r\n\r\n        subtitles.polish(\r\n            fix_rtl=self.subtitles_fix_rtl,\r\n            remove_duplicates=self.subtitles_remove_duplicates,\r\n        )\r\n\r\n        if subrip_conversion:\r\n            subtitles_format = SubtitlesFormatType.SUBRIP\r\n            content = subtitles.to_srt().dump()\r\n\r\n        else:\r\n            subtitles_format = SubtitlesFormatType.WEBVTT\r\n            content = subtitles.dump()\r\n\r\n        return SubtitlesData(\r\n            language_code=media_data.language,\r\n            language_name=self.parse_language_name(media_data=media_data),\r\n            subtitles_format=subtitles_format,\r\n            content=content,\r\n            content_encoding=subtitles.encoding,\r\n            special_type=self.detect_subtitles_type(subtitles_media=media_data),\r\n        )\r\n\r\n    async def download_segments(self, playlist: m3u8.M3U8) -> list[bytes]:\r\n        responses = await asyncio.gather(\r\n            *[\r\n                self._async_session.get(url=segment.absolute_uri)\r\n                for segment in playlist.segments\r\n            ],\r\n        )\r\n\r\n        responses_data = []\r\n\r\n        for result in responses:\r\n            try:\r\n                result.raise_for_status()\r\n                responses_data.append(result.content)\r\n\r\n            except Exception as e:\r\n                raise DownloadError(\"One of the subtitles segments failed to download.\") from e\r\n\r\n        return responses_data\r\n\r\n    def find_matching_media(self, main_playlist: m3u8.M3U8,\r\n                            filters: dict[str, str | list[str]] | None = None) -> list[m3u8.Media]:\r\n        results: list[m3u8.Media] = []\r\n        config_filters: dict | None = self.config.get(self._playlist_filters_config_category)\r\n        playlist_filters: dict[str, Union[str, List[str]]] | None\r\n\r\n        if config_filters:\r\n            # Merge filtering dictionaries into a single dictionary\r\n            playlist_filters = merge_dict_values(\r\n                *[dict_item for dict_item in (filters, config_filters)\r\n                  if dict_item is not None],\r\n            )\r\n\r\n        else:\r\n            playlist_filters = filters\r\n\r\n        for media in main_playlist.media:\r\n            if not playlist_filters:\r\n                results.append(media)\r\n                continue\r\n\r\n            is_valid = True\r\n\r\n            for filter_name, filter_value in playlist_filters.items():\r\n                try:\r\n                    filter_name_enum = HLSScraper.M3U8Attribute(filter_name)\r\n                    attribute_value = getattr(media, filter_name_enum.name.lower(), None)\r\n\r\n                    if (attribute_value is None) or (\r\n                            isinstance(filter_value, list) and\r\n                            attribute_value.casefold() not in (x.casefold() for x in filter_value)\r\n                    ) or (\r\n                            isinstance(filter_value, str) and filter_value.casefold() != attribute_value.casefold()\r\n                    ):\r\n                        is_valid = False\r\n                        break\r\n\r\n                except Exception:\r\n                    is_valid = False\r\n\r\n            if is_valid:\r\n                results.append(media)\r\n\r\n        return results\r\n\r\n    def find_matching_subtitles(self, main_playlist: m3u8.M3U8,\r\n                                language_filter: list[str] | None = None) -> list[m3u8.Media]:\r\n        _filters = self._subtitles_filters\r\n\r\n        if language_filter:\r\n            _filters[self.M3U8Attribute.LANGUAGE.value] = language_filter\r\n\r\n        return self.find_matching_media(main_playlist=main_playlist, filters=_filters)\r\n\r\n\r\nclass ScraperFactory:\r\n    _scraper_classes_cache: list[type[Scraper]] | None = None\r\n    _scraper_instances_cache: dict[type[Scraper], Scraper] = {}\r\n    _currently_initializing: list[type[Scraper]] = []  # Used to prevent infinite recursion\r\n\r\n    @classmethod\r\n    def get_initialized_scrapers(cls) -> list[Scraper]:\r\n        \"\"\"\r\n        Get a list of all previously initialized scrapers.\r\n\r\n        Returns:\r\n            list[Scraper]: A list of initialized scrapers.\r\n        \"\"\"\r\n        return list(cls._scraper_instances_cache.values())\r\n\r\n    @classmethod\r\n    def get_scraper_classes(cls) -> list[type[Scraper]]:\r\n        \"\"\"\r\n        Find all scraper classes in the scrapers directory.\r\n\r\n        Returns:\r\n            list[Scraper]: A Scraper subclass.\r\n        \"\"\"\r\n        if cls._scraper_classes_cache is not None:\r\n            return cls._scraper_classes_cache\r\n\r\n        cls._scraper_classes_cache = []\r\n        scraper_modules_paths = Path(__file__).parent.glob(f\"*{SCRAPER_MODULES_SUFFIX}.py\")\r\n\r\n        for scraper_module_path in scraper_modules_paths:\r\n            sys.path.append(str(scraper_module_path))\r\n\r\n            module = importlib.import_module(f\"{PACKAGE_NAME}.scrapers.{scraper_module_path.stem}\")\r\n\r\n            # Find all 'Scraper' subclasses\r\n            for _, obj in inspect.getmembers(module,\r\n                                             predicate=lambda x: inspect.isclass(x) and issubclass(x, Scraper)):\r\n                # Skip object if it's an abstract or imported from another module\r\n                if not inspect.isabstract(obj) and obj.__module__ == module.__name__:\r\n                    cls._scraper_classes_cache.append(obj)\r\n\r\n        return cls._scraper_classes_cache\r\n\r\n    @classmethod\r\n    def _get_scraper_instance(cls, scraper_class: type[ScraperT], kwargs: dict | None = None,\r\n                              extract_scraper_config: bool = False) -> ScraperT:\r\n        \"\"\"\r\n        Initialize and return a scraper instance.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT]): A scraper class to initialize.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            extract_scraper_config (bool, optional): Whether the passed 'config_data' (within kwargs)\r\n                is a main config dictionary, and scraper's config should be extracted from it. Defaults to False.\r\n\r\n        Returns:\r\n            Scraper: An instance of the given scraper class.\r\n        \"\"\"\r\n        logger.debug(f\"Initializing '{scraper_class.name}' scraper...\")\r\n        kwargs = kwargs or {}\r\n\r\n        if scraper_class not in cls._scraper_instances_cache:\r\n            logger.debug(f\"'{scraper_class.name}' scraper not found in cache, creating a new instance...\")\r\n\r\n            if scraper_class in cls._currently_initializing:\r\n                raise ScraperError(f\"'{scraper_class.name}' scraper is already being initialized.\\n\"\r\n                                   f\"Make sure there are no circular dependencies between scrapers.\")\r\n\r\n            cls._currently_initializing.append(scraper_class)\r\n\r\n            if extract_scraper_config:\r\n                if kwargs.get(\"config_data\"):\r\n                    required_scrapers_ids = [scraper_class.id, *scraper_class.uses_scrapers]\r\n                    kwargs[\"config_data\"] = (\r\n                        {scraper_id: kwargs[\"config_data\"][scraper_id] for scraper_id in required_scrapers_ids\r\n                         if kwargs[\"config_data\"].get(scraper_id)}\r\n                    )\r\n\r\n                else:\r\n                    kwargs[\"config_data\"] = None\r\n\r\n            cls._scraper_instances_cache[scraper_class] = scraper_class(**kwargs)\r\n            cls._currently_initializing.remove(scraper_class)\r\n\r\n        else:\r\n            logger.debug(f\"Cached '{scraper_class.name}' scraper instance found and will be used.\")\r\n\r\n        return cls._scraper_instances_cache[scraper_class]  # type: ignore[return-value]\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ..., extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[True] = ...) -> ScraperT:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[False] = ...) -> ScraperT | None:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ..., extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[True] = ...) -> Scraper:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ..., extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[False] = ...) -> Scraper | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def get_scraper_instance(cls, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,\r\n                             url: str | None = None, kwargs: dict | None = None, extract_scraper_config: bool = False,\r\n                             raise_error: bool = True) -> Scraper | None:\r\n        \"\"\"\r\n        Find, initialize and return a scraper that matches the given URL or ID.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT] | None, optional): A scraper class to initialize. Defaults to None.\r\n            scraper_id (str | None, optional): ID of a scraper to initialize. Defaults to None.\r\n            url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            extract_scraper_config (bool, optional): Whether the passed 'config_data' (within kwargs)\r\n            raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.\r\n\r\n        Returns:\r\n            ScraperT | Scraper | None: An instance of a scraper that matches the given URL or ID,\r\n                None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If no scraper was found and raise_error is True.\r\n        \"\"\"\r\n        if scraper_class:\r\n            return cls._get_scraper_instance(scraper_class=scraper_class, kwargs=kwargs,\r\n                                             extract_scraper_config=extract_scraper_config)\r\n\r\n        if not (scraper_id or url):\r\n            raise ValueError(\"At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.\")\r\n\r\n        if scraper_id:\r\n            logger.debug(f\"Searching for a scraper object with ID '{scraper_id}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.id == scraper_id:\r\n                    return cls._get_scraper_instance(scraper_class=scraper, kwargs=kwargs,\r\n                                                     extract_scraper_config=extract_scraper_config)\r\n\r\n        elif url:\r\n            logger.debug(f\"Searching for a scraper object that matches URL '{url}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.match_url(url) is not None:\r\n                    return cls._get_scraper_instance(scraper_class=scraper, kwargs=kwargs,\r\n                                                     extract_scraper_config=extract_scraper_config)\r\n\r\n        error_message = \"No matching scraper was found.\"\r\n\r\n        if raise_error:\r\n            raise ValueError(error_message)\r\n\r\n        logger.debug(error_message)\r\n        return None\r\n\r\n\r\nclass ScraperError(Exception):\r\n    pass\r\n\r\n\r\nclass DownloadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass PlaylistLoadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass SubtitlesDownloadError(ScraperError):\r\n    def __init__(self, language_code: str, language_name: str | None = None, special_type: SubtitlesType | None = None,\r\n                 original_exc: Exception | None = None, *args: Any, **kwargs: dict[str, Any]):\r\n        \"\"\"\r\n        Initialize a SubtitlesDownloadError instance.\r\n\r\n        Args:\r\n            language_code (str): Language code of the subtitles that failed to download.\r\n            language_name (str | None, optional): Language name of the subtitles that failed to download.\r\n            special_type (SubtitlesType | None, optional): Type of the subtitles that failed to download.\r\n            original_exc (Exception | None, optional): The original exception that caused the error.\r\n        \"\"\"\r\n        super().__init__(*args, **kwargs)\r\n        self.language_code = language_code\r\n        self.language_name = language_name\r\n        self.special_type = special_type\r\n        self.original_exc = original_exc\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/scraper.py b/isubrip/scrapers/scraper.py
--- a/isubrip/scrapers/scraper.py	(revision f040b9612ca4349900c8d4bb1178838dc8a97be1)
+++ b/isubrip/scrapers/scraper.py	(date 1723918271896)
@@ -13,7 +13,7 @@
 import httpx
 import m3u8
 
-from isubrip.config import Config, ConfigSetting
+from isubrip.config import ConfigCategory
 from isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX
 from isubrip.data_structures import (
     MainPlaylist,
@@ -92,32 +92,15 @@
             verify_ssl (bool | None, optional): Whether to verify SSL certificates. Defaults to None.
             config_data (dict | None, optional): A dictionary containing scraper's configuration data. Defaults to None.
         """
-        self.config = Config(config_data=config_data.get(self.id) if config_data else None)
-
-        # Add a "user-agent" setting by default to all scrapers
-        self.config.add_settings([
-            ConfigSetting(
-                key="timeout",
-                value_type=Union[int, float],
-                required=False,
-            ),
-            ConfigSetting(
-                key="user-agent",
-                value_type=str,
-                required=False,
-            ),
-            ConfigSetting(
-                key="proxy",
-                value_type=str,
-                required=False,
-            ),
-            ConfigSetting(
-                key="verify-ssl",
-                value_type=bool,
-                required=False,
-            ),
-        ],
-            check_config=False)
+        class ScraperConfig(ConfigCategory):
+            """
+            A class that specifies the configuration settings for a scraper.
+            These will be automatically detected, and then validated and parsed, if a conf
+            """
+            timeout: int | float | None
+            user_agent: str | None
+            proxy: str | None
+            verify_ssl: bool | None
 
         self._user_agent: str
         self._proxy: str | None
Index: isubrip/resources/default_config.toml
===================================================================
diff --git a/isubrip/resources/default_config.toml b/isubrip/resources/default_config.toml
deleted file mode 100644
--- a/isubrip/resources/default_config.toml	(revision f040b9612ca4349900c8d4bb1178838dc8a97be1)
+++ /dev/null	(revision f040b9612ca4349900c8d4bb1178838dc8a97be1)
@@ -1,23 +0,0 @@
-# This is the default config file, containing all the default settings.
-# Settings set on the user config will override these.
-
-[general]
-check-for-updates = true
-log-rotation-size = 15
-
-[downloads]
-folder = "."
-merge-playlists = false
-overwrite-existing = false
-zip = true
-
-[subtitles]
-fix-rtl = false
-remove-duplicates = true
-convert-to-srt = false
-
-[subtitles.webvtt]
-subrip-alignment-conversion = false
-
-[scrapers]
-user-agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36"
Index: isubrip/constants.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport datetime as dt\r\nimport logging\r\nfrom pathlib import Path\r\nfrom tempfile import gettempdir\r\n\r\n# General\r\nPACKAGE_NAME = \"isubrip\"\r\nPACKAGE_VERSION = \"2.5.6\"\r\n\r\n# Async\r\nEVENT_LOOP = asyncio.get_event_loop()\r\n\r\n# Logging\r\nPREORDER_MESSAGE = (\"'{movie_name}' is currently unavailable on {scraper_name}, \"\r\n                    \"and will be available on {preorder_date}.\")\r\n\r\nANSI_COLORS = {\r\n    logging.DEBUG: \"\\x1b[37;20m\",  # Light Grey\r\n    logging.INFO: \"\\x1b[38;20m\",  # Grey\r\n    logging.WARNING: \"\\x1b[33;20m\",  # Yellow\r\n    logging.ERROR: \"\\x1b[31;20m\",  # Red\r\n    logging.CRITICAL: \"\\x1b[31;1m\",  # Bold Red\r\n    }\r\nRESET_COLOR = \"\\x1b[0m\"\r\n\r\nLOGGING_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\r\nLOGGING_FILE_METADATA = \"[%(asctime)s | %(levelname)s | %(threadName)s | %(filename)s::%(funcName)s::%(lineno)d] \"\r\n\r\n# Downloads\r\nARCHIVE_FORMAT = \"zip\"\r\n\r\n# Paths\r\nDEFAULT_CONFIG_PATH = Path(__file__).parent / \"resources\" / \"default_config.toml\"\r\nDATA_FOLDER_PATH = Path.home() / f\".{PACKAGE_NAME}\"\r\nSCRAPER_MODULES_SUFFIX = \"_scraper\"\r\nTEMP_FOLDER_PATH = Path(gettempdir()) / PACKAGE_NAME\r\n\r\n# Config Paths\r\nUSER_CONFIG_FILE_NAME = \"config.toml\"\r\nUSER_CONFIG_FILE = DATA_FOLDER_PATH / USER_CONFIG_FILE_NAME\r\n\r\n# Logging Paths\r\nLOG_FILES_PATH = DATA_FOLDER_PATH / \"logs\"\r\nLOG_FILE_NAME = f\"{PACKAGE_NAME}_{dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\r\n\r\n# Other\r\nTITLE_REPLACEMENT_STRINGS = {  # Replacements will be done by the order of the keys.\r\n    \": \": \".\", \":\": \".\", \" - \": \"-\", \", \": \".\", \". \": \".\", \" \": \".\", \"|\": \".\", \"/\": \".\", \"…\": \".\",\r\n    \"<\": \"\", \">\": \"\", \"(\": \"\", \")\": \"\", '\"': \"\", \"?\": \"\", \"*\": \"\",\r\n}\r\nWINDOWS_RESERVED_FILE_NAMES = (\"CON\", \"PRN\", \"AUX\", \"NUL\", \"COM1\", \"COM2\", \"COM3\", \"COM4\", \"COM5\", \"COM6\", \"COM7\",\r\n                               \"COM8\", \"COM9\", \"LPT1\", \"LPT2\", \"LPT3\", \"LPT4\", \"LPT5\", \"LPT6\", \"LPT7\", \"LPT8\", \"LPT9\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/constants.py b/isubrip/constants.py
--- a/isubrip/constants.py	(revision f040b9612ca4349900c8d4bb1178838dc8a97be1)
+++ b/isubrip/constants.py	(date 1723916115788)
@@ -40,7 +40,7 @@
 
 # Config Paths
 USER_CONFIG_FILE_NAME = "config.toml"
-USER_CONFIG_FILE = DATA_FOLDER_PATH / USER_CONFIG_FILE_NAME
+USER_CONFIG_FILE_PATH = DATA_FOLDER_PATH / USER_CONFIG_FILE_NAME
 
 # Logging Paths
 LOG_FILES_PATH = DATA_FOLDER_PATH / "logs"
