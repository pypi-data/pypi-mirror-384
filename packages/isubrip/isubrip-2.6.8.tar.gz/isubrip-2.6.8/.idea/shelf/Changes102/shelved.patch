Index: isubrip/__main__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport logging\r\nfrom pathlib import Path\r\nimport shutil\r\nimport sys\r\nfrom typing import List\r\n\r\nimport requests\r\nfrom requests.utils import default_user_agent\r\n\r\nfrom isubrip.config import Config, ConfigError, ConfigSetting, SpecialConfigType\r\nfrom isubrip.constants import (\r\n    ARCHIVE_FORMAT,\r\n    DATA_FOLDER_PATH,\r\n    DEFAULT_CONFIG_PATH,\r\n    LOG_FILE_NAME,\r\n    LOG_FILES_PATH,\r\n    PACKAGE_NAME,\r\n    PACKAGE_VERSION,\r\n    PREORDER_MESSAGE,\r\n    TEMP_FOLDER_PATH,\r\n    USER_CONFIG_FILE,\r\n)\r\nfrom isubrip.data_structures import (\r\n    Episode,\r\n    MediaData,\r\n    Movie,\r\n    ScrapedMediaResponse,\r\n    Season,\r\n    Series,\r\n    SubtitlesData,\r\n    SubtitlesDownloadResults,\r\n)\r\nfrom isubrip.logger import CustomLogFileFormatter, CustomStdoutFormatter, logger\r\nfrom isubrip.scrapers.scraper import PlaylistLoadError, Scraper, ScraperError, ScraperFactory, SubtitlesDownloadError\r\nfrom isubrip.subtitle_formats.webvtt import Caption as WebVTTCaption\r\nfrom isubrip.utils import (\r\n    TempDirGenerator,\r\n    download_subtitles_to_file,\r\n    format_media_description,\r\n    format_release_name,\r\n    format_subtitles_description,\r\n    generate_non_conflicting_path,\r\n    raise_for_status,\r\n    single_to_list,\r\n)\r\n\r\nLOG_ROTATION_SIZE: int | None = None\r\n\r\nBASE_CONFIG_SETTINGS = [\r\n    ConfigSetting(\r\n        key=\"check-for-updates\",\r\n        value_type=bool,\r\n        category=\"general\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"log_rotation_size\",\r\n        value_type=str,\r\n        category=\"general\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"add-release-year-to-series\",\r\n        value_type=bool,\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"folder\",\r\n        value_type=str,\r\n        category=\"downloads\",\r\n        required=True,\r\n        special_type=SpecialConfigType.EXISTING_FOLDER_PATH,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"languages\",\r\n        value_type=List[str],\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"overwrite-existing\",\r\n        value_type=bool,\r\n        category=\"downloads\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"zip\",\r\n        value_type=bool,\r\n        category=\"downloads\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"fix-rtl\",\r\n        value_type=bool,\r\n        category=\"subtitles\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"rtl-languages\",\r\n        value_type=List[str],\r\n        category=\"subtitles\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"remove-duplicates\",\r\n        value_type=bool,\r\n        category=\"subtitles\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"convert-to-srt\",\r\n        value_type=bool,\r\n        category=\"subtitles\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"subrip-alignment-conversion\",\r\n        value_type=bool,\r\n        category=(\"subtitles\", \"webvtt\"),\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"user-agent\",\r\n        value_type=str,\r\n        category=\"scrapers\",\r\n        required=True,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"proxy\",\r\n        value_type=str,\r\n        category=\"scrapers\",\r\n        required=False,\r\n    ),\r\n    ConfigSetting(\r\n        key=\"verify-ssl\",\r\n        value_type=bool,\r\n        category=\"scrapers\",\r\n        required=False,\r\n    ),\r\n]\r\n\r\n\r\ndef main() -> None:\r\n    try:\r\n        # Assure at least one argument was passed\r\n        if len(sys.argv) < 2:\r\n            print_usage()\r\n            exit(0)\r\n\r\n        if not DATA_FOLDER_PATH.is_dir():\r\n            DATA_FOLDER_PATH.mkdir(parents=True)\r\n\r\n        setup_loggers(stdout_loglevel=logging.INFO,\r\n                      file_loglevel=logging.DEBUG)\r\n\r\n        cli_args = \" \".join(sys.argv[1:])\r\n        logger.debug(f\"CLI Command: {PACKAGE_NAME} {cli_args}\")\r\n        logger.debug(f\"Python version: {sys.version}\")\r\n        logger.debug(f\"Package version: {PACKAGE_VERSION}\")\r\n        logger.debug(f\"OS: {sys.platform}\")\r\n\r\n        config = generate_config()\r\n        update_settings(config)\r\n\r\n        if config.general.get(\"check-for-updates\", True):\r\n            check_for_updates(current_package_version=PACKAGE_VERSION)\r\n\r\n        urls = single_to_list(sys.argv[1:])\r\n        download(urls=urls, config=config)\r\n\r\n    except Exception as ex:\r\n        logger.error(f\"Error: {ex}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        exit(1)\r\n\r\n    finally:\r\n        if log_rotation_size := LOG_ROTATION_SIZE:\r\n            handle_log_rotation(log_rotation_size=log_rotation_size)\r\n\r\n        # NOTE: This will only close scrapers that were initialized using the ScraperFactory.\r\n        for scraper in ScraperFactory.get_initialized_scrapers():\r\n            scraper.close()\r\n\r\n        TempDirGenerator.cleanup()\r\n\r\n\r\ndef download(urls: list[str], config: Config) -> None:\r\n    \"\"\"\r\n    Download subtitles from a given URL.\r\n\r\n    Args:\r\n        urls (list[str]): A list of URLs to download subtitles from.\r\n        config (Config): A config to use for downloading subtitles.\r\n    \"\"\"\r\n    for url in urls:\r\n        try:\r\n            logger.info(f\"Scraping '{url}'...\")\r\n\r\n            scraper = ScraperFactory.get_scraper_instance(url=url,\r\n                                                          kwargs={\"config_data\": config.data.get(\"scrapers\")},\r\n                                                          extract_scraper_config=True)\r\n            scraper.config.check()  # Recheck config after scraper settings were loaded\r\n\r\n            try:\r\n                logger.debug(f\"Fetching '{url}'...\")\r\n                scraper_response: ScrapedMediaResponse = scraper.get_data(url=url)\r\n\r\n            except ScraperError as e:\r\n                logger.error(f\"Error: {e}\")\r\n                logger.debug(\"Debug information:\", exc_info=True)\r\n                continue\r\n\r\n            media_data = scraper_response.media_data\r\n            playlist_scraper = ScraperFactory.get_scraper_instance(scraper_id=scraper_response.playlist_scraper,\r\n                                                                   kwargs={\"config_data\": config.data.get(\"scrapers\")},\r\n                                                                   extract_scraper_config=True)\r\n\r\n            if not media_data:\r\n                logger.error(f\"Error: No supported media was found for {url}.\")\r\n                continue\r\n\r\n            for media_item in media_data:\r\n                try:\r\n                    object_type_str = media_item.__class__.__name__.lower()\r\n\r\n                    logger.info(f\"Found {object_type_str}: {format_media_description(media_data=media_item)}\")\r\n                    download_media(scraper=playlist_scraper, media_item=media_item, config=config)\r\n\r\n                except Exception as e:\r\n                    if len(media_data) > 1:\r\n                        logger.warning(f\"Error scraping media item \"\r\n                                       f\"'{format_media_description(media_data=media_item)}': {e}\\n\"\r\n                                       f\"Skipping to next media item...\")\r\n                        logger.debug(\"Debug information:\", exc_info=True)\r\n                        continue\r\n\r\n                    raise\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error while scraping '{url}': {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            continue\r\n\r\n\r\ndef download_media(scraper: Scraper, media_item: MediaData, config: Config) -> None:\r\n    \"\"\"\r\n    Download a media item.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_item (MediaData): A media data item to download subtitles for.\r\n        config (Config): A config to use for downloading subtitles.\r\n    \"\"\"\r\n    if isinstance(media_item, Series):\r\n        for season in media_item.seasons:\r\n            download_media(scraper=scraper, media_item=season, config=config)\r\n\r\n    elif isinstance(media_item, Season):\r\n        for episode in media_item.episodes:\r\n            logger.info(f\"{format_media_description(media_data=episode, shortened=True)}:\")\r\n            download_media_item(scraper=scraper, media_item=episode, config=config)\r\n\r\n    elif isinstance(media_item, (Movie, Episode)):\r\n        download_media_item(scraper=scraper, media_item=media_item, config=config)\r\n\r\n\r\ndef download_media_item(scraper: Scraper, media_item: Movie | Episode, config: Config) -> None:\r\n    if media_item.playlist:\r\n        download_subtitles_kwargs = {\r\n            \"download_path\": Path(config.downloads[\"folder\"]),\r\n            \"language_filter\": config.downloads.get(\"languages\"),\r\n            \"convert_to_srt\": config.subtitles.get(\"convert-to-srt\", False),\r\n            \"overwrite_existing\": config.downloads.get(\"overwrite-existing\", False),\r\n            \"zip_files\": config.downloads.get(\"zip\", False),\r\n        }\r\n\r\n        try:\r\n            results = download_subtitles(scraper=scraper,\r\n                                         media_data=media_item,\r\n                                         **download_subtitles_kwargs)\r\n\r\n            success_count = len(results.successful_subtitles)\r\n            failed_count = len(results.failed_subtitles)\r\n\r\n            if success_count or failed_count:\r\n                logger.info(f\"{success_count}/{success_count + failed_count} matching subtitles \"\r\n                            f\"were successfully downloaded.\")\r\n\r\n            else:\r\n                logger.info(\"No matching subtitles were found.\")\r\n\r\n            return  # noqa: TRY300\r\n\r\n        except PlaylistLoadError:\r\n            pass\r\n\r\n    # We get here if there is no playlist, or there is one, but it failed to load\r\n    if isinstance(media_item, Movie) and media_item.preorder_availability_date:\r\n        preorder_date_str = media_item.preorder_availability_date.strftime(\"%Y-%m-%d\")\r\n        logger.info(PREORDER_MESSAGE.format(movie_name=media_item.name, scraper_name=scraper.name,\r\n                                            preorder_date=preorder_date_str))\r\n\r\n    else:\r\n        logger.error(\"No valid playlist was found.\")\r\n\r\n\r\ndef check_for_updates(current_package_version: str) -> None:\r\n    \"\"\"\r\n    Check and print if a newer version of the package is available, and log accordingly.\r\n\r\n    Args:\r\n        current_package_version (str): The current version of the package.\r\n    \"\"\"\r\n    api_url = f\"https://pypi.org/pypi/{PACKAGE_NAME}/json\"\r\n    logger.debug(\"Checking for package updates on PyPI...\")\r\n    try:\r\n        response = requests.get(\r\n            url=api_url,\r\n            headers={\"Accept\": \"application/json\"},\r\n            timeout=5,\r\n        )\r\n        raise_for_status(response)\r\n        response_data = response.json()\r\n\r\n        pypi_latest_version = response_data[\"info\"][\"version\"]\r\n\r\n        if pypi_latest_version != current_package_version:\r\n            logger.warning(f\"You are currently using version '{current_package_version}' of '{PACKAGE_NAME}', \"\r\n                           f\"however version '{pypi_latest_version}' is available.\"\r\n                           f'\\nConsider upgrading by running \"python3 -m pip install --upgrade {PACKAGE_NAME}\"\\n')\r\n\r\n        else:\r\n            logger.debug(f\"Latest version of '{PACKAGE_NAME}' ({current_package_version}) is currently installed.\")\r\n\r\n    except Exception as e:\r\n        logger.warning(f\"Update check failed: {e}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        return\r\n\r\n\r\ndef download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,\r\n                       language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                       overwrite_existing: bool = True, zip_files: bool = False) -> SubtitlesDownloadResults:\r\n    \"\"\"\r\n    Download subtitles for the given media data.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        zip_files (bool, optional): Whether to unite the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n\r\n    Returns:\r\n        SubtitlesDownloadResults: A SubtitlesDownloadResults object containing the results of the download.\r\n    \"\"\"\r\n    temp_dir_name = generate_media_folder_name(media_data=media_data, source=scraper.abbreviation)\r\n    temp_download_path = TempDirGenerator.generate(directory_name=temp_dir_name)\r\n\r\n    successful_downloads: list[SubtitlesData] = []\r\n    failed_downloads: list[SubtitlesDownloadError] = []\r\n    temp_downloads: list[Path] = []\r\n\r\n    for subtitles_data in scraper.get_subtitles(main_playlist=media_data.playlist,  # type: ignore[arg-type]\r\n                                                language_filter=language_filter,\r\n                                                subrip_conversion=convert_to_srt):\r\n        language_info = format_subtitles_description(language_code=subtitles_data.language_code,\r\n                                                     language_name=subtitles_data.language_name,\r\n                                                     special_type=subtitles_data.special_type)\r\n\r\n        if isinstance(subtitles_data, SubtitlesDownloadError):\r\n            logger.warning(f\"Failed to download '{language_info}' subtitles. Skipping...\")\r\n            logger.debug(\"Debug information:\", exc_info=subtitles_data.original_exc)\r\n            failed_downloads.append(subtitles_data)\r\n            continue\r\n\r\n        try:\r\n            temp_downloads.append(download_subtitles_to_file(\r\n                media_data=media_data,\r\n                subtitles_data=subtitles_data,\r\n                output_path=temp_download_path,\r\n                source_abbreviation=scraper.abbreviation,\r\n                overwrite=overwrite_existing,\r\n            ))\r\n\r\n            logger.info(f\"'{language_info}' subtitles were successfully downloaded.\")\r\n            successful_downloads.append(subtitles_data)\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error: Failed to save '{language_info}' subtitles: {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            failed_downloads.append(\r\n                SubtitlesDownloadError(\r\n                    language_code=subtitles_data.language_code,\r\n                    language_name=subtitles_data.language_name,\r\n                    special_type=subtitles_data.special_type,\r\n                    original_exc=e,\r\n                ),\r\n            )\r\n\r\n    if not zip_files or len(temp_downloads) == 1:\r\n        for file_path in temp_downloads:\r\n            if overwrite_existing:\r\n                new_path = download_path / file_path.name\r\n\r\n            else:\r\n                new_path = generate_non_conflicting_path(file_path=download_path / file_path.name)\r\n\r\n            # str conversion needed only for Python <= 3.8 - https://github.com/python/cpython/issues/76870\r\n            shutil.move(src=str(file_path), dst=new_path)\r\n\r\n    elif len(temp_downloads) > 0:\r\n        archive_path = Path(shutil.make_archive(\r\n            base_name=str(temp_download_path.parent / temp_download_path.name),\r\n            format=ARCHIVE_FORMAT,\r\n            root_dir=temp_download_path,\r\n        ))\r\n\r\n        file_name = generate_media_folder_name(media_data=media_data,\r\n                                               source=scraper.abbreviation) + f\".{ARCHIVE_FORMAT}\"\r\n\r\n        if overwrite_existing:\r\n            destination_path = download_path / file_name\r\n\r\n        else:\r\n            destination_path = generate_non_conflicting_path(file_path=download_path / file_name)\r\n\r\n        shutil.move(src=str(archive_path), dst=destination_path)\r\n\r\n    return SubtitlesDownloadResults(\r\n        media_data=media_data,\r\n        successful_subtitles=successful_downloads,\r\n        failed_subtitles=failed_downloads,\r\n        is_zip=zip_files,\r\n    )\r\n\r\n\r\ndef handle_log_rotation(log_rotation_size: int) -> None:\r\n    \"\"\"\r\n    Handle log rotation and remove old log files if needed.\r\n\r\n    Args:\r\n        log_rotation_size (int): Maximum amount of log files to keep.\r\n    \"\"\"\r\n    sorted_log_files = sorted(LOG_FILES_PATH.glob(\"*.log\"), key=lambda file: file.stat().st_mtime, reverse=True)\r\n\r\n    if len(sorted_log_files) > log_rotation_size:\r\n        for log_file in sorted_log_files[log_rotation_size:]:\r\n            log_file.unlink()\r\n\r\n\r\ndef generate_config() -> Config:\r\n    \"\"\"\r\n    Generate a config object using config files, and validate it.\r\n\r\n    Returns:\r\n        Config: A config object.\r\n\r\n    Raises:\r\n        ConfigException: If there is a general config error.\r\n        MissingConfigValue: If a required config value is missing.\r\n        InvalidConfigValue: If a config value is invalid.\r\n    \"\"\"\r\n    if not DEFAULT_CONFIG_PATH.is_file():\r\n        raise ConfigError(\"Default config file could not be found.\")\r\n\r\n    config = Config(config_settings=BASE_CONFIG_SETTINGS)\r\n\r\n    logger.debug(\"Loading default config data...\")\r\n\r\n    with DEFAULT_CONFIG_PATH.open('r') as data:\r\n        config.loads(config_data=data.read(), check_config=True)\r\n\r\n    logger.debug(\"Default config data loaded and validated successfully.\")\r\n\r\n    # If logs folder doesn't exist, create it (also handles data folder)\r\n    if not DATA_FOLDER_PATH.is_dir():\r\n        logger.debug(f\"'{DATA_FOLDER_PATH}' directory could not be found and will be created.\")\r\n        DATA_FOLDER_PATH.mkdir(parents=True, exist_ok=True)\r\n        LOG_FILES_PATH.mkdir()\r\n\r\n    else:\r\n        if not LOG_FILES_PATH.is_dir():\r\n            logger.debug(f\"'{LOG_FILES_PATH}' directory could not be found and will be created.\")\r\n            LOG_FILES_PATH.mkdir()\r\n\r\n        # If a user config file exists, add it to config_files\r\n        if USER_CONFIG_FILE.is_file():\r\n            logger.info(f\"User config file detected at '{USER_CONFIG_FILE}' and will be used.\")\r\n\r\n            with USER_CONFIG_FILE.open('r') as data:\r\n                config.loads(config_data=data.read(), check_config=True)\r\n\r\n            logger.debug(\"User config file loaded and validated successfully.\")\r\n\r\n    return config\r\n\r\n\r\ndef generate_media_folder_name(media_data: Movie | Episode, source: str | None = None) -> str:\r\n    \"\"\"\r\n    Generate a folder name for media data.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        str: A folder name for the media data.\r\n    \"\"\"\r\n    if isinstance(media_data, Movie):\r\n        return format_release_name(\r\n            title=media_data.name,\r\n            release_date=media_data.release_date,\r\n            media_source=source,\r\n        )\r\n\r\n    # elif isinstance(media_data, Episode):\r\n    return format_release_name(\r\n        title=media_data.series_name,\r\n        season_number=media_data.season_number,\r\n        media_source=source,\r\n    )\r\n\r\n\r\ndef generate_temp_media_path(media_data: Movie | Episode, source: str | None = None) -> Path:\r\n    \"\"\"\r\n    Generate a temporary directory for downloading media data.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        Path: A path to the temporary folder.\r\n    \"\"\"\r\n    temp_folder_name = generate_media_folder_name(media_data=media_data, source=source)\r\n    path = generate_non_conflicting_path(file_path=TEMP_FOLDER_PATH / temp_folder_name, has_extension=False)\r\n\r\n    return TempDirGenerator.generate(directory_name=path.name)\r\n\r\n\r\ndef update_settings(config: Config) -> None:\r\n    \"\"\"\r\n    Update settings according to config.\r\n\r\n    Args:\r\n        config (Config): An instance of a config to set settings according to.\r\n    \"\"\"\r\n    Scraper.subtitles_fix_rtl = config.subtitles[\"fix-rtl\"]\r\n    Scraper.subtitles_remove_duplicates = config.subtitles[\"remove-duplicates\"]\r\n    Scraper.default_user_agent = config.scrapers.get(\"user-agent\", default_user_agent())\r\n    Scraper.default_proxy = config.scrapers.get(\"proxy\")\r\n    Scraper.default_verify_ssl = config.scrapers.get(\"verify-ssl\", True)\r\n\r\n    if not Scraper.default_verify_ssl:\r\n        import urllib3\r\n        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\n    WebVTTCaption.subrip_alignment_conversion = (\r\n        config.subtitles.get(\"webvtt\", {}).get(\"subrip-alignment-conversion\", False)\r\n    )\r\n\r\n    if log_rotation := config.general.get(\"log-rotation-size\"):\r\n        global LOG_ROTATION_SIZE\r\n        LOG_ROTATION_SIZE = log_rotation\r\n\r\n\r\ndef print_usage() -> None:\r\n    \"\"\"Print usage information.\"\"\"\r\n    logger.info(f\"Usage: {PACKAGE_NAME} <iTunes movie URL> [iTunes movie URL...]\")\r\n\r\n\r\ndef setup_loggers(stdout_loglevel: int, file_loglevel: int) -> None:\r\n    \"\"\"\r\n    Configure loggers.\r\n\r\n    Args:\r\n        stdout_loglevel (int): Log level for STDOUT logger.\r\n        file_loglevel (int): Log level for logfile logger.\r\n    \"\"\"\r\n    logger.setLevel(logging.DEBUG)\r\n\r\n    # Setup STDOUT logger\r\n    stdout_handler = logging.StreamHandler(sys.stdout)\r\n    stdout_handler.setLevel(stdout_loglevel)\r\n    stdout_handler.setFormatter(CustomStdoutFormatter())\r\n    logger.addHandler(stdout_handler)\r\n\r\n    # Setup logfile logger\r\n    if not LOG_FILES_PATH.is_dir():\r\n        logger.debug(\"Logs directory could not be found and will be created.\")\r\n        LOG_FILES_PATH.mkdir()\r\n\r\n    logfile_path = generate_non_conflicting_path(file_path=LOG_FILES_PATH / LOG_FILE_NAME)\r\n    logfile_handler = logging.FileHandler(filename=logfile_path, encoding=\"utf-8\")\r\n    logfile_handler.setLevel(file_loglevel)\r\n    logfile_handler.setFormatter(CustomLogFileFormatter())\r\n    logger.debug(f\"Log file location: '{logfile_path}'\")\r\n    logger.addHandler(logfile_handler)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/__main__.py b/isubrip/__main__.py
--- a/isubrip/__main__.py	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/isubrip/__main__.py	(date 1715678910565)
@@ -1,9 +1,11 @@
 from __future__ import annotations
 
+import asyncio
 import logging
 from pathlib import Path
 import shutil
 import sys
+from timeit import default_timer as timer
 from typing import List
 
 import requests
@@ -143,7 +145,9 @@
 ]
 
 
-def main() -> None:
+async def main() -> None:
+    start_time = timer()
+
     try:
         # Assure at least one argument was passed
         if len(sys.argv) < 2:
@@ -153,7 +157,7 @@
         if not DATA_FOLDER_PATH.is_dir():
             DATA_FOLDER_PATH.mkdir(parents=True)
 
-        setup_loggers(stdout_loglevel=logging.INFO,
+        setup_loggers(stdout_loglevel=logging.DEBUG,
                       file_loglevel=logging.DEBUG)
 
         cli_args = " ".join(sys.argv[1:])
@@ -169,7 +173,7 @@
             check_for_updates(current_package_version=PACKAGE_VERSION)
 
         urls = single_to_list(sys.argv[1:])
-        download(urls=urls, config=config)
+        await download(urls=urls, config=config)
 
     except Exception as ex:
         logger.error(f"Error: {ex}")
@@ -183,11 +187,16 @@
         # NOTE: This will only close scrapers that were initialized using the ScraperFactory.
         for scraper in ScraperFactory.get_initialized_scrapers():
             scraper.close()
+            await scraper.async_close()
 
         TempDirGenerator.cleanup()
 
+        if logger.handlers:  # If logger was initialized
+            runtime = timer() - start_time
+            logger.debug(f"Total runtime: {runtime:.2f} seconds")
 
-def download(urls: list[str], config: Config) -> None:
+
+async def download(urls: list[str], config: Config) -> None:
     """
     Download subtitles from a given URL.
 
@@ -227,7 +236,7 @@
                     object_type_str = media_item.__class__.__name__.lower()
 
                     logger.info(f"Found {object_type_str}: {format_media_description(media_data=media_item)}")
-                    download_media(scraper=playlist_scraper, media_item=media_item, config=config)
+                    await download_media(scraper=playlist_scraper, media_item=media_item, config=config)
 
                 except Exception as e:
                     if len(media_data) > 1:
@@ -245,7 +254,7 @@
             continue
 
 
-def download_media(scraper: Scraper, media_item: MediaData, config: Config) -> None:
+async def download_media(scraper: Scraper, media_item: MediaData, config: Config) -> None:
     """
     Download a media item.
 
@@ -254,20 +263,21 @@
         media_item (MediaData): A media data item to download subtitles for.
         config (Config): A config to use for downloading subtitles.
     """
+    # TODO: This logic awaits each episode. Perhaps Download multiple episodes async at the same time?
     if isinstance(media_item, Series):
         for season in media_item.seasons:
-            download_media(scraper=scraper, media_item=season, config=config)
+            await download_media(scraper=scraper, media_item=season, config=config)
 
     elif isinstance(media_item, Season):
         for episode in media_item.episodes:
             logger.info(f"{format_media_description(media_data=episode, shortened=True)}:")
-            download_media_item(scraper=scraper, media_item=episode, config=config)
+            await download_media_item(scraper=scraper, media_item=episode, config=config)
 
     elif isinstance(media_item, (Movie, Episode)):
-        download_media_item(scraper=scraper, media_item=media_item, config=config)
+        await download_media_item(scraper=scraper, media_item=media_item, config=config)
 
 
-def download_media_item(scraper: Scraper, media_item: Movie | Episode, config: Config) -> None:
+async def download_media_item(scraper: Scraper, media_item: Movie | Episode, config: Config) -> None:
     if media_item.playlist:
         download_subtitles_kwargs = {
             "download_path": Path(config.downloads["folder"]),
@@ -278,9 +288,9 @@
         }
 
         try:
-            results = download_subtitles(scraper=scraper,
-                                         media_data=media_item,
-                                         **download_subtitles_kwargs)
+            results = await download_subtitles(scraper=scraper,
+                                               media_data=media_item,
+                                               **download_subtitles_kwargs)
 
             success_count = len(results.successful_subtitles)
             failed_count = len(results.failed_subtitles)
@@ -341,9 +351,9 @@
         return
 
 
-def download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,
-                       language_filter: list[str] | None = None, convert_to_srt: bool = False,
-                       overwrite_existing: bool = True, zip_files: bool = False) -> SubtitlesDownloadResults:
+async def download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,
+                             language_filter: list[str] | None = None, convert_to_srt: bool = False,
+                             overwrite_existing: bool = True, zip_files: bool = False) -> SubtitlesDownloadResults:
     """
     Download subtitles for the given media data.
 
@@ -368,9 +378,12 @@
     failed_downloads: list[SubtitlesDownloadError] = []
     temp_downloads: list[Path] = []
 
-    for subtitles_data in scraper.get_subtitles(main_playlist=media_data.playlist,  # type: ignore[arg-type]
-                                                language_filter=language_filter,
-                                                subrip_conversion=convert_to_srt):
+    subtitles_download_coroutines = await scraper.download_matching_subtitles(main_playlist=media_data.playlist,  # type: ignore[arg-type]
+                                                                              language_filter=language_filter,
+                                                                              subrip_conversion=convert_to_srt)
+
+    for subtitles_data_coroutine in asyncio.as_completed(subtitles_download_coroutines):
+        subtitles_data = await subtitles_data_coroutine
         language_info = format_subtitles_description(language_code=subtitles_data.language_code,
                                                      language_name=subtitles_data.language_name,
                                                      special_type=subtitles_data.special_type)
@@ -607,4 +620,8 @@
 
 
 if __name__ == "__main__":
-    main()
+    from timeit import default_timer as timer
+    start_time = timer()
+    asyncio.run(main(), debug=True)
+    runtime = timer() - start_time
+    print(f"Total runtime: {runtime:.2f} seconds")
Index: isubrip/subtitle_formats/webvtt.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABCMeta\r\nimport re\r\nfrom typing import TYPE_CHECKING, Any, ClassVar\r\n\r\nfrom isubrip.data_structures import SubtitlesFormatType\r\nfrom isubrip.subtitle_formats.subrip import SubRipCaptionBlock\r\nfrom isubrip.subtitle_formats.subtitles import RTL_CHAR, Subtitles, SubtitlesBlock, SubtitlesCaptionBlock\r\nfrom isubrip.utils import split_subtitles_timestamp\r\n\r\nif TYPE_CHECKING:\r\n    from datetime import time\r\n\r\n# WebVTT Documentation:\r\n# https://www.w3.org/TR/webvtt1/#cues\r\n# https://developer.mozilla.org/en-US/docs/Web/API/WebVTT_API#webvtt_cues\r\n\r\n\r\nclass WebVTTBlock(SubtitlesBlock, metaclass=ABCMeta):\r\n    \"\"\"\r\n    Abstract base class for WEBVTT cue blocks.\r\n    \"\"\"\r\n    is_caption_block: bool = False\r\n\r\n\r\nclass Caption(SubtitlesCaptionBlock, WebVTTBlock):\r\n    \"\"\"An object representing a WebVTT caption block.\"\"\"\r\n    subrip_alignment_conversion: ClassVar[bool] = False\r\n\r\n    is_caption_block: bool = True\r\n\r\n    def __init__(self, start_time: time, end_time: time, payload: str, settings: str = \"\", identifier: str = \"\"):\r\n        \"\"\"\r\n        Initialize a new object representing a WebVTT caption block.\r\n\r\n        Args:\r\n            start_time (time): Cue start time.\r\n            end_time (time): Cue end time.\r\n            settings (str): Cue settings.\r\n            payload (str): Cue payload.\r\n        \"\"\"\r\n        super().__init__(start_time=start_time, end_time=end_time, payload=payload)\r\n        self.identifier = identifier\r\n        self.settings = settings\r\n\r\n    def to_srt(self) -> SubRipCaptionBlock:\r\n        # Add a {\\an8} tag at the start of the payload if it has 'line:0.00%' in the settings\r\n        if \"line:0.00%\" in self.settings and self.subrip_alignment_conversion:\r\n            # If the payload starts with an RTL control char, add the tag after it\r\n            if self.payload.startswith(RTL_CHAR):\r\n                payload = RTL_CHAR + WEBVTT_ALIGN_TOP_TAG + self.payload[len(RTL_CHAR):]\r\n\r\n            else:\r\n                payload = WEBVTT_ALIGN_TOP_TAG + self.payload\r\n\r\n        else:\r\n            payload = self.payload\r\n\r\n        return SubRipCaptionBlock(start_time=self.start_time, end_time=self.end_time, payload=payload)\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        return isinstance(other, type(self)) and \\\r\n            self.start_time == other.start_time and self.end_time == other.end_time and self.payload == other.payload\r\n\r\n    def __str__(self) -> str:\r\n        result_str = \"\"\r\n        time_format = \"%H:%M:%S.%f\"\r\n\r\n        # Add identifier (if it exists)\r\n        if self.identifier:\r\n            result_str += f\"{self.identifier}\\n\"\r\n\r\n        result_str += f\"{self.start_time.strftime(time_format)[:-3]} --> {self.end_time.strftime(time_format)[:-3]}\"\r\n\r\n        if self.settings:\r\n            result_str += f\" {self.settings}\"\r\n\r\n        result_str += f\"\\n{self.payload}\"\r\n\r\n        return result_str\r\n\r\n\r\nclass Comment(WebVTTBlock):\r\n    \"\"\"An object representing a WebVTT comment block.\"\"\"\r\n    header = \"NOTE\"\r\n\r\n    def __init__(self, payload: str, inline: bool = False) -> None:\r\n        \"\"\"\r\n        Initialize a new object representing a WebVTT comment block.\r\n\r\n        Args:\r\n            payload (str): Comment payload.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.payload = payload\r\n        self.inline = inline\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        return isinstance(other, type(self)) and self.inline == other.inline and self.payload == other.payload\r\n\r\n    def __str__(self) -> str:\r\n        if self.inline:\r\n            return f\"{self.header} {self.payload}\"\r\n\r\n        if self.payload:\r\n            return f\"{self.header}\\n{self.payload}\"\r\n\r\n        return self.header\r\n\r\n\r\nclass Style(WebVTTBlock):\r\n    \"\"\"An object representing a WebVTT style block.\"\"\"\r\n    header = \"STYLE\"\r\n\r\n    def __init__(self, payload: str) -> None:\r\n        \"\"\"\r\n        Initialize a new object representing a WebVTT style block.\r\n\r\n        Args:\r\n            payload (str): Style payload.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.payload = payload\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        return isinstance(other, type(self)) and self.payload == other.payload\r\n\r\n    def __str__(self) -> str:\r\n        return f\"{self.header}\\n{self.payload}\"\r\n\r\n\r\nclass Region(WebVTTBlock):\r\n    \"\"\"An object representing a WebVTT region block.\"\"\"\r\n    header = \"REGION\"\r\n\r\n    def __init__(self, payload: str) -> None:\r\n        \"\"\"\r\n        Initialize a new object representing a WebVTT region block.\r\n\r\n        Args:\r\n            payload (str): Region payload.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.payload = payload\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        return isinstance(other, type(self)) and self.payload == other.payload\r\n\r\n    def __str__(self) -> str:\r\n        return f\"{self.header} {self.payload}\"\r\n\r\n\r\nclass WebVTTSubtitles(Subtitles[WebVTTBlock]):\r\n    \"\"\"An object representing a WebVTT subtitles file.\"\"\"\r\n    format = SubtitlesFormatType.WEBVTT\r\n\r\n    def _dumps(self) -> str:\r\n        \"\"\"\r\n        Dump subtitles to a string representing the subtitles in a WebVTT format.\r\n\r\n        Returns:\r\n            str: The subtitles in a string using a WebVTT format.\r\n        \"\"\"\r\n        subtitles_str = \"WEBVTT\\n\\n\"\r\n\r\n        for block in self.blocks:\r\n            subtitles_str += str(block) + \"\\n\\n\"\r\n\r\n        return subtitles_str.rstrip('\\n')\r\n\r\n    def _loads(self, data: str) -> None:\r\n        \"\"\"\r\n        Load and parse WebVTT subtitles data from a string.\r\n\r\n        Args:\r\n            data (bytes): Subtitles data to load.\r\n        \"\"\"\r\n        prev_line: str = \"\"\r\n        lines_iterator = iter(data.splitlines())\r\n\r\n        for line in lines_iterator:\r\n            # If the line is a timestamp\r\n            if caption_block_regex := re.match(WEBVTT_CAPTION_BLOCK_REGEX, line):\r\n                # If previous line wasn't empty, add it as an identifier\r\n                if prev_line:\r\n                    caption_identifier = prev_line\r\n\r\n                else:\r\n                    caption_identifier = \"\"\r\n\r\n                caption_timestamps = split_subtitles_timestamp(caption_block_regex.group(1))\r\n                caption_settings = caption_block_regex.group(2)\r\n                caption_payload = \"\"\r\n\r\n                for additional_line in lines_iterator:\r\n                    if not additional_line:\r\n                        line = additional_line\r\n                        break\r\n\r\n                    caption_payload += additional_line + \"\\n\"\r\n\r\n                caption_payload = caption_payload.rstrip(\"\\n\")\r\n                self.blocks.append(Caption(\r\n                    identifier=caption_identifier,\r\n                    start_time=caption_timestamps[0],\r\n                    end_time=caption_timestamps[1],\r\n                    settings=caption_settings,\r\n                    payload=caption_payload))\r\n\r\n            elif comment_block_regex := re.match(WEBVTT_COMMENT_HEADER_REGEX, line):\r\n                comment_payload = \"\"\r\n                inline = False\r\n\r\n                if comment_block_regex.group(1) is not None:\r\n                    comment_payload += comment_block_regex.group(1) + \"\\n\"\r\n                    inline = True\r\n\r\n                for additional_line in lines_iterator:\r\n                    if not additional_line:\r\n                        line = additional_line\r\n                        break\r\n\r\n                    comment_payload += additional_line + \"\\n\"\r\n\r\n                self.blocks.append(Comment(comment_payload.rstrip(\"\\n\"), inline=inline))\r\n\r\n            elif line.rstrip(' \\t') == Region.header:\r\n                region_payload = \"\"\r\n\r\n                for additional_line in lines_iterator:\r\n                    if not additional_line:\r\n                        line = additional_line\r\n                        break\r\n\r\n                    region_payload += additional_line + \"\\n\"\r\n\r\n                self.blocks.append(Region(region_payload.rstrip(\"\\n\")))\r\n\r\n            elif line.rstrip(' \\t') == Style.header:\r\n                style_payload = \"\"\r\n\r\n                for additional_line in lines_iterator:\r\n                    if not additional_line:\r\n                        line = additional_line\r\n                        break\r\n\r\n                    style_payload += additional_line + \"\\n\"\r\n\r\n                self.blocks.append(Style(style_payload.rstrip(\"\\n\")))\r\n\r\n            prev_line = line\r\n\r\n    def remove_head_blocks(self) -> None:\r\n        \"\"\"\r\n        Remove all head blocks (Style / Region) from the subtitles.\r\n\r\n        NOTE:\r\n            Comment blocks are removed as well if they are before the first caption block (since they're probably\r\n            related to the head blocks).\r\n        \"\"\"\r\n        for block in self.blocks:\r\n            if isinstance(block, Caption):\r\n                break\r\n\r\n            if isinstance(block, (Comment, Style, Region)):\r\n                self.blocks.remove(block)\r\n\r\n\r\n# --- Constants ---\r\nWEBVTT_PERCENTAGE_REGEX = r\"\\d{1,3}(?:\\.\\d+)?%\"\r\nWEBVTT_CAPTION_TIMINGS_REGEX = \\\r\n    r\"(?:[0-5]\\d:)?[0-5]\\d:[0-5]\\d[\\.,]\\d{3}[ \\t]+-->[ \\t]+(?:[0-5]\\d:)?[0-5]\\d:[0-5]\\d[\\.,]\\d{3}\"\r\n\r\nWEBVTT_CAPTION_SETTING_ALIGNMENT_REGEX = r\"align:(?:start|center|middle|end|left|right)\"\r\nWEBVTT_CAPTION_SETTING_LINE_REGEX = rf\"line:(?:{WEBVTT_PERCENTAGE_REGEX}|-?\\d+%)(?:,(?:start|center|middle|end))?\"\r\nWEBVTT_CAPTION_SETTING_POSITION_REGEX = rf\"position:{WEBVTT_PERCENTAGE_REGEX}(?:,(?:start|center|middle|end))?\"\r\nWEBVTT_CAPTION_SETTING_REGION_REGEX = r\"region:(?:(?!(?:-->)|\\t)\\S)+\"\r\nWEBVTT_CAPTION_SETTING_SIZE_REGEX = rf\"size:{WEBVTT_PERCENTAGE_REGEX}\"\r\nWEBVTT_CAPTION_SETTING_VERTICAL_REGEX = r\"vertical:(?:lr|rl)\"\r\n\r\nWEBVTT_CAPTION_SETTINGS_REGEX = (\"(?:\"\r\n                                 f\"(?:{WEBVTT_CAPTION_SETTING_ALIGNMENT_REGEX})|\"\r\n                                 f\"(?:{WEBVTT_CAPTION_SETTING_LINE_REGEX})|\"\r\n                                 f\"(?:{WEBVTT_CAPTION_SETTING_POSITION_REGEX})|\"\r\n                                 f\"(?:{WEBVTT_CAPTION_SETTING_REGION_REGEX})|\"\r\n                                 f\"(?:{WEBVTT_CAPTION_SETTING_SIZE_REGEX})|\"\r\n                                 f\"(?:{WEBVTT_CAPTION_SETTING_VERTICAL_REGEX})|\"\r\n                                 f\"(?:[ \\t]+)\"\r\n                                 \")*\")\r\n\r\nWEBVTT_CAPTION_BLOCK_REGEX = re.compile(rf\"^({WEBVTT_CAPTION_TIMINGS_REGEX})[ \\t]*({WEBVTT_CAPTION_SETTINGS_REGEX})?\")\r\nWEBVTT_COMMENT_HEADER_REGEX = re.compile(rf\"^{Comment.header}(?:$|[ \\t])(.+)?\")\r\n\r\nWEBVTT_ALIGN_TOP_TAG = \"{\\\\an8}\"\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/subtitle_formats/webvtt.py b/isubrip/subtitle_formats/webvtt.py
--- a/isubrip/subtitle_formats/webvtt.py	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/isubrip/subtitle_formats/webvtt.py	(date 1715678910551)
@@ -251,6 +251,15 @@
 
             prev_line = line
 
+    def append_subtitles(self: WebVTTSubtitles,
+                         subtitles: WebVTTSubtitles) -> WebVTTSubtitles:
+        if subtitles.blocks:
+            # subtitle_copy = subtitles.copy()  # TODO - Use copy instead of using references to original blocks
+            subtitles.remove_head_blocks()
+            self.add_blocks(subtitles.blocks)
+
+        return self
+
     def remove_head_blocks(self) -> None:
         """
         Remove all head blocks (Style / Region) from the subtitles.
Index: isubrip/subtitle_formats/subtitles.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC, abstractmethod\r\nfrom datetime import time\r\nfrom typing import TYPE_CHECKING, Any, ClassVar, Generic, TypeVar\r\n\r\nfrom isubrip.logger import logger\r\n\r\nif TYPE_CHECKING:\r\n    from isubrip.data_structures import SubtitlesFormatType\r\n    from isubrip.subtitle_formats.subrip import SubRipCaptionBlock, SubRipSubtitles\r\n\r\nRTL_CONTROL_CHARS = ('\\u200e', '\\u200f', '\\u202a', '\\u202b', '\\u202c', '\\u202d', '\\u202e')\r\nRTL_CHAR = '\\u202b'\r\nRTL_LANGUAGES = [\"ar\", \"he\", \"he-il\"]\r\n\r\nSubtitlesT = TypeVar('SubtitlesT', bound='Subtitles')\r\nSubtitlesBlockT = TypeVar('SubtitlesBlockT', bound='SubtitlesBlock')\r\n\r\n\r\nclass SubtitlesBlock(ABC):\r\n    \"\"\"\r\n    Abstract base class for subtitles blocks.\r\n\r\n    Attributes:\r\n        modified (bool): Whether the block has been modified.\r\n    \"\"\"\r\n\r\n    def __init__(self) -> None:\r\n        self.modified: bool = False\r\n\r\n    @abstractmethod\r\n    def __str__(self) -> str:\r\n        pass\r\n\r\n    @abstractmethod\r\n    def __eq__(self, other: Any) -> bool:\r\n        pass\r\n\r\n\r\nclass SubtitlesCaptionBlock(SubtitlesBlock, ABC):\r\n    \"\"\"\r\n    A base class for subtitles caption blocks.\r\n\r\n    Attributes:\r\n        start_time (time): Start timestamp of the caption block.\r\n        end_time (time): End timestamp of the caption block.\r\n        payload (str): Caption block's payload.\r\n    \"\"\"\r\n\r\n    def __init__(self, start_time: time, end_time: time, payload: str):\r\n        \"\"\"\r\n        Initialize a new SubtitlesCaptionBlock object.\r\n\r\n        Args:\r\n            start_time: Start timestamp of the caption block.\r\n            end_time: End timestamp of the caption block.\r\n            payload: Caption block's payload.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.start_time = start_time\r\n        self.end_time = end_time\r\n        self.payload = payload\r\n\r\n    def fix_rtl(self) -> None:\r\n        \"\"\"Fix payload's text direction to RTL.\"\"\"\r\n        previous_payload = self.payload\r\n\r\n        # Remove previous RTL-related formatting\r\n        for char in RTL_CONTROL_CHARS:\r\n            self.payload = self.payload.replace(char, '')\r\n\r\n        # Add RLM char at the start of every line\r\n        self.payload = RTL_CHAR + self.payload.replace(\"\\n\", f\"\\n{RTL_CHAR}\")\r\n\r\n        if self.payload != previous_payload:\r\n            self.modified = True\r\n\r\n    @abstractmethod\r\n    def to_srt(self) -> SubRipCaptionBlock:\r\n        \"\"\"\r\n        Convert WebVTT caption block to SRT caption block.\r\n\r\n        Returns:\r\n            SubRipCaptionBlock: The caption block in SRT format.\r\n        \"\"\"\r\n        ...\r\n\r\n\r\nclass Subtitles(Generic[SubtitlesBlockT], ABC):\r\n    \"\"\"\r\n    An object representing subtitles, made out of blocks.\r\n\r\n    Attributes:\r\n        _modified (bool): Whether the subtitles have been modified.\r\n        format (SubtitlesFormatType): [Class Attribute] Format of the subtitles (contains name and file extension).\r\n        language_code (str): Language code of the subtitles.\r\n        blocks (list[SubtitlesBlock]): A list of subtitles blocks that make up the subtitles.\r\n        encoding (str): Encoding of the subtitles.\r\n        raw_data (bytes | None): Raw data of the subtitles.\r\n    \"\"\"\r\n    format: ClassVar[SubtitlesFormatType]\r\n\r\n    def __init__(self, data: bytes | None, language_code: str, encoding: str = \"utf-8\"):\r\n        \"\"\"\r\n        Initialize a new Subtitles object.\r\n\r\n        Args:\r\n            data (bytes | None): Raw data of the subtitles.\r\n            language_code (str): Language code of the subtitles.\r\n            encoding (str, optional): Encoding of the subtitles. Defaults to \"utf-8\".\r\n        \"\"\"\r\n        self._modified = False\r\n        self.raw_data = None\r\n\r\n        self.blocks: list[SubtitlesBlockT] = []\r\n\r\n        self.language_code = language_code\r\n        self.encoding = encoding\r\n\r\n        if data:\r\n            self.raw_data = data\r\n            self._load(data=data)\r\n\r\n    def __add__(self: SubtitlesT, obj: SubtitlesBlockT | SubtitlesT) -> SubtitlesT:\r\n        \"\"\"\r\n        Add a new subtitles block, or append blocks from another subtitles object.\r\n\r\n        Args:\r\n            obj (SubtitlesBlock | Subtitles): A subtitles block or another subtitles object.\r\n\r\n        Returns:\r\n            Subtitles: The current subtitles object.\r\n        \"\"\"\r\n        if isinstance(obj, SubtitlesBlock):\r\n            self.add_blocks(obj)\r\n\r\n        elif isinstance(obj, self.__class__):\r\n            self.append_subtitles(obj)\r\n\r\n        else:\r\n            logger.warning(f\"Cannot add object of type '{type(obj)}' to '{type(self)}' object. Skipping...\")\r\n\r\n        return self\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        return isinstance(other, type(self)) and self.blocks == other.blocks\r\n\r\n    def __str__(self) -> str:\r\n        return self.dumps()\r\n\r\n    def _dump(self) -> bytes:\r\n        \"\"\"\r\n        Dump subtitles object to bytes representing the subtitles.\r\n\r\n        Returns:\r\n            bytes: The subtitles in a bytes object.\r\n        \"\"\"\r\n        return self._dumps().encode(encoding=self.encoding)\r\n\r\n    @abstractmethod\r\n    def _dumps(self) -> str:\r\n        \"\"\"\r\n        Dump subtitles object to a string representing the subtitles.\r\n\r\n        Returns:\r\n            str: The subtitles in a string format.\r\n        \"\"\"\r\n        ...\r\n\r\n    def _load(self, data: bytes) -> None:\r\n        \"\"\"\r\n        Load and parse subtitles data from bytes.\r\n\r\n        Args:\r\n            data (bytes): Subtitles data to load.\r\n        \"\"\"\r\n        parsed_data = data.decode(encoding=self.encoding)\r\n        self._loads(data=parsed_data)\r\n\r\n    @abstractmethod\r\n    def _loads(self, data: str) -> None:\r\n        \"\"\"\r\n        Load and parse subtitles data from a string.\r\n\r\n        Args:\r\n            data (bytes): Subtitles data to load.\r\n        \"\"\"\r\n        ...\r\n\r\n    def dump(self) -> bytes:\r\n        \"\"\"\r\n        Dump subtitles to a bytes object representing the subtitles.\r\n        Returns the original raw subtitles data if they have not been modified, and raw data is available.\r\n\r\n        Returns:\r\n            bytes: The subtitles in a bytes object.\r\n        \"\"\"\r\n        if self.raw_data is not None and not self.modified():\r\n            logger.debug(\"Returning original raw data as subtitles have not been modified.\")\r\n            return self.raw_data\r\n\r\n        return self._dump()\r\n\r\n    def dumps(self) -> str:\r\n        \"\"\"\r\n        Dump subtitles to a string representing the subtitles.\r\n        Returns the original raw subtitles data if they have not been modified, and raw data is available.\r\n\r\n        Returns:\r\n\r\n        \"\"\"\r\n        if self.raw_data is not None and not self.modified():\r\n            logger.debug(\"Returning original raw data (decoded) as subtitles have not been modified.\")\r\n            return self.raw_data.decode(encoding=self.encoding)\r\n\r\n        return self._dumps()\r\n\r\n    def add_blocks(self: SubtitlesT,\r\n                   blocks: SubtitlesBlockT | list[SubtitlesBlockT],\r\n                   set_modified: bool = True) -> SubtitlesT:\r\n        \"\"\"\r\n        Add a new subtitles block to current subtitles.\r\n\r\n        Args:\r\n            blocks (SubtitlesBlock | list[SubtitlesBlock]):\r\n                A block object or a list of block objects to append.\r\n            set_modified (bool, optional): Whether to set the subtitles as modified. Defaults to True.\r\n\r\n        Returns:\r\n            Subtitles: The current subtitles object.\r\n        \"\"\"\r\n        if isinstance(blocks, list):\r\n            if not blocks:\r\n                return self\r\n\r\n            self.blocks.extend(blocks)\r\n\r\n        else:\r\n            self.blocks.append(blocks)\r\n\r\n        if set_modified:\r\n            self._modified = True\r\n\r\n        return self\r\n\r\n    def append_subtitles(self: SubtitlesT,\r\n                         subtitles: SubtitlesT) -> SubtitlesT:\r\n        \"\"\"\r\n        Append subtitles to an existing subtitles object.\r\n\r\n        Args:\r\n            subtitles (Subtitles): Subtitles object to append to current subtitles.\r\n\r\n        Returns:\r\n            Subtitles: The current subtitles object.\r\n        \"\"\"\r\n        if not subtitles.blocks:\r\n            return self\r\n\r\n        for block in subtitles.blocks:\r\n            self.add_blocks(block)\r\n\r\n        return self\r\n\r\n    def polish(self: SubtitlesT,\r\n               fix_rtl: bool = False,\r\n               remove_duplicates: bool = True,\r\n               ) -> SubtitlesT:\r\n        \"\"\"\r\n        Apply various fixes to subtitles.\r\n\r\n        Args:\r\n            fix_rtl (bool, optional): Whether to fix text direction of RTL languages. Defaults to False.\r\n            remove_duplicates (bool, optional): Whether to remove duplicate captions. Defaults to True.\r\n\r\n        Returns:\r\n            Subtitles: The current subtitles object.\r\n        \"\"\"\r\n        fix_rtl = (fix_rtl and self.language_code in RTL_LANGUAGES)\r\n\r\n        if not any((\r\n                fix_rtl,\r\n                remove_duplicates,\r\n        )):\r\n            return self\r\n\r\n        previous_block: SubtitlesBlockT | None = None\r\n\r\n        for block in self.blocks:\r\n            if fix_rtl:\r\n                block.fix_rtl()\r\n\r\n            if remove_duplicates and previous_block is not None and block == previous_block:\r\n                self.blocks.remove(previous_block)\r\n                self._modified = True\r\n\r\n            previous_block = block\r\n\r\n        return self\r\n\r\n    def modified(self) -> bool:\r\n        \"\"\"\r\n        Check if the subtitles have been modified (by checking if any of its blocks have been modified).\r\n\r\n        Returns:\r\n            bool: True if the subtitles have been modified, False otherwise.\r\n        \"\"\"\r\n        return self._modified or any(block.modified for block in self.blocks)\r\n\r\n    def to_srt(self) -> SubRipSubtitles:\r\n        \"\"\"\r\n        Convert subtitles to SRT format.\r\n\r\n        Returns:\r\n            SubRipSubtitles: The subtitles in SRT format.\r\n        \"\"\"\r\n        from isubrip.subtitle_formats.subrip import SubRipSubtitles\r\n\r\n        subrip_subtitles = SubRipSubtitles(\r\n            data=None,\r\n            language_code=self.language_code,\r\n            encoding=self.encoding,\r\n        )\r\n        subrip_blocks = [block.to_srt() for block in self.blocks if isinstance(block, SubtitlesCaptionBlock)]\r\n        subrip_subtitles.add_blocks(subrip_blocks)\r\n\r\n        return subrip_subtitles\r\n\r\n\r\ndef split_timestamp(timestamp: str) -> tuple[time, time]:\r\n    \"\"\"\r\n    Split a subtitles timestamp into start and end.\r\n\r\n    Args:\r\n        timestamp (str): A subtitles timestamp. For example: \"00:00:00.000 --> 00:00:00.000\"\r\n\r\n    Returns:\r\n        tuple(time, time): A tuple containing start and end times as a datetime object.\r\n    \"\"\"\r\n    # Support ',' character in timestamp's milliseconds (used in SubRip format).\r\n    timestamp = timestamp.replace(',', '.')\r\n\r\n    start_time, end_time = timestamp.split(\" --> \")\r\n    return time.fromisoformat(start_time), time.fromisoformat(end_time)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/subtitle_formats/subtitles.py b/isubrip/subtitle_formats/subtitles.py
--- a/isubrip/subtitle_formats/subtitles.py	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/isubrip/subtitle_formats/subtitles.py	(date 1715678910543)
@@ -255,11 +255,8 @@
         Returns:
             Subtitles: The current subtitles object.
         """
-        if not subtitles.blocks:
-            return self
-
-        for block in subtitles.blocks:
-            self.add_blocks(block)
+        if blocks := subtitles.blocks:
+            self.add_blocks(blocks)
 
         return self
 
Index: pyproject.toml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>[tool.poetry]\r\nname = \"isubrip\"\r\nversion = \"2.5.4\"\r\ndescription = \"A Python package for scraping and downloading subtitles from AppleTV / iTunes movie pages.\"\r\nlicense = \"MIT\"\r\nauthors = [\"Michael Yochpaz\"]\r\nreadme = \"README.md\"\r\nhomepage = \"https://github.com/MichaelYochpaz/iSubRip\"\r\nrepository = \"https://github.com/MichaelYochpaz/iSubRip\"\r\nkeywords = [\r\n    \"iTunes\",\r\n    \"AppleTV\",\r\n    \"movies\",\r\n    \"subtitles\",\r\n    \"scrape\",\r\n    \"scraper\",\r\n    \"download\",\r\n    \"m3u8\"\r\n]\r\nclassifiers = [\r\n    \"Development Status :: 5 - Production/Stable\",\r\n    \"Intended Audience :: End Users/Desktop\",\r\n    \"Intended Audience :: Developers\",\r\n    \"Operating System :: Microsoft :: Windows\",\r\n    \"Operating System :: MacOS\",\r\n    \"Operating System :: POSIX :: Linux\",\r\n    \"Topic :: Utilities\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Programming Language :: Python :: 3.8\",\r\n    \"Programming Language :: Python :: 3.9\",\r\n    \"Programming Language :: Python :: 3.10\",\r\n    \"Programming Language :: Python :: 3.11\",\r\n    \"Programming Language :: Python :: 3.12\",\r\n]\r\npackages = [\r\n    { include = \"isubrip\" },\r\n]\r\ninclude = [\r\n    \"isubrip/resources\", \"README.md\", \"LICENSE\"\r\n]\r\n\r\n[tool.mypy]\r\ndisallow_untyped_defs = true\r\nexplicit_package_bases = true\r\nignore_missing_imports = true\r\npython_version = \"3.8\"\r\nwarn_return_any = true\r\n\r\n[tool.poetry.scripts]\r\nisubrip = \"isubrip.__main__:main\"\r\n\r\n[tool.poetry.urls]\r\n\"Bug Reports\" = \"https://github.com/MichaelYochpaz/iSubRip/issues\"\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\nrequests = \"^2.31.0\"\r\nhttpx = {extras = [\"http2\"], version = \"^0.27.0\"}\r\nm3u8 = \"^4.1.0\"\r\nmergedeep = \"^1.3.4\"\r\npydantic = \"^2.7.0\"\r\ntomli = \"^2.0.1\"\r\n\r\n\r\n[tool.poetry.group.dev.dependencies]\r\nmypy = \"^1.10.0\"\r\nruff = \"^0.4.2\"\r\ntypes-requests = \"^2.31.0.20240406\"\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry_bumpversion.file.\"isubrip/constants.py\"]\r\nsearch = 'PACKAGE_VERSION = \"{current_version}\"'\r\nreplace = 'PACKAGE_VERSION = \"{new_version}\"'\r\n\r\n[tool.poetry_bumpversion.file.\"README.md\"]\r\nsearch = 'Latest version: {current_version}'\r\nreplace = 'Latest version: {new_version}'\r\n\r\n\r\n[tool.ruff]\r\nline-length = 120\r\ntarget-version = \"py38\"\r\n\r\n[tool.ruff.lint]\r\nselect = [\r\n    \"ARG\",\r\n    \"ASYNC\",\r\n    \"B\",\r\n    \"C4\",\r\n    \"COM\",\r\n    \"E\",\r\n    \"F\",\r\n    \"FA\",\r\n    \"I\",\r\n    \"INP\",\r\n    \"ISC\",\r\n    \"N\",\r\n    \"PIE\",\r\n    \"PGH\",\r\n    \"PT\",\r\n    \"PTH\",\r\n    \"Q\",\r\n    \"RSE\",\r\n    \"RET\",\r\n    \"RUF\",\r\n    \"S\",\r\n    \"SIM\",\r\n    \"SLF\",\r\n    \"T20\",\r\n    \"TCH\",\r\n    \"TID\",\r\n    \"TRY\",\r\n    \"UP\",\r\n]\r\nignore = [\r\n    \"C416\",\r\n    \"Q000\",\r\n    \"RUF010\",\r\n    \"RUF012\",\r\n    \"SIM108\",\r\n    \"TD002\",\r\n    \"TD003\",\r\n    \"TRY003\",\r\n]\r\nunfixable = [\"ARG\"]\r\n\r\n[tool.ruff.lint.flake8-tidy-imports]\r\nban-relative-imports = \"all\"\r\n\r\n[tool.ruff.lint.flake8-quotes]\r\ndocstring-quotes = \"double\"\r\n\r\n[tool.ruff.lint.isort]\r\nforce-sort-within-sections = true\r\n\r\n[tool.ruff.lint.pyupgrade]\r\nkeep-runtime-typing = true
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pyproject.toml b/pyproject.toml
--- a/pyproject.toml	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/pyproject.toml	(date 1715678910594)
@@ -59,6 +59,7 @@
 m3u8 = "^4.1.0"
 mergedeep = "^1.3.4"
 pydantic = "^2.7.0"
+rich = "^13.7.1"
 tomli = "^2.0.1"
 
 
Index: isubrip/scrapers/itunes_scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport re\r\nfrom typing import TYPE_CHECKING, Iterator\r\n\r\nimport m3u8\r\nfrom requests.exceptions import HTTPError\r\n\r\nfrom isubrip.data_structures import SubtitlesData, SubtitlesFormatType\r\nfrom isubrip.logger import logger\r\nfrom isubrip.scrapers.scraper import HLSScraper, PlaylistLoadError, ScraperError, ScraperFactory, SubtitlesDownloadError\r\nfrom isubrip.subtitle_formats.webvtt import WebVTTSubtitles\r\nfrom isubrip.utils import merge_dict_values, raise_for_status\r\n\r\nif TYPE_CHECKING:\r\n    from isubrip.data_structures import Movie, ScrapedMediaResponse\r\n\r\n\r\nclass ItunesScraper(HLSScraper):\r\n    \"\"\"An iTunes movie data scraper.\"\"\"\r\n    id = \"itunes\"\r\n    name = \"iTunes\"\r\n    abbreviation = \"iT\"\r\n    url_regex = re.compile(r\"(?i)(?P<base_url>https?://itunes\\.apple\\.com/(?:(?P<country_code>[a-z]{2})/)?(?P<media_type>movie|tv-show|tv-season|show)/(?:(?P<media_name>[\\w\\-%]+)/)?(?P<media_id>id\\d{9,10}))(?:\\?(?P<url_params>.*))?\")\r\n    subtitles_class = WebVTTSubtitles\r\n    is_movie_scraper = True\r\n    uses_scrapers = [\"appletv\"]\r\n\r\n    _subtitles_filters = {\r\n        HLSScraper.M3U8Attribute.GROUP_ID.value: [\"subtitles_ak\", \"subtitles_vod-ak-amt.tv.apple.com\"],\r\n        **HLSScraper._subtitles_filters,  # noqa: SLF001\r\n    }\r\n\r\n    def __init__(self,  user_agent: str | None = None, config_data: dict | None = None):\r\n        super().__init__(user_agent=user_agent, config_data=config_data)\r\n        self._appletv_scraper = ScraperFactory.get_scraper_instance(\r\n            scraper_id=\"appletv\",\r\n            kwargs={\"config_data\": config_data},\r\n            extract_scraper_config=True,\r\n            raise_error=True,\r\n        )\r\n\r\n    def get_data(self, url: str) -> ScrapedMediaResponse[Movie]:\r\n        \"\"\"\r\n        Scrape iTunes to find info about a movie, and it's M3U8 main_playlist.\r\n\r\n        Args:\r\n            url (str): An iTunes store movie URL.\r\n\r\n        Raises:\r\n            InvalidURL: `itunes_url` is not a valid iTunes store movie URL.\r\n            PageLoadError: HTML page did not load properly.\r\n            HTTPError: HTTP request failed.\r\n\r\n        Returns:\r\n            Movie: A Movie (NamedTuple) object with movie's name, and an M3U8 object of the main_playlist\r\n            if the main_playlist is found. None otherwise.\r\n        \"\"\"\r\n        regex_match = self.match_url(url, raise_error=True)\r\n        url = regex_match.group(1)\r\n        logger.debug(f\"Scraping iTunes URL: {url}.\")\r\n        response = self._session.get(url=url, allow_redirects=False)\r\n\r\n        try:\r\n            raise_for_status(response=response)\r\n\r\n        except HTTPError as e:\r\n            if response.status_code == 404:\r\n                raise ScraperError(\r\n                    \"Media not found. This could indicate that the provided URL is invalid.\",\r\n                ) from e\r\n\r\n            raise\r\n\r\n        redirect_location = response.headers.get(\"Location\")\r\n\r\n        if response.status_code != 301 or not redirect_location:\r\n            logger.debug(f\"iTunes URL: {url} did not redirect to an Apple TV URL.\\n\"\r\n                         f\"Response status code: {response.status_code}.\\n\"\r\n                         f\"Response headers:\\n{response.headers}.\\n\"\r\n                         f\"Response data:\\n{response.text}.\")\r\n            raise ScraperError(\"Apple TV redirect URL not found.\")\r\n\r\n        if not self._appletv_scraper.match_url(redirect_location):\r\n            logger.debug(f\"iTunes URL: {url} redirected to an invalid Apple TV URL: '{redirect_location}'.\")\r\n            raise ScraperError(\"Redirect URL is not a valid Apple TV URL.\")\r\n\r\n        return self._appletv_scraper.get_data(redirect_location)\r\n\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData | SubtitlesDownloadError]:\r\n        language_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None\r\n        main_playlist_m3u8 = self.load_m3u8(url=main_playlist)\r\n\r\n        if main_playlist_m3u8 is None:\r\n            raise PlaylistLoadError(\"Could not load M3U8 playlist.\")\r\n\r\n        playlist_filters = (merge_dict_values(self._subtitles_filters, language_filters)\r\n                            if language_filters\r\n                            else self._subtitles_filters)\r\n\r\n        matched_media_items = self.get_media_playlists(main_playlist=main_playlist_m3u8,\r\n                                                       playlist_filters=playlist_filters)\r\n\r\n        for matched_media in matched_media_items:\r\n            language_name = matched_media.name.replace(' (forced)', '').strip()\r\n            language_code = matched_media.language\r\n            special_type = self.detect_subtitles_type(subtitles_media=matched_media)\r\n\r\n            try:\r\n                m3u8_data = self._session.get(url=matched_media.absolute_uri)\r\n                matched_media_playlist = m3u8.loads(content=m3u8_data.text, uri=matched_media.absolute_uri)\r\n\r\n                subtitles_segments = self._download_segments(matched_media_playlist.segments)\r\n                subtitles = self.subtitles_class(data=subtitles_segments[0], language_code=language_code)\r\n\r\n                for segment in subtitles_segments[1:]:\r\n                    segment_subtitles_obj = self.subtitles_class(data=segment, language_code=language_code)\r\n                    segment_subtitles_obj.remove_head_blocks()\r\n                    subtitles.append_subtitles(segment_subtitles_obj)\r\n\r\n                subtitles.polish(\r\n                    fix_rtl=self.subtitles_fix_rtl,\r\n                    remove_duplicates=self.subtitles_remove_duplicates,\r\n                )\r\n\r\n                language_name = matched_media.name.replace(' (forced)', '').strip()\r\n\r\n                if subrip_conversion:\r\n                    subtitles_format = SubtitlesFormatType.SUBRIP\r\n                    content = subtitles.to_srt().dump()\r\n\r\n                else:\r\n                    subtitles_format = SubtitlesFormatType.WEBVTT\r\n                    content = subtitles.dump()\r\n\r\n                yield SubtitlesData(\r\n                    language_code=language_code,\r\n                    language_name=language_name,\r\n                    subtitles_format=subtitles_format,\r\n                    content=content,\r\n                    content_encoding=subtitles.encoding,\r\n                    special_type=special_type,\r\n                )\r\n\r\n            except Exception as e:\r\n                yield SubtitlesDownloadError(\r\n                    language_code=language_code,\r\n                    language_name=language_name,\r\n                    special_type=special_type,\r\n                    original_exc=e,\r\n                )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/itunes_scraper.py b/isubrip/scrapers/itunes_scraper.py
--- a/isubrip/scrapers/itunes_scraper.py	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/isubrip/scrapers/itunes_scraper.py	(date 1715678910518)
@@ -1,9 +1,10 @@
 from __future__ import annotations
 
+import asyncio
 import re
 from typing import TYPE_CHECKING, Iterator
 
-import m3u8
+from m3u8.model import Media, M3U8
 from requests.exceptions import HTTPError
 
 from isubrip.data_structures import SubtitlesData, SubtitlesFormatType
@@ -87,66 +88,5 @@
 
         return self._appletv_scraper.get_data(redirect_location)
 
-    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,
-                      subrip_conversion: bool = False) -> Iterator[SubtitlesData | SubtitlesDownloadError]:
-        language_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None
-        main_playlist_m3u8 = self.load_m3u8(url=main_playlist)
-
-        if main_playlist_m3u8 is None:
-            raise PlaylistLoadError("Could not load M3U8 playlist.")
-
-        playlist_filters = (merge_dict_values(self._subtitles_filters, language_filters)
-                            if language_filters
-                            else self._subtitles_filters)
-
-        matched_media_items = self.get_media_playlists(main_playlist=main_playlist_m3u8,
-                                                       playlist_filters=playlist_filters)
-
-        for matched_media in matched_media_items:
-            language_name = matched_media.name.replace(' (forced)', '').strip()
-            language_code = matched_media.language
-            special_type = self.detect_subtitles_type(subtitles_media=matched_media)
-
-            try:
-                m3u8_data = self._session.get(url=matched_media.absolute_uri)
-                matched_media_playlist = m3u8.loads(content=m3u8_data.text, uri=matched_media.absolute_uri)
-
-                subtitles_segments = self._download_segments(matched_media_playlist.segments)
-                subtitles = self.subtitles_class(data=subtitles_segments[0], language_code=language_code)
-
-                for segment in subtitles_segments[1:]:
-                    segment_subtitles_obj = self.subtitles_class(data=segment, language_code=language_code)
-                    segment_subtitles_obj.remove_head_blocks()
-                    subtitles.append_subtitles(segment_subtitles_obj)
-
-                subtitles.polish(
-                    fix_rtl=self.subtitles_fix_rtl,
-                    remove_duplicates=self.subtitles_remove_duplicates,
-                )
-
-                language_name = matched_media.name.replace(' (forced)', '').strip()
-
-                if subrip_conversion:
-                    subtitles_format = SubtitlesFormatType.SUBRIP
-                    content = subtitles.to_srt().dump()
-
-                else:
-                    subtitles_format = SubtitlesFormatType.WEBVTT
-                    content = subtitles.dump()
-
-                yield SubtitlesData(
-                    language_code=language_code,
-                    language_name=language_name,
-                    subtitles_format=subtitles_format,
-                    content=content,
-                    content_encoding=subtitles.encoding,
-                    special_type=special_type,
-                )
-
-            except Exception as e:
-                yield SubtitlesDownloadError(
-                    language_code=language_code,
-                    language_name=language_name,
-                    special_type=special_type,
-                    original_exc=e,
-                )
+    def parse_language_name(self, media_data: Media) -> str:
+        return media_data.name.replace(' (forced)', '').strip()
Index: isubrip/scrapers/appletv_scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport datetime as dt\r\nfrom enum import Enum\r\nimport fnmatch\r\nimport re\r\nfrom typing import Iterator\r\n\r\nfrom requests.exceptions import HTTPError\r\n\r\nfrom isubrip.data_structures import Episode, Movie, ScrapedMediaResponse, Season, Series, SubtitlesData\r\nfrom isubrip.logger import logger\r\nfrom isubrip.scrapers.scraper import HLSScraper, ScraperError\r\nfrom isubrip.subtitle_formats.webvtt import WebVTTSubtitles\r\nfrom isubrip.utils import convert_epoch_to_datetime, parse_url_params, raise_for_status\r\n\r\n\r\nclass AppleTVScraper(HLSScraper):\r\n    \"\"\"An Apple TV scraper.\"\"\"\r\n    id = \"appletv\"\r\n    name = \"Apple TV\"\r\n    abbreviation = \"ATV\"\r\n    url_regex = re.compile(r\"(?i)(?P<base_url>https?://tv\\.apple\\.com/(?:(?P<country_code>[a-z]{2})/)?(?P<media_type>movie|episode|season|show)/(?:(?P<media_name>[\\w\\-%]+)/)?(?P<media_id>umc\\.cmc\\.[a-z\\d]{23,25}))(?:\\?(?P<url_params>.*))?\")\r\n    subtitles_class = WebVTTSubtitles\r\n    is_movie_scraper = True\r\n    is_series_scraper = True\r\n    uses_scrapers = [\"itunes\"]\r\n    default_storefront = \"US\"\r\n    storefronts_mapping = {\r\n        \"AF\": \"143610\", \"AO\": \"143564\", \"AI\": \"143538\", \"AL\": \"143575\", \"AD\": \"143611\", \"AE\": \"143481\", \"AR\": \"143505\",\r\n        \"AM\": \"143524\", \"AG\": \"143540\", \"AU\": \"143460\", \"AT\": \"143445\", \"AZ\": \"143568\", \"BE\": \"143446\", \"BJ\": \"143576\",\r\n        \"BF\": \"143578\", \"BD\": \"143490\", \"BG\": \"143526\", \"BH\": \"143559\", \"BS\": \"143539\", \"BA\": \"143612\", \"BY\": \"143565\",\r\n        \"BZ\": \"143555\", \"BM\": \"143542\", \"BO\": \"143556\", \"BR\": \"143503\", \"BB\": \"143541\", \"BN\": \"143560\", \"BT\": \"143577\",\r\n        \"BW\": \"143525\", \"CF\": \"143623\", \"CA\": \"143455\", \"CH\": \"143459\", \"CL\": \"143483\", \"CN\": \"143465\", \"CI\": \"143527\",\r\n        \"CM\": \"143574\", \"CD\": \"143613\", \"CG\": \"143582\", \"CO\": \"143501\", \"CV\": \"143580\", \"CR\": \"143495\", \"KY\": \"143544\",\r\n        \"CY\": \"143557\", \"CZ\": \"143489\", \"DE\": \"143443\", \"DM\": \"143545\", \"DK\": \"143458\", \"DO\": \"143508\", \"DZ\": \"143563\",\r\n        \"EC\": \"143509\", \"EG\": \"143516\", \"ES\": \"143454\", \"EE\": \"143518\", \"ET\": \"143569\", \"FI\": \"143447\", \"FJ\": \"143583\",\r\n        \"FR\": \"143442\", \"FM\": \"143591\", \"GA\": \"143614\", \"GB\": \"143444\", \"GE\": \"143615\", \"GH\": \"143573\", \"GN\": \"143616\",\r\n        \"GM\": \"143584\", \"GW\": \"143585\", \"GR\": \"143448\", \"GD\": \"143546\", \"GT\": \"143504\", \"GY\": \"143553\", \"HK\": \"143463\",\r\n        \"HN\": \"143510\", \"HR\": \"143494\", \"HU\": \"143482\", \"ID\": \"143476\", \"IN\": \"143467\", \"IE\": \"143449\", \"IQ\": \"143617\",\r\n        \"IS\": \"143558\", \"IL\": \"143491\", \"IT\": \"143450\", \"JM\": \"143511\", \"JO\": \"143528\", \"JP\": \"143462\", \"KZ\": \"143517\",\r\n        \"KE\": \"143529\", \"KG\": \"143586\", \"KH\": \"143579\", \"KN\": \"143548\", \"KR\": \"143466\", \"KW\": \"143493\", \"LA\": \"143587\",\r\n        \"LB\": \"143497\", \"LR\": \"143588\", \"LY\": \"143567\", \"LC\": \"143549\", \"LI\": \"143522\", \"LK\": \"143486\", \"LT\": \"143520\",\r\n        \"LU\": \"143451\", \"LV\": \"143519\", \"MO\": \"143515\", \"MA\": \"143620\", \"MC\": \"143618\", \"MD\": \"143523\", \"MG\": \"143531\",\r\n        \"MV\": \"143488\", \"MX\": \"143468\", \"MK\": \"143530\", \"ML\": \"143532\", \"MT\": \"143521\", \"MM\": \"143570\", \"ME\": \"143619\",\r\n        \"MN\": \"143592\", \"MZ\": \"143593\", \"MR\": \"143590\", \"MS\": \"143547\", \"MU\": \"143533\", \"MW\": \"143589\", \"MY\": \"143473\",\r\n        \"NA\": \"143594\", \"NE\": \"143534\", \"NG\": \"143561\", \"NI\": \"143512\", \"NL\": \"143452\", \"NO\": \"143457\", \"NP\": \"143484\",\r\n        \"NR\": \"143606\", \"NZ\": \"143461\", \"OM\": \"143562\", \"PK\": \"143477\", \"PA\": \"143485\", \"PE\": \"143507\", \"PH\": \"143474\",\r\n        \"PW\": \"143595\", \"PG\": \"143597\", \"PL\": \"143478\", \"PT\": \"143453\", \"PY\": \"143513\", \"PS\": \"143596\", \"QA\": \"143498\",\r\n        \"RO\": \"143487\", \"RU\": \"143469\", \"RW\": \"143621\", \"SA\": \"143479\", \"SN\": \"143535\", \"SG\": \"143464\", \"SB\": \"143601\",\r\n        \"SL\": \"143600\", \"SV\": \"143506\", \"RS\": \"143500\", \"ST\": \"143598\", \"SR\": \"143554\", \"SK\": \"143496\", \"SI\": \"143499\",\r\n        \"SE\": \"143456\", \"SZ\": \"143602\", \"SC\": \"143599\", \"TC\": \"143552\", \"TD\": \"143581\", \"TH\": \"143475\", \"TJ\": \"143603\",\r\n        \"TM\": \"143604\", \"TO\": \"143608\", \"TT\": \"143551\", \"TN\": \"143536\", \"TR\": \"143480\", \"TW\": \"143470\", \"TZ\": \"143572\",\r\n        \"UG\": \"143537\", \"UA\": \"143492\", \"UY\": \"143514\", \"US\": \"143441\", \"UZ\": \"143566\", \"VC\": \"143550\", \"VE\": \"143502\",\r\n        \"VG\": \"143543\", \"VN\": \"143471\", \"VU\": \"143609\", \"WS\": \"143607\", \"XK\": \"143624\", \"YE\": \"143571\", \"ZA\": \"143472\",\r\n        \"ZM\": \"143622\", \"ZW\": \"143605\",\r\n    }\r\n\r\n    _api_base_url = \"https://tv.apple.com/api/uts/v3\"\r\n    _api_base_params = {\r\n        \"utscf\": \"OjAAAAAAAAA~\",\r\n        \"caller\": \"js\",\r\n        \"v\": \"66\",\r\n        \"pfm\": \"web\",\r\n    }\r\n\r\n    class Channel(Enum):\r\n        \"\"\"\r\n        An Enum representing AppleTV channels.\r\n        Value represents the channel ID as used by the API.\r\n        \"\"\"\r\n        APPLE_TV_PLUS = \"tvs.sbd.4000\"\r\n        DISNEY_PLUS = \"tvs.sbd.1000216\"\r\n        ITUNES = \"tvs.sbd.9001\"\r\n        HULU = \"tvs.sbd.10000\"\r\n        MAX = \"tvs.sbd.9050\"\r\n        NETFLIX = \"tvs.sbd.9000\"\r\n        PRIME_VIDEO = \"tvs.sbd.12962\"\r\n        STARZ = \"tvs.sbd.1000308\"\r\n\r\n    def __init__(self, user_agent: str | None = None, config_data: dict | None = None):\r\n        super().__init__(user_agent=user_agent, config_data=config_data)\r\n        self._storefront_locale_mapping_cache: dict[str, str] = {}\r\n\r\n    def _decide_locale(self, preferred_locales: str | list[str], default_locale: str, locales: list[str]) -> str:\r\n        \"\"\"\r\n        Decide which locale to use.\r\n\r\n        Args:\r\n            preferred_locales (str | list[str]): The preferred locales to use.\r\n            default_locale (str): The default locale to use if there is no match.\r\n            locales (list[str]): The locales to search in.\r\n\r\n        Returns:\r\n            str: The locale to use.\r\n        \"\"\"\r\n        if isinstance(preferred_locales, str):\r\n            preferred_locales = [preferred_locales]\r\n\r\n        for locale in preferred_locales:\r\n            if locale in locales:\r\n                return locale.replace(\"_\", \"-\")\r\n\r\n        if result := fnmatch.filter(locales, \"en_*\"):\r\n            return result[0].replace(\"_\", \"-\")\r\n\r\n        return default_locale\r\n\r\n    def _fetch_api_data(self, storefront_id: str, endpoint: str, additional_params: dict | None = None) -> dict:\r\n        \"\"\"\r\n        Send a request to AppleTV's API and return the JSON response.\r\n\r\n        Args:\r\n            endpoint (str): The endpoint to send the request to.\r\n            additional_params (dict[str, str]): Additional parameters to send with the request.\r\n\r\n        Returns:\r\n            dict: The JSON response.\r\n\r\n        Raises:\r\n            HttpError: If an HTTP error response is received.\r\n        \"\"\"\r\n        logger.debug(f\"Preparing to fetch '{endpoint}' using storefront '{storefront_id}'.\")\r\n\r\n        if storefront_cached_local := self._storefront_locale_mapping_cache.get(storefront_id):\r\n            logger.debug(f\"Using cached locale for storefront '{storefront_id}': '{storefront_cached_local}'.\")\r\n            locale = storefront_cached_local\r\n\r\n        else:\r\n            storefront_data = \\\r\n                self._get_configuration_data(storefront_id=storefront_id)[\"applicationProps\"][\"storefront\"]\r\n\r\n            default_locale = storefront_data[\"defaultLocale\"]\r\n            available_locales = storefront_data[\"localesSupported\"]\r\n\r\n            logger.debug(f\"Available locales for storefront '{storefront_id}': {available_locales}'. \"\r\n                         f\"Storefront's default locale: '{default_locale}'.\")\r\n\r\n            locale = self._decide_locale(\r\n                preferred_locales=[\"en_US\", \"en_GB\"],\r\n                default_locale=default_locale,\r\n                locales=available_locales,\r\n            )\r\n\r\n            logger.debug(f\"Selected locale for storefront '{storefront_id}': '{locale}'\")\r\n\r\n            self._storefront_locale_mapping_cache[storefront_id] = locale\r\n\r\n        request_params = self._generate_api_request_params(storefront_id=storefront_id, locale=locale)\r\n\r\n        if additional_params:\r\n            request_params.update(additional_params)\r\n\r\n        response = self._session.get(url=f\"{self._api_base_url}{endpoint}\", params=request_params)\r\n\r\n        try:\r\n            raise_for_status(response)\r\n\r\n        except HTTPError as e:\r\n            if response.status_code == 404:\r\n                raise ScraperError(\r\n                    \"Media not found. This could indicate that the provided URL is invalid.\",\r\n                ) from e\r\n\r\n            raise\r\n\r\n        response_json: dict = response.json()\r\n        response_data: dict = response_json.get(\"data\", {})\r\n\r\n        return response_data\r\n\r\n    def _generate_api_request_params(self, storefront_id: str,\r\n                                     locale: str | None = None, utsk: str | None = None) -> dict:\r\n        \"\"\"\r\n        Generate request params for the AppleTV's API.\r\n\r\n        Args:\r\n            storefront_id (str): ID of the storefront to use.\r\n            locale (str | None, optional): ID of the locale to use. Defaults to None.\r\n            utsk (str | None, optional): utsk data. Defaults to None.\r\n\r\n        Returns:\r\n            dict: The request params, generated from the given arguments.\r\n        \"\"\"\r\n        params = self._api_base_params.copy()\r\n        params[\"sf\"] = storefront_id\r\n\r\n        if utsk:\r\n            params[\"utsk\"] = utsk\r\n\r\n        if locale:\r\n            params[\"locale\"] = locale\r\n\r\n        return params\r\n\r\n    def _get_configuration_data(self, storefront_id: str) -> dict:\r\n        \"\"\"\r\n        Get configuration data for the given storefront ID.\r\n\r\n        Args:\r\n            storefront_id (str): The ID of the storefront to get the configuration data for.\r\n\r\n        Returns:\r\n            dict: The configuration data.\r\n        \"\"\"\r\n        logger.debug(f\"Fetching configuration data for storefront '{storefront_id}'...\")\r\n        url = f\"{self._api_base_url}/configurations\"\r\n        params = self._generate_api_request_params(storefront_id=storefront_id)\r\n        response = self._session.get(url=url, params=params)\r\n        raise_for_status(response)\r\n        logger.debug(\"Configuration data fetched successfully.\")\r\n\r\n        response_data: dict = response.json()[\"data\"]\r\n        return response_data\r\n\r\n    def _map_playables_by_channel(self, playables: list[dict]) -> dict[str, dict]:\r\n        \"\"\"\r\n        Map playables by channel name.\r\n\r\n        Args:\r\n            playables (list[dict]): Playables data to map.\r\n\r\n        Returns:\r\n            dict: The mapped playables (in a `channel_name (str): [playables]` format).\r\n        \"\"\"\r\n        mapped_playables: dict = {}\r\n\r\n        for playable in playables:\r\n            if channel_id := playable.get(\"channelId\"):\r\n                mapped_playables.setdefault(channel_id, []).append(playable)\r\n\r\n        return mapped_playables\r\n\r\n    def get_movie_data(self, storefront_id: str, movie_id: str) -> ScrapedMediaResponse[Movie]:\r\n        data = self._fetch_api_data(\r\n            storefront_id=storefront_id,\r\n            endpoint=f\"/movies/{movie_id}\",\r\n        )\r\n\r\n        mapped_playables = self._map_playables_by_channel(playables=data[\"playables\"].values())\r\n        logger.debug(f\"Available channels for movie '{movie_id}': \"\r\n                     f\"{' '.join(list(mapped_playables.keys()))}\")\r\n\r\n        if self.Channel.ITUNES.value not in mapped_playables:\r\n            if self.Channel.APPLE_TV_PLUS.value in mapped_playables:\r\n                raise ScraperError(\"Scraping AppleTV+ content is not currently supported.\")\r\n\r\n            raise ScraperError(\"No iTunes playables could be found.\")\r\n\r\n        return_data = []\r\n\r\n        for playable_data in mapped_playables[self.Channel.ITUNES.value]:\r\n            return_data.append(self._extract_itunes_movie_data(playable_data))\r\n\r\n        if len(return_data) > 1:\r\n            logger.debug(f\"{len(return_data)} iTunes playables were found for movie '{movie_id}'.\")\r\n\r\n        return ScrapedMediaResponse(\r\n            media_data=return_data,\r\n            metadata_scraper=self.id,\r\n            playlist_scraper=\"itunes\",\r\n            original_data=data,\r\n        )\r\n\r\n    def _extract_itunes_movie_data(self, playable_data: dict) -> Movie:\r\n        \"\"\"\r\n        Extract movie data from an AppleTV's API iTunes playable data.\r\n\r\n        Args:\r\n            playable_data (dict): The playable data from the AppleTV API.\r\n\r\n        Returns:\r\n            Movie: A Movie object.\r\n        \"\"\"\r\n        itunes_movie_id = playable_data[\"itunesMediaApiData\"][\"id\"]\r\n        appletv_movie_id = playable_data[\"canonicalId\"]\r\n        movie_title = playable_data[\"canonicalMetadata\"][\"movieTitle\"]\r\n        movie_release_date = convert_epoch_to_datetime(playable_data[\"canonicalMetadata\"][\"releaseDate\"] // 1000)\r\n\r\n        movie_playlists = []\r\n        movie_duration = None\r\n\r\n        if offers := playable_data[\"itunesMediaApiData\"].get(\"offers\"):\r\n            for offer in offers:\r\n                if (playlist := offer.get(\"hlsUrl\")) and offer[\"hlsUrl\"] not in movie_playlists:\r\n                    movie_playlists.append(playlist)\r\n\r\n            if movie_duration_int := offers[0].get(\"durationInMilliseconds\"):\r\n                movie_duration = dt.timedelta(milliseconds=movie_duration_int)\r\n\r\n        if movie_expected_release_date := playable_data[\"itunesMediaApiData\"].get(\"futureRentalAvailabilityDate\"):\r\n            movie_expected_release_date = dt.datetime.strptime(movie_expected_release_date, \"%Y-%m-%d\")\r\n\r\n        return Movie(\r\n            id=itunes_movie_id,\r\n            referer_id=appletv_movie_id,\r\n            name=movie_title,\r\n            release_date=movie_release_date,\r\n            duration=movie_duration,\r\n            preorder_availability_date=movie_expected_release_date,\r\n            playlist=movie_playlists if movie_playlists else None,\r\n        )\r\n\r\n    def get_episode_data(self, storefront_id: str, episode_id: str) -> ScrapedMediaResponse[Episode]:\r\n        raise NotImplementedError(\"Series scraping is not currently supported.\")\r\n\r\n    def get_season_data(self, storefront_id: str, season_id: str, show_id: str) -> ScrapedMediaResponse[Season]:\r\n        raise NotImplementedError(\"Series scraping is not currently supported.\")\r\n\r\n    def get_show_data(self, storefront_id: str, show_id: str) -> ScrapedMediaResponse[Series]:\r\n        raise NotImplementedError(\"Series scraping is not currently supported.\")\r\n\r\n    def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        regex_match = self.match_url(url=url, raise_error=True)\r\n        url_data = regex_match.groupdict()\r\n\r\n        media_type = url_data[\"media_type\"]\r\n\r\n        if storefront_code := url_data.get(\"country_code\"):\r\n            storefront_code = storefront_code.upper()\r\n\r\n        else:\r\n            storefront_code = self.default_storefront\r\n\r\n        media_id = url_data[\"media_id\"]\r\n\r\n        if storefront_code not in self.storefronts_mapping:\r\n            raise ScraperError(f\"ID mapping for storefront '{storefront_code}' could not be found.\")\r\n\r\n        storefront_id = self.storefronts_mapping[storefront_code]\r\n\r\n        if media_type == \"movie\":\r\n            return self.get_movie_data(storefront_id=storefront_id, movie_id=media_id)\r\n\r\n        if media_type == \"episode\":\r\n            return self.get_episode_data(storefront_id=storefront_id, episode_id=media_id)\r\n\r\n        if media_type == \"season\":\r\n            if (url_params := url_data.get(\"url_params\")) and (show_id := parse_url_params(url_params).get(\"showId\")):\r\n                return self.get_season_data(storefront_id=storefront_id, season_id=media_id, show_id=show_id)\r\n\r\n            raise ScraperError(\"Invalid AppleTV URL: Missing 'showId' parameter.\")\r\n\r\n        if media_type == \"show\":\r\n            return self.get_show_data(storefront_id=storefront_id, show_id=media_id)\r\n\r\n        raise ScraperError(f\"Invalid media type '{media_type}'.\")\r\n\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData]:\r\n        raise NotImplementedError(\"Subtitles scraping for AppleTV+ is not currently supported.\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/appletv_scraper.py b/isubrip/scrapers/appletv_scraper.py
--- a/isubrip/scrapers/appletv_scraper.py	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/isubrip/scrapers/appletv_scraper.py	(date 1715678910511)
@@ -345,7 +345,3 @@
             return self.get_show_data(storefront_id=storefront_id, show_id=media_id)
 
         raise ScraperError(f"Invalid media type '{media_type}'.")
-
-    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,
-                      subrip_conversion: bool = False) -> Iterator[SubtitlesData]:
-        raise NotImplementedError("Subtitles scraping for AppleTV+ is not currently supported.")
Index: isubrip/scrapers/scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC, abstractmethod\r\nimport asyncio\r\nfrom enum import Enum\r\nimport importlib\r\nimport inspect\r\nfrom pathlib import Path\r\nimport re\r\nimport sys\r\nfrom typing import TYPE_CHECKING, Any, ClassVar, Iterator, List, Literal, Type, TypeVar, Union, overload\r\n\r\nimport httpx\r\nimport m3u8\r\nfrom m3u8 import M3U8, Media, Segment, SegmentList\r\nimport requests\r\nimport requests.utils\r\n\r\nfrom isubrip.config import Config, ConfigSetting\r\nfrom isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX\r\nfrom isubrip.data_structures import ScrapedMediaResponse, SubtitlesData, SubtitlesType\r\nfrom isubrip.logger import logger\r\nfrom isubrip.utils import SingletonMeta, merge_dict_values, single_to_list\r\n\r\nif TYPE_CHECKING:\r\n    from types import TracebackType\r\n\r\n    from isubrip.subtitle_formats.subtitles import Subtitles\r\n\r\nScraperT = TypeVar(\"ScraperT\", bound=\"Scraper\")\r\n\r\n\r\nclass Scraper(ABC, metaclass=SingletonMeta):\r\n    \"\"\"\r\n    A base class for scrapers.\r\n\r\n    Attributes:\r\n        default_user_agent (str): [Class Attribute]\r\n            Default user agent to use if no other user agent is specified when making requests.\r\n        default_proxy (str | None): [Class Attribute] Default proxy to use when making requests.\r\n        default_verify_ssl (bool): [Class Attribute] Whether to verify SSL certificates by default.\r\n        subtitles_fix_rtl (bool): [Class Attribute] Whether to fix RTL from downloaded subtitles.\r\n            A list of languages to fix RTL on. If None, a default list will be used.\r\n        subtitles_remove_duplicates (bool): [Class Attribute]\r\n            Whether to remove duplicate lines from downloaded subtitles.\r\n\r\n        id (str): [Class Attribute] ID of the scraper.\r\n        name (str): [Class Attribute] Name of the scraper.\r\n        abbreviation (str): [Class Attribute] Abbreviation of the scraper.\r\n        url_regex (re.Pattern | list[re.Pattern]): [Class Attribute] A RegEx pattern to find URLs matching the service.\r\n        subtitles_class (type[Subtitles]): [Class Attribute] Class of the subtitles format returned by the scraper.\r\n        is_movie_scraper (bool): [Class Attribute] Whether the scraper is for movies.\r\n        is_series_scraper (bool): [Class Attribute] Whether the scraper is for series.\r\n        uses_scrapers (list[str]): [Class Attribute] A list of IDs for other scraper classes that this scraper uses.\r\n            This assures that the config data for the other scrapers is passed as well.\r\n        _session (requests.Session): A requests session to use for making requests.\r\n        _user_agent (str): A user agent to use when making requests.\r\n        _proxy (str | None): A proxy to use when making requests.\r\n        _verify_ssl (bool): Whether to verify SSL certificates when making requests.\r\n        config (Config): A Config object containing the scraper's configuration.\r\n    \"\"\"\r\n    default_user_agent: ClassVar[str] = requests.utils.default_user_agent()\r\n    default_proxy: ClassVar[str | None] = None\r\n    default_verify_ssl: ClassVar[bool] = True\r\n    subtitles_fix_rtl: ClassVar[bool] = False\r\n    subtitles_remove_duplicates: ClassVar[bool] = True\r\n\r\n    id: ClassVar[str]\r\n    name: ClassVar[str]\r\n    abbreviation: ClassVar[str]\r\n    url_regex: ClassVar[re.Pattern | list[re.Pattern]]\r\n    subtitles_class: ClassVar[type[Subtitles]]\r\n    is_movie_scraper: ClassVar[bool] = False\r\n    is_series_scraper: ClassVar[bool] = False\r\n    uses_scrapers: ClassVar[list[str]] = []\r\n\r\n    def __init__(self, user_agent: str | None = None, proxy: str | None = None,\r\n                 verify_ssl: bool | None = None, config_data: dict | None = None):\r\n        \"\"\"\r\n        Initialize a Scraper object.\r\n\r\n        Args:\r\n            user_agent (str | None, optional): A user agent to use when making requests. Defaults to None.\r\n            proxy (str | None, optional): A proxy to use when making requests. Defaults to None.\r\n            verify_ssl (bool | None, optional): Whether to verify SSL certificates. Defaults to None.\r\n            config_data (dict | None, optional): A dictionary containing scraper's configuration data. Defaults to None.\r\n        \"\"\"\r\n        self._session = requests.Session()\r\n        self.config = Config(config_data=config_data.get(self.id) if config_data else None)\r\n\r\n        # Add a \"user-agent\" setting by default to all scrapers\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                key=\"user-agent\",\r\n                value_type=str,\r\n                required=False,\r\n            ),\r\n            ConfigSetting(\r\n                key=\"proxy\",\r\n                value_type=str,\r\n                required=False,\r\n            ),\r\n            ConfigSetting(\r\n                key=\"verify-ssl\",\r\n                value_type=bool,\r\n                required=False,\r\n            ),\r\n        ],\r\n            check_config=False)\r\n\r\n        self._user_agent: str\r\n        self._proxy: str | None\r\n        self._verify_ssl: bool\r\n\r\n        # User-Agent Configuration\r\n        if user_agent is not None:\r\n            self._user_agent = user_agent\r\n\r\n        elif \"user-agent\" in self.config:\r\n            self._user_agent = self.config[\"user-agent\"]\r\n\r\n        else:\r\n            self._user_agent = self.default_user_agent\r\n\r\n        if self._user_agent != self.default_user_agent:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with user-agent: '{user_agent}'.\")\r\n\r\n        # Proxy Configuration\r\n        if proxy is not None:\r\n            self._proxy = proxy\r\n\r\n        elif \"proxy\" in self.config:\r\n            self._proxy = self.config[\"proxy\"]\r\n\r\n        else:\r\n            self._proxy = self.default_proxy\r\n\r\n        if self._proxy != self.default_proxy:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with proxy: '{proxy}'.\")\r\n\r\n        # SSL Verification Configuration\r\n        if verify_ssl is not None:\r\n            self._verify_ssl = verify_ssl\r\n\r\n        elif \"verify-ssl\" in self.config:\r\n            self._verify_ssl = self.config[\"verify-ssl\"]\r\n\r\n        else:\r\n            self._verify_ssl = self.default_verify_ssl\r\n\r\n        if self._verify_ssl != self.default_verify_ssl:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with SSL verification set to: '{verify_ssl}'.\")\r\n\r\n        self._session.headers.update({\"User-Agent\": self._user_agent})\r\n\r\n        if self._proxy:\r\n            self._session.proxies.update({\"http\": self._proxy, \"https\": self._proxy})\r\n\r\n        self._session.verify = self._verify_ssl\r\n\r\n        if not self._verify_ssl:\r\n            import urllib3\r\n            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[True] = ...) -> re.Match:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[False] = ...) -> re.Match | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def match_url(cls, url: str, raise_error: bool = False) -> re.Match | None:\r\n        \"\"\"\r\n        Checks if a URL matches scraper's url regex.\r\n\r\n        Args:\r\n            url (str): A URL to check against the regex.\r\n            raise_error (bool, optional): Whether to raise an error instead of returning None if the URL doesn't match.\r\n\r\n        Returns:\r\n            re.Match | None: A Match object if the URL matches the regex, None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If the URL doesn't match the regex and raise_error is True.\r\n        \"\"\"\r\n        if isinstance(cls.url_regex, re.Pattern) and (match_result := re.fullmatch(pattern=cls.url_regex, string=url)):\r\n            return match_result\r\n\r\n        if isinstance(cls.url_regex, list):\r\n            for url_regex_item in cls.url_regex:\r\n                if result := re.fullmatch(pattern=url_regex_item, string=url):\r\n                    return result\r\n\r\n        if raise_error:\r\n            raise ValueError(f\"URL '{url}' doesn't match the URL regex of {cls.name}.\")\r\n\r\n        return None\r\n\r\n    def __enter__(self) -> Scraper:\r\n        return self\r\n\r\n    def __exit__(self, exc_type: Type[BaseException] | None,\r\n                 exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:\r\n        self.close()\r\n\r\n    def close(self) -> None:\r\n        self._session.close()\r\n\r\n    @abstractmethod\r\n    def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        \"\"\"\r\n        Scrape media information about the media on a URL.\r\n\r\n        Args:\r\n            url (str): A URL to get media information about.\r\n\r\n        Returns:\r\n            ScrapedMediaResponse: A ScrapedMediaResponse object containing scraped media information.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData | SubtitlesDownloadError]:\r\n        \"\"\"\r\n        Find and yield subtitles data from a main_playlist.\r\n\r\n        Args:\r\n            main_playlist(str | list[str]): A URL or a list of URLs (for redundancy) of the main playlist.\r\n            language_filter (list[str] | str | None, optional):\r\n                A language or a list of languages to filter for. Defaults to None.\r\n            subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.\r\n\r\n        Yields:\r\n            SubtitlesData | SubtitlesDownloadError: A SubtitlesData object containing subtitles data,\r\n                or a SubtitlesDownloadError object if an error occurred.\r\n        \"\"\"\r\n\r\n\r\nclass AsyncScraper(Scraper, ABC):\r\n    \"\"\"A base class for scrapers that utilize async requests.\"\"\"\r\n    def __init__(self,  user_agent: str | None = None, config_data: dict | None = None):\r\n        super().__init__(user_agent=user_agent, config_data=config_data)\r\n        self._async_session = httpx.AsyncClient(\r\n            headers={\"User-Agent\": self._user_agent},\r\n            proxy=(httpx.Proxy(url=self._proxy) if self._proxy else None),\r\n            verify=self._verify_ssl,\r\n        )\r\n\r\n    def close(self) -> None:\r\n        asyncio.get_event_loop().run_until_complete(self._async_session.aclose())\r\n        super().close()\r\n\r\n    async def _async_close(self) -> None:\r\n        await self._async_session.aclose()\r\n\r\n\r\nclass HLSScraper(AsyncScraper, ABC):\r\n    \"\"\"A base class for HLS (m3u8) scrapers.\"\"\"\r\n    class M3U8Attribute(Enum):\r\n        \"\"\"\r\n        An enum representing all possible M3U8 attributes.\r\n        Names / Keys represent M3U8 Media object attributes (should be converted to lowercase),\r\n        and values represent the name of the key for config usage.\r\n        \"\"\"\r\n        ASSOC_LANGUAGE = \"assoc-language\"\r\n        AUTOSELECT = \"autoselect\"\r\n        CHARACTERISTICS = \"characteristics\"\r\n        CHANNELS = \"channels\"\r\n        DEFAULT = \"default\"\r\n        FORCED = \"forced\"\r\n        GROUP_ID = \"group-id\"\r\n        INSTREAM_ID = \"instream-id\"\r\n        LANGUAGE = \"language\"\r\n        NAME = \"name\"\r\n        STABLE_RENDITION_ID = \"stable-rendition-id\"\r\n        TYPE = \"type\"\r\n\r\n    _playlist_filters_config_category = \"playlist-filters\"\r\n    _subtitles_filters: dict[str, Any] = {\r\n        M3U8Attribute.TYPE.value: \"SUBTITLES\",\r\n    }\r\n\r\n    def __init__(self,  user_agent: str | None = None, config_data: dict | None = None):\r\n        super().__init__(user_agent=user_agent, config_data=config_data)\r\n\r\n        # Add M3U8 filters settings\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                category=self._playlist_filters_config_category,\r\n                key=m3u8_attribute.value,\r\n                value_type=Union[str, List[str]],\r\n                required=False,\r\n            ) for m3u8_attribute in self.M3U8Attribute],\r\n            check_config=False)\r\n\r\n    def _download_segments(self, segments: SegmentList[Segment]) -> list[bytes]:\r\n        \"\"\"\r\n        Download M3U8 segments asynchronously.\r\n\r\n        Args:\r\n            segments (m3u8.SegmentList[m3u8.Segment]): List of segments to download.\r\n\r\n        Returns:\r\n            list[bytes]: List of downloaded segments.\r\n        \"\"\"\r\n        return asyncio.get_event_loop().run_until_complete(self._download_segments_async(segments))\r\n\r\n    async def _download_segments_async(self, segments: SegmentList[Segment]) -> list[bytes]:\r\n        \"\"\"\r\n        Download M3U8 segments asynchronously.\r\n\r\n        Args:\r\n            segments (m3u8.SegmentList[m3u8.Segment]): List of segments to download.\r\n\r\n        Returns:\r\n            list[bytes]: List of downloaded segments.\r\n        \"\"\"\r\n        async_tasks = [\r\n            self._download_segment_async(url=segment.absolute_uri)\r\n            for segment in segments\r\n        ]\r\n\r\n        return await asyncio.gather(*async_tasks)\r\n\r\n    async def _download_segment_async(self, url: str) -> bytes:\r\n        \"\"\"\r\n        Download an M3U8 segment asynchronously.\r\n\r\n        Args:\r\n            url (str): URL of the segment to download.\r\n\r\n        Returns:\r\n            bytes: Downloaded segment.\r\n        \"\"\"\r\n        response = await self._async_session.get(url)\r\n        return response.content\r\n\r\n    def load_m3u8(self, url: str | list[str], headers: dict | None = None) -> M3U8 | None:\r\n        \"\"\"\r\n        Load an M3U8 playlist from a URL to an M3U8 object.\r\n        Multiple URLs can be given, in which case the first one that loads successfully will be returned.\r\n        The method uses caching to avoid loading the same playlist multiple times.\r\n\r\n        Args:\r\n            url (str | list[str]): URL of the M3U8 playlist to load. Can also be a list of URLs (for redundancy).\r\n            headers (dict | None, optional): A dictionary of headers to use when making the request.\r\n                Defaults to None (results in using session's configured headers).\r\n\r\n        Returns:\r\n            m3u8.M3U8: An M3U8 object representing the playlist.\r\n        \"\"\"\r\n        _headers = headers or self._session.headers\r\n\r\n        for url_item in single_to_list(url):\r\n            try:\r\n                response = self._session.get(url=url_item, headers=_headers)\r\n\r\n            except Exception as e:\r\n                logger.debug(f\"Failed to load M3U8 playlist '{url_item}': {e}\")\r\n                continue\r\n\r\n            if not response.text:\r\n                raise PlaylistLoadError(\"Received empty response for playlist from server.\")\r\n\r\n            return m3u8.loads(content=response.text, uri=url_item)\r\n\r\n        return None\r\n\r\n    @staticmethod\r\n    def detect_subtitles_type(subtitles_media: Media) -> SubtitlesType | None:\r\n        \"\"\"\r\n        Detect the subtitles type (Closed Captions, Forced, etc.) from an M3U8 Media object.\r\n\r\n        Args:\r\n            subtitles_media (m3u8.Media): Subtitles Media object to detect the type of.\r\n\r\n        Returns:\r\n            SubtitlesType | None: The type of the subtitles, None for regular subtitles.\r\n        \"\"\"\r\n        if subtitles_media.forced == \"YES\":\r\n            return SubtitlesType.FORCED\r\n\r\n        if subtitles_media.characteristics is not None and \"public.accessibility\" in subtitles_media.characteristics:\r\n            return SubtitlesType.CC\r\n\r\n        return None\r\n\r\n    def get_media_playlists(self, main_playlist: M3U8,\r\n                            playlist_filters: dict[str, str | list[str]] | None = None) -> list[Media]:\r\n        \"\"\"\r\n        Find and yield playlists of media within an M3U8 main_playlist using optional filters.\r\n\r\n        Args:\r\n            main_playlist (m3u8.M3U8): An M3U8 object of the main main_playlist.\r\n            playlist_filters (dict[str, str | list[str], optional):\r\n                A dictionary of filters to use when searching for subtitles.\r\n                Will be added to filters set by the config. Defaults to None.\r\n\r\n        Returns:\r\n            list[Media]: A list of  matching Media objects.\r\n        \"\"\"\r\n        results = []\r\n        config_filters: dict | None = self.config.get(self._playlist_filters_config_category)\r\n        # Merge filtering dictionaries to a single dictionary\r\n        playlist_filters = merge_dict_values(*[item for item in (playlist_filters, config_filters) if item])\r\n\r\n        for media in main_playlist.media:\r\n            if not playlist_filters:\r\n                results.append(media)\r\n                continue\r\n\r\n            is_valid = True\r\n\r\n            for filter_name, filter_value in playlist_filters.items():\r\n                try:\r\n                    filter_name_enum = HLSScraper.M3U8Attribute(filter_name)\r\n                    attribute_value = getattr(media, filter_name_enum.name.lower(), None)\r\n\r\n                    if (attribute_value is None) or (\r\n                            isinstance(filter_value, list) and\r\n                            attribute_value.casefold() not in (x.casefold() for x in filter_value)\r\n                    ) or (\r\n                            isinstance(filter_value, str) and filter_value.casefold() != attribute_value.casefold()\r\n                    ):\r\n                        is_valid = False\r\n                        break\r\n\r\n                except Exception:\r\n                    is_valid = False\r\n\r\n            if is_valid:\r\n                results.append(media)\r\n\r\n        return results\r\n\r\n\r\nclass ScraperFactory:\r\n    _scraper_classes_cache: list[type[Scraper]] | None = None\r\n    _scraper_instances_cache: dict[type[Scraper], Scraper] = {}\r\n    _currently_initializing: list[type[Scraper]] = []  # Used to prevent infinite recursion\r\n\r\n    @classmethod\r\n    def get_initialized_scrapers(cls) -> list[Scraper]:\r\n        \"\"\"\r\n        Get a list of all previously initialized scrapers.\r\n\r\n        Returns:\r\n            list[Scraper]: A list of initialized scrapers.\r\n        \"\"\"\r\n        return list(cls._scraper_instances_cache.values())\r\n\r\n    @classmethod\r\n    def get_scraper_classes(cls) -> list[type[Scraper]]:\r\n        \"\"\"\r\n        Find all scraper classes in the scrapers directory.\r\n\r\n        Returns:\r\n            list[Scraper]: A Scraper subclass.\r\n        \"\"\"\r\n        if cls._scraper_classes_cache is not None:\r\n            return cls._scraper_classes_cache\r\n\r\n        cls._scraper_classes_cache = []\r\n        scraper_modules_paths = Path(__file__).parent.glob(f\"*{SCRAPER_MODULES_SUFFIX}.py\")\r\n\r\n        for scraper_module_path in scraper_modules_paths:\r\n            sys.path.append(str(scraper_module_path))\r\n\r\n            module = importlib.import_module(f\"{PACKAGE_NAME}.scrapers.{scraper_module_path.stem}\")\r\n\r\n            # Find all 'Scraper' subclasses\r\n            for _, obj in inspect.getmembers(module,\r\n                                             predicate=lambda x: inspect.isclass(x) and issubclass(x, Scraper)):\r\n                # Skip object if it's an abstract or imported from another module\r\n                if not inspect.isabstract(obj) and obj.__module__ == module.__name__:\r\n                    cls._scraper_classes_cache.append(obj)\r\n\r\n        return cls._scraper_classes_cache\r\n\r\n    @classmethod\r\n    def _get_scraper_instance(cls, scraper_class: type[ScraperT], kwargs: dict | None = None,\r\n                              extract_scraper_config: bool = False) -> ScraperT:\r\n        \"\"\"\r\n        Initialize and return a scraper instance.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT]): A scraper class to initialize.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            extract_scraper_config (bool, optional): Whether the passed 'config_data' (within kwargs)\r\n                is a main config dictionary, and scraper's config should be extracted from it. Defaults to False.\r\n\r\n        Returns:\r\n            Scraper: An instance of the given scraper class.\r\n        \"\"\"\r\n        logger.debug(f\"Initializing '{scraper_class.name}' scraper...\")\r\n        kwargs = kwargs or {}\r\n\r\n        if scraper_class not in cls._scraper_instances_cache:\r\n            logger.debug(f\"'{scraper_class.name}' scraper not found in cache, creating a new instance...\")\r\n\r\n            if scraper_class in cls._currently_initializing:\r\n                raise ScraperError(f\"'{scraper_class.name}' scraper is already being initialized.\\n\"\r\n                                   f\"Make sure there are no circular dependencies between scrapers.\")\r\n\r\n            cls._currently_initializing.append(scraper_class)\r\n\r\n            if extract_scraper_config:\r\n                if kwargs.get(\"config_data\"):\r\n                    required_scrapers_ids = [scraper_class.id, *scraper_class.uses_scrapers]\r\n                    kwargs[\"config_data\"] = (\r\n                        {scraper_id: kwargs[\"config_data\"][scraper_id] for scraper_id in required_scrapers_ids\r\n                         if kwargs[\"config_data\"].get(scraper_id)}\r\n                    )\r\n\r\n                else:\r\n                    kwargs[\"config_data\"] = None\r\n\r\n            cls._scraper_instances_cache[scraper_class] = scraper_class(**kwargs)\r\n            cls._currently_initializing.remove(scraper_class)\r\n\r\n        else:\r\n            logger.debug(f\"Cached '{scraper_class.name}' scraper instance found and will be used.\")\r\n\r\n        return cls._scraper_instances_cache[scraper_class]  # type: ignore[return-value]\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ..., extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[True] = ...) -> ScraperT:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[False] = ...) -> ScraperT | None:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ..., extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[True] = ...) -> Scraper:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ..., extract_scraper_config: bool = ...,\r\n                             raise_error: Literal[False] = ...) -> Scraper | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def get_scraper_instance(cls, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,\r\n                             url: str | None = None, kwargs: dict | None = None, extract_scraper_config: bool = False,\r\n                             raise_error: bool = True) -> Scraper | None:\r\n        \"\"\"\r\n        Find, initialize and return a scraper that matches the given URL or ID.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT] | None, optional): A scraper class to initialize. Defaults to None.\r\n            scraper_id (str | None, optional): ID of a scraper to initialize. Defaults to None.\r\n            url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            extract_scraper_config (bool, optional): Whether the passed 'config_data' (within kwargs)\r\n            raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.\r\n\r\n        Returns:\r\n            ScraperT | Scraper | None: An instance of a scraper that matches the given URL or ID,\r\n                None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If no scraper was found and raise_error is True.\r\n        \"\"\"\r\n        if scraper_class:\r\n            return cls._get_scraper_instance(scraper_class=scraper_class, kwargs=kwargs,\r\n                                             extract_scraper_config=extract_scraper_config)\r\n\r\n        if not (scraper_id or url):\r\n            raise ValueError(\"At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.\")\r\n\r\n        if scraper_id:\r\n            logger.debug(f\"Searching for a scraper object with ID '{scraper_id}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.id == scraper_id:\r\n                    return cls._get_scraper_instance(scraper_class=scraper, kwargs=kwargs,\r\n                                                     extract_scraper_config=extract_scraper_config)\r\n\r\n        elif url:\r\n            logger.debug(f\"Searching for a scraper object that matches URL '{url}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.match_url(url) is not None:\r\n                    return cls._get_scraper_instance(scraper_class=scraper, kwargs=kwargs,\r\n                                                     extract_scraper_config=extract_scraper_config)\r\n\r\n        error_message = \"No matching scraper was found.\"\r\n\r\n        if raise_error:\r\n            raise ValueError(error_message)\r\n\r\n        logger.debug(error_message)\r\n        return None\r\n\r\n\r\nclass ScraperError(Exception):\r\n    pass\r\n\r\n\r\nclass PlaylistLoadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass SubtitlesDownloadError(ScraperError):\r\n    def __init__(self, language_code: str, language_name: str | None = None, special_type: SubtitlesType | None = None,\r\n                 original_exc: Exception | None = None, *args: Any, **kwargs: dict[str, Any]):\r\n        \"\"\"\r\n        Initialize a SubtitlesDownloadError instance.\r\n\r\n        Args:\r\n            language_code (str): Language code of the subtitles that failed to download.\r\n            language_name (str | None, optional): Language name of the subtitles that failed to download.\r\n            special_type (SubtitlesType | None, optional): Type of the subtitles that failed to download.\r\n            original_exc (Exception | None, optional): The original exception that caused the error.\r\n        \"\"\"\r\n        super().__init__(*args, **kwargs)\r\n        self.language_code = language_code\r\n        self.language_name = language_name\r\n        self.special_type = special_type\r\n        self.original_exc = original_exc\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/scraper.py b/isubrip/scrapers/scraper.py
--- a/isubrip/scrapers/scraper.py	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/isubrip/scrapers/scraper.py	(date 1715678910535)
@@ -1,7 +1,8 @@
 from __future__ import annotations
 
-from abc import ABC, abstractmethod
 import asyncio
+from abc import ABC, abstractmethod
+from asyncio import Task
 from enum import Enum
 import importlib
 import inspect
@@ -12,13 +13,13 @@
 
 import httpx
 import m3u8
-from m3u8 import M3U8, Media, Segment, SegmentList
+from m3u8 import M3U8, Media
 import requests
 import requests.utils
 
 from isubrip.config import Config, ConfigSetting
 from isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX
-from isubrip.data_structures import ScrapedMediaResponse, SubtitlesData, SubtitlesType
+from isubrip.data_structures import ScrapedMediaResponse, SubtitlesData, SubtitlesType, SubtitlesFormatType
 from isubrip.logger import logger
 from isubrip.utils import SingletonMeta, merge_dict_values, single_to_list
 
@@ -86,6 +87,9 @@
             config_data (dict | None, optional): A dictionary containing scraper's configuration data. Defaults to None.
         """
         self._session = requests.Session()
+        self._async_session = httpx.AsyncClient(
+            limits=httpx.Limits(max_connections=100),
+        )
         self.config = Config(config_data=config_data.get(self.id) if config_data else None)
 
         # Add a "user-agent" setting by default to all scrapers
@@ -151,12 +155,16 @@
         if self._verify_ssl != self.default_verify_ssl:
             logger.debug(f"Initializing '{self.name}' scraper with SSL verification set to: '{verify_ssl}'.")
 
+        # Update session settings according to configurations
         self._session.headers.update({"User-Agent": self._user_agent})
+        self._async_session.headers.update({"User-Agent": self._user_agent})
 
         if self._proxy:
             self._session.proxies.update({"http": self._proxy, "https": self._proxy})
+            self._async_session.proxies = {"http": self._proxy, "https": self._proxy}
 
         self._session.verify = self._verify_ssl
+        self._async_session.verify = self._verify_ssl
 
         if not self._verify_ssl:
             import urllib3
@@ -207,6 +215,9 @@
                  exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:
         self.close()
 
+    async def async_close(self) -> None:
+        await self._async_session.aclose()
+
     def close(self) -> None:
         self._session.close()
 
@@ -223,8 +234,9 @@
         """
 
     @abstractmethod
-    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | None = None,
-                      subrip_conversion: bool = False) -> Iterator[SubtitlesData | SubtitlesDownloadError]:
+    async def download_matching_subtitles(self, main_playlist: str | list[str],
+                                          language_filter: list[str] | None = None,
+                                          subrip_conversion: bool = False) -> list[Task[SubtitlesData | SubtitlesDownloadError]]:
         """
         Find and yield subtitles data from a main_playlist.
 
@@ -234,31 +246,12 @@
                 A language or a list of languages to filter for. Defaults to None.
             subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.
 
-        Yields:
-            SubtitlesData | SubtitlesDownloadError: A SubtitlesData object containing subtitles data,
-                or a SubtitlesDownloadError object if an error occurred.
+        Returns:
+            list[Task[SubtitlesData | SubtitlesDownloadError]]: A list of tasks that download matching subtitles.
         """
 
 
-class AsyncScraper(Scraper, ABC):
-    """A base class for scrapers that utilize async requests."""
-    def __init__(self,  user_agent: str | None = None, config_data: dict | None = None):
-        super().__init__(user_agent=user_agent, config_data=config_data)
-        self._async_session = httpx.AsyncClient(
-            headers={"User-Agent": self._user_agent},
-            proxy=(httpx.Proxy(url=self._proxy) if self._proxy else None),
-            verify=self._verify_ssl,
-        )
-
-    def close(self) -> None:
-        asyncio.get_event_loop().run_until_complete(self._async_session.aclose())
-        super().close()
-
-    async def _async_close(self) -> None:
-        await self._async_session.aclose()
-
-
-class HLSScraper(AsyncScraper, ABC):
+class HLSScraper(Scraper, ABC):
     """A base class for HLS (m3u8) scrapers."""
     class M3U8Attribute(Enum):
         """
@@ -297,49 +290,20 @@
             ) for m3u8_attribute in self.M3U8Attribute],
             check_config=False)
 
-    def _download_segments(self, segments: SegmentList[Segment]) -> list[bytes]:
-        """
-        Download M3U8 segments asynchronously.
-
-        Args:
-            segments (m3u8.SegmentList[m3u8.Segment]): List of segments to download.
-
-        Returns:
-            list[bytes]: List of downloaded segments.
+    def parse_language_name(self, media_data: Media) -> str:
         """
-        return asyncio.get_event_loop().run_until_complete(self._download_segments_async(segments))
-
-    async def _download_segments_async(self, segments: SegmentList[Segment]) -> list[bytes]:
-        """
-        Download M3U8 segments asynchronously.
+        Parse the language name from an M3U8 Media object.
+        Can be overridden in subclasses for normalization.
 
         Args:
-            segments (m3u8.SegmentList[m3u8.Segment]): List of segments to download.
+            media_data (m3u8.Media): Media object to parse the language name from.
 
         Returns:
-            list[bytes]: List of downloaded segments.
-        """
-        async_tasks = [
-            self._download_segment_async(url=segment.absolute_uri)
-            for segment in segments
-        ]
-
-        return await asyncio.gather(*async_tasks)
-
-    async def _download_segment_async(self, url: str) -> bytes:
+            str: The language name.
         """
-        Download an M3U8 segment asynchronously.
-
-        Args:
-            url (str): URL of the segment to download.
+        return media_data.name
 
-        Returns:
-            bytes: Downloaded segment.
-        """
-        response = await self._async_session.get(url)
-        return response.content
-
-    def load_m3u8(self, url: str | list[str], headers: dict | None = None) -> M3U8 | None:
+    async def load_m3u8(self, url: str | list[str], headers: dict | None = None) -> M3U8 | None:
         """
         Load an M3U8 playlist from a URL to an M3U8 object.
         Multiple URLs can be given, in which case the first one that loads successfully will be returned.
@@ -354,10 +318,11 @@
             m3u8.M3U8: An M3U8 object representing the playlist.
         """
         _headers = headers or self._session.headers
+        result: M3U8 | None = None
 
         for url_item in single_to_list(url):
             try:
-                response = self._session.get(url=url_item, headers=_headers)
+                response = await self._async_session.get(url=url_item, headers=_headers, timeout=5)
 
             except Exception as e:
                 logger.debug(f"Failed to load M3U8 playlist '{url_item}': {e}")
@@ -366,9 +331,10 @@
             if not response.text:
                 raise PlaylistLoadError("Received empty response for playlist from server.")
 
-            return m3u8.loads(content=response.text, uri=url_item)
+            result = m3u8.loads(content=response.text, uri=url_item)
+            break
 
-        return None
+        return result
 
     @staticmethod
     def detect_subtitles_type(subtitles_media: Media) -> SubtitlesType | None:
@@ -389,10 +355,99 @@
 
         return None
 
+    async def download_matching_subtitles(self, main_playlist: str | list[str],
+                                          language_filter: list[str] | str | None = None,
+                                          subrip_conversion: bool = False) -> list[Task[SubtitlesData | SubtitlesDownloadError]]:
+        language_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None
+        logger.debug(f"Fetching main M3U8 playlist from: {main_playlist}.")  # TODO: Remove
+        main_playlist_m3u8 = await self.load_m3u8(url=main_playlist)
+
+        if main_playlist_m3u8 is None:
+            raise PlaylistLoadError("Could not load main M3U8 playlist.")
+
+        playlist_filters = (merge_dict_values(self._subtitles_filters, language_filters)
+                            if language_filters
+                            else self._subtitles_filters)
+
+        matching_media_items = self.get_media_playlists(main_playlist=main_playlist_m3u8,
+                                                        playlist_filters=playlist_filters)
+
+        if not matching_media_items:
+            logger.debug("No matching subtitles were found.")
+            return []
+
+        return [
+            asyncio.create_task(self.download_subtitles(media_data=matching_media, subrip_conversion=subrip_conversion))
+            for matching_media in matching_media_items]
+
+    async def download_subtitles(self, media_data: Media, subrip_conversion: bool = False) -> SubtitlesData:
+        logger.debug(f"Downloading subtitles from: {media_data.absolute_uri}.")  # TODO: Remove this line.
+        playlist_m3u8 = await self.load_m3u8(url=media_data.absolute_uri)
+
+        if playlist_m3u8 is None:
+            raise PlaylistLoadError("Could not load subtitles M3U8 playlist.")
+
+        from timeit import default_timer as timer  # TODO: Remove
+        start = timer()  # TODO: Remove
+        downloaded_segments = await self.download_subtitles_segments(playlist=playlist_m3u8)
+        logger.debug(f"Downloaded subtitles segments in {timer() - start:.2f} seconds.")  # TODO: Remove
+        subtitles = self.subtitles_class(data=downloaded_segments[0], language_code=media_data.language)
+
+        if len(downloaded_segments) > 1:
+            for segment_data in downloaded_segments[1:]:
+                segment_subtitles_obj = self.subtitles_class(data=segment_data, language_code=media_data.language)
+                subtitles.append_subtitles(segment_subtitles_obj)
+
+        subtitles.polish(
+            fix_rtl=self.subtitles_fix_rtl,
+            remove_duplicates=self.subtitles_remove_duplicates,
+        )
+
+        if subrip_conversion:
+            subtitles_format = SubtitlesFormatType.SUBRIP
+            content = subtitles.to_srt().dump()
+
+        else:
+            subtitles_format = SubtitlesFormatType.WEBVTT
+            content = subtitles.dump()
+
+        return SubtitlesData(
+            language_code=media_data.language,
+            language_name=self.parse_language_name(media_data=media_data),
+            subtitles_format=subtitles_format,
+            content=content,
+            content_encoding=subtitles.encoding,
+            special_type=self.detect_subtitles_type(subtitles_media=media_data),
+        )
+
+    async def download_subtitles_segments(self, playlist: M3U8) -> list[bytes]:
+        from timeit import default_timer as timer  # TODO: Remove
+        start = timer()  # TODO: Remove
+
+        responses = await asyncio.gather(
+            *[
+                self._async_session.get(url=segment.absolute_uri)
+                for segment in playlist.segments
+            ],
+        )
+
+        responses_data = []
+
+        for result in responses:
+            try:
+                result.raise_for_status()
+                responses_data.append(result.content)
+
+            except Exception as e:
+                raise DownloadError("Failed to download one of the subtitles segments.") from e
+
+        logger.debug(f"Downloaded subtitles segments in {timer() - start:.2f} seconds.")  # TODO: Remove
+        return responses_data
+
     def get_media_playlists(self, main_playlist: M3U8,
                             playlist_filters: dict[str, str | list[str]] | None = None) -> list[Media]:
         """
-        Find and yield playlists of media within an M3U8 main_playlist using optional filters.
+        Find and return a list of media playlists within an M3U8 main_playlist using optional filters.
 
         Args:
             main_playlist (m3u8.M3U8): An M3U8 object of the main main_playlist.
@@ -405,7 +460,7 @@
         """
         results = []
         config_filters: dict | None = self.config.get(self._playlist_filters_config_category)
-        # Merge filtering dictionaries to a single dictionary
+        # Merge filtering dictionaries into a single dictionary
         playlist_filters = merge_dict_values(*[item for item in (playlist_filters, config_filters) if item])
 
         for media in main_playlist.media:
@@ -612,6 +667,10 @@
 
 class ScraperError(Exception):
     pass
+
+
+class DownloadError(ScraperError):
+    pass
 
 
 class PlaylistLoadError(ScraperError):
Index: poetry.lock
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is automatically @generated by Poetry 1.7.1 and should not be changed by hand.\r\n\r\n[[package]]\r\nname = \"annotated-types\"\r\nversion = \"0.6.0\"\r\ndescription = \"Reusable constraint types to use with typing.Annotated\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"annotated_types-0.6.0-py3-none-any.whl\", hash = \"sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43\"},\r\n    {file = \"annotated_types-0.6.0.tar.gz\", hash = \"sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\"},\r\n]\r\n\r\n[package.dependencies]\r\ntyping-extensions = {version = \">=4.0.0\", markers = \"python_version < \\\"3.9\\\"\"}\r\n\r\n[[package]]\r\nname = \"anyio\"\r\nversion = \"4.3.0\"\r\ndescription = \"High level compatibility layer for multiple asynchronous event loop implementations\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"anyio-4.3.0-py3-none-any.whl\", hash = \"sha256:048e05d0f6caeed70d731f3db756d35dcc1f35747c8c403364a8332c630441b8\"},\r\n    {file = \"anyio-4.3.0.tar.gz\", hash = \"sha256:f75253795a87df48568485fd18cdd2a3fa5c4f7c5be8e5e36637733fce06fed6\"},\r\n]\r\n\r\n[package.dependencies]\r\nexceptiongroup = {version = \">=1.0.2\", markers = \"python_version < \\\"3.11\\\"\"}\r\nidna = \">=2.8\"\r\nsniffio = \">=1.1\"\r\ntyping-extensions = {version = \">=4.1\", markers = \"python_version < \\\"3.11\\\"\"}\r\n\r\n[package.extras]\r\ndoc = [\"Sphinx (>=7)\", \"packaging\", \"sphinx-autodoc-typehints (>=1.2.0)\", \"sphinx-rtd-theme\"]\r\ntest = [\"anyio[trio]\", \"coverage[toml] (>=7)\", \"exceptiongroup (>=1.2.0)\", \"hypothesis (>=4.0)\", \"psutil (>=5.9)\", \"pytest (>=7.0)\", \"pytest-mock (>=3.6.1)\", \"trustme\", \"uvloop (>=0.17)\"]\r\ntrio = [\"trio (>=0.23)\"]\r\n\r\n[[package]]\r\nname = \"backports-datetime-fromisoformat\"\r\nversion = \"2.0.1\"\r\ndescription = \"Backport of Python 3.11's datetime.fromisoformat\"\r\noptional = false\r\npython-versions = \">3\"\r\nfiles = [\r\n    {file = \"backports-datetime-fromisoformat-2.0.1.tar.gz\", hash = \"sha256:1b6afca7f47019c22df43062cde73c1af65fbdebc66520f352c690d52fd27127\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:b739ccd3f36244f618f1fbc21d89894d9dc9d1d75a68762fcf917d433df38ae3\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:afd072ca32f2ca4e838e0f7b61a56168d98837ee9a182c567a49a834e07c2b98\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:1a136d85f8b1db4747aa9e56a8caa0ba77c5c25b761b18e2169ea7b1b516f012\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5d3a0579958ade7db62c8238163e05d46a4de61c99cebb40031ed7409a44d5f6\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:199df62af8feff5da0f4953fdc4a6994bcd7dbfe1db95901d8b93d05feda2ab5\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:afe32e60a471831058ede14fc226d9f14120e6dc67d66fbbd36e1724826ad70b\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:a1ba7e226a9694b20b713867f71b5ed2f662603c39875f14f968608d331fc96a\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:403f155deecbe94d43d0679a74abb5c9ac441422a9ececcfde030fb133865659\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b4d2ee049997d3aa2e714489cb3c34864fb0f25786e7a4ff04ac9d82af58b453\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:20aa422769af9f72ca41d83238d4a3a008d6cd74bcff0a08befb11b0018d6aa5\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:8ea8d85c3c9afa4ad51b6644d26516d43493f44c2131c12a2ba959433f4417f6\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp37-cp37m-win_amd64.whl\", hash = \"sha256:812b8c34e88a7d9615c604f1a0473a4e6d664aba94086bffb0c55627f9e3fb68\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:df5365930320b7a9d404cd6f7bc13988e28355e812aa42e21aa5c93443dcdd2e\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:fe3e3968c8dce4a44da2da81a6031e992a4ee62d130c2536696d215a4db2ce3c\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:36a4abb678ab0d6a1965d70e21e424bcf7a52086a7afb1c5f13243a3d44fa2dd\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:96b7e806ade09a91d8ce195c197fc799d8fbe6b8ea9cde21f8a01f1090e51e33\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:002a77bd4f549ff5e80f1ef4a9b69982746dd6190786b90abe3d9c69c9883ce4\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:7b4ad0013a96b656ebf85079921ffb211623a1e28ff4981b3927690a2ed6df54\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:065421723e735ce8f68dbb4486f07562ce8556ed543ceaa012189b9aa209f303\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:a4bf1bec08bc84095ee379202466c948fe12cff1442f58ee1a91fac4c5164c97\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:1836aff09b8317e179cc7288856b61a450515d4b411f0ab723dc9692dfa5362e\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:815f85a581b142bcf34632c3ce26f7e21003f101ce88b5649631330e334bbe35\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6a6986cfd3bc40b41465a6c54c18a30ca8110333d0b71f6062af136db11c8ff0\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:82741e732d71f78b44a8c3b95f33b3630e7bfbdb02e3fede3938cdf15d5b6a83\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:4eac27abb51ee84e08d1dd1e908c16cae2078c217ff5b54092e6cb92107b4c6c\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:3b730d72061523be9600bcd281ef353f7f73b1df095adbbdc364aac8f430c44c\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-pp310-pypy310_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f6e8f28f4a68539192473f427ed86794931502d186e2fffa1926250550c1335a\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-pp37-pypy37_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0cef151f1df77e413dc179607edb5bee11949ca5890e81c0bb742d96fec753fe\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-pp38-pypy38_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c28c95d6df2a44fa3540e18e484596c03e8ff7112e2f93b664f482fe3a88720b\"},\r\n    {file = \"backports_datetime_fromisoformat-2.0.1-pp39-pypy39_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:91042b53de903e3725209ad6d69b6994ae4819614c0decd62d05dfea23f35e2b\"},\r\n]\r\n\r\n[[package]]\r\nname = \"certifi\"\r\nversion = \"2024.2.2\"\r\ndescription = \"Python package for providing Mozilla's CA Bundle.\"\r\noptional = false\r\npython-versions = \">=3.6\"\r\nfiles = [\r\n    {file = \"certifi-2024.2.2-py3-none-any.whl\", hash = \"sha256:dc383c07b76109f368f6106eee2b593b04a011ea4d55f652c6ca24a754d1cdd1\"},\r\n    {file = \"certifi-2024.2.2.tar.gz\", hash = \"sha256:0569859f95fc761b18b45ef421b1290a0f65f147e92a1e5eb3e635f9a5e4e66f\"},\r\n]\r\n\r\n[[package]]\r\nname = \"charset-normalizer\"\r\nversion = \"3.3.2\"\r\ndescription = \"The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\"\r\noptional = false\r\npython-versions = \">=3.7.0\"\r\nfiles = [\r\n    {file = \"charset-normalizer-3.3.2.tar.gz\", hash = \"sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-win32.whl\", hash = \"sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73\"},\r\n    {file = \"charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-win32.whl\", hash = \"sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab\"},\r\n    {file = \"charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_s390x.whl\", hash = \"sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-win32.whl\", hash = \"sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7\"},\r\n    {file = \"charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-win32.whl\", hash = \"sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4\"},\r\n    {file = \"charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-win32.whl\", hash = \"sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25\"},\r\n    {file = \"charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-win32.whl\", hash = \"sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f\"},\r\n    {file = \"charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d\"},\r\n    {file = \"charset_normalizer-3.3.2-py3-none-any.whl\", hash = \"sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc\"},\r\n]\r\n\r\n[[package]]\r\nname = \"exceptiongroup\"\r\nversion = \"1.2.1\"\r\ndescription = \"Backport of PEP 654 (exception groups)\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"exceptiongroup-1.2.1-py3-none-any.whl\", hash = \"sha256:5258b9ed329c5bbdd31a309f53cbfb0b155341807f6ff7606a1e801a891b29ad\"},\r\n    {file = \"exceptiongroup-1.2.1.tar.gz\", hash = \"sha256:a4785e48b045528f5bfe627b6ad554ff32def154f42372786903b7abcfe1aa16\"},\r\n]\r\n\r\n[package.extras]\r\ntest = [\"pytest (>=6)\"]\r\n\r\n[[package]]\r\nname = \"h11\"\r\nversion = \"0.14.0\"\r\ndescription = \"A pure-Python, bring-your-own-I/O implementation of HTTP/1.1\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"h11-0.14.0-py3-none-any.whl\", hash = \"sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761\"},\r\n    {file = \"h11-0.14.0.tar.gz\", hash = \"sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d\"},\r\n]\r\n\r\n[[package]]\r\nname = \"h2\"\r\nversion = \"4.1.0\"\r\ndescription = \"HTTP/2 State-Machine based protocol implementation\"\r\noptional = false\r\npython-versions = \">=3.6.1\"\r\nfiles = [\r\n    {file = \"h2-4.1.0-py3-none-any.whl\", hash = \"sha256:03a46bcf682256c95b5fd9e9a99c1323584c3eec6440d379b9903d709476bc6d\"},\r\n    {file = \"h2-4.1.0.tar.gz\", hash = \"sha256:a83aca08fbe7aacb79fec788c9c0bac936343560ed9ec18b82a13a12c28d2abb\"},\r\n]\r\n\r\n[package.dependencies]\r\nhpack = \">=4.0,<5\"\r\nhyperframe = \">=6.0,<7\"\r\n\r\n[[package]]\r\nname = \"hpack\"\r\nversion = \"4.0.0\"\r\ndescription = \"Pure-Python HPACK header compression\"\r\noptional = false\r\npython-versions = \">=3.6.1\"\r\nfiles = [\r\n    {file = \"hpack-4.0.0-py3-none-any.whl\", hash = \"sha256:84a076fad3dc9a9f8063ccb8041ef100867b1878b25ef0ee63847a5d53818a6c\"},\r\n    {file = \"hpack-4.0.0.tar.gz\", hash = \"sha256:fc41de0c63e687ebffde81187a948221294896f6bdc0ae2312708df339430095\"},\r\n]\r\n\r\n[[package]]\r\nname = \"httpcore\"\r\nversion = \"1.0.5\"\r\ndescription = \"A minimal low-level HTTP client.\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"httpcore-1.0.5-py3-none-any.whl\", hash = \"sha256:421f18bac248b25d310f3cacd198d55b8e6125c107797b609ff9b7a6ba7991b5\"},\r\n    {file = \"httpcore-1.0.5.tar.gz\", hash = \"sha256:34a38e2f9291467ee3b44e89dd52615370e152954ba21721378a87b2960f7a61\"},\r\n]\r\n\r\n[package.dependencies]\r\ncertifi = \"*\"\r\nh11 = \">=0.13,<0.15\"\r\n\r\n[package.extras]\r\nasyncio = [\"anyio (>=4.0,<5.0)\"]\r\nhttp2 = [\"h2 (>=3,<5)\"]\r\nsocks = [\"socksio (==1.*)\"]\r\ntrio = [\"trio (>=0.22.0,<0.26.0)\"]\r\n\r\n[[package]]\r\nname = \"httpx\"\r\nversion = \"0.27.0\"\r\ndescription = \"The next generation HTTP client.\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"httpx-0.27.0-py3-none-any.whl\", hash = \"sha256:71d5465162c13681bff01ad59b2cc68dd838ea1f10e51574bac27103f00c91a5\"},\r\n    {file = \"httpx-0.27.0.tar.gz\", hash = \"sha256:a0cb88a46f32dc874e04ee956e4c2764aba2aa228f650b06788ba6bda2962ab5\"},\r\n]\r\n\r\n[package.dependencies]\r\nanyio = \"*\"\r\ncertifi = \"*\"\r\nh2 = {version = \">=3,<5\", optional = true, markers = \"extra == \\\"http2\\\"\"}\r\nhttpcore = \"==1.*\"\r\nidna = \"*\"\r\nsniffio = \"*\"\r\n\r\n[package.extras]\r\nbrotli = [\"brotli\", \"brotlicffi\"]\r\ncli = [\"click (==8.*)\", \"pygments (==2.*)\", \"rich (>=10,<14)\"]\r\nhttp2 = [\"h2 (>=3,<5)\"]\r\nsocks = [\"socksio (==1.*)\"]\r\n\r\n[[package]]\r\nname = \"hyperframe\"\r\nversion = \"6.0.1\"\r\ndescription = \"HTTP/2 framing layer for Python\"\r\noptional = false\r\npython-versions = \">=3.6.1\"\r\nfiles = [\r\n    {file = \"hyperframe-6.0.1-py3-none-any.whl\", hash = \"sha256:0ec6bafd80d8ad2195c4f03aacba3a8265e57bc4cff261e802bf39970ed02a15\"},\r\n    {file = \"hyperframe-6.0.1.tar.gz\", hash = \"sha256:ae510046231dc8e9ecb1a6586f63d2347bf4c8905914aa84ba585ae85f28a914\"},\r\n]\r\n\r\n[[package]]\r\nname = \"idna\"\r\nversion = \"3.7\"\r\ndescription = \"Internationalized Domain Names in Applications (IDNA)\"\r\noptional = false\r\npython-versions = \">=3.5\"\r\nfiles = [\r\n    {file = \"idna-3.7-py3-none-any.whl\", hash = \"sha256:82fee1fc78add43492d3a1898bfa6d8a904cc97d8427f683ed8e798d07761aa0\"},\r\n    {file = \"idna-3.7.tar.gz\", hash = \"sha256:028ff3aadf0609c1fd278d8ea3089299412a7a8b9bd005dd08b9f8285bcb5cfc\"},\r\n]\r\n\r\n[[package]]\r\nname = \"m3u8\"\r\nversion = \"4.1.0\"\r\ndescription = \"Python m3u8 parser\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"m3u8-4.1.0-py3-none-any.whl\", hash = \"sha256:981daed09f57b7590721b6437278e49f2c36c1bceaa8fbe48f585e1745571d17\"},\r\n    {file = \"m3u8-4.1.0.tar.gz\", hash = \"sha256:3b9d7e5bafbaae89f2464cb16f397887d8decf6b1b48d8de58711414dc1c7b45\"},\r\n]\r\n\r\n[package.dependencies]\r\nbackports-datetime-fromisoformat = {version = \"*\", markers = \"python_version < \\\"3.11\\\"\"}\r\n\r\n[[package]]\r\nname = \"mergedeep\"\r\nversion = \"1.3.4\"\r\ndescription = \"A deep merge function for \uD83D\uDC0D.\"\r\noptional = false\r\npython-versions = \">=3.6\"\r\nfiles = [\r\n    {file = \"mergedeep-1.3.4-py3-none-any.whl\", hash = \"sha256:70775750742b25c0d8f36c55aed03d24c3384d17c951b3175d898bd778ef0307\"},\r\n    {file = \"mergedeep-1.3.4.tar.gz\", hash = \"sha256:0096d52e9dad9939c3d975a774666af186eda617e6ca84df4c94dec30004f2a8\"},\r\n]\r\n\r\n[[package]]\r\nname = \"mypy\"\r\nversion = \"1.10.0\"\r\ndescription = \"Optional static typing for Python\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"mypy-1.10.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:da1cbf08fb3b851ab3b9523a884c232774008267b1f83371ace57f412fe308c2\"},\r\n    {file = \"mypy-1.10.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:12b6bfc1b1a66095ab413160a6e520e1dc076a28f3e22f7fb25ba3b000b4ef99\"},\r\n    {file = \"mypy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9e36fb078cce9904c7989b9693e41cb9711e0600139ce3970c6ef814b6ebc2b2\"},\r\n    {file = \"mypy-1.10.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:2b0695d605ddcd3eb2f736cd8b4e388288c21e7de85001e9f85df9187f2b50f9\"},\r\n    {file = \"mypy-1.10.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:cd777b780312ddb135bceb9bc8722a73ec95e042f911cc279e2ec3c667076051\"},\r\n    {file = \"mypy-1.10.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:3be66771aa5c97602f382230165b856c231d1277c511c9a8dd058be4784472e1\"},\r\n    {file = \"mypy-1.10.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:8b2cbaca148d0754a54d44121b5825ae71868c7592a53b7292eeb0f3fdae95ee\"},\r\n    {file = \"mypy-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1ec404a7cbe9fc0e92cb0e67f55ce0c025014e26d33e54d9e506a0f2d07fe5de\"},\r\n    {file = \"mypy-1.10.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:e22e1527dc3d4aa94311d246b59e47f6455b8729f4968765ac1eacf9a4760bc7\"},\r\n    {file = \"mypy-1.10.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:a87dbfa85971e8d59c9cc1fcf534efe664d8949e4c0b6b44e8ca548e746a8d53\"},\r\n    {file = \"mypy-1.10.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:a781f6ad4bab20eef8b65174a57e5203f4be627b46291f4589879bf4e257b97b\"},\r\n    {file = \"mypy-1.10.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:b808e12113505b97d9023b0b5e0c0705a90571c6feefc6f215c1df9381256e30\"},\r\n    {file = \"mypy-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8f55583b12156c399dce2df7d16f8a5095291354f1e839c252ec6c0611e86e2e\"},\r\n    {file = \"mypy-1.10.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:4cf18f9d0efa1b16478c4c129eabec36148032575391095f73cae2e722fcf9d5\"},\r\n    {file = \"mypy-1.10.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:bc6ac273b23c6b82da3bb25f4136c4fd42665f17f2cd850771cb600bdd2ebeda\"},\r\n    {file = \"mypy-1.10.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:9fd50226364cd2737351c79807775136b0abe084433b55b2e29181a4c3c878c0\"},\r\n    {file = \"mypy-1.10.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:f90cff89eea89273727d8783fef5d4a934be2fdca11b47def50cf5d311aff727\"},\r\n    {file = \"mypy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fcfc70599efde5c67862a07a1aaf50e55bce629ace26bb19dc17cece5dd31ca4\"},\r\n    {file = \"mypy-1.10.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:075cbf81f3e134eadaf247de187bd604748171d6b79736fa9b6c9685b4083061\"},\r\n    {file = \"mypy-1.10.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:3f298531bca95ff615b6e9f2fc0333aae27fa48052903a0ac90215021cdcfa4f\"},\r\n    {file = \"mypy-1.10.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:fa7ef5244615a2523b56c034becde4e9e3f9b034854c93639adb667ec9ec2976\"},\r\n    {file = \"mypy-1.10.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:3236a4c8f535a0631f85f5fcdffba71c7feeef76a6002fcba7c1a8e57c8be1ec\"},\r\n    {file = \"mypy-1.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4a2b5cdbb5dd35aa08ea9114436e0d79aceb2f38e32c21684dcf8e24e1e92821\"},\r\n    {file = \"mypy-1.10.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:92f93b21c0fe73dc00abf91022234c79d793318b8a96faac147cd579c1671746\"},\r\n    {file = \"mypy-1.10.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:28d0e038361b45f099cc086d9dd99c15ff14d0188f44ac883010e172ce86c38a\"},\r\n    {file = \"mypy-1.10.0-py3-none-any.whl\", hash = \"sha256:f8c083976eb530019175aabadb60921e73b4f45736760826aa1689dda8208aee\"},\r\n    {file = \"mypy-1.10.0.tar.gz\", hash = \"sha256:3d087fcbec056c4ee34974da493a826ce316947485cef3901f511848e687c131\"},\r\n]\r\n\r\n[package.dependencies]\r\nmypy-extensions = \">=1.0.0\"\r\ntomli = {version = \">=1.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\r\ntyping-extensions = \">=4.1.0\"\r\n\r\n[package.extras]\r\ndmypy = [\"psutil (>=4.0)\"]\r\ninstall-types = [\"pip\"]\r\nmypyc = [\"setuptools (>=50)\"]\r\nreports = [\"lxml\"]\r\n\r\n[[package]]\r\nname = \"mypy-extensions\"\r\nversion = \"1.0.0\"\r\ndescription = \"Type system extensions for programs checked with the mypy type checker.\"\r\noptional = false\r\npython-versions = \">=3.5\"\r\nfiles = [\r\n    {file = \"mypy_extensions-1.0.0-py3-none-any.whl\", hash = \"sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d\"},\r\n    {file = \"mypy_extensions-1.0.0.tar.gz\", hash = \"sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782\"},\r\n]\r\n\r\n[[package]]\r\nname = \"pydantic\"\r\nversion = \"2.7.1\"\r\ndescription = \"Data validation using Python type hints\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"pydantic-2.7.1-py3-none-any.whl\", hash = \"sha256:e029badca45266732a9a79898a15ae2e8b14840b1eabbb25844be28f0b33f3d5\"},\r\n    {file = \"pydantic-2.7.1.tar.gz\", hash = \"sha256:e9dbb5eada8abe4d9ae5f46b9939aead650cd2b68f249bb3a8139dbe125803cc\"},\r\n]\r\n\r\n[package.dependencies]\r\nannotated-types = \">=0.4.0\"\r\npydantic-core = \"2.18.2\"\r\ntyping-extensions = \">=4.6.1\"\r\n\r\n[package.extras]\r\nemail = [\"email-validator (>=2.0.0)\"]\r\n\r\n[[package]]\r\nname = \"pydantic-core\"\r\nversion = \"2.18.2\"\r\ndescription = \"Core functionality for Pydantic validation and serialization\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:9e08e867b306f525802df7cd16c44ff5ebbe747ff0ca6cf3fde7f36c05a59a81\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:f0a21cbaa69900cbe1a2e7cad2aa74ac3cf21b10c3efb0fa0b80305274c0e8a2\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0680b1f1f11fda801397de52c36ce38ef1c1dc841a0927a94f226dea29c3ae3d\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:95b9d5e72481d3780ba3442eac863eae92ae43a5f3adb5b4d0a1de89d42bb250\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c4fcf5cd9c4b655ad666ca332b9a081112cd7a58a8b5a6ca7a3104bc950f2038\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9b5155ff768083cb1d62f3e143b49a8a3432e6789a3abee8acd005c3c7af1c74\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:553ef617b6836fc7e4df130bb851e32fe357ce36336d897fd6646d6058d980af\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:b89ed9eb7d616ef5714e5590e6cf7f23b02d0d539767d33561e3675d6f9e3857\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:75f7e9488238e920ab6204399ded280dc4c307d034f3924cd7f90a38b1829563\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:ef26c9e94a8c04a1b2924149a9cb081836913818e55681722d7f29af88fe7b38\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-none-win32.whl\", hash = \"sha256:182245ff6b0039e82b6bb585ed55a64d7c81c560715d1bad0cbad6dfa07b4027\"},\r\n    {file = \"pydantic_core-2.18.2-cp310-none-win_amd64.whl\", hash = \"sha256:e23ec367a948b6d812301afc1b13f8094ab7b2c280af66ef450efc357d2ae543\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:219da3f096d50a157f33645a1cf31c0ad1fe829a92181dd1311022f986e5fbe3\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:cc1cfd88a64e012b74e94cd00bbe0f9c6df57049c97f02bb07d39e9c852e19a4\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:05b7133a6e6aeb8df37d6f413f7705a37ab4031597f64ab56384c94d98fa0e90\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:224c421235f6102e8737032483f43c1a8cfb1d2f45740c44166219599358c2cd\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b14d82cdb934e99dda6d9d60dc84a24379820176cc4a0d123f88df319ae9c150\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2728b01246a3bba6de144f9e3115b532ee44bd6cf39795194fb75491824a1413\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:470b94480bb5ee929f5acba6995251ada5e059a5ef3e0dfc63cca287283ebfa6\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:997abc4df705d1295a42f95b4eec4950a37ad8ae46d913caeee117b6b198811c\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:75250dbc5290e3f1a0f4618db35e51a165186f9034eff158f3d490b3fed9f8a0\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:4456f2dca97c425231d7315737d45239b2b51a50dc2b6f0c2bb181fce6207664\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-none-win32.whl\", hash = \"sha256:269322dcc3d8bdb69f054681edff86276b2ff972447863cf34c8b860f5188e2e\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-none-win_amd64.whl\", hash = \"sha256:800d60565aec896f25bc3cfa56d2277d52d5182af08162f7954f938c06dc4ee3\"},\r\n    {file = \"pydantic_core-2.18.2-cp311-none-win_arm64.whl\", hash = \"sha256:1404c69d6a676245199767ba4f633cce5f4ad4181f9d0ccb0577e1f66cf4c46d\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:fb2bd7be70c0fe4dfd32c951bc813d9fe6ebcbfdd15a07527796c8204bd36242\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:6132dd3bd52838acddca05a72aafb6eab6536aa145e923bb50f45e78b7251043\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d7d904828195733c183d20a54230c0df0eb46ec746ea1a666730787353e87182\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:c9bd70772c720142be1020eac55f8143a34ec9f82d75a8e7a07852023e46617f\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:2b8ed04b3582771764538f7ee7001b02e1170223cf9b75dff0bc698fadb00cf3\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:e6dac87ddb34aaec85f873d737e9d06a3555a1cc1a8e0c44b7f8d5daeb89d86f\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7ca4ae5a27ad7a4ee5170aebce1574b375de390bc01284f87b18d43a3984df72\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:886eec03591b7cf058467a70a87733b35f44707bd86cf64a615584fd72488b7c\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:ca7b0c1f1c983e064caa85f3792dd2fe3526b3505378874afa84baf662e12241\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:4b4356d3538c3649337df4074e81b85f0616b79731fe22dd11b99499b2ebbdf3\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-none-win32.whl\", hash = \"sha256:8b172601454f2d7701121bbec3425dd71efcb787a027edf49724c9cefc14c038\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-none-win_amd64.whl\", hash = \"sha256:b1bd7e47b1558ea872bd16c8502c414f9e90dcf12f1395129d7bb42a09a95438\"},\r\n    {file = \"pydantic_core-2.18.2-cp312-none-win_arm64.whl\", hash = \"sha256:98758d627ff397e752bc339272c14c98199c613f922d4a384ddc07526c86a2ec\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:9fdad8e35f278b2c3eb77cbdc5c0a49dada440657bf738d6905ce106dc1de439\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:1d90c3265ae107f91a4f279f4d6f6f1d4907ac76c6868b27dc7fb33688cfb347\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:390193c770399861d8df9670fb0d1874f330c79caaca4642332df7c682bf6b91\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:82d5d4d78e4448683cb467897fe24e2b74bb7b973a541ea1dcfec1d3cbce39fb\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:4774f3184d2ef3e14e8693194f661dea5a4d6ca4e3dc8e39786d33a94865cefd\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d4d938ec0adf5167cb335acb25a4ee69a8107e4984f8fbd2e897021d9e4ca21b\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e0e8b1be28239fc64a88a8189d1df7fad8be8c1ae47fcc33e43d4be15f99cc70\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:868649da93e5a3d5eacc2b5b3b9235c98ccdbfd443832f31e075f54419e1b96b\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:78363590ef93d5d226ba21a90a03ea89a20738ee5b7da83d771d283fd8a56761\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:852e966fbd035a6468fc0a3496589b45e2208ec7ca95c26470a54daed82a0788\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-none-win32.whl\", hash = \"sha256:6a46e22a707e7ad4484ac9ee9f290f9d501df45954184e23fc29408dfad61350\"},\r\n    {file = \"pydantic_core-2.18.2-cp38-none-win_amd64.whl\", hash = \"sha256:d91cb5ea8b11607cc757675051f61b3d93f15eca3cefb3e6c704a5d6e8440f4e\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:ae0a8a797a5e56c053610fa7be147993fe50960fa43609ff2a9552b0e07013e8\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:042473b6280246b1dbf530559246f6842b56119c2926d1e52b631bdc46075f2a\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1a388a77e629b9ec814c1b1e6b3b595fe521d2cdc625fcca26fbc2d44c816804\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:e25add29b8f3b233ae90ccef2d902d0ae0432eb0d45370fe315d1a5cf231004b\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:f459a5ce8434614dfd39bbebf1041952ae01da6bed9855008cb33b875cb024c0\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:eff2de745698eb46eeb51193a9f41d67d834d50e424aef27df2fcdee1b153845\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a8309f67285bdfe65c372ea3722b7a5642680f3dba538566340a9d36e920b5f0\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:f93a8a2e3938ff656a7c1bc57193b1319960ac015b6e87d76c76bf14fe0244b4\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:22057013c8c1e272eb8d0eebc796701167d8377441ec894a8fed1af64a0bf399\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:cfeecd1ac6cc1fb2692c3d5110781c965aabd4ec5d32799773ca7b1456ac636b\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-none-win32.whl\", hash = \"sha256:0d69b4c2f6bb3e130dba60d34c0845ba31b69babdd3f78f7c0c8fae5021a253e\"},\r\n    {file = \"pydantic_core-2.18.2-cp39-none-win_amd64.whl\", hash = \"sha256:d9319e499827271b09b4e411905b24a426b8fb69464dfa1696258f53a3334641\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:a1874c6dd4113308bd0eb568418e6114b252afe44319ead2b4081e9b9521fe75\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:ccdd111c03bfd3666bd2472b674c6899550e09e9f298954cfc896ab92b5b0e6d\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e18609ceaa6eed63753037fc06ebb16041d17d28199ae5aba0052c51449650a9\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6e5c584d357c4e2baf0ff7baf44f4994be121e16a2c88918a5817331fc7599d7\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:43f0f463cf89ace478de71a318b1b4f05ebc456a9b9300d027b4b57c1a2064fb\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:e1b395e58b10b73b07b7cf740d728dd4ff9365ac46c18751bf8b3d8cca8f625a\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:0098300eebb1c837271d3d1a2cd2911e7c11b396eac9661655ee524a7f10587b\"},\r\n    {file = \"pydantic_core-2.18.2-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:36789b70d613fbac0a25bb07ab3d9dba4d2e38af609c020cf4d888d165ee0bf3\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:3f9a801e7c8f1ef8718da265bba008fa121243dfe37c1cea17840b0944dfd72c\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:3a6515ebc6e69d85502b4951d89131ca4e036078ea35533bb76327f8424531ce\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:20aca1e2298c56ececfd8ed159ae4dde2df0781988c97ef77d5c16ff4bd5b400\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:223ee893d77a310a0391dca6df00f70bbc2f36a71a895cecd9a0e762dc37b349\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:2334ce8c673ee93a1d6a65bd90327588387ba073c17e61bf19b4fd97d688d63c\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:cbca948f2d14b09d20268cda7b0367723d79063f26c4ffc523af9042cad95592\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:b3ef08e20ec49e02d5c6717a91bb5af9b20f1805583cb0adfe9ba2c6b505b5ae\"},\r\n    {file = \"pydantic_core-2.18.2-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:c6fdc8627910eed0c01aed6a390a252fe3ea6d472ee70fdde56273f198938374\"},\r\n    {file = \"pydantic_core-2.18.2.tar.gz\", hash = \"sha256:2e29d20810dfc3043ee13ac7d9e25105799817683348823f305ab3f349b9386e\"},\r\n]\r\n\r\n[package.dependencies]\r\ntyping-extensions = \">=4.6.0,<4.7.0 || >4.7.0\"\r\n\r\n[[package]]\r\nname = \"requests\"\r\nversion = \"2.31.0\"\r\ndescription = \"Python HTTP for Humans.\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"requests-2.31.0-py3-none-any.whl\", hash = \"sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f\"},\r\n    {file = \"requests-2.31.0.tar.gz\", hash = \"sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\"},\r\n]\r\n\r\n[package.dependencies]\r\ncertifi = \">=2017.4.17\"\r\ncharset-normalizer = \">=2,<4\"\r\nidna = \">=2.5,<4\"\r\nurllib3 = \">=1.21.1,<3\"\r\n\r\n[package.extras]\r\nsocks = [\"PySocks (>=1.5.6,!=1.5.7)\"]\r\nuse-chardet-on-py3 = [\"chardet (>=3.0.2,<6)\"]\r\n\r\n[[package]]\r\nname = \"ruff\"\r\nversion = \"0.4.2\"\r\ndescription = \"An extremely fast Python linter and code formatter, written in Rust.\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"ruff-0.4.2-py3-none-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl\", hash = \"sha256:8d14dc8953f8af7e003a485ef560bbefa5f8cc1ad994eebb5b12136049bbccc5\"},\r\n    {file = \"ruff-0.4.2-py3-none-macosx_10_12_x86_64.whl\", hash = \"sha256:24016ed18db3dc9786af103ff49c03bdf408ea253f3cb9e3638f39ac9cf2d483\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0e2e06459042ac841ed510196c350ba35a9b24a643e23db60d79b2db92af0c2b\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:3afabaf7ba8e9c485a14ad8f4122feff6b2b93cc53cd4dad2fd24ae35112d5c5\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:799eb468ea6bc54b95527143a4ceaf970d5aa3613050c6cff54c85fda3fde480\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl\", hash = \"sha256:ec4ba9436a51527fb6931a8839af4c36a5481f8c19e8f5e42c2f7ad3a49f5069\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:6a2243f8f434e487c2a010c7252150b1fdf019035130f41b77626f5655c9ca22\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8772130a063f3eebdf7095da00c0b9898bd1774c43b336272c3e98667d4fb8fa\"},\r\n    {file = \"ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6ab165ef5d72392b4ebb85a8b0fbd321f69832a632e07a74794c0e598e7a8376\"},\r\n    {file = \"ruff-0.4.2-py3-none-musllinux_1_2_aarch64.whl\", hash = \"sha256:1f32cadf44c2020e75e0c56c3408ed1d32c024766bd41aedef92aa3ca28eef68\"},\r\n    {file = \"ruff-0.4.2-py3-none-musllinux_1_2_armv7l.whl\", hash = \"sha256:22e306bf15e09af45ca812bc42fa59b628646fa7c26072555f278994890bc7ac\"},\r\n    {file = \"ruff-0.4.2-py3-none-musllinux_1_2_i686.whl\", hash = \"sha256:82986bb77ad83a1719c90b9528a9dd663c9206f7c0ab69282af8223566a0c34e\"},\r\n    {file = \"ruff-0.4.2-py3-none-musllinux_1_2_x86_64.whl\", hash = \"sha256:652e4ba553e421a6dc2a6d4868bc3b3881311702633eb3672f9f244ded8908cd\"},\r\n    {file = \"ruff-0.4.2-py3-none-win32.whl\", hash = \"sha256:7891ee376770ac094da3ad40c116258a381b86c7352552788377c6eb16d784fe\"},\r\n    {file = \"ruff-0.4.2-py3-none-win_amd64.whl\", hash = \"sha256:5ec481661fb2fd88a5d6cf1f83403d388ec90f9daaa36e40e2c003de66751798\"},\r\n    {file = \"ruff-0.4.2-py3-none-win_arm64.whl\", hash = \"sha256:cbd1e87c71bca14792948c4ccb51ee61c3296e164019d2d484f3eaa2d360dfaf\"},\r\n    {file = \"ruff-0.4.2.tar.gz\", hash = \"sha256:33bcc160aee2520664bc0859cfeaebc84bb7323becff3f303b8f1f2d81cb4edc\"},\r\n]\r\n\r\n[[package]]\r\nname = \"sniffio\"\r\nversion = \"1.3.1\"\r\ndescription = \"Sniff out which async library your code is running under\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"sniffio-1.3.1-py3-none-any.whl\", hash = \"sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2\"},\r\n    {file = \"sniffio-1.3.1.tar.gz\", hash = \"sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc\"},\r\n]\r\n\r\n[[package]]\r\nname = \"tomli\"\r\nversion = \"2.0.1\"\r\ndescription = \"A lil' TOML parser\"\r\noptional = false\r\npython-versions = \">=3.7\"\r\nfiles = [\r\n    {file = \"tomli-2.0.1-py3-none-any.whl\", hash = \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\"},\r\n    {file = \"tomli-2.0.1.tar.gz\", hash = \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"},\r\n]\r\n\r\n[[package]]\r\nname = \"types-requests\"\r\nversion = \"2.31.0.20240406\"\r\ndescription = \"Typing stubs for requests\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"types-requests-2.31.0.20240406.tar.gz\", hash = \"sha256:4428df33c5503945c74b3f42e82b181e86ec7b724620419a2966e2de604ce1a1\"},\r\n    {file = \"types_requests-2.31.0.20240406-py3-none-any.whl\", hash = \"sha256:6216cdac377c6b9a040ac1c0404f7284bd13199c0e1bb235f4324627e8898cf5\"},\r\n]\r\n\r\n[package.dependencies]\r\nurllib3 = \">=2\"\r\n\r\n[[package]]\r\nname = \"typing-extensions\"\r\nversion = \"4.11.0\"\r\ndescription = \"Backported and Experimental Type Hints for Python 3.8+\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"typing_extensions-4.11.0-py3-none-any.whl\", hash = \"sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a\"},\r\n    {file = \"typing_extensions-4.11.0.tar.gz\", hash = \"sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0\"},\r\n]\r\n\r\n[[package]]\r\nname = \"urllib3\"\r\nversion = \"2.2.1\"\r\ndescription = \"HTTP library with thread-safe connection pooling, file post, and more.\"\r\noptional = false\r\npython-versions = \">=3.8\"\r\nfiles = [\r\n    {file = \"urllib3-2.2.1-py3-none-any.whl\", hash = \"sha256:450b20ec296a467077128bff42b73080516e71b56ff59a60a02bef2232c4fa9d\"},\r\n    {file = \"urllib3-2.2.1.tar.gz\", hash = \"sha256:d0570876c61ab9e520d776c38acbbb5b05a776d3f9ff98a5c8fd5162a444cf19\"},\r\n]\r\n\r\n[package.extras]\r\nbrotli = [\"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\"]\r\nh2 = [\"h2 (>=4,<5)\"]\r\nsocks = [\"pysocks (>=1.5.6,!=1.5.7,<2.0)\"]\r\nzstd = [\"zstandard (>=0.18.0)\"]\r\n\r\n[metadata]\r\nlock-version = \"2.0\"\r\npython-versions = \"^3.8\"\r\ncontent-hash = \"cca87ee43501511b80014c60654703b2dcd1e1588fb61ce5630f1cdb53fd9494\"\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/poetry.lock b/poetry.lock
--- a/poetry.lock	(revision 99fc7a7f8aace4d7305fb2f0777747ee443821b0)
+++ b/poetry.lock	(date 1715678910574)
@@ -319,6 +319,41 @@
 [package.dependencies]
 backports-datetime-fromisoformat = {version = "*", markers = "python_version < \"3.11\""}
 
+[[package]]
+name = "markdown-it-py"
+version = "3.0.0"
+description = "Python port of markdown-it. Markdown parsing, done right!"
+optional = false
+python-versions = ">=3.8"
+files = [
+    {file = "markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb"},
+    {file = "markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1"},
+]
+
+[package.dependencies]
+mdurl = ">=0.1,<1.0"
+
+[package.extras]
+benchmarking = ["psutil", "pytest", "pytest-benchmark"]
+code-style = ["pre-commit (>=3.0,<4.0)"]
+compare = ["commonmark (>=0.9,<1.0)", "markdown (>=3.4,<4.0)", "mistletoe (>=1.0,<2.0)", "mistune (>=2.0,<3.0)", "panflute (>=2.3,<3.0)"]
+linkify = ["linkify-it-py (>=1,<3)"]
+plugins = ["mdit-py-plugins"]
+profiling = ["gprof2dot"]
+rtd = ["jupyter_sphinx", "mdit-py-plugins", "myst-parser", "pyyaml", "sphinx", "sphinx-copybutton", "sphinx-design", "sphinx_book_theme"]
+testing = ["coverage", "pytest", "pytest-cov", "pytest-regressions"]
+
+[[package]]
+name = "mdurl"
+version = "0.1.2"
+description = "Markdown URL utilities"
+optional = false
+python-versions = ">=3.7"
+files = [
+    {file = "mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8"},
+    {file = "mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba"},
+]
+
 [[package]]
 name = "mergedeep"
 version = "1.3.4"
@@ -498,6 +533,20 @@
 [package.dependencies]
 typing-extensions = ">=4.6.0,<4.7.0 || >4.7.0"
 
+[[package]]
+name = "pygments"
+version = "2.18.0"
+description = "Pygments is a syntax highlighting package written in Python."
+optional = false
+python-versions = ">=3.8"
+files = [
+    {file = "pygments-2.18.0-py3-none-any.whl", hash = "sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a"},
+    {file = "pygments-2.18.0.tar.gz", hash = "sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199"},
+]
+
+[package.extras]
+windows-terminal = ["colorama (>=0.4.6)"]
+
 [[package]]
 name = "requests"
 version = "2.31.0"
@@ -519,30 +568,49 @@
 socks = ["PySocks (>=1.5.6,!=1.5.7)"]
 use-chardet-on-py3 = ["chardet (>=3.0.2,<6)"]
 
+[[package]]
+name = "rich"
+version = "13.7.1"
+description = "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal"
+optional = false
+python-versions = ">=3.7.0"
+files = [
+    {file = "rich-13.7.1-py3-none-any.whl", hash = "sha256:4edbae314f59eb482f54e9e30bf00d33350aaa94f4bfcd4e9e3110e64d0d7222"},
+    {file = "rich-13.7.1.tar.gz", hash = "sha256:9be308cb1fe2f1f57d67ce99e95af38a1e2bc71ad9813b0e247cf7ffbcc3a432"},
+]
+
+[package.dependencies]
+markdown-it-py = ">=2.2.0"
+pygments = ">=2.13.0,<3.0.0"
+typing-extensions = {version = ">=4.0.0,<5.0", markers = "python_version < \"3.9\""}
+
+[package.extras]
+jupyter = ["ipywidgets (>=7.5.1,<9)"]
+
 [[package]]
 name = "ruff"
-version = "0.4.2"
+version = "0.4.4"
 description = "An extremely fast Python linter and code formatter, written in Rust."
 optional = false
 python-versions = ">=3.7"
 files = [
-    {file = "ruff-0.4.2-py3-none-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:8d14dc8953f8af7e003a485ef560bbefa5f8cc1ad994eebb5b12136049bbccc5"},
-    {file = "ruff-0.4.2-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:24016ed18db3dc9786af103ff49c03bdf408ea253f3cb9e3638f39ac9cf2d483"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0e2e06459042ac841ed510196c350ba35a9b24a643e23db60d79b2db92af0c2b"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:3afabaf7ba8e9c485a14ad8f4122feff6b2b93cc53cd4dad2fd24ae35112d5c5"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:799eb468ea6bc54b95527143a4ceaf970d5aa3613050c6cff54c85fda3fde480"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:ec4ba9436a51527fb6931a8839af4c36a5481f8c19e8f5e42c2f7ad3a49f5069"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6a2243f8f434e487c2a010c7252150b1fdf019035130f41b77626f5655c9ca22"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:8772130a063f3eebdf7095da00c0b9898bd1774c43b336272c3e98667d4fb8fa"},
-    {file = "ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6ab165ef5d72392b4ebb85a8b0fbd321f69832a632e07a74794c0e598e7a8376"},
-    {file = "ruff-0.4.2-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:1f32cadf44c2020e75e0c56c3408ed1d32c024766bd41aedef92aa3ca28eef68"},
-    {file = "ruff-0.4.2-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:22e306bf15e09af45ca812bc42fa59b628646fa7c26072555f278994890bc7ac"},
-    {file = "ruff-0.4.2-py3-none-musllinux_1_2_i686.whl", hash = "sha256:82986bb77ad83a1719c90b9528a9dd663c9206f7c0ab69282af8223566a0c34e"},
-    {file = "ruff-0.4.2-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:652e4ba553e421a6dc2a6d4868bc3b3881311702633eb3672f9f244ded8908cd"},
-    {file = "ruff-0.4.2-py3-none-win32.whl", hash = "sha256:7891ee376770ac094da3ad40c116258a381b86c7352552788377c6eb16d784fe"},
-    {file = "ruff-0.4.2-py3-none-win_amd64.whl", hash = "sha256:5ec481661fb2fd88a5d6cf1f83403d388ec90f9daaa36e40e2c003de66751798"},
-    {file = "ruff-0.4.2-py3-none-win_arm64.whl", hash = "sha256:cbd1e87c71bca14792948c4ccb51ee61c3296e164019d2d484f3eaa2d360dfaf"},
-    {file = "ruff-0.4.2.tar.gz", hash = "sha256:33bcc160aee2520664bc0859cfeaebc84bb7323becff3f303b8f1f2d81cb4edc"},
+    {file = "ruff-0.4.4-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:29d44ef5bb6a08e235c8249294fa8d431adc1426bfda99ed493119e6f9ea1bf6"},
+    {file = "ruff-0.4.4-py3-none-macosx_11_0_arm64.whl", hash = "sha256:c4efe62b5bbb24178c950732ddd40712b878a9b96b1d02b0ff0b08a090cbd891"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4c8e2f1e8fc12d07ab521a9005d68a969e167b589cbcaee354cb61e9d9de9c15"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:60ed88b636a463214905c002fa3eaab19795679ed55529f91e488db3fe8976ab"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b90fc5e170fc71c712cc4d9ab0e24ea505c6a9e4ebf346787a67e691dfb72e85"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:8e7e6ebc10ef16dcdc77fd5557ee60647512b400e4a60bdc4849468f076f6eef"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b9ddb2c494fb79fc208cd15ffe08f32b7682519e067413dbaf5f4b01a6087bcd"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c51c928a14f9f0a871082603e25a1588059b7e08a920f2f9fa7157b5bf08cfe9"},
+    {file = "ruff-0.4.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b5eb0a4bfd6400b7d07c09a7725e1a98c3b838be557fee229ac0f84d9aa49c36"},
+    {file = "ruff-0.4.4-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:b1867ee9bf3acc21778dcb293db504692eda5f7a11a6e6cc40890182a9f9e595"},
+    {file = "ruff-0.4.4-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:1aecced1269481ef2894cc495647392a34b0bf3e28ff53ed95a385b13aa45768"},
+    {file = "ruff-0.4.4-py3-none-musllinux_1_2_i686.whl", hash = "sha256:9da73eb616b3241a307b837f32756dc20a0b07e2bcb694fec73699c93d04a69e"},
+    {file = "ruff-0.4.4-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:958b4ea5589706a81065e2a776237de2ecc3e763342e5cc8e02a4a4d8a5e6f95"},
+    {file = "ruff-0.4.4-py3-none-win32.whl", hash = "sha256:cb53473849f011bca6e754f2cdf47cafc9c4f4ff4570003a0dad0b9b6890e876"},
+    {file = "ruff-0.4.4-py3-none-win_amd64.whl", hash = "sha256:424e5b72597482543b684c11def82669cc6b395aa8cc69acc1858b5ef3e5daae"},
+    {file = "ruff-0.4.4-py3-none-win_arm64.whl", hash = "sha256:39df0537b47d3b597293edbb95baf54ff5b49589eb7ff41926d8243caa995ea6"},
+    {file = "ruff-0.4.4.tar.gz", hash = "sha256:f87ea42d5cdebdc6a69761a9d0bc83ae9b3b30d0ad78952005ba6568d6c022af"},
 ]
 
 [[package]]
@@ -612,4 +680,4 @@
 [metadata]
 lock-version = "2.0"
 python-versions = "^3.8"
-content-hash = "cca87ee43501511b80014c60654703b2dcd1e1588fb61ce5630f1cdb53fd9494"
+content-hash = "ab72df06738ee4a8e7aaf669e36f2e119021a4e38e41eee72f2cfeb0abf37fd6"
