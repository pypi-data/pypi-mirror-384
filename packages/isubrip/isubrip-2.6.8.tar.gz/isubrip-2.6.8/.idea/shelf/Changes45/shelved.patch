Index: isubrip/scrapers/appletv_scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport datetime as dt\r\nfrom enum import Enum\r\nimport fnmatch\r\n\r\nfrom isubrip.data_structures import Episode, Movie, ScrapedMediaResponse, Season, Series, MediaData\r\nfrom isubrip.logger import logger\r\nfrom isubrip.scrapers.scraper import M3U8Scraper, MovieScraper, ScraperException, SeriesScraper\r\nfrom isubrip.subtitle_formats.webvtt import WebVTTSubtitles\r\nfrom isubrip.utils import convert_epoch_to_datetime, parse_url_params\r\n\r\n\r\nclass AppleTVScraper(M3U8Scraper, MovieScraper, SeriesScraper):\r\n    \"\"\"An Apple TV scraper.\"\"\"\r\n    id = \"appletv\"\r\n    name = \"Apple TV\"  # (iTunes content is redirected to the iTunes scraper)\r\n    abbreviation = \"ATV\"\r\n    url_regex = r\"(?P<base_url>https?://tv\\.apple\\.com/(?:(?P<country_code>[a-z]{2})/)?(?P<media_type>movie|episode|season|show)/(?:(?P<media_name>[\\w\\-%]+)/)?(?P<media_id>umc\\.cmc\\.[a-z\\d]{23,25}))(?:\\?(?P<url_params>(?:).*))?\"  # noqa: E501\r\n    subtitles_class = WebVTTSubtitles\r\n    is_movie_scraper = True\r\n    is_series_scraper = True\r\n    uses_scrapers = [\"itunes\"]\r\n\r\n    _api_base_url = \"https://tv.apple.com/api/uts/v3\"\r\n    _api_base_params = {\r\n        \"utscf\": \"OjAAAAAAAAA~\",\r\n        \"caller\": \"js\",\r\n        \"v\": \"66\",\r\n        \"pfm\": \"web\",\r\n    }\r\n    _default_storefront = \"US\"  # Has to be uppercase\r\n    _storefronts_mapping = {\r\n        \"AF\": \"143610\", \"AO\": \"143564\", \"AI\": \"143538\", \"AL\": \"143575\", \"AD\": \"143611\", \"AE\": \"143481\", \"AR\": \"143505\",\r\n        \"AM\": \"143524\", \"AG\": \"143540\", \"AU\": \"143460\", \"AT\": \"143445\", \"AZ\": \"143568\", \"BE\": \"143446\", \"BJ\": \"143576\",\r\n        \"BF\": \"143578\", \"BD\": \"143490\", \"BG\": \"143526\", \"BH\": \"143559\", \"BS\": \"143539\", \"BA\": \"143612\", \"BY\": \"143565\",\r\n        \"BZ\": \"143555\", \"BM\": \"143542\", \"BO\": \"143556\", \"BR\": \"143503\", \"BB\": \"143541\", \"BN\": \"143560\", \"BT\": \"143577\",\r\n        \"BW\": \"143525\", \"CF\": \"143623\", \"CA\": \"143455\", \"CH\": \"143459\", \"CL\": \"143483\", \"CN\": \"143465\", \"CI\": \"143527\",\r\n        \"CM\": \"143574\", \"CD\": \"143613\", \"CG\": \"143582\", \"CO\": \"143501\", \"CV\": \"143580\", \"CR\": \"143495\", \"KY\": \"143544\",\r\n        \"CY\": \"143557\", \"CZ\": \"143489\", \"DE\": \"143443\", \"DM\": \"143545\", \"DK\": \"143458\", \"DO\": \"143508\", \"DZ\": \"143563\",\r\n        \"EC\": \"143509\", \"EG\": \"143516\", \"ES\": \"143454\", \"EE\": \"143518\", \"ET\": \"143569\", \"FI\": \"143447\", \"FJ\": \"143583\",\r\n        \"FR\": \"143442\", \"FM\": \"143591\", \"GA\": \"143614\", \"GB\": \"143444\", \"GE\": \"143615\", \"GH\": \"143573\", \"GN\": \"143616\",\r\n        \"GM\": \"143584\", \"GW\": \"143585\", \"GR\": \"143448\", \"GD\": \"143546\", \"GT\": \"143504\", \"GY\": \"143553\", \"HK\": \"143463\",\r\n        \"HN\": \"143510\", \"HR\": \"143494\", \"HU\": \"143482\", \"ID\": \"143476\", \"IN\": \"143467\", \"IE\": \"143449\", \"IQ\": \"143617\",\r\n        \"IS\": \"143558\", \"IL\": \"143491\", \"IT\": \"143450\", \"JM\": \"143511\", \"JO\": \"143528\", \"JP\": \"143462\", \"KZ\": \"143517\",\r\n        \"KE\": \"143529\", \"KG\": \"143586\", \"KH\": \"143579\", \"KN\": \"143548\", \"KR\": \"143466\", \"KW\": \"143493\", \"LA\": \"143587\",\r\n        \"LB\": \"143497\", \"LR\": \"143588\", \"LY\": \"143567\", \"LC\": \"143549\", \"LI\": \"143522\", \"LK\": \"143486\", \"LT\": \"143520\",\r\n        \"LU\": \"143451\", \"LV\": \"143519\", \"MO\": \"143515\", \"MA\": \"143620\", \"MC\": \"143618\", \"MD\": \"143523\", \"MG\": \"143531\",\r\n        \"MV\": \"143488\", \"MX\": \"143468\", \"MK\": \"143530\", \"ML\": \"143532\", \"MT\": \"143521\", \"MM\": \"143570\", \"ME\": \"143619\",\r\n        \"MN\": \"143592\", \"MZ\": \"143593\", \"MR\": \"143590\", \"MS\": \"143547\", \"MU\": \"143533\", \"MW\": \"143589\", \"MY\": \"143473\",\r\n        \"NA\": \"143594\", \"NE\": \"143534\", \"NG\": \"143561\", \"NI\": \"143512\", \"NL\": \"143452\", \"NO\": \"143457\", \"NP\": \"143484\",\r\n        \"NR\": \"143606\", \"NZ\": \"143461\", \"OM\": \"143562\", \"PK\": \"143477\", \"PA\": \"143485\", \"PE\": \"143507\", \"PH\": \"143474\",\r\n        \"PW\": \"143595\", \"PG\": \"143597\", \"PL\": \"143478\", \"PT\": \"143453\", \"PY\": \"143513\", \"PS\": \"143596\", \"QA\": \"143498\",\r\n        \"RO\": \"143487\", \"RU\": \"143469\", \"RW\": \"143621\", \"SA\": \"143479\", \"SN\": \"143535\", \"SG\": \"143464\", \"SB\": \"143601\",\r\n        \"SL\": \"143600\", \"SV\": \"143506\", \"RS\": \"143500\", \"ST\": \"143598\", \"SR\": \"143554\", \"SK\": \"143496\", \"SI\": \"143499\",\r\n        \"SE\": \"143456\", \"SZ\": \"143602\", \"SC\": \"143599\", \"TC\": \"143552\", \"TD\": \"143581\", \"TH\": \"143475\", \"TJ\": \"143603\",\r\n        \"TM\": \"143604\", \"TO\": \"143608\", \"TT\": \"143551\", \"TN\": \"143536\", \"TR\": \"143480\", \"TW\": \"143470\", \"TZ\": \"143572\",\r\n        \"UG\": \"143537\", \"UA\": \"143492\", \"UY\": \"143514\", \"US\": \"143441\", \"UZ\": \"143566\", \"VC\": \"143550\", \"VE\": \"143502\",\r\n        \"VG\": \"143543\", \"VN\": \"143471\", \"VU\": \"143609\", \"WS\": \"143607\", \"XK\": \"143624\", \"YE\": \"143571\", \"ZA\": \"143472\",\r\n        \"ZM\": \"143622\", \"ZW\": \"143605\",\r\n    }\r\n\r\n    class Channel(Enum):\r\n        \"\"\"\r\n        An Enum representing AppleTV channels.\r\n        Value represents the channel ID as used by the API.\r\n        \"\"\"\r\n        APPLE_TV_PLUS = \"tvs.sbd.4000\"\r\n        DISNEY_PLUS = \"tvs.sbd.1000216\"\r\n        ITUNES = \"tvs.sbd.9001\"\r\n        HULU = \"tvs.sbd.10000\"\r\n        MAX = \"tvs.sbd.9050\"\r\n        NETFLIX = \"tvs.sbd.9000\"\r\n        PRIME_VIDEO = \"tvs.sbd.12962\"\r\n        STARZ = \"tvs.sbd.1000308\"\r\n\r\n    def __init__(self, config_data: dict | None = None):\r\n        super().__init__(config_data=config_data)\r\n        self._config_data = config_data\r\n        self._storefront_locale_mapping_cache: dict[str, str] = {}\r\n\r\n    def _decide_locale(self, preferred_locales: str | list[str], default_locale: str, locales: list[str]) -> str:\r\n        \"\"\"\r\n        Decide which locale to use.\r\n\r\n        Args:\r\n            preferred_locales (str | list[str]): The preferred locales to use.\r\n            default_locale (str): The default locale to use if there is no match.\r\n            locales (list[str]): The locales to search in.\r\n\r\n        Returns:\r\n            str: The locale to use.\r\n        \"\"\"\r\n        if isinstance(preferred_locales, str):\r\n            preferred_locales = [preferred_locales]\r\n\r\n        for locale in preferred_locales:\r\n            if locale in locales:\r\n                return locale.replace(\"_\", \"-\")\r\n\r\n        if result := fnmatch.filter(locales, \"en_*\"):\r\n            return result[0].replace(\"_\", \"-\")\r\n\r\n        return default_locale\r\n\r\n    def _fetch_api_data(self, storefront_id: str, endpoint: str, additional_params: dict | None = None) -> dict:\r\n        \"\"\"\r\n        Send a request to AppleTV's API and return the JSON response.\r\n\r\n        Args:\r\n            endpoint (str): The endpoint to send the request to.\r\n            additional_params (dict[str, str]): Additional parameters to send with the request.\r\n\r\n        Returns:\r\n            dict: The JSON response.\r\n\r\n        Raises:\r\n            HttpError: If an HTTP error response is received.\r\n        \"\"\"\r\n        logger.debug(f\"Calling API endpoint '{endpoint}' using storefront '{storefront_id}'.\")\r\n\r\n        if storefront_cached_local := self._storefront_locale_mapping_cache.get(storefront_id):\r\n            logger.debug(f\"Using cached locale for storefront '{storefront_id}': '{storefront_cached_local}'.\")\r\n            locale = storefront_cached_local\r\n\r\n        else:\r\n            storefront_data = \\\r\n                self._get_configuration_data(storefront_id=storefront_id)[\"applicationProps\"][\"storefront\"]\r\n\r\n            default_locale = storefront_data[\"defaultLocale\"]\r\n            available_locales = storefront_data[\"localesSupported\"]\r\n\r\n            logger.debug(f\"Available locales for storefront '{storefront_id}': {available_locales}'.\"\r\n                         f\"\\nStorefront's default locale: '{default_locale}'.\")\r\n\r\n            locale = self._decide_locale(\r\n                preferred_locales=[\"en_US\", \"en_GB\"],\r\n                default_locale=default_locale,\r\n                locales=available_locales,\r\n            )\r\n\r\n            logger.debug(f\"Determined locale for storefront '{storefront_id}': '{locale}'\")\r\n\r\n            self._storefront_locale_mapping_cache[storefront_id] = locale\r\n\r\n        request_params = self._generate_api_request_params(storefront_id=storefront_id, locale=locale)\r\n\r\n        if additional_params:\r\n            request_params.update(additional_params)\r\n\r\n        # Send request to fetch media data\r\n        response = self._session.get(url=f\"{self._api_base_url}{endpoint}\", params=request_params)\r\n        logger.debug(f\"API endpoint '{endpoint}' on storefront '{storefront_id}' responded with status code: \"\r\n                     f\"'{response.status_code}'.\")\r\n\r\n        response.raise_for_status()\r\n        response_json = response.json()\r\n\r\n        return response_json.get(\"data\", {})\r\n\r\n    def _generate_api_request_params(self, storefront_id: str,\r\n                                     locale: str | None = None, utsk: str | None = None) -> dict:\r\n        \"\"\"\r\n        Generate request params for the AppleTV's API.\r\n\r\n        Args:\r\n            storefront_id (str): ID of the storefront to use.\r\n            locale (str | None, optional): ID of the locale to use. Defaults to None.\r\n            utsk (str | None, optional): utsk data. Defaults to None.\r\n\r\n        Returns:\r\n            dict: The request params, generated from the given arguments.\r\n        \"\"\"\r\n        params = self._api_base_params.copy()\r\n        params[\"sf\"] = storefront_id\r\n\r\n        if utsk:\r\n            params[\"utsk\"] = utsk\r\n\r\n        if locale:\r\n            params[\"locale\"] = locale\r\n\r\n        return params\r\n\r\n    def _get_configuration_data(self, storefront_id: str) -> dict:\r\n        \"\"\"\r\n        Get configuration data for the given storefront ID.\r\n\r\n        Args:\r\n            storefront_id (str): The ID of the storefront to get the configuration data for.\r\n\r\n        Returns:\r\n            dict: The configuration data.\r\n        \"\"\"\r\n        logger.debug(f\"Fetching configuration data for storefront '{storefront_id}'...\")\r\n        url = f\"{self._api_base_url}/configurations\"\r\n        params = self._generate_api_request_params(storefront_id=storefront_id)\r\n        response = self._session.get(url=url, params=params)\r\n        response.raise_for_status()\r\n        logger.debug(\"Configuration data fetched successfully.\")\r\n\r\n        return response.json()[\"data\"]\r\n\r\n    def _map_playables_by_channel(self, playables: list[dict]) -> dict[str, dict]:\r\n        \"\"\"\r\n        Map playables by channel name.\r\n\r\n        Args:\r\n            playables (list[dict]): Playables data to map.\r\n\r\n        Returns:\r\n            dict: The mapped playables (in a `channel_name (str): [playables]` format).\r\n        \"\"\"\r\n        mapped_playables: dict = {}\r\n\r\n        for playable in playables:\r\n            if channel_id := playable.get(\"channelId\"):\r\n                mapped_playables.setdefault(channel_id, []).append(playable)\r\n\r\n        return mapped_playables\r\n\r\n    def get_movie_data(self, storefront_id: str, movie_id: str) -> ScrapedMediaResponse[Movie]:\r\n        data = self._fetch_api_data(\r\n            storefront_id=storefront_id,\r\n            endpoint=f\"/movies/{movie_id}\",\r\n        )\r\n\r\n        mapped_playables = self._map_playables_by_channel(playables=data[\"playables\"].values())\r\n        logger.debug(f\"Available channels for movie '{movie_id}': \"\r\n                     f\"{' '.join(list(mapped_playables.keys()))}\")\r\n\r\n        if self.Channel.ITUNES.value not in mapped_playables:\r\n            if self.Channel.APPLE_TV_PLUS.value in mapped_playables:\r\n                raise ScraperException(\"Scraping AppleTV+ content is not currently supported.\")\r\n\r\n            else:\r\n                raise ScraperException(\"No iTunes playables could be found.\")\r\n\r\n        return_data = []\r\n\r\n        for playable_data in mapped_playables[self.Channel.ITUNES.value]:\r\n            return_data.append(self._extract_itunes_movie_data(playable_data))\r\n\r\n        if len(return_data) > 1:\r\n            logger.debug(f\"{len(return_data)} iTunes playables were found for movie '{movie_id}'.\")\r\n\r\n        else:\r\n            return_data = return_data[0]\r\n\r\n        return ScrapedMediaResponse(\r\n            media_data=return_data,\r\n            metadata_scraper=self.id,\r\n            playlist_scraper=\"itunes\",\r\n            original_data=data,\r\n        )\r\n\r\n    def _extract_itunes_movie_data(self, playable_data: dict) -> Movie:\r\n        \"\"\"\r\n        Extract movie data from an AppleTV's API iTunes playable data.\r\n\r\n        Args:\r\n            playable_data (dict): The playable data from the AppleTV API.\r\n\r\n        Returns:\r\n            Movie: A Movie object.\r\n        \"\"\"\r\n        itunes_movie_id = playable_data[\"itunesMediaApiData\"][\"id\"]\r\n        appletv_movie_id = playable_data[\"canonicalId\"]\r\n        movie_title = playable_data[\"canonicalMetadata\"][\"movieTitle\"]\r\n        movie_release_date = convert_epoch_to_datetime(playable_data[\"canonicalMetadata\"][\"releaseDate\"] // 1000)\r\n\r\n        movie_playlists = []\r\n        movie_duration = None\r\n\r\n        if offers := playable_data[\"itunesMediaApiData\"].get(\"offers\"):\r\n            for offer in offers:\r\n                if (playlist := offer.get(\"hlsUrl\")) and offer[\"hlsUrl\"] not in movie_playlists:\r\n                    movie_playlists.append(playlist)\r\n\r\n            if movie_duration_int := offers[0].get(\"durationInMilliseconds\"):\r\n                movie_duration = dt.timedelta(milliseconds=movie_duration_int)\r\n\r\n        if movie_expected_release_date := playable_data[\"itunesMediaApiData\"].get(\"futureRentalAvailabilityDate\"):\r\n            dt.datetime.strptime(movie_expected_release_date, \"%Y-%m-%d\")\r\n\r\n        return Movie(\r\n            id=itunes_movie_id,\r\n            referer_id=appletv_movie_id,\r\n            name=movie_title,\r\n            release_date=movie_release_date,\r\n            duration=movie_duration,\r\n            preorder_availability_date=movie_expected_release_date,\r\n            playlist=movie_playlists if movie_playlists else None,\r\n        )\r\n\r\n    def get_episode_data(self, storefront_id: str, episode_id: str) -> ScrapedMediaResponse[Episode]:\r\n        raise NotImplementedError(\"Series scraping is not currently supported.\")\r\n\r\n    def get_season_data(self, storefront_id: str, season_id: str, show_id: str) -> ScrapedMediaResponse[Season]:\r\n        raise NotImplementedError(\"Series scraping is not currently supported.\")\r\n\r\n    def get_show_data(self, storefront_id: str, show_id: str) -> ScrapedMediaResponse[Series]:\r\n        raise NotImplementedError(\"Series scraping is not currently supported.\")\r\n\r\n    def get_data(self, url: str) -> ScrapedMediaResponse[MediaData]:\r\n        regex_match = self.match_url(url, raise_error=True)\r\n        url_data = regex_match.groupdict()\r\n\r\n        media_type = url_data[\"media_type\"]\r\n\r\n        if storefront_code := url_data.get(\"country_code\"):\r\n            storefront_code = storefront_code.upper()\r\n\r\n        else:\r\n            storefront_code = self._default_storefront\r\n\r\n        media_id = url_data[\"media_id\"]\r\n\r\n        if storefront_code not in self._storefronts_mapping:\r\n            raise ScraperException(f\"ID mapping for storefront '{storefront_code}' could not be found.\")\r\n\r\n        storefront_id = self._storefronts_mapping[storefront_code]\r\n\r\n        if media_type == \"movie\":\r\n            return self.get_movie_data(storefront_id=storefront_id, movie_id=media_id)\r\n\r\n        elif media_type == \"episode\":\r\n            return self.get_episode_data(storefront_id=storefront_id, episode_id=media_id)\r\n\r\n        elif media_type == \"season\":\r\n            if url_params := url_data.get(\"url_params\"):\r\n                if show_id := parse_url_params(url_params).get(\"showId\"):\r\n                    return self.get_season_data(storefront_id=storefront_id, season_id=media_id, show_id=show_id)\r\n\r\n            raise ScraperException(\"Invalid AppleTV URL: Missing 'showId' parameter.\")\r\n\r\n        elif media_type == \"show\":\r\n            return self.get_show_data(storefront_id=storefront_id, show_id=media_id)\r\n\r\n        else:\r\n            raise ScraperException(f\"Invalid media type '{media_type}'.\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/appletv_scraper.py b/isubrip/scrapers/appletv_scraper.py
--- a/isubrip/scrapers/appletv_scraper.py	(revision 9fce8c61d89eb65de1a4a0c329bc31fc213e2991)
+++ b/isubrip/scrapers/appletv_scraper.py	(date 1689972067193)
@@ -281,7 +281,7 @@
                 movie_duration = dt.timedelta(milliseconds=movie_duration_int)
 
         if movie_expected_release_date := playable_data["itunesMediaApiData"].get("futureRentalAvailabilityDate"):
-            dt.datetime.strptime(movie_expected_release_date, "%Y-%m-%d")
+            movie_expected_release_date = dt.datetime.strptime(movie_expected_release_date, "%Y-%m-%d")
 
         return Movie(
             id=itunes_movie_id,
Index: isubrip/scrapers/scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport importlib\r\nimport inspect\r\nimport os\r\nimport re\r\nimport sys\r\nfrom abc import abstractmethod, ABC\r\nfrom enum import Enum\r\nfrom glob import glob\r\nfrom pathlib import Path\r\nfrom typing import Any, ClassVar, Iterator, List, Literal, overload, Union, TypeVar\r\n\r\nimport aiohttp\r\nimport m3u8\r\nimport requests\r\nimport requests.utils\r\nfrom m3u8 import M3U8, Media, Segment, SegmentList\r\n\r\nfrom isubrip.config import Config, ConfigSetting\r\nfrom isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX\r\nfrom isubrip.data_structures import SubtitlesData, SubtitlesFormatType, SubtitlesType, ScrapedMediaResponse\r\nfrom isubrip.logger import logger\r\nfrom isubrip.subtitle_formats.subtitles import Subtitles\r\nfrom isubrip.utils import merge_dict_values, single_to_list, SingletonMeta\r\n\r\n\r\nScraperT = TypeVar(\"ScraperT\", bound=\"Scraper\")\r\n\r\n\r\nclass Scraper(ABC, metaclass=SingletonMeta):\r\n    \"\"\"\r\n    A base class for scrapers.\r\n\r\n    Attributes:\r\n        default_user_agent (str): [Class Attribute]\r\n            Default user agent to use if no other user agent is specified when making requests.\r\n        subtitles_fix_rtl (bool): [Class Attribute] Whether to fix RTL from downloaded subtitles.\r\n        subtitles_fix_rtl_languages (list[str] | None): [Class Attribute]\r\n            A list of languages to fix RTL on. If None, a default list will be used.\r\n        subtitles_remove_duplicates (bool): [Class Attribute]\r\n            Whether to remove duplicate lines from downloaded subtitles.\r\n\r\n        id (str): [Class Attribute] ID of the scraper.\r\n        name (str): [Class Attribute] Name of the scraper.\r\n        abbreviation (str): [Class Attribute] Abbreviation of the scraper.\r\n        url_regex (str): [Class Attribute] A RegEx pattern to find URLs matching the service.\r\n        subtitles_class (type[Subtitles]): [Class Attribute] Class of the subtitles format returned by the scraper.\r\n        is_movie_scraper (bool): [Class Attribute] Whether the scraper is for movies.\r\n        is_series_scraper (bool): [Class Attribute] Whether the scraper is for series.\r\n        uses_scrapers (list[str]): [Class Attribute] A list of IDs for other scraper classes that this scraper uses.\r\n            This assures that the config data for the other scrapers is passed as well.\r\n        _session (requests.Session): A requests session to use for making requests.\r\n        config (Config): A Config object containing the scraper's configuration.\r\n    \"\"\"\r\n    default_user_agent: ClassVar[str] = requests.utils.default_user_agent()\r\n    subtitles_fix_rtl: ClassVar[bool] = False\r\n    subtitles_fix_rtl_languages: ClassVar[list | None] = [\"ar\", \"he\"]\r\n    subtitles_remove_duplicates: ClassVar[bool] = True\r\n\r\n    id: ClassVar[str]\r\n    name: ClassVar[str]\r\n    abbreviation: ClassVar[str]\r\n    url_regex: ClassVar[str | list[str]]\r\n    subtitles_class: ClassVar[type[Subtitles]]\r\n    is_movie_scraper: ClassVar[bool] = False\r\n    is_series_scraper: ClassVar[bool] = False\r\n    uses_scrapers: ClassVar[list[str]] = []\r\n\r\n    def __init__(self, config_data: dict | None = None):\r\n        \"\"\"\r\n        Initialize a Scraper object.\r\n\r\n        Args:\r\n            config_data (dict | None, optional): A dictionary containing scraper's configuration data. Defaults to None.\r\n        \"\"\"\r\n        self._session = requests.Session()\r\n        self._config_data = config_data\r\n        self.config = Config(config_data=config_data.get(self.id) if config_data else None)\r\n\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                key=\"user-agent\",\r\n                type=str,\r\n                required=False,\r\n            )],\r\n            check_config=False)\r\n\r\n        self._session.headers.update({\"User-Agent\": self.config.get(\"user-agent\") or self.default_user_agent})\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[True] = ...) -> re.Match:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[False] = ...) -> re.Match | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def match_url(cls, url: str, raise_error: bool = False) -> re.Match | None:\r\n        \"\"\"\r\n        Checks if a URL matches scraper's url regex.\r\n\r\n        Args:\r\n            url (str): A URL to check against the regex.\r\n            raise_error (bool, optional): Whether to raise an error instead of returning None if the URL doesn't match.\r\n\r\n        Returns:\r\n            re.Match | None: A Match object if the URL matches the regex, None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If the URL doesn't match the regex and raise_error is True.\r\n        \"\"\"\r\n        if isinstance(cls.url_regex, str):\r\n            return re.fullmatch(pattern=cls.url_regex, string=url, flags=re.IGNORECASE)\r\n\r\n        else:  # isinstance(cls.url_regex, (list, tuple)):\r\n            for url_regex_item in cls.url_regex:\r\n                if result := re.fullmatch(pattern=url_regex_item, string=url, flags=re.IGNORECASE):\r\n                    return result\r\n\r\n        if raise_error:\r\n            raise ValueError(f\"URL '{url}' doesn't match the URL regex of {cls.name}.\")\r\n\r\n        return None\r\n\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        self.close()\r\n\r\n    def close(self):\r\n        self._session.close()\r\n\r\n    @abstractmethod\r\n    def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        \"\"\"\r\n        Scrape media information about the media on a URL.\r\n\r\n        Args:\r\n            url (str): A URL to get media information about.\r\n\r\n        Returns:\r\n            ScrapedMediaResponse: A ScrapedMediaResponse object containing scraped media information.\r\n        \"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData]:\r\n        \"\"\"\r\n        Find and yield subtitles data from a main_playlist.\r\n\r\n        Args:\r\n            main_playlist (M3U8): Main playlist of the media to search for subtitles in.\r\n            language_filter (list[str], optional): A list of languages to filter for.\r\n            subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.\r\n\r\n        Yields:\r\n            SubtitlesData: A SubtitlesData object for each subtitle found\r\n                in the main playlist (matching the filters, if given).\r\n        \"\"\"\r\n        pass\r\n\r\n\r\nclass MovieScraper(Scraper, ABC):\r\n    \"\"\"A base class for movie scrapers.\"\"\"\r\n    is_movie_scraper = True\r\n\r\n\r\nclass SeriesScraper(Scraper, ABC):\r\n    \"\"\"A base class for series scrapers.\"\"\"\r\n    is_series_scraper = True\r\n\r\n\r\nclass AsyncScraper(Scraper, ABC):\r\n    \"\"\"A base class for scrapers that utilize async requests.\"\"\"\r\n    def __init__(self, config_data: dict | None = None):\r\n        super().__init__(config_data)\r\n        self.async_session = aiohttp.ClientSession()\r\n        self.async_session.headers.update(self._session.headers)\r\n\r\n    def close(self):\r\n        asyncio.get_event_loop().run_until_complete(self._async_close())\r\n        super().close()\r\n\r\n    async def _async_close(self):\r\n        await self.async_session.close()\r\n\r\n\r\nclass M3U8Scraper(AsyncScraper, ABC):\r\n    \"\"\"A base class for M3U8 scrapers.\"\"\"\r\n    playlist_filters_config_category = \"playlist-filters\"\r\n\r\n    class M3U8Attribute(Enum):\r\n        \"\"\"\r\n        An enum representing all possible M3U8 attributes.\r\n        Names / Keys represent M3U8 Media object attributes (should be converted to lowercase),\r\n        and values represent the name of the key for config usage.\r\n        \"\"\"\r\n        ASSOC_LANGUAGE = \"assoc-language\"\r\n        AUTOSELECT = \"autoselect\"\r\n        CHARACTERISTICS = \"characteristics\"\r\n        CHANNELS = \"channels\"\r\n        DEFAULT = \"default\"\r\n        FORCED = \"forced\"\r\n        GROUP_ID = \"group-id\"\r\n        INSTREAM_ID = \"instream-id\"\r\n        LANGUAGE = \"language\"\r\n        NAME = \"name\"\r\n        STABLE_RENDITION_ID = \"stable-rendition-id\"\r\n        TYPE = \"type\"\r\n\r\n    def __init__(self, config_data: dict | None = None):\r\n        super().__init__(config_data)\r\n\r\n        if self.config is None:\r\n            self.config = Config()\r\n\r\n        # Add M3U8 filters settings\r\n        self.config.add_settings([\r\n            ConfigSetting(\r\n                category=self.playlist_filters_config_category,\r\n                key=m3u8_attribute.value,\r\n                type=Union[str, List[str]],\r\n                required=False,\r\n            ) for m3u8_attribute in self.M3U8Attribute],\r\n            check_config=False)\r\n\r\n        self._m3u8_cache: dict[str, M3U8] = {}\r\n\r\n    def _download_segments_async(self, segments: SegmentList[Segment]) -> list[bytes]:\r\n        \"\"\"\r\n        Download M3U8 segments asynchronously.\r\n\r\n        Args:\r\n            segments (m3u8.SegmentList[m3u8.Segment]): List of segments to download.\r\n\r\n        Returns:\r\n            list[bytes]: List of downloaded segments.\r\n        \"\"\"\r\n        loop = asyncio.get_event_loop()\r\n        async_tasks = [loop.create_task(self._download_segment_async(segment.absolute_uri)) for segment in segments]\r\n        segments_bytes = loop.run_until_complete(asyncio.gather(*async_tasks))\r\n\r\n        return list(segments_bytes)\r\n\r\n    async def _download_segment_async(self, url: str) -> bytes:\r\n        \"\"\"\r\n        Download an M3U8 segment asynchronously.\r\n\r\n        Args:\r\n            url (str): URL of the segment to download.\r\n\r\n        Returns:\r\n            bytes: Downloaded segment.\r\n        \"\"\"\r\n        async with self.async_session.get(url) as response:\r\n            return await response.read()\r\n\r\n    @overload\r\n    def load_m3u8(self, url: str | list[str], raise_error: Literal[True] = ...) -> M3U8:\r\n        ...\r\n\r\n    @overload\r\n    def load_m3u8(self, url: str | list[str], raise_error: Literal[False] = ...) -> M3U8 | None:\r\n        ...\r\n\r\n    def load_m3u8(self, url: str | list[str], raise_error: bool = False) -> M3U8 | None:\r\n        \"\"\"\r\n        Load an M3U8 playlist from a URL to an M3U8 object.\r\n        Multiple URLs can be given, in which case the first one that loads successfully will be returned.\r\n        The method uses caching to avoid loading the same playlist multiple times.\r\n\r\n        Args:\r\n            url (str | list[str]: URL of the M3U8 playlist to load.\r\n            raise_error (bool, optional): Whether to raise an error if none of the playlists loaded successfully.\r\n                If set to false, a None value will be returned instead. Defaults to False.\r\n\r\n        Returns:\r\n            m3u8.M3U8: An M3U8 object representing the playlist.\r\n        \"\"\"\r\n        errors = {}\r\n\r\n        for _url in single_to_list(url):\r\n            if _url in self._m3u8_cache:\r\n                return self._m3u8_cache[_url]\r\n\r\n            else:\r\n                try:\r\n                    self._m3u8_cache[_url] = m3u8.load(uri=_url, timeout=5)\r\n                    return self._m3u8_cache[_url]\r\n\r\n                except Exception as e:\r\n                    errors[_url] = e\r\n                    continue\r\n\r\n        if raise_error:\r\n            errors_str = \"\\n\".join([f\"{url}: {error}\" for url, error in errors.items()])\r\n            raise ScraperException(f\"Failed to load M3U8 playlist: {url}:\\n{errors_str}\")\r\n\r\n        return None\r\n\r\n    def _map_session_data(self, playlist_data: M3U8) -> dict[str, Any]:\r\n        \"\"\"\r\n        Create and return a dictionary of session data from an M3U8 playlist.\r\n\r\n        Args:\r\n            playlist_data (m3u8.M3U8): M3U8 playlist to map session data from.\r\n\r\n        Returns:\r\n            dict[str, Any]: Dictionary of session data.\r\n        \"\"\"\r\n        session_data = {}\r\n\r\n        if playlist_data.session_data:\r\n            for session_data_item in playlist_data.session_data:\r\n                session_data[session_data_item.data_id] = session_data_item.value\r\n\r\n        return session_data\r\n\r\n    @staticmethod\r\n    def detect_subtitles_type(subtitles_media: Media) -> SubtitlesType | None:\r\n        \"\"\"\r\n        Detect the subtitles type (Closed Captions, Forced, etc.) from an M3U8 Media object.\r\n\r\n        Args:\r\n            subtitles_media (m3u8.Media): Subtitles Media object to detect the type of.\r\n\r\n        Returns:\r\n            SubtitlesType | None: The type of the subtitles, None for regular subtitles.\r\n        \"\"\"\r\n        if subtitles_media.forced == \"YES\":\r\n            return SubtitlesType.FORCED\r\n\r\n        elif subtitles_media.characteristics is not None and \"public.accessibility\" in subtitles_media.characteristics:\r\n            return SubtitlesType.CC\r\n\r\n        return None\r\n\r\n    def get_media_playlists(self, main_playlist: M3U8,\r\n                            playlist_filters: dict[str, str | list[str]] | None = None,\r\n                            include_default_filters: bool = True) -> list[Media]:\r\n        \"\"\"\r\n        Find and yield playlists of media within an M3U8 main_playlist using optional filters.\r\n\r\n        Args:\r\n            main_playlist (m3u8.M3U8): An M3U8 object of the main main_playlist.\r\n            playlist_filters (dict[str, str | list[str], optional):\r\n                A dictionary of filters to use when searching for subtitles.\r\n                Will be added to filters set by the config (unless `include_default_filters` is set to false).\r\n                Defaults to None.\r\n            include_default_filters (bool, optional): Whether to include the default filters set by the config or not.\r\n                Defaults to True.\r\n\r\n        Returns:\r\n            list[Media]: A list of  matching Media objects.\r\n        \"\"\"\r\n        results = []\r\n        default_filters: dict | None = self.config.get(M3U8Scraper.playlist_filters_config_category)\r\n\r\n        if include_default_filters and default_filters:\r\n            if not playlist_filters:\r\n                playlist_filters = default_filters\r\n\r\n            else:\r\n                playlist_filters = merge_dict_values(default_filters, playlist_filters)\r\n\r\n        for media in main_playlist.media:\r\n            if not playlist_filters:\r\n                results.append(media)\r\n                continue\r\n\r\n            is_valid = True\r\n\r\n            for filter_name, filter_value in playlist_filters.items():\r\n                try:\r\n                    filter_name_enum = M3U8Scraper.M3U8Attribute(filter_name)\r\n                    attribute_value = getattr(media, filter_name_enum.name.lower(), None)\r\n\r\n                    if (attribute_value is None) or (\r\n                            isinstance(filter_value, list) and\r\n                            attribute_value.casefold() not in (x.casefold() for x in filter_value)\r\n                    ) or (\r\n                            isinstance(filter_value, str) and filter_value.casefold() != attribute_value.casefold()\r\n                    ):\r\n                        is_valid = False\r\n                        break\r\n\r\n                except Exception:\r\n                    is_valid = False\r\n\r\n            if is_valid:\r\n                results.append(media)\r\n\r\n        return results\r\n\r\n    def get_subtitles(self, main_playlist: str | list[str], language_filter: list[str] | str | None = None,\r\n                      subrip_conversion: bool = False) -> Iterator[SubtitlesData]:\r\n        \"\"\"\r\n        Find and yield subtitles for a movie using optional filters.\r\n\r\n        Args:\r\n            main_playlist(str | list[str]): A URL or a list of URLs (for redundancy) of the main playlist.\r\n            language_filter (list[str] | str | None, optional):\r\n                A language or a list of languages to filter for. Defaults to None.\r\n            subrip_conversion (bool, optional): Whether to convert and return the subtitles as an SRT file or not.\r\n                Defaults to False.\r\n\r\n        Yields:\r\n            SubtitlesData: A SubtitlesData NamedTuple with a matching playlist, and it's metadata.\r\n        \"\"\"\r\n        playlist_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None\r\n        main_playlist_m3u8 = self.load_m3u8(main_playlist)\r\n        matched_media_items = self.get_media_playlists(main_playlist=main_playlist_m3u8,\r\n                                                       playlist_filters=playlist_filters)\r\n\r\n        for matched_media in matched_media_items:\r\n            try:\r\n                matched_media_playlist = m3u8.load(matched_media.absolute_uri)\r\n                subtitles = self.subtitles_class(language_code=matched_media.language)\r\n                for segment in self._download_segments_async(matched_media_playlist.segments):\r\n                    subtitles.append_subtitles(subtitles.loads(segment.decode(\"utf-8\")))\r\n\r\n                subtitles.polish(\r\n                    fix_rtl=self.subtitles_fix_rtl,\r\n                    rtl_languages=self.subtitles_fix_rtl_languages,\r\n                    remove_duplicates=self.subtitles_remove_duplicates,\r\n                )\r\n\r\n                yield SubtitlesData(\r\n                    language_code=matched_media.language,\r\n                    language_name=matched_media.name,\r\n                    subtitles_format=SubtitlesFormatType.SUBRIP if subrip_conversion else SubtitlesFormatType.WEBVTT,\r\n                    content=subtitles.to_srt().dump() if subrip_conversion else subtitles.dump(),\r\n                    special_type=self.detect_subtitles_type(matched_media),\r\n                )\r\n\r\n            except Exception:\r\n                continue\r\n\r\n\r\nclass ScraperFactory(metaclass=SingletonMeta):\r\n    def __init__(self):\r\n        self._scraper_classes_cache: list[type[Scraper]] | None = None\r\n        self._scraper_instances_cache: dict[type[Scraper], Scraper] = {}\r\n        self._currently_initializing: list[type[Scraper]] = []  # Used to prevent infinite recursion\r\n\r\n    def get_initialized_scrapers(self) -> list[Scraper]:\r\n        \"\"\"\r\n        Get a list of all previously initialized scrapers.\r\n\r\n        Returns:\r\n            list[Scraper]: A list of initialized scrapers.\r\n        \"\"\"\r\n        return list(self._scraper_instances_cache.values())\r\n\r\n    def get_scraper_classes(self) -> Iterator[type[Scraper]]:\r\n        \"\"\"\r\n        Iterate over all scraper classes.\r\n\r\n        Yields:\r\n            type[Scraper]: A Scraper subclass.\r\n        \"\"\"\r\n        if self._scraper_classes_cache is not None:\r\n            return self._scraper_classes_cache\r\n\r\n        else:\r\n            scraper_modules_paths = glob(os.path.dirname(__file__) + f\"/*{SCRAPER_MODULES_SUFFIX}.py\")\r\n\r\n            for scraper_module_path in scraper_modules_paths:\r\n                sys.path.append(scraper_module_path)\r\n\r\n                module = importlib.import_module(f\"{PACKAGE_NAME}.scrapers.{Path(scraper_module_path).stem}\")\r\n\r\n                # Find all 'Scraper' subclasses\r\n                for _, obj in inspect.getmembers(module,\r\n                                                 predicate=lambda x: inspect.isclass(x) and issubclass(x, Scraper)):\r\n                    # Skip object if it's an abstract or imported from another module\r\n                    if not inspect.isabstract(obj) and obj.__module__ == module.__name__:\r\n                        if any((obj.is_movie_scraper, obj.is_series_scraper)):\r\n                            yield obj\r\n\r\n            return\r\n\r\n    def _get_scraper_instance(self, scraper_class: type[ScraperT],\r\n                              scrapers_config_data: dict | None = None) -> ScraperT:\r\n        \"\"\"\r\n        Initialize and return a scraper instance.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT]): A scraper class to initialize.\r\n            scrapers_config_data (dict, optional): A dictionary containing scrapers config data to use\r\n                when creating a new scraper. Defaults to None.\r\n\r\n        Returns:\r\n            Scraper: An instance of the given scraper class.\r\n        \"\"\"\r\n        logger.debug(f\"Initializing '{scraper_class.name}' scraper...\")\r\n\r\n        if scraper_class not in self._scraper_instances_cache:\r\n            logger.debug(f\"'{scraper_class.name}' scraper not found in cache, creating a new instance...\")\r\n\r\n            if scraper_class in self._currently_initializing:\r\n                raise ScraperException(f\"'{scraper_class.name}' scraper is already being initialized.\\n\"\r\n                                       f\"Make sure there are no circular dependencies between scrapers.\")\r\n\r\n            self._currently_initializing.append(scraper_class)\r\n\r\n            # Set config data for the scraper and its dependencies, if any\r\n            if not scrapers_config_data:\r\n                config_data = None\r\n\r\n            else:\r\n                required_scrapers_ids = [scraper_class.id] + scraper_class.uses_scrapers\r\n                config_data = \\\r\n                    {scraper_id: scrapers_config_data[scraper_id] for scraper_id in required_scrapers_ids\r\n                     if scrapers_config_data.get(scraper_id)}\r\n\r\n            self._scraper_instances_cache[scraper_class] = scraper_class(config_data=config_data)\r\n            self._currently_initializing.remove(scraper_class)\r\n\r\n        else:\r\n            logger.debug(f\"'{scraper_class.id}' scraper found in cache. Cached instance will be used.\")\r\n\r\n        return self._scraper_instances_cache[scraper_class]  # type: ignore[return-value]\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[True] = ...) -> ScraperT:\r\n        ...\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[False] = ...) -> ScraperT | None:\r\n        ...\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[True] = ...) -> Scraper:\r\n        ...\r\n\r\n    @overload\r\n    def get_scraper_instance(self, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., config_data: dict | None = ...,\r\n                             raise_error: Literal[False] = ...) -> Scraper | None:\r\n        ...\r\n\r\n    def get_scraper_instance(self, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,\r\n                             url: str | None = None, config_data: dict | None = None,\r\n                             raise_error: bool = True) -> Scraper | None:\r\n        \"\"\"\r\n        Find, initialize and return a scraper that matches the given URL or ID.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT] | None, optional): A scraper class to initialize. Defaults to None.\r\n            scraper_id (str | None, optional): ID of a scraper to initialize. Defaults to None.\r\n            url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.\r\n            config_data (dict, optional): A dictionary containing scrapers config data to use\r\n                when creating a new scraper. Defaults to None.\r\n            raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.\r\n\r\n        Returns:\r\n            ScraperT | Scraper | None: An instance of a scraper that matches the given URL or ID,\r\n                None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If no scraper was found and raise_error is True.\r\n        \"\"\"\r\n        if scraper_class:\r\n            return self._get_scraper_instance(scraper_class=scraper_class,\r\n                                              scrapers_config_data=config_data)\r\n\r\n        elif scraper_id or url:\r\n            if scraper_id:\r\n                logger.debug(f\"Searching for a scraper object with ID '{scraper_id}'...\")\r\n                for scraper in self.get_scraper_classes():\r\n                    if scraper.id == scraper_id:\r\n                        return self._get_scraper_instance(scraper_class=scraper, scrapers_config_data=config_data)\r\n\r\n            elif url:\r\n                logger.debug(f\"Searching for a scraper object that matches URL '{url}'...\")\r\n                for scraper in self.get_scraper_classes():\r\n                    if scraper.match_url(url) is not None:\r\n                        return self._get_scraper_instance(scraper_class=scraper, scrapers_config_data=config_data)\r\n\r\n            if raise_error:\r\n                raise ValueError(f\"No matching scraper was found for URL '{url}'\")\r\n\r\n            else:\r\n                logger.debug(\"No matching scraper was found.\")\r\n                return None\r\n\r\n        else:\r\n            raise ValueError(\"At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.\")\r\n\r\n\r\nclass ScraperException(Exception):\r\n    pass\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/scraper.py b/isubrip/scrapers/scraper.py
--- a/isubrip/scrapers/scraper.py	(revision 9fce8c61d89eb65de1a4a0c329bc31fc213e2991)
+++ b/isubrip/scrapers/scraper.py	(date 1689974988821)
@@ -342,6 +342,25 @@
 
         return None
 
+    def find_valid_playlist(self, playlists: list[str] | str) -> M3U8 | None:
+        """
+        Find and return a valid M3U8 playlist from a list of playlists.
+
+        Args:
+            playlists (list[str] | str): List of playlists to check (list[str]). Can also be a single playlist (str).
+
+        Returns:
+            m3u8.M3U8 | None: A successfully loaded M3U8 playlist, or None if none of the playlists loaded successfully.
+        """
+        for playlist in single_to_list(playlists):  # type: str
+            try:
+                return self.load_m3u8(playlist)
+
+            except Exception:
+                continue
+
+        return None
+
     def get_media_playlists(self, main_playlist: M3U8,
                             playlist_filters: dict[str, str | list[str]] | None = None,
                             include_default_filters: bool = True) -> list[Media]:
@@ -415,7 +434,11 @@
             SubtitlesData: A SubtitlesData NamedTuple with a matching playlist, and it's metadata.
         """
         playlist_filters = {self.M3U8Attribute.LANGUAGE.value: language_filter} if language_filter else None
-        main_playlist_m3u8 = self.load_m3u8(main_playlist)
+        main_playlist_m3u8 = self.find_valid_playlist(main_playlist)
+
+        if main_playlist_m3u8 is None:
+            raise ScraperException("Failed to load main playlist.")
+
         matched_media_items = self.get_media_playlists(main_playlist=main_playlist_m3u8,
                                                        playlist_filters=playlist_filters)
 
