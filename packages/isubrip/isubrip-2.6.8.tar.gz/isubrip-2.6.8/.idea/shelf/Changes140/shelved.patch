Index: isubrip/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABCMeta\r\nimport datetime as dt\r\nfrom functools import lru_cache\r\nfrom pathlib import Path\r\nimport re\r\nimport secrets\r\nimport shutil\r\nimport sys\r\nfrom typing import TYPE_CHECKING, Any, Literal, Type, overload\r\n\r\nfrom isubrip.constants import TEMP_FOLDER_PATH, TITLE_REPLACEMENT_STRINGS, WINDOWS_RESERVED_FILE_NAMES\r\nfrom isubrip.data_structures import (\r\n    Episode,\r\n    MediaBase,\r\n    Movie,\r\n    Season,\r\n    Series,\r\n    SubtitlesData,\r\n    SubtitlesFormatType,\r\n    SubtitlesType,\r\n    T,\r\n)\r\nfrom isubrip.logger import logger\r\n\r\nif TYPE_CHECKING:\r\n    from os import PathLike\r\n    from types import TracebackType\r\n\r\n    import httpx\r\n    from pydantic import BaseModel, ValidationError\r\n\r\n\r\nclass SingletonMeta(ABCMeta):\r\n    \"\"\"\r\n    A metaclass that implements the Singleton pattern.\r\n    When a class using this metaclass is initialized, it will return the same instance every time.\r\n    \"\"\"\r\n    _instances: dict[object, object] = {}\r\n\r\n    def __call__(cls, *args: Any, **kwargs: Any) -> object:\r\n        if cls._instances.get(cls) is None:\r\n            cls._instances[cls] = super().__call__(*args, **kwargs)\r\n\r\n        return cls._instances[cls]\r\n\r\n\r\nclass TempDirGenerator:\r\n    \"\"\"A class for generating temporary directories, and disposing them once the object is destroyed.\"\"\"\r\n    _generated_temp_directories: list[Path] = []\r\n\r\n    def __exit__(self, exc_type: Type[BaseException] | None,\r\n                 exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:\r\n        self.cleanup()\r\n\r\n    @classmethod\r\n    def generate(cls, directory_name: str | None = None) -> Path:\r\n        \"\"\"\r\n        Generate a temporary directory within 'TEMP_FOLDER_PATH'.\r\n\r\n        Args:\r\n            directory_name (str | None, optional): Name of the directory to generate.\r\n                If not specified, a random string will be generated. Defaults to None.\r\n\r\n        Returns:\r\n            Path: Path to the generated directory.\r\n        \"\"\"\r\n        directory_name = directory_name or secrets.token_hex(5)\r\n        full_path = TEMP_FOLDER_PATH / directory_name\r\n\r\n        if full_path.is_dir():\r\n            if full_path in cls._generated_temp_directories:  # Generated by this class\r\n                logger.debug(f\"Using previously generated temporary directory: '{full_path}'.\")\r\n                return full_path\r\n\r\n            logger.debug(f\"Temporary directory '{full_path}' already exists. \"\r\n                         f\"Emptying the directory from all contents...\")\r\n            shutil.rmtree(full_path)\r\n            full_path.mkdir(parents=True)\r\n\r\n        else:\r\n            full_path.mkdir(parents=True)\r\n            logger.debug(f\"Temporary directory has been generated: '{full_path}'\")\r\n\r\n        cls._generated_temp_directories.append(full_path)\r\n        return full_path\r\n\r\n    @classmethod\r\n    def cleanup(cls) -> None:\r\n        \"\"\"Remove all temporary directories generated by this object.\"\"\"\r\n        for temp_directory in cls._generated_temp_directories:\r\n            logger.debug(f\"Removing temporary directory: '{temp_directory}'\")\r\n\r\n            try:\r\n                shutil.rmtree(temp_directory)\r\n\r\n            except Exception as e:\r\n                logger.debug(f\"Failed to remove temporary directory '{temp_directory}': {e}\")\r\n\r\n        cls._generated_temp_directories = []\r\n\r\n\r\ndef convert_epoch_to_datetime(epoch_timestamp: int) -> dt.datetime:\r\n    \"\"\"\r\n    Convert an epoch timestamp to a datetime object.\r\n\r\n    Args:\r\n        epoch_timestamp (int): Epoch timestamp.\r\n\r\n    Returns:\r\n        datetime: A datetime object representing the timestamp.\r\n    \"\"\"\r\n    if epoch_timestamp >= 0:\r\n        return dt.datetime.fromtimestamp(epoch_timestamp)\r\n\r\n    return dt.datetime(1970, 1, 1) + dt.timedelta(seconds=epoch_timestamp)\r\n\r\n\r\ndef download_subtitles_to_file(media_data: Movie | Episode, subtitles_data: SubtitlesData, output_path: str | PathLike,\r\n                               source_abbreviation: str | None = None, overwrite: bool = False) -> Path:\r\n    \"\"\"\r\n    Download subtitles to a file.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): An object containing media data.\r\n        subtitles_data (SubtitlesData): A SubtitlesData object containing subtitles data.\r\n        output_path (str | PathLike): Path to the output folder.\r\n        source_abbreviation (str | None, optional): Abbreviation of the source the subtitles are downloaded from.\r\n            Defaults to None.\r\n        overwrite (bool, optional): Whether to overwrite files if they already exist. Defaults to True.\r\n\r\n    Returns:\r\n        Path: Path to the downloaded subtitles file.\r\n\r\n    Raises:\r\n        ValueError: If the path in `output_path` does not exist.\r\n    \"\"\"\r\n    output_path = Path(output_path)\r\n\r\n    if not output_path.is_dir():\r\n        raise ValueError(f\"Invalid path: {output_path}\")\r\n\r\n    if isinstance(media_data, Movie):\r\n        file_name = format_release_name(title=media_data.name,\r\n                                        release_date=media_data.release_date,\r\n                                        media_source=source_abbreviation,\r\n                                        language_code=subtitles_data.language_code,\r\n                                        subtitles_type=subtitles_data.special_type,\r\n                                        file_format=subtitles_data.subtitles_format)\r\n    else:  # isinstance(media_data, Episode):\r\n        file_name = format_release_name(title=media_data.series_name,\r\n                                        release_date=media_data.release_date,\r\n                                        season_number=media_data.season_number,\r\n                                        episode_number=media_data.episode_number,\r\n                                        episode_name=media_data.episode_name,\r\n                                        media_source=source_abbreviation,\r\n                                        language_code=subtitles_data.language_code,\r\n                                        subtitles_type=subtitles_data.special_type,\r\n                                        file_format=subtitles_data.subtitles_format)\r\n\r\n    file_path = output_path / file_name\r\n\r\n    if file_path.exists() and not overwrite:\r\n        file_path = generate_non_conflicting_path(file_path=file_path)\r\n\r\n    with file_path.open('wb') as f:\r\n        f.write(subtitles_data.content)\r\n\r\n    return file_path\r\n\r\ndef format_config_validation_error(exc: ValidationError) -> str:\r\n    validation_errors = exc.errors()\r\n    error_str = \"\"\r\n\r\n    for validation_error in validation_errors:\r\n        value: Any = validation_error['input']\r\n        value_type: str = type(value).__name__\r\n        location: list[str] = [str(item) for item in validation_error[\"loc\"]]\r\n        error_msg: str = validation_error['msg']\r\n\r\n        # When the expected type is a union, Pydantic returns several errors for each type,\r\n        # with the type being the last item in the location list\r\n        if (\r\n                isinstance(location[-1], str) and\r\n                (location[-1].endswith(']') or location[-1] in ('str', 'int', 'float', 'bool'))\r\n        ):\r\n            location.pop()\r\n\r\n        if len(location) > 1:\r\n            location_str = \".\".join(location)\r\n\r\n        else:\r\n            location_str = location[0]\r\n\r\n        error_str += f\"'{location_str}' (value: '{value}', type: '{value_type}'): {error_msg}\\n\"\r\n\r\n    return error_str\r\n\r\n\r\ndef format_media_description(media_data: MediaBase, shortened: bool = False) -> str:\r\n    \"\"\"\r\n    Generate a short description string of a media object.\r\n\r\n    Args:\r\n        media_data (MediaBase): An object containing media data.\r\n        shortened (bool, optional): Whether to generate a shortened description. Defaults to False.\r\n\r\n    Returns:\r\n        str: A short description string of the media object.\r\n    \"\"\"\r\n    if isinstance(media_data, Movie):\r\n        release_year = (\r\n            media_data.release_date.year\r\n            if isinstance(media_data.release_date, dt.datetime)\r\n            else media_data.release_date\r\n        )\r\n        description_str = f\"{media_data.name} [{release_year}]\"\r\n\r\n        if media_data.id:\r\n            description_str += f\" (ID: {media_data.id})\"\r\n\r\n        return description_str\r\n\r\n    if isinstance(media_data, Series):\r\n        description_str = f\"{media_data.series_name}\"\r\n\r\n        if media_data.series_release_date:\r\n            if isinstance(media_data.series_release_date, dt.datetime):\r\n                description_str += f\" [{media_data.series_release_date.year}]\"\r\n\r\n            else:\r\n                description_str += f\" [{media_data.series_release_date}]\"\r\n\r\n        if media_data.id:\r\n            description_str += f\" (ID: {media_data.id})\"\r\n\r\n        return description_str\r\n\r\n    if isinstance(media_data, Season):\r\n        if shortened:\r\n            description_str = f\"Season {media_data.season_number:02d}\"\r\n\r\n        else:\r\n            description_str = f\"{media_data.series_name} - Season {media_data.season_number:02d}\"\r\n\r\n        if media_data.season_name:\r\n            description_str += f\" - {media_data.season_name}\"\r\n\r\n        if media_data.id:\r\n            description_str += f\" (ID: {media_data.id})\"\r\n\r\n        return description_str\r\n\r\n    if isinstance(media_data, Episode):\r\n        if shortened:\r\n            description_str = f\"S{media_data.season_number:02d}E{media_data.episode_number:02d}\"\r\n\r\n        else:\r\n            description_str = (f\"{media_data.series_name} - \"\r\n                               f\"S{media_data.season_number:02d}E{media_data.episode_number:02d}\")\r\n\r\n        if media_data.episode_name:\r\n            description_str += f\" - {media_data.episode_name}\"\r\n\r\n        if media_data.id:\r\n            description_str += f\" (ID: {media_data.id})\"\r\n\r\n        return description_str\r\n\r\n    raise ValueError(f\"Unsupported media type: '{type(media_data)}'\")\r\n\r\n\r\n@lru_cache\r\ndef format_release_name(title: str,\r\n                        release_date: dt.datetime | int | None = None,\r\n                        season_number: int | None = None,\r\n                        episode_number: int | None = None,\r\n                        episode_name: str | None = None,\r\n                        media_source: str | None = None,\r\n                        source_type: str | None = \"WEB\",\r\n                        additional_info: str | list[str] | None = None,\r\n                        language_code: str | None = None,\r\n                        subtitles_type: SubtitlesType | None = None,\r\n                        file_format: str | SubtitlesFormatType | None = None) -> str:\r\n    \"\"\"\r\n    Format a release name.\r\n\r\n    Args:\r\n        title (str): Media title.\r\n        release_date (int | None, optional): Release date (datetime), or year (int) of the media. Defaults to None.\r\n        season_number (int | None, optional): Season number. Defaults to None.\r\n        episode_number (int | None, optional): Episode number. Defaults to None.\r\n        episode_name (str | None, optional): Episode name. Defaults to None.\r\n        media_source (str | None, optional): Media source name (full or abbreviation). Defaults to None.\r\n        source_type(str | None, optional): General source type (WEB, BluRay, etc.). Defaults to None.\r\n        additional_info (list[str] | str | None, optional): Additional info to add to the file name. Defaults to None.\r\n        language_code (str | None, optional): Language code. Defaults to None.\r\n        subtitles_type (SubtitlesType | None, optional): Subtitles type. Defaults to None.\r\n        file_format (SubtitlesFormat | str | None, optional): File format to use.  Defaults to None.\r\n\r\n    Returns:\r\n        str: Generated file name.\r\n    \"\"\"\r\n    file_name = standardize_title(title).rstrip('.')\r\n\r\n    if release_date is not None:\r\n        if isinstance(release_date, dt.datetime):\r\n            release_year = release_date.year\r\n\r\n        else:\r\n            release_year = release_date\r\n\r\n        file_name += f\".{release_year}\"\r\n\r\n    if season_number is not None and episode_number is not None:\r\n        file_name += f\".S{season_number:02}E{episode_number:02}\"\r\n\r\n    if episode_name is not None:\r\n        file_name += f\".{standardize_title(episode_name).rstrip('.')}\"\r\n\r\n    if media_source is not None:\r\n        file_name += f\".{media_source}\"\r\n\r\n    if source_type is not None:\r\n        file_name += f\".{source_type}\"\r\n\r\n    if additional_info is not None:\r\n        if isinstance(additional_info, (list, tuple)):\r\n            additional_info = '.'.join(additional_info)\r\n\r\n        file_name += f\".{additional_info}\"\r\n\r\n    if language_code is not None:\r\n        file_name += f\".{language_code}\"\r\n\r\n    if subtitles_type is not None:\r\n        file_name += f\".{subtitles_type.value.lower()}\"\r\n\r\n    if file_format is not None:\r\n        if isinstance(file_format, SubtitlesFormatType):\r\n            file_format = file_format.value.file_extension\r\n\r\n        file_name += f\".{file_format}\"\r\n\r\n    return file_name\r\n\r\n\r\ndef format_subtitles_description(language_code: str, language_name: str | None = None,\r\n                                 special_type: SubtitlesType | None = None) -> str:\r\n    if language_name:\r\n        language_str = f\"{language_name} ({language_code})\"\r\n\r\n    else:\r\n        language_str = language_code\r\n\r\n    if special_type:\r\n        language_str += f\" [{special_type.value}]\"\r\n\r\n    return language_str\r\n\r\n\r\ndef get_model_field(model: BaseModel | None, field: str, convert_to_dict: bool = False) -> Any:\r\n    \"\"\"\r\n    Get a field from a Pydantic model.\r\n\r\n    Args:\r\n        model (BaseModel | None): A Pydantic model.\r\n        field (str): Field name to retrieve.\r\n        convert_to_dict (bool, optional): Whether to convert the field value to a dictionary. Defaults to False.\r\n\r\n    Returns:\r\n        Any: The field value.\r\n    \"\"\"\r\n    if model and hasattr(model, field):\r\n        field_value = getattr(model, field)\r\n\r\n        if convert_to_dict:\r\n            return field_value.dict()\r\n\r\n        return field_value\r\n\r\n    return None\r\n\r\n\r\n\r\ndef generate_non_conflicting_path(file_path: Path, has_extension: bool = True) -> Path:\r\n    \"\"\"\r\n    Generate a non-conflicting path for a file.\r\n    If the file already exists, a number will be added to the end of the file name.\r\n\r\n    Args:\r\n        file_path (Path): Path to a file.\r\n        has_extension (bool, optional): Whether the name of the file includes file extension. Defaults to True.\r\n\r\n    Returns:\r\n        Path: A non-conflicting file path.\r\n    \"\"\"\r\n    if isinstance(file_path, str):\r\n        file_path = Path(file_path)\r\n\r\n    if not file_path.exists():\r\n        return file_path\r\n\r\n    i = 1\r\n    while True:\r\n        if has_extension:\r\n            new_file_path = file_path.parent / f\"{file_path.stem}-{i}{file_path.suffix}\"\r\n\r\n        else:\r\n            new_file_path = file_path.parent / f\"{file_path}-{i}\"\r\n\r\n        if not new_file_path.exists():\r\n            return new_file_path\r\n\r\n        i += 1\r\n\r\n\r\ndef merge_dict_values(*dictionaries: dict) -> dict:\r\n    \"\"\"\r\n    A function for merging the values of multiple dictionaries using the same keys.\r\n    If a key already exists, the value will be added to a list of values mapped to that key.\r\n\r\n    Examples:\r\n        merge_dict_values({'a': 1, 'b': 3}, {'a': 2, 'b': 4}) -> {'a': [1, 2], 'b': [3, 4]}\r\n        merge_dict_values({'a': 1, 'b': 2}, {'a': 1, 'b': [2, 3]}) -> {'a': 1, 'b': [2, 3]}\r\n\r\n    Note:\r\n        This function support only merging of lists or single items (no tuples or other iterables),\r\n        and without any nesting (lists within lists).\r\n\r\n    Args:\r\n        *dictionaries (dict): Dictionaries to merge.\r\n\r\n    Returns:\r\n        dict: A merged dictionary.\r\n    \"\"\"\r\n    _dictionaries: list[dict] = [d for d in dictionaries if d]\r\n\r\n    if len(_dictionaries) == 0:\r\n        return {}\r\n\r\n    if len(_dictionaries) == 1:\r\n        return _dictionaries[0]\r\n\r\n    result: dict = {}\r\n\r\n    for _dict in _dictionaries:\r\n        for key, value in _dict.items():\r\n            if key in result:\r\n                if isinstance(result[key], list):\r\n                    if isinstance(value, list):\r\n                        result[key].extend(value)\r\n                    else:\r\n                        result[key].append(value)\r\n                else:\r\n                    if isinstance(value, list):\r\n                        result[key] = [result[key], *value]\r\n                    else:\r\n                        result[key] = [result[key], value]\r\n            else:\r\n                result[key] = value\r\n\r\n    return result\r\n\r\n\r\ndef normalize_config_name(name: str) -> str:\r\n    \"\"\"\r\n    Normalize a config category / field name (used for creating an alias).\r\n\r\n    Args:\r\n        name (str): The name to normalize.\r\n\r\n    Returns:\r\n        str: The normalized name.\r\n    \"\"\"\r\n    return name.lower().replace('_', '-')\r\n\r\n\r\ndef raise_for_status(response: httpx.Response) -> None:\r\n    \"\"\"\r\n    Raise an exception if the response status code is invalid.\r\n    Uses 'response.raise_for_status()' internally, with additional logging.\r\n\r\n    Args:\r\n        response (httpx.Response): A response object.\r\n    \"\"\"\r\n    truncation_threshold = 1500\r\n\r\n    if not response.is_error:\r\n        return\r\n\r\n    if len(response.text) > truncation_threshold:\r\n        # Truncate the response as in some cases there could be an unexpected long HTML response\r\n        response_text = response.text[:truncation_threshold].rstrip() + \" <TRUNCATED...>\"\r\n\r\n    else:\r\n        response_text = response.text\r\n\r\n    logger.debug(f\"Response status code: {response.status_code}\")\r\n\r\n    if response.headers.get('Content-Type'):\r\n        logger.debug(f\"Response type: {response.headers['Content-Type']}\")\r\n\r\n    logger.debug(f\"Response text: {response_text}\")\r\n\r\n    response.raise_for_status()\r\n\r\n\r\ndef parse_url_params(url_params: str) -> dict:\r\n    \"\"\"\r\n    Parse GET parameters from a URL to a dictionary.\r\n\r\n    Args:\r\n        url_params (str): URL parameters. (e.g. 'param1=value1&param2=value2')\r\n\r\n    Returns:\r\n        dict: A dictionary containing the URL parameters.\r\n    \"\"\"\r\n    url_params = url_params.split('?')[-1].rstrip('&')\r\n    params_list = url_params.split('&')\r\n\r\n    if len(params_list) == 0 or \\\r\n            (len(params_list) == 1 and '=' not in params_list[0]):\r\n        return {}\r\n\r\n    return {key: value for key, value in (param.split('=') for param in params_list)}\r\n\r\n\r\n@overload\r\ndef return_first_valid(*values: T | None, raise_error: Literal[True] = ...) -> T:\r\n    ...\r\n\r\n\r\n@overload\r\ndef return_first_valid(*values: T | None, raise_error: Literal[False] = ...) -> T | None:\r\n    ...\r\n\r\n\r\ndef return_first_valid(*values: T | None, raise_error: bool = False) -> T | None:\r\n    \"\"\"\r\n    Return the first non-None value from a list of values.\r\n\r\n    Args:\r\n        *values (T): Values to check.\r\n        raise_error (bool, optional): Whether to raise an error if all values are None. Defaults to False.\r\n\r\n    Returns:\r\n        T | None: The first non-None value, or None if all values are None and `raise_error` is False.\r\n\r\n    Raises:\r\n        ValueError: If all values are None and `raise_error` is True.\r\n    \"\"\"\r\n    for value in values:\r\n        if value is not None:\r\n            return value\r\n\r\n    if raise_error:\r\n        raise ValueError(\"No valid value found.\")\r\n\r\n    return None\r\n\r\ndef single_to_list(obj: Any) -> list:\r\n    \"\"\"\r\n    Convert a single non-iterable object to a list.\r\n    If None is passed, an empty list will be returned.\r\n\r\n    Args:\r\n        obj: Object to convert.\r\n\r\n    Returns:\r\n        list: A list containing the object.\r\n            If the object is already an iterable, it will be converted to a list.\r\n    \"\"\"\r\n    if isinstance(obj, list):\r\n        return obj\r\n\r\n    if obj is None:\r\n        return []\r\n\r\n    # tuple (not a namedtuple) or a set\r\n    if (isinstance(obj, tuple) and not hasattr(obj, '_fields')) or isinstance(obj, set):\r\n        return list(obj)\r\n\r\n    return [obj]\r\n\r\n\r\ndef split_subtitles_timestamp(timestamp: str) -> tuple[dt.time, dt.time]:\r\n    \"\"\"\r\n    Split a subtitles timestamp into start and end.\r\n\r\n    Args:\r\n        timestamp (str): A subtitles timestamp. For example: \"00:00:00.000 --> 00:00:00.000\"\r\n\r\n    Returns:\r\n        tuple(time, time): A tuple containing start and end times as a datetime object.\r\n    \"\"\"\r\n    # Support ',' character in timestamp's milliseconds (used in SubRip format).\r\n    timestamp = timestamp.replace(',', '.')\r\n\r\n    start_time, end_time = timestamp.split(\" --> \")\r\n    return dt.time.fromisoformat(start_time), dt.time.fromisoformat(end_time)\r\n\r\n\r\n@lru_cache\r\ndef standardize_title(title: str) -> str:\r\n    \"\"\"\r\n    Format movie title to a standardized title that can be used as a file name.\r\n\r\n    Args:\r\n        title (str): A movie title.\r\n\r\n    Returns:\r\n        str: The movie title, in a file-name-friendly format.\r\n    \"\"\"\r\n    title = title.strip()\r\n\r\n    for string, replacement_string in TITLE_REPLACEMENT_STRINGS.items():\r\n        title = title.replace(string, replacement_string)\r\n\r\n    title = re.sub(r\"\\.+\", \".\", title)  # Replace multiple dots with a single dot\r\n\r\n    # If running on Windows, rename Windows reserved names to allow file creation\r\n    if sys.platform == 'win32':\r\n        split_title = title.split('.')\r\n\r\n        if split_title[0].upper() in WINDOWS_RESERVED_FILE_NAMES:\r\n            if len(split_title) > 1:\r\n                return split_title[0] + split_title[1] + '.'.join(split_title[2:])\r\n\r\n            if len(split_title) == 1:\r\n                return \"_\" + title\r\n\r\n    return title\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/utils.py b/isubrip/utils.py
--- a/isubrip/utils.py	(revision ea78d7310b108e0a7e0392d467d7b2aef5731fea)
+++ b/isubrip/utils.py	(date 1725475135273)
@@ -560,29 +560,25 @@
 
     return None
 
-def single_to_list(obj: Any) -> list:
+def single_string_to_list(item: str | list[str]) -> list[str]:
     """
-    Convert a single non-iterable object to a list.
+    Convert a single string to a list containing the string.
     If None is passed, an empty list will be returned.
 
     Args:
-        obj: Object to convert.
+        item (str | list[str]): A string or a list of strings.
 
     Returns:
         list: A list containing the object.
             If the object is already an iterable, it will be converted to a list.
     """
-    if isinstance(obj, list):
-        return obj
-
-    if obj is None:
+    if item is None:
         return []
 
-    # tuple (not a namedtuple) or a set
-    if (isinstance(obj, tuple) and not hasattr(obj, '_fields')) or isinstance(obj, set):
-        return list(obj)
+    if isinstance(item, list):
+        return item
 
-    return [obj]
+    return [item]
 
 
 def split_subtitles_timestamp(timestamp: str) -> tuple[dt.time, dt.time]:
Index: isubrip/__main__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nimport asyncio\r\nimport logging\r\nfrom pathlib import Path\r\nimport shutil\r\nimport sys\r\n\r\nimport httpx\r\nfrom pydantic_core._pydantic_core import ValidationError\r\n\r\nfrom isubrip.config import Config\r\nfrom isubrip.constants import (\r\n    ARCHIVE_FORMAT,\r\n    DATA_FOLDER_PATH,\r\n    EVENT_LOOP,\r\n    LOG_FILE_NAME,\r\n    LOG_FILES_PATH,\r\n    PACKAGE_NAME,\r\n    PACKAGE_VERSION,\r\n    PREORDER_MESSAGE,\r\n    TEMP_FOLDER_PATH,\r\n)\r\nfrom isubrip.data_structures import (\r\n    Episode,\r\n    MediaData,\r\n    Movie,\r\n    ScrapedMediaResponse,\r\n    Season,\r\n    Series,\r\n    SubtitlesData,\r\n    SubtitlesDownloadResults,\r\n)\r\nfrom isubrip.logger import CustomLogFileFormatter, CustomStdoutFormatter, logger\r\nfrom isubrip.scrapers.scraper import PlaylistLoadError, Scraper, ScraperError, ScraperFactory, SubtitlesDownloadError\r\nfrom isubrip.subtitle_formats.webvtt import WebVTTCaptionBlock\r\nfrom isubrip.utils import (\r\n    TempDirGenerator,\r\n    download_subtitles_to_file,\r\n    format_config_validation_error,\r\n    format_media_description,\r\n    format_release_name,\r\n    format_subtitles_description,\r\n    generate_non_conflicting_path,\r\n    get_model_field,\r\n    raise_for_status,\r\n    single_to_list,\r\n)\r\n\r\nLOG_ROTATION_SIZE: int | None = None\r\n\r\n\r\n\r\ndef main() -> None:\r\n    try:\r\n        # Assure at least one argument was passed\r\n        if len(sys.argv) < 2:\r\n            print_usage()\r\n            exit(0)\r\n\r\n        if not DATA_FOLDER_PATH.is_dir():\r\n            DATA_FOLDER_PATH.mkdir(parents=True)\r\n\r\n        setup_loggers(stdout_loglevel=logging.INFO,\r\n                      file_loglevel=logging.DEBUG)\r\n\r\n        cli_args = \" \".join(sys.argv[1:])\r\n        logger.debug(f\"CLI Command: {PACKAGE_NAME} {cli_args}\")\r\n        logger.debug(f\"Python version: {sys.version}\")\r\n        logger.debug(f\"Package version: {PACKAGE_VERSION}\")\r\n        logger.debug(f\"OS: {sys.platform}\")\r\n\r\n        try:\r\n            config = Config()\r\n\r\n        except ValidationError as e:\r\n            logger.error(\"Invalid configuration - the following errors were found in the configuration file:\\n\"\r\n                         \"---\\n\" +\r\n                         format_config_validation_error(exc=e) +\r\n                         \"---\\n\"\r\n                         \"Please update your configuration to resolve the issue.\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            exit(1)\r\n\r\n        update_settings(config=config)\r\n\r\n        if config.general.check_for_updates:\r\n            check_for_updates(current_package_version=PACKAGE_VERSION)\r\n\r\n        urls = single_to_list(sys.argv[1:])\r\n        EVENT_LOOP.run_until_complete(download(urls=urls, config=config))\r\n\r\n    except Exception as ex:\r\n        logger.error(f\"Error: {ex}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        exit(1)\r\n\r\n    finally:\r\n        if log_rotation_size := LOG_ROTATION_SIZE:\r\n            handle_log_rotation(log_rotation_size=log_rotation_size)\r\n\r\n        # NOTE: This will only close scrapers that were initialized using the ScraperFactory.\r\n        async_cleanup_coroutines = []\r\n        for scraper in ScraperFactory.get_initialized_scrapers():\r\n            logger.debug(f\"Requests count for '{scraper.name}' scraper: {scraper.requests_count}\")\r\n            scraper.close()\r\n            async_cleanup_coroutines.append(scraper.async_close())\r\n\r\n        EVENT_LOOP.run_until_complete(asyncio.gather(*async_cleanup_coroutines))\r\n        TempDirGenerator.cleanup()\r\n\r\n\r\nasync def download(urls: list[str], config: Config) -> None:\r\n    \"\"\"\r\n    Download subtitles from a given URL.\r\n\r\n    Args:\r\n        urls (list[str]): A list of URLs to download subtitles from.\r\n        config (Config): A config to use for downloading subtitles.\r\n    \"\"\"\r\n    scrapers_configs = {\r\n        scraper_id: get_model_field(config.scrapers, scraper_id) for scraper_id in config.scrapers.model_fields\r\n    }\r\n\r\n    for url in urls:\r\n        try:\r\n            logger.info(f\"Scraping '{url}'...\")\r\n\r\n            scraper = ScraperFactory.get_scraper_instance(url=url, scrapers_configs=scrapers_configs)\r\n\r\n            try:\r\n                logger.debug(f\"Fetching '{url}'...\")\r\n                scraper_response: ScrapedMediaResponse = await scraper.get_data(url=url)\r\n\r\n            except ScraperError as e:\r\n                logger.error(f\"Error: {e}\")\r\n                logger.debug(\"Debug information:\", exc_info=True)\r\n                continue\r\n\r\n            media_data = scraper_response.media_data\r\n            playlist_scraper = ScraperFactory.get_scraper_instance(scraper_id=scraper_response.playlist_scraper,\r\n                                                                   scrapers_configs=scrapers_configs)\r\n\r\n            if not media_data:\r\n                logger.error(f\"Error: No supported media was found for {url}.\")\r\n                continue\r\n\r\n            for media_item in media_data:\r\n                try:\r\n                    logger.info(f\"Found {media_item.media_type}: {format_media_description(media_data=media_item)}\")\r\n                    await download_media(scraper=playlist_scraper,\r\n                                         media_item=media_item,\r\n                                         download_path=config.downloads.folder,\r\n                                         language_filter=config.downloads.languages,\r\n                                         convert_to_srt=config.subtitles.convert_to_srt,\r\n                                         overwrite_existing=config.downloads.overwrite_existing,\r\n                                         archive=config.downloads.zip)\r\n\r\n                except Exception as e:\r\n                    if len(media_data) > 1:\r\n                        logger.warning(f\"Error scraping media item \"\r\n                                       f\"'{format_media_description(media_data=media_item)}': {e}\\n\"\r\n                                       f\"Skipping to next media item...\")\r\n                        logger.debug(\"Debug information:\", exc_info=True)\r\n                        continue\r\n\r\n                    raise\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error while scraping '{url}': {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            continue\r\n\r\n\r\nasync def download_media(scraper: Scraper, media_item: MediaData, download_path: Path,\r\n                              language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                              overwrite_existing: bool = True, archive: bool = False) -> None:\r\n    \"\"\"\r\n    Download a media item.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_item (MediaData): A media data item to download subtitles for.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        archive (bool, optional): Whether to archive the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n    \"\"\"\r\n    if isinstance(media_item, Series):\r\n        for season in media_item.seasons:\r\n            await download_media(media_item=season, scraper=scraper, download_path=download_path,\r\n                                 language_filter=language_filter, convert_to_srt=convert_to_srt,\r\n                                 overwrite_existing=overwrite_existing, archive=archive)\r\n\r\n    elif isinstance(media_item, Season):\r\n        for episode in media_item.episodes:\r\n            logger.info(f\"{format_media_description(media_data=episode, shortened=True)}:\")\r\n            await download_media_item(media_item=episode, scraper=scraper, download_path=download_path,\r\n                                 language_filter=language_filter, convert_to_srt=convert_to_srt,\r\n                                 overwrite_existing=overwrite_existing, archive=archive)\r\n\r\n    elif isinstance(media_item, (Movie, Episode)):\r\n        await download_media_item(media_item=media_item, scraper=scraper, download_path=download_path,\r\n                                 language_filter=language_filter, convert_to_srt=convert_to_srt,\r\n                                 overwrite_existing=overwrite_existing, archive=archive)\r\n\r\n\r\nasync def download_media_item(scraper: Scraper, media_item: Movie | Episode, download_path: Path,\r\n                              language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                              overwrite_existing: bool = True, archive: bool = False) -> None:\r\n    \"\"\"\r\n    Download subtitles for a single media item.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_item (Movie | Episode): A movie or episode data object.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        archive (bool, optional): Whether to archive the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n    \"\"\"\r\n    ex: Exception | None = None\r\n\r\n    if media_item.playlist:\r\n        try:\r\n            results = await download_subtitles(\r\n                scraper=scraper,\r\n                media_data=media_item,\r\n                download_path=download_path,\r\n                language_filter=language_filter,\r\n                convert_to_srt=convert_to_srt,\r\n                overwrite_existing=overwrite_existing,\r\n                archive=archive,\r\n            )\r\n\r\n            success_count = len(results.successful_subtitles)\r\n            failed_count = len(results.failed_subtitles)\r\n\r\n            if success_count or failed_count:\r\n                logger.info(f\"{success_count}/{success_count + failed_count} matching subtitles \"\r\n                            f\"were successfully downloaded.\")\r\n\r\n            else:\r\n                logger.info(\"No matching subtitles were found.\")\r\n\r\n            return  # noqa: TRY300\r\n\r\n        except PlaylistLoadError as e:\r\n            ex = e\r\n\r\n    # We get here if there is no playlist, or there is one, but it failed to load\r\n    if isinstance(media_item, Movie) and media_item.preorder_availability_date:\r\n        preorder_date_str = media_item.preorder_availability_date.strftime(\"%Y-%m-%d\")\r\n        logger.info(PREORDER_MESSAGE.format(movie_name=media_item.name, scraper_name=scraper.name,\r\n                                            preorder_date=preorder_date_str))\r\n\r\n    else:\r\n        if ex:\r\n            logger.error(f\"Error: {ex}\")\r\n\r\n        else:\r\n            logger.error(\"Error: No valid playlist was found.\")\r\n\r\n\r\ndef check_for_updates(current_package_version: str) -> None:\r\n    \"\"\"\r\n    Check and print if a newer version of the package is available, and log accordingly.\r\n\r\n    Args:\r\n        current_package_version (str): The current version of the package.\r\n    \"\"\"\r\n    api_url = f\"https://pypi.org/pypi/{PACKAGE_NAME}/json\"\r\n    logger.debug(\"Checking for package updates on PyPI...\")\r\n    try:\r\n        response = httpx.get(\r\n            url=api_url,\r\n            headers={\"Accept\": \"application/json\"},\r\n            timeout=5,\r\n        )\r\n        raise_for_status(response)\r\n        response_data = response.json()\r\n\r\n        pypi_latest_version = response_data[\"info\"][\"version\"]\r\n\r\n        if pypi_latest_version != current_package_version:\r\n            logger.warning(f\"You are currently using version '{current_package_version}' of '{PACKAGE_NAME}', \"\r\n                           f\"however version '{pypi_latest_version}' is available.\"\r\n                           f'\\nConsider upgrading by running \"pip install --upgrade {PACKAGE_NAME}\"\\n')\r\n\r\n        else:\r\n            logger.debug(f\"Latest version of '{PACKAGE_NAME}' ({current_package_version}) is currently installed.\")\r\n\r\n    except Exception as e:\r\n        logger.warning(f\"Update check failed: {e}\")\r\n        logger.debug(\"Debug information:\", exc_info=True)\r\n        return\r\n\r\n\r\nasync def download_subtitles(scraper: Scraper, media_data: Movie | Episode, download_path: Path,\r\n                             language_filter: list[str] | None = None, convert_to_srt: bool = False,\r\n                             overwrite_existing: bool = True, archive: bool = False) -> SubtitlesDownloadResults:\r\n    \"\"\"\r\n    Download subtitles for the given media data.\r\n\r\n    Args:\r\n        scraper (Scraper): A Scraper object to use for downloading subtitles.\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        download_path (Path): Path to a folder where the subtitles will be downloaded to.\r\n        language_filter (list[str] | None): List of specific languages to download subtitles for.\r\n            None for all languages (no filter). Defaults to None.\r\n        convert_to_srt (bool, optional): Whether to convert the subtitles to SRT format. Defaults to False.\r\n        overwrite_existing (bool, optional): Whether to overwrite existing subtitles. Defaults to True.\r\n        archive (bool, optional): Whether to archive the subtitles into a single zip file\r\n            (only if there are multiple subtitles).\r\n\r\n    Returns:\r\n        SubtitlesDownloadResults: A SubtitlesDownloadResults object containing the results of the download.\r\n    \"\"\"\r\n    temp_dir_name = generate_media_folder_name(media_data=media_data, source=scraper.abbreviation)\r\n    temp_download_path = TempDirGenerator.generate(directory_name=temp_dir_name)\r\n\r\n    successful_downloads: list[SubtitlesData] = []\r\n    failed_downloads: list[SubtitlesDownloadError] = []\r\n    temp_downloads: list[Path] = []\r\n\r\n    if not media_data.playlist:\r\n        raise PlaylistLoadError(\"No playlist was found for provided media data.\")\r\n\r\n    main_playlist = await scraper.load_playlist(url=media_data.playlist)  # type: ignore[func-returns-value]\r\n\r\n    if not main_playlist:\r\n        raise PlaylistLoadError(\"Failed to load the main playlist.\")\r\n\r\n    matching_subtitles = scraper.find_matching_subtitles(main_playlist=main_playlist,  # type: ignore[var-annotated]\r\n                                                         language_filter=language_filter)\r\n\r\n    logger.debug(f\"{len(matching_subtitles)} matching subtitles were found.\")\r\n\r\n    for matching_subtitles_item in matching_subtitles:\r\n        subtitles_data = await scraper.download_subtitles(media_data=matching_subtitles_item,\r\n                                                          subrip_conversion=convert_to_srt)\r\n        language_info = format_subtitles_description(language_code=subtitles_data.language_code,\r\n                                                     language_name=subtitles_data.language_name,\r\n                                                     special_type=subtitles_data.special_type)\r\n\r\n        if isinstance(subtitles_data, SubtitlesDownloadError):\r\n            logger.warning(f\"Failed to download '{language_info}' subtitles. Skipping...\")\r\n            logger.debug(\"Debug information:\", exc_info=subtitles_data.original_exc)\r\n            failed_downloads.append(subtitles_data)\r\n            continue\r\n\r\n        try:\r\n            temp_downloads.append(download_subtitles_to_file(\r\n                media_data=media_data,\r\n                subtitles_data=subtitles_data,\r\n                output_path=temp_download_path,\r\n                source_abbreviation=scraper.abbreviation,\r\n                overwrite=overwrite_existing,\r\n            ))\r\n\r\n            logger.info(f\"'{language_info}' subtitles were successfully downloaded.\")\r\n            successful_downloads.append(subtitles_data)\r\n\r\n        except Exception as e:\r\n            logger.error(f\"Error: Failed to save '{language_info}' subtitles: {e}\")\r\n            logger.debug(\"Debug information:\", exc_info=True)\r\n            failed_downloads.append(\r\n                SubtitlesDownloadError(\r\n                    language_code=subtitles_data.language_code,\r\n                    language_name=subtitles_data.language_name,\r\n                    special_type=subtitles_data.special_type,\r\n                    original_exc=e,\r\n                ),\r\n            )\r\n\r\n    if not archive or len(temp_downloads) == 1:\r\n        for file_path in temp_downloads:\r\n            if overwrite_existing:\r\n                new_path = download_path / file_path.name\r\n\r\n            else:\r\n                new_path = generate_non_conflicting_path(file_path=download_path / file_path.name)\r\n\r\n            # str conversion needed only for Python <= 3.8 - https://github.com/python/cpython/issues/76870\r\n            shutil.move(src=str(file_path), dst=new_path)\r\n\r\n    elif len(temp_downloads) > 0:\r\n        archive_path = Path(shutil.make_archive(\r\n            base_name=str(temp_download_path.parent / temp_download_path.name),\r\n            format=ARCHIVE_FORMAT,\r\n            root_dir=temp_download_path,\r\n        ))\r\n\r\n        file_name = generate_media_folder_name(media_data=media_data,\r\n                                               source=scraper.abbreviation) + f\".{ARCHIVE_FORMAT}\"\r\n\r\n        if overwrite_existing:\r\n            destination_path = download_path / file_name\r\n\r\n        else:\r\n            destination_path = generate_non_conflicting_path(file_path=download_path / file_name)\r\n\r\n        shutil.move(src=str(archive_path), dst=destination_path)\r\n\r\n    return SubtitlesDownloadResults(\r\n        media_data=media_data,\r\n        successful_subtitles=successful_downloads,\r\n        failed_subtitles=failed_downloads,\r\n        is_archive=archive,\r\n    )\r\n\r\n\r\ndef handle_log_rotation(log_rotation_size: int) -> None:\r\n    \"\"\"\r\n    Handle log rotation and remove old log files if needed.\r\n\r\n    Args:\r\n        log_rotation_size (int): Maximum amount of log files to keep.\r\n    \"\"\"\r\n    sorted_log_files = sorted(LOG_FILES_PATH.glob(\"*.log\"), key=lambda file: file.stat().st_mtime, reverse=True)\r\n\r\n    if len(sorted_log_files) > log_rotation_size:\r\n        for log_file in sorted_log_files[log_rotation_size:]:\r\n            log_file.unlink()\r\n\r\n\r\ndef generate_media_folder_name(media_data: Movie | Episode, source: str | None = None) -> str:\r\n    \"\"\"\r\n    Generate a folder name for media data.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        str: A folder name for the media data.\r\n    \"\"\"\r\n    if isinstance(media_data, Movie):\r\n        return format_release_name(\r\n            title=media_data.name,\r\n            release_date=media_data.release_date,\r\n            media_source=source,\r\n        )\r\n\r\n    # elif isinstance(media_data, Episode):\r\n    return format_release_name(\r\n        title=media_data.series_name,\r\n        season_number=media_data.season_number,\r\n        episode_number=media_data.episode_number,\r\n        media_source=source,\r\n    )\r\n\r\n\r\ndef generate_temp_media_path(media_data: Movie | Episode, source: str | None = None) -> Path:\r\n    \"\"\"\r\n    Generate a temporary directory for downloading media data.\r\n\r\n    Args:\r\n        media_data (Movie | Episode): A movie or episode data object.\r\n        source (str | None, optional): Abbreviation of the source to use for file names. Defaults to None.\r\n\r\n    Returns:\r\n        Path: A path to the temporary folder.\r\n    \"\"\"\r\n    temp_folder_name = generate_media_folder_name(media_data=media_data, source=source)\r\n    path = generate_non_conflicting_path(file_path=TEMP_FOLDER_PATH / temp_folder_name, has_extension=False)\r\n\r\n    return TempDirGenerator.generate(directory_name=path.name)\r\n\r\n\r\ndef update_settings(config: Config) -> None:\r\n    \"\"\"\r\n    Update settings according to config.\r\n\r\n    Args:\r\n        config (Config): An instance of a config to set settings according to.\r\n    \"\"\"\r\n    Scraper.subtitles_fix_rtl = config.subtitles.fix_rtl\r\n    Scraper.subtitles_remove_duplicates = config.subtitles.remove_duplicates\r\n    Scraper.default_timeout = config.scrapers.default.timeout\r\n    Scraper.default_user_agent = config.scrapers.default.user_agent\r\n    Scraper.default_proxy = config.scrapers.default.proxy\r\n    Scraper.default_verify_ssl = config.scrapers.default.verify_ssl\r\n\r\n    WebVTTCaptionBlock.subrip_alignment_conversion = (\r\n        config.subtitles.webvtt.subrip_alignment_conversion\r\n    )\r\n\r\n    if log_rotation := config.general.log_rotation_size:\r\n        global LOG_ROTATION_SIZE\r\n        LOG_ROTATION_SIZE = log_rotation\r\n\r\n\r\ndef print_usage() -> None:\r\n    \"\"\"Print usage information.\"\"\"\r\n    logger.info(f\"Usage: {PACKAGE_NAME} <iTunes movie URL> [iTunes movie URL...]\")\r\n\r\n\r\ndef setup_loggers(stdout_loglevel: int, file_loglevel: int) -> None:\r\n    \"\"\"\r\n    Configure loggers.\r\n\r\n    Args:\r\n        stdout_loglevel (int): Log level for STDOUT logger.\r\n        file_loglevel (int): Log level for logfile logger.\r\n    \"\"\"\r\n    logger.setLevel(logging.DEBUG)\r\n\r\n    # Setup STDOUT logger\r\n    stdout_handler = logging.StreamHandler(sys.stdout)\r\n    stdout_handler.setLevel(stdout_loglevel)\r\n    stdout_handler.setFormatter(CustomStdoutFormatter())\r\n    logger.addHandler(stdout_handler)\r\n\r\n    # Setup logfile logger\r\n    if not LOG_FILES_PATH.is_dir():\r\n        logger.debug(\"Logs directory could not be found and will be created.\")\r\n        LOG_FILES_PATH.mkdir()\r\n\r\n    logfile_path = generate_non_conflicting_path(file_path=LOG_FILES_PATH / LOG_FILE_NAME)\r\n    logfile_handler = logging.FileHandler(filename=logfile_path, encoding=\"utf-8\")\r\n    logfile_handler.setLevel(file_loglevel)\r\n    logfile_handler.setFormatter(CustomLogFileFormatter())\r\n    logger.debug(f\"Log file location: '{logfile_path}'\")\r\n    logger.addHandler(logfile_handler)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/__main__.py b/isubrip/__main__.py
--- a/isubrip/__main__.py	(revision ea78d7310b108e0a7e0392d467d7b2aef5731fea)
+++ b/isubrip/__main__.py	(date 1725475197565)
@@ -44,7 +44,7 @@
     generate_non_conflicting_path,
     get_model_field,
     raise_for_status,
-    single_to_list,
+    single_string_to_list,
 )
 
 LOG_ROTATION_SIZE: int | None = None
@@ -61,7 +61,7 @@
         if not DATA_FOLDER_PATH.is_dir():
             DATA_FOLDER_PATH.mkdir(parents=True)
 
-        setup_loggers(stdout_loglevel=logging.INFO,
+        setup_loggers(stdout_loglevel=logging.DEBUG,
                       file_loglevel=logging.DEBUG)
 
         cli_args = " ".join(sys.argv[1:])
@@ -87,8 +87,8 @@
         if config.general.check_for_updates:
             check_for_updates(current_package_version=PACKAGE_VERSION)
 
-        urls = single_to_list(sys.argv[1:])
-        EVENT_LOOP.run_until_complete(download(urls=urls, config=config))
+        EVENT_LOOP.run_until_complete(download(urls=single_string_to_list(item=sys.argv[1:]),
+                                               config=config))
 
     except Exception as ex:
         logger.error(f"Error: {ex}")
@@ -96,6 +96,7 @@
         exit(1)
 
     finally:
+        # TODO: Change from const. Perhaps utilize `setup_loggers` or `update_settings` to set log rotation size.
         if log_rotation_size := LOG_ROTATION_SIZE:
             handle_log_rotation(log_rotation_size=log_rotation_size)
 
Index: isubrip/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC\r\nfrom pathlib import Path\r\nfrom typing import TYPE_CHECKING, List, Tuple, Type\r\n\r\nfrom pydantic import BaseModel, ConfigDict, Field, create_model, field_validator\r\nfrom pydantic_core._pydantic_core import PydanticCustomError\r\nfrom pydantic_settings import BaseSettings, PydanticBaseSettingsSource, SettingsConfigDict, TomlConfigSettingsSource\r\n\r\nfrom isubrip.constants import USER_CONFIG_FILE_PATH\r\nfrom isubrip.scrapers.scraper import DefaultScraperConfig, ScraperFactory\r\nfrom isubrip.utils import normalize_config_name\r\n\r\n\r\nclass ConfigCategory(BaseModel, ABC):\r\n    \"\"\"A base class for settings categories.\"\"\"\r\n    model_config = ConfigDict(\r\n        extra='allow',\r\n        alias_generator=normalize_config_name,\r\n    )\r\n\r\n\r\nclass GeneralCategory(ConfigCategory):\r\n    check_for_updates: bool = Field(default=True)\r\n    log_rotation_size: int = Field(default=15)\r\n\r\n\r\nclass DownloadsCategory(ConfigCategory):\r\n    folder: Path = Field(default=Path.cwd().resolve())\r\n    languages: List[str] = Field(default=[])\r\n    overwrite_existing: bool = Field(default=False)\r\n    zip: bool = Field(default=False)\r\n\r\n    @field_validator('folder')\r\n    @classmethod\r\n    def assure_path_exists(cls, value: Path) -> Path:\r\n        if value.exists():\r\n            if not value.is_dir():\r\n                raise PydanticCustomError(\r\n                    \"invalid_path\",\r\n                    \"Path is not a directory.\",\r\n                )\r\n\r\n        else:\r\n            raise PydanticCustomError(\r\n                \"invalid_path\",\r\n                \"Path does not exist.\")\r\n\r\n        return value\r\n\r\n\r\nclass WebVTTSubcategory(ConfigCategory):\r\n    subrip_alignment_conversion: bool = Field(default=False)\r\n\r\n\r\nclass SubtitlesCategory(ConfigCategory):\r\n    fix_rtl: bool = Field(default=False)\r\n    remove_duplicates: bool = Field(default=True)\r\n    convert_to_srt: bool = Field(default=False)\r\n    webvtt: WebVTTSubcategory = WebVTTSubcategory()\r\n\r\n\r\nclass ScrapersCategory(ConfigCategory):\r\n    default: DefaultScraperConfig = Field(default_factory=DefaultScraperConfig)\r\n\r\n\r\n# Resolve mypy errors as mypy doesn't support dynamic models.\r\nif TYPE_CHECKING:\r\n    DynamicScrapersCategory = ScrapersCategory\r\n\r\nelse:\r\n    # A config model that's dynamically created based on the available scrapers and their configurations.\r\n    DynamicScrapersCategory = create_model(\r\n        'DynamicScrapersCategory',\r\n        __base__=ScrapersCategory,\r\n        **{\r\n            scraper.id: (scraper.ScraperConfig, Field(default_factory=scraper.ScraperConfig))\r\n            for scraper in ScraperFactory.get_scraper_classes()\r\n        },  # type: ignore[call-overload]\r\n    )\r\n\r\n\r\nclass Config(BaseSettings):\r\n    model_config = SettingsConfigDict(\r\n        extra='forbid',\r\n        alias_generator=normalize_config_name,\r\n        toml_file=USER_CONFIG_FILE_PATH,\r\n    )\r\n\r\n    general: GeneralCategory = Field(default_factory=GeneralCategory)\r\n    downloads: DownloadsCategory = Field(default_factory=DownloadsCategory)\r\n    subtitles: SubtitlesCategory = Field(default_factory=SubtitlesCategory)\r\n    scrapers: DynamicScrapersCategory = Field(default_factory=DynamicScrapersCategory)\r\n\r\n    @classmethod\r\n    def settings_customise_sources(\r\n        cls,\r\n        settings_cls: Type[BaseSettings],\r\n        init_settings: PydanticBaseSettingsSource,\r\n        env_settings: PydanticBaseSettingsSource,\r\n        dotenv_settings: PydanticBaseSettingsSource,\r\n        file_secret_settings: PydanticBaseSettingsSource,\r\n    ) -> Tuple[PydanticBaseSettingsSource, ...]:\r\n        return (\r\n            init_settings,\r\n            TomlConfigSettingsSource(settings_cls),\r\n            env_settings,\r\n            dotenv_settings,\r\n            file_secret_settings,\r\n        )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/config.py b/isubrip/config.py
--- a/isubrip/config.py	(revision ea78d7310b108e0a7e0392d467d7b2aef5731fea)
+++ b/isubrip/config.py	(date 1725474359205)
@@ -28,7 +28,7 @@
 
 class DownloadsCategory(ConfigCategory):
     folder: Path = Field(default=Path.cwd().resolve())
-    languages: List[str] = Field(default=[])
+    languages: List[str] = Field(default=[])  # TODO: Use Pydantic's Language type.
     overwrite_existing: bool = Field(default=False)
     zip: bool = Field(default=False)
 
Index: isubrip/scrapers/scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import annotations\r\n\r\nfrom abc import ABC, abstractmethod\r\nimport asyncio\r\nfrom enum import Enum\r\nimport importlib\r\nimport inspect\r\nfrom pathlib import Path\r\nimport re\r\nimport sys\r\nfrom typing import TYPE_CHECKING, Any, ClassVar, List, Literal, Type, TypeVar, Union, overload\r\n\r\nimport httpx\r\nimport m3u8\r\nfrom pydantic import BaseModel, ConfigDict, Field, create_model\r\n\r\nfrom isubrip.constants import PACKAGE_NAME, SCRAPER_MODULES_SUFFIX\r\nfrom isubrip.data_structures import (\r\n    MainPlaylist,\r\n    PlaylistMediaItem,\r\n    ScrapedMediaResponse,\r\n    SubtitlesData,\r\n    SubtitlesFormatType,\r\n    SubtitlesType,\r\n)\r\nfrom isubrip.logger import logger\r\nfrom isubrip.utils import (\r\n    SingletonMeta,\r\n    get_model_field,\r\n    merge_dict_values,\r\n    normalize_config_name,\r\n    return_first_valid,\r\n    single_to_list,\r\n)\r\n\r\nif TYPE_CHECKING:\r\n    from types import TracebackType\r\n\r\n    from isubrip.subtitle_formats.subtitles import Subtitles\r\n\r\n\r\nScraperT = TypeVar(\"ScraperT\", bound=\"Scraper\")\r\n\r\n\r\nclass ScraperConfigBase(BaseModel, ABC):\r\n    \"\"\"\r\n    A Pydantic BaseModel for base class for scraper's configuration classes.\r\n    Also serves for setting default configuration settings for all scrapers.\r\n\r\n    Attributes:\r\n        timeout (int | float): Timeout to use when making requests.\r\n        user_agent (st): User agent to use when making requests.\r\n        proxy (str | None): Proxy to use when making requests.\r\n        verify_ssl (bool): Whether to verify SSL certificates.\r\n    \"\"\"\r\n    model_config = ConfigDict(\r\n        extra='forbid',\r\n        alias_generator=normalize_config_name,\r\n    )\r\n\r\n    timeout: int | float | None = Field(default=None)\r\n    user_agent: str | None = Field(default=None, alias=\"user-agent\")\r\n    proxy: str | None = Field(default=None)\r\n    verify_ssl: bool | None = Field(default=None, alias=\"verify-ssl\")\r\n\r\n\r\nclass DefaultScraperConfig(ScraperConfigBase):\r\n    \"\"\"\r\n    A Pydantic BaseModel for base class for scraper's configuration classes.\r\n    Also serves for setting default configuration settings for all scrapers.\r\n\r\n    Attributes:\r\n        timeout (int | float): Timeout to use when making requests.\r\n        user_agent (st): User agent to use when making requests.\r\n        proxy (str | None): Proxy to use when making requests.\r\n        verify_ssl (bool): Whether to verify SSL certificates.\r\n    \"\"\"\r\n    timeout: int | float = Field(default=10)\r\n    user_agent: str = Field(\r\n        default=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\",    # noqa: E501\r\n        alias=\"user-agent\",\r\n    )\r\n    proxy: str | None = Field(default=None)\r\n    verify_ssl: bool = Field(default=True, alias=\"verify-ssl\")\r\n\r\n\r\nclass ScraperConfigSubcategory(BaseModel, ABC):\r\n    \"\"\"A Pydantic BaseModel for a scraper's configuration subcategory (which can be set under 'ScraperConfig').\"\"\"\r\n    model_config = ConfigDict(\r\n        extra='forbid',\r\n        alias_generator=normalize_config_name,\r\n    )\r\n\r\n\r\nclass Scraper(ABC, metaclass=SingletonMeta):\r\n    \"\"\"\r\n    A base class for scrapers.\r\n\r\n    Attributes:\r\n        default_user_agent (str): [Class Attribute]\r\n            Default user agent to use if no other user agent is specified when making requests.\r\n        default_proxy (str | None): [Class Attribute] Default proxy to use when making requests.\r\n        default_verify_ssl (bool): [Class Attribute] Whether to verify SSL certificates by default.\r\n        subtitles_fix_rtl (bool): [Class Attribute] Whether to fix RTL from downloaded subtitles.\r\n            A list of languages to fix RTL on. If None, a default list will be used.\r\n        subtitles_remove_duplicates (bool): [Class Attribute]\r\n            Whether to remove duplicate lines from downloaded subtitles.\r\n\r\n        id (str): [Class Attribute] ID of the scraper (must be unique).\r\n        name (str): [Class Attribute] Name of the scraper.\r\n        abbreviation (str): [Class Attribute] Abbreviation of the scraper.\r\n        url_regex (re.Pattern | list[re.Pattern]): [Class Attribute] A RegEx pattern to find URLs matching the service.\r\n        subtitles_class (type[Subtitles]): [Class Attribute] Class of the subtitles format returned by the scraper.\r\n        is_movie_scraper (bool): [Class Attribute] Whether the scraper is for movies.\r\n        is_series_scraper (bool): [Class Attribute] Whether the scraper is for series.\r\n        uses_scrapers (list[str]): [Class Attribute] A list of IDs for other scraper classes that this scraper uses.\r\n            This assures that the config data for the other scrapers is passed as well.\r\n        _session (httpx.Client): A synchronous HTTP client session.\r\n        _async_session (httpx.AsyncClient): An asynchronous HTTP client session.\r\n\r\n    Notes:\r\n        Each scraper implements its own `ScraperConfig` class (which can be overridden and updated),\r\n         inheriting from `ScraperConfigBase`, which sets configurable options for the scraper.\r\n    \"\"\"\r\n    default_timeout: ClassVar[int | float] = 10\r\n    default_user_agent: ClassVar[str] = httpx._client.USER_AGENT  # noqa: SLF001\r\n    default_proxy: ClassVar[str | None] = None\r\n    default_verify_ssl: ClassVar[bool] = True\r\n    subtitles_fix_rtl: ClassVar[bool] = False\r\n    subtitles_remove_duplicates: ClassVar[bool] = True\r\n\r\n    id: ClassVar[str]\r\n    name: ClassVar[str]\r\n    abbreviation: ClassVar[str]\r\n    url_regex: ClassVar[re.Pattern | list[re.Pattern]]\r\n    subtitles_class: ClassVar[type[Subtitles]]\r\n    is_movie_scraper: ClassVar[bool] = False\r\n    is_series_scraper: ClassVar[bool] = False\r\n    uses_scrapers: ClassVar[list[str]] = []\r\n\r\n    class ScraperConfig(ScraperConfigBase):\r\n        \"\"\"Set a default configuration for all scrapers using the default fields.\"\"\"\r\n\r\n\r\n    def __init__(self, timeout: int | float | None = None, user_agent: str | None = None,\r\n                 proxy: str | None = None, verify_ssl: bool | None = None, config: ScraperConfig | None = None):\r\n        \"\"\"\r\n        Initialize a Scraper object.\r\n\r\n        Args:\r\n            timeout (int | float | None, optional): A timeout to use when making requests. Defaults to None.\r\n            user_agent (str | None, optional): A user agent to use when making requests. Defaults to None.\r\n            proxy (str | None, optional): A proxy to use when making requests. Defaults to None.\r\n            verify_ssl (bool | None, optional): Whether to verify SSL certificates. Defaults to None.\r\n            config (ScraperConfig | None, optional): An optional config object\r\n                containing scraper configuration settings. Defaults to None.\r\n        \"\"\"\r\n        self._config = config\r\n        self._timeout = return_first_valid(timeout,\r\n                                           get_model_field(model=config, field='timeout'),\r\n                                           self.default_timeout,\r\n                                           raise_error=True)\r\n        self._user_agent = return_first_valid(user_agent,\r\n                                              get_model_field(model=config, field='user_agent'),\r\n                                              self.default_user_agent,\r\n                                              raise_error=True)\r\n        self._proxy = return_first_valid(proxy,\r\n                                         get_model_field(model=config, field='proxy'),\r\n                                         self.default_proxy)\r\n        self._verify_ssl = return_first_valid(verify_ssl,\r\n                                              get_model_field(model=config, field='verify_ssl'),\r\n                                              self.default_verify_ssl,\r\n                                              raise_error=True)\r\n\r\n        if self._timeout != self.default_timeout:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with custom timeout: '{self._timeout}'.\")\r\n\r\n        if self._user_agent != self.default_user_agent:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with custom user-agent: '{self._user_agent}'.\")\r\n\r\n        if self._proxy != self.default_proxy:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with proxy: '{self._proxy}'.\")\r\n\r\n        if self._verify_ssl != self.default_verify_ssl:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with SSL verification set to: '{self._verify_ssl}'.\")\r\n\r\n        self._requests_counter = 0\r\n        clients_params: dict[str, Any] = {\r\n            \"headers\": {\"User-Agent\": self._user_agent},\r\n            \"verify\": self._verify_ssl,\r\n            \"proxy\": self._proxy,\r\n            \"timeout\": float(self._timeout),\r\n        }\r\n        self._session = httpx.Client(\r\n            **clients_params,\r\n            event_hooks={\r\n                \"request\": [self._increment_requests_counter],\r\n            },\r\n        )\r\n        self._async_session = httpx.AsyncClient(\r\n            **clients_params,\r\n            event_hooks={\r\n                \"request\": [self._async_increment_requests_counter],\r\n            },\r\n        )\r\n\r\n        # Update session settings according to configurations\r\n        self._session.headers.update({\"User-Agent\": self._user_agent})\r\n        self._async_session.headers.update({\"User-Agent\": self._user_agent})\r\n\r\n        if not self._verify_ssl:\r\n            import urllib3\r\n            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\n    def _increment_requests_counter(self, request: httpx.Request) -> None:  # noqa: ARG002\r\n        self._requests_counter += 1\r\n\r\n    async def _async_increment_requests_counter(self, request: httpx.Request) -> None:  # noqa: ARG002\r\n        self._requests_counter += 1\r\n\r\n    @property\r\n    def requests_count(self) -> int:\r\n        return self._requests_counter\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[True] = ...) -> re.Match:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def match_url(cls, url: str, raise_error: Literal[False] = ...) -> re.Match | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def match_url(cls, url: str, raise_error: bool = False) -> re.Match | None:\r\n        \"\"\"\r\n        Checks if a URL matches scraper's url regex.\r\n\r\n        Args:\r\n            url (str): A URL to check against the regex.\r\n            raise_error (bool, optional): Whether to raise an error instead of returning None if the URL doesn't match.\r\n\r\n        Returns:\r\n            re.Match | None: A Match object if the URL matches the regex, None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If the URL doesn't match the regex and raise_error is True.\r\n        \"\"\"\r\n        if isinstance(cls.url_regex, re.Pattern) and (match_result := re.fullmatch(pattern=cls.url_regex, string=url)):\r\n            return match_result\r\n\r\n        if isinstance(cls.url_regex, list):\r\n            for url_regex_item in cls.url_regex:\r\n                if result := re.fullmatch(pattern=url_regex_item, string=url):\r\n                    return result\r\n\r\n        if raise_error:\r\n            raise ValueError(f\"URL '{url}' doesn't match the URL regex of {cls.name}.\")\r\n\r\n        return None\r\n\r\n    def __enter__(self) -> Scraper:\r\n        return self\r\n\r\n    def __exit__(self, exc_type: Type[BaseException] | None,\r\n                 exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:\r\n        self.close()\r\n\r\n    async def async_close(self) -> None:\r\n        await self._async_session.aclose()\r\n\r\n    def close(self) -> None:\r\n        self._session.close()\r\n\r\n    @abstractmethod\r\n    async def get_data(self, url: str) -> ScrapedMediaResponse:\r\n        \"\"\"\r\n        Scrape media information about the media on a URL.\r\n\r\n        Args:\r\n            url (str): A URL to get media information about.\r\n\r\n        Returns:\r\n            ScrapedMediaResponse: A ScrapedMediaResponse object containing scraped media information.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    async def download_subtitles(self, media_data: PlaylistMediaItem, subrip_conversion: bool = False) -> SubtitlesData:\r\n        \"\"\"\r\n        Download subtitles from a media object.\r\n\r\n        Args:\r\n            media_data (PlaylistMediaItem): A media object to download subtitles from.\r\n            subrip_conversion (bool, optional): Whether to convert the subtitles to SubRip format. Defaults to False.\r\n\r\n        Returns:\r\n            SubtitlesData: A SubtitlesData object containing downloaded subtitles.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def find_matching_media(self, main_playlist: MainPlaylist,\r\n                            filters: dict[str, str | list[str]] | None = None) -> list:\r\n        \"\"\"\r\n        Find media items that match the given filters in the main playlist (or all media items if no filters are given).\r\n\r\n        Args:\r\n            main_playlist (MainPlaylist): Main playlist to search for media items in.\r\n            filters (dict[str, str | list[str]] | None, optional): A dictionary of filters to match media items against.\r\n                Defaults to None.\r\n\r\n        Returns:\r\n            list: A list of media items that match the given filters.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def find_matching_subtitles(self, main_playlist: MainPlaylist,\r\n                                language_filter: list[str] | None = None) -> list[PlaylistMediaItem]:\r\n        \"\"\"\r\n        Find subtitles that match the given language filter in the main playlist.\r\n\r\n        Args:\r\n            main_playlist (MainPlaylist): Main playlist to search for subtitles in.\r\n            language_filter (list[str] | None, optional): A list of language codes to filter subtitles by.\r\n                Defaults to None.\r\n\r\n        Returns:\r\n            list[PlaylistMediaItem]: A list of subtitles that match the given language filter.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    async def load_playlist(self, url: str | list[str], headers: dict | None = None) -> MainPlaylist | None:\r\n        \"\"\"\r\n        Load a playlist from a URL to a representing object.\r\n        Multiple URLs can be given, in which case the first one that loads successfully will be returned.\r\n\r\n        Args:\r\n            url (str | list[str]): URL of the M3U8 playlist to load. Can also be a list of URLs (for redundancy).\r\n            headers (dict | None, optional): A dictionary of headers to use when making the request.\r\n                Defaults to None (results in using session's configured headers).\r\n\r\n        Returns:\r\n            MainPlaylist | None: A playlist object (matching the type), or None if the playlist couldn't be loaded.\r\n        \"\"\"\r\n\r\n\r\nclass HLSScraper(Scraper, ABC):\r\n    \"\"\"A base class for HLS (m3u8) scrapers.\"\"\"\r\n    class M3U8Attribute(Enum):\r\n        \"\"\"\r\n        An enum representing all possible M3U8 attributes.\r\n        Names / Keys represent M3U8 Media object attributes (should be converted to lowercase),\r\n        and values represent the name of the key for config usage.\r\n        \"\"\"\r\n        ASSOC_LANGUAGE = \"assoc-language\"\r\n        AUTOSELECT = \"autoselect\"\r\n        CHARACTERISTICS = \"characteristics\"\r\n        CHANNELS = \"channels\"\r\n        DEFAULT = \"default\"\r\n        FORCED = \"forced\"\r\n        GROUP_ID = \"group-id\"\r\n        INSTREAM_ID = \"instream-id\"\r\n        LANGUAGE = \"language\"\r\n        NAME = \"name\"\r\n        STABLE_RENDITION_ID = \"stable-rendition-id\"\r\n        TYPE = \"type\"\r\n\r\n    default_playlist_filters: ClassVar[dict[str, str | list[str] | None] | None] = None\r\n\r\n    _subtitles_filters: dict[str, str | list[str]] = {\r\n        M3U8Attribute.TYPE.value: \"SUBTITLES\",\r\n    }\r\n\r\n    # Resolve mypy errors as mypy doesn't support dynamic models.\r\n    if TYPE_CHECKING:\r\n        PlaylistFiltersSubcategory = ScraperConfigSubcategory\r\n\r\n    else:\r\n        PlaylistFiltersSubcategory = create_model(\r\n            \"PlaylistFiltersSubcategory\",\r\n            __base__=ScraperConfigSubcategory,\r\n            **{\r\n                m3u8_attribute.value: (Union[str, List[str], None],\r\n                                       Field(alias=normalize_config_name(m3u8_attribute.value), default=None))\r\n                for m3u8_attribute in M3U8Attribute\r\n            },  # type: ignore[call-overload]\r\n        )\r\n\r\n\r\n    class ScraperConfig(Scraper.ScraperConfig):\r\n        playlist_filters: HLSScraper.PlaylistFiltersSubcategory = Field(  # type: ignore[valid-type]\r\n            default_factory=lambda: HLSScraper.PlaylistFiltersSubcategory(),\r\n        )\r\n\r\n\r\n    def __init__(self, playlist_filters: dict[str, str | list[str] | None] | None = None,\r\n                 *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self._playlist_filters = return_first_valid(playlist_filters,\r\n                                                    get_model_field(model=self._config,\r\n                                                                    field='playlist_filters',\r\n                                                                    convert_to_dict=True),\r\n                                                    self.default_playlist_filters)\r\n\r\n        if self._playlist_filters != self.default_playlist_filters:\r\n            logger.debug(f\"Initializing '{self.name}' scraper with custom playlist filters: {self._playlist_filters}.\")\r\n\r\n    def parse_language_name(self, media_data: m3u8.Media) -> str | None:\r\n        \"\"\"\r\n        Parse the language name from an M3U8 Media object.\r\n        Can be overridden in subclasses for normalization.\r\n\r\n        Args:\r\n            media_data (m3u8.Media): Media object to parse the language name from.\r\n\r\n        Returns:\r\n            str | None: The language name if found, None otherwise.\r\n        \"\"\"\r\n        name: str | None = media_data.name\r\n        return name\r\n\r\n    async def load_playlist(self, url: str | list[str], headers: dict | None = None) -> m3u8.M3U8 | None:\r\n        _headers = headers or self._session.headers\r\n        result: m3u8.M3U8 | None = None\r\n\r\n        for url_item in single_to_list(url):\r\n            try:\r\n                response = await self._async_session.get(url=url_item, headers=_headers, timeout=5)\r\n\r\n            except Exception as e:\r\n                logger.debug(f\"Failed to load M3U8 playlist '{url_item}': {e}\")\r\n                continue\r\n\r\n            if not response.text:\r\n                raise PlaylistLoadError(\"Received empty response for playlist from server.\")\r\n\r\n            result = m3u8.loads(content=response.text, uri=url_item)\r\n            break\r\n\r\n        return result\r\n\r\n    @staticmethod\r\n    def detect_subtitles_type(subtitles_media: m3u8.Media) -> SubtitlesType | None:\r\n        \"\"\"\r\n        Detect the subtitles type (Closed Captions, Forced, etc.) from an M3U8 Media object.\r\n\r\n        Args:\r\n            subtitles_media (m3u8.Media): Subtitles Media object to detect the type of.\r\n\r\n        Returns:\r\n            SubtitlesType | None: The type of the subtitles, None for regular subtitles.\r\n        \"\"\"\r\n        if subtitles_media.forced == \"YES\":\r\n            return SubtitlesType.FORCED\r\n\r\n        if subtitles_media.characteristics is not None and \"public.accessibility\" in subtitles_media.characteristics:\r\n            return SubtitlesType.CC\r\n\r\n        return None\r\n\r\n    async def download_subtitles(self, media_data: m3u8.Media, subrip_conversion: bool = False) -> SubtitlesData:\r\n        playlist_m3u8 = await self.load_playlist(url=media_data.absolute_uri)\r\n\r\n        if playlist_m3u8 is None:\r\n            raise PlaylistLoadError(\"Could not load subtitles M3U8 playlist.\")\r\n\r\n        downloaded_segments = await self.download_segments(playlist=playlist_m3u8)\r\n        subtitles = self.subtitles_class(data=downloaded_segments[0], language_code=media_data.language)\r\n\r\n        if len(downloaded_segments) > 1:\r\n            for segment_data in downloaded_segments[1:]:\r\n                segment_subtitles_obj = self.subtitles_class(data=segment_data, language_code=media_data.language)\r\n                subtitles.append_subtitles(segment_subtitles_obj)\r\n\r\n        subtitles.polish(\r\n            fix_rtl=self.subtitles_fix_rtl,\r\n            remove_duplicates=self.subtitles_remove_duplicates,\r\n        )\r\n\r\n        if subrip_conversion:\r\n            subtitles_format = SubtitlesFormatType.SUBRIP\r\n            content = subtitles.to_srt().dump()\r\n\r\n        else:\r\n            subtitles_format = SubtitlesFormatType.WEBVTT\r\n            content = subtitles.dump()\r\n\r\n        return SubtitlesData(\r\n            language_code=media_data.language,\r\n            language_name=self.parse_language_name(media_data=media_data),\r\n            subtitles_format=subtitles_format,\r\n            content=content,\r\n            content_encoding=subtitles.encoding,\r\n            special_type=self.detect_subtitles_type(subtitles_media=media_data),\r\n        )\r\n\r\n    async def download_segments(self, playlist: m3u8.M3U8) -> list[bytes]:\r\n        responses = await asyncio.gather(\r\n            *[\r\n                self._async_session.get(url=segment.absolute_uri)\r\n                for segment in playlist.segments\r\n            ],\r\n        )\r\n\r\n        responses_data = []\r\n\r\n        for result in responses:\r\n            try:\r\n                result.raise_for_status()\r\n                responses_data.append(result.content)\r\n\r\n            except Exception as e:\r\n                raise DownloadError(\"One of the subtitles segments failed to download.\") from e\r\n\r\n        return responses_data\r\n\r\n    def find_matching_media(self, main_playlist: m3u8.M3U8,\r\n                            filters: dict[str, str | list[str]] | None = None) -> list[m3u8.Media]:\r\n        results: list[m3u8.Media] = []\r\n        playlist_filters: dict[str, Union[str, List[str]]] | None\r\n\r\n        if self._playlist_filters:\r\n            # Merge filtering dictionaries into a single dictionary\r\n            playlist_filters = merge_dict_values(\r\n                *[dict_item for dict_item in (filters, self._playlist_filters)\r\n                  if dict_item is not None],\r\n            )\r\n\r\n        else:\r\n            playlist_filters = filters\r\n\r\n        for media in main_playlist.media:\r\n            if not playlist_filters:\r\n                results.append(media)\r\n                continue\r\n\r\n            is_valid = True\r\n\r\n            for filter_name, filter_value in playlist_filters.items():\r\n                # Skip filter if its value is None\r\n                if filter_value is None:\r\n                    continue\r\n\r\n                try:\r\n                    filter_name_enum = HLSScraper.M3U8Attribute(filter_name)\r\n                    attribute_value = getattr(media, filter_name_enum.name.lower(), None)\r\n\r\n                    if (attribute_value is None) or (\r\n                            isinstance(filter_value, list) and\r\n                            attribute_value.casefold() not in (x.casefold() for x in filter_value)\r\n                    ) or (\r\n                            isinstance(filter_value, str) and filter_value.casefold() != attribute_value.casefold()\r\n                    ):\r\n                        is_valid = False\r\n                        break\r\n\r\n                except Exception:\r\n                    is_valid = False\r\n\r\n            if is_valid:\r\n                results.append(media)\r\n\r\n        return results\r\n\r\n    def find_matching_subtitles(self, main_playlist: m3u8.M3U8,\r\n                                language_filter: list[str] | None = None) -> list[m3u8.Media]:\r\n        _filters = self._subtitles_filters\r\n\r\n        if language_filter:\r\n            _filters[self.M3U8Attribute.LANGUAGE.value] = language_filter\r\n\r\n        return self.find_matching_media(main_playlist=main_playlist, filters=_filters)\r\n\r\n\r\nclass ScraperFactory:\r\n    _scraper_classes_cache: list[type[Scraper]] | None = None\r\n    _scraper_instances_cache: dict[type[Scraper], Scraper] = {}\r\n    _currently_initializing: list[type[Scraper]] = []  # Used to prevent infinite recursion\r\n\r\n    @classmethod\r\n    def get_initialized_scrapers(cls) -> list[Scraper]:\r\n        \"\"\"\r\n        Get a list of all previously initialized scrapers.\r\n\r\n        Returns:\r\n            list[Scraper]: A list of initialized scrapers.\r\n        \"\"\"\r\n        return list(cls._scraper_instances_cache.values())\r\n\r\n    @classmethod\r\n    def get_scraper_classes(cls) -> list[type[Scraper]]:\r\n        \"\"\"\r\n        Find all scraper classes in the scrapers directory.\r\n\r\n        Returns:\r\n            list[Scraper]: A Scraper subclass.\r\n        \"\"\"\r\n        if cls._scraper_classes_cache is not None:\r\n            return cls._scraper_classes_cache\r\n\r\n        cls._scraper_classes_cache = []\r\n        scraper_modules_paths = Path(__file__).parent.glob(f\"*{SCRAPER_MODULES_SUFFIX}.py\")\r\n\r\n        for scraper_module_path in scraper_modules_paths:\r\n            sys.path.append(str(scraper_module_path))\r\n\r\n            module = importlib.import_module(f\"{PACKAGE_NAME}.scrapers.{scraper_module_path.stem}\")\r\n\r\n            # Find all 'Scraper' subclasses\r\n            for _, obj in inspect.getmembers(module,\r\n                                             predicate=lambda x: inspect.isclass(x) and issubclass(x, Scraper)):\r\n                # Skip object if it's an abstract or imported from another module\r\n                if not inspect.isabstract(obj) and obj.__module__ == module.__name__:\r\n                    cls._scraper_classes_cache.append(obj)\r\n\r\n        return cls._scraper_classes_cache\r\n\r\n    @classmethod\r\n    def _get_scraper_instance(cls, scraper_class: type[ScraperT], kwargs: dict | None = None,\r\n                              scraper_config: Scraper.ScraperConfig | None = None) -> ScraperT:\r\n        \"\"\"\r\n        Initialize and return a scraper instance.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT]): A scraper class to initialize.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            scraper_config (ScraperT.ScraperConfig | None, optional):\r\n                A scraper configuration object to pass to the scraper. Defaults to None.\r\n\r\n        Returns:\r\n            Scraper: An instance of the given scraper class.\r\n        \"\"\"\r\n        logger.debug(f\"Initializing '{scraper_class.name}' scraper...\")\r\n        kwargs = kwargs or {}\r\n        kwargs.update({\"config\": scraper_config})\r\n\r\n        if scraper_class not in cls._scraper_instances_cache:\r\n            logger.debug(f\"'{scraper_class.name}' scraper not found in cache, creating a new instance...\")\r\n\r\n            if scraper_class in cls._currently_initializing:\r\n                raise ScraperError(f\"'{scraper_class.name}' scraper is already being initialized.\\n\"\r\n                                   f\"Make sure there are no circular dependencies between scrapers.\")\r\n\r\n            cls._currently_initializing.append(scraper_class)\r\n\r\n            cls._scraper_instances_cache[scraper_class] = scraper_class(**kwargs)\r\n            cls._currently_initializing.remove(scraper_class)\r\n\r\n        else:\r\n            logger.debug(f\"Cached '{scraper_class.name}' scraper instance found and will be used.\")\r\n\r\n        return cls._scraper_instances_cache[scraper_class]  # type: ignore[return-value]\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[True] = ...) -> ScraperT:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: type[ScraperT], scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[False] = ...) -> ScraperT | None:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[True] = ...) -> Scraper:\r\n        ...\r\n\r\n    @classmethod\r\n    @overload\r\n    def get_scraper_instance(cls, scraper_class: None = ..., scraper_id: str | None = ...,\r\n                             url: str | None = ..., kwargs: dict | None = ...,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: Literal[False] = ...) -> Scraper | None:\r\n        ...\r\n\r\n    @classmethod\r\n    def get_scraper_instance(cls, scraper_class: type[Scraper] | None = None, scraper_id: str | None = None,\r\n                             url: str | None = None, kwargs: dict | None = None,\r\n                             scrapers_configs: dict[str, Scraper.ScraperConfig] | None = None,\r\n                             raise_error: bool = True) -> Scraper | None:\r\n        \"\"\"\r\n        Find, initialize and return a scraper that matches the given URL or ID.\r\n\r\n        Args:\r\n            scraper_class (type[ScraperT] | None, optional): A scraper class to initialize. Defaults to None.\r\n            scraper_id (str | None, optional): ID of a scraper to initialize. Defaults to None.\r\n            url (str | None, optional): A URL to match a scraper for to initialize. Defaults to None.\r\n            kwargs (dict | None, optional): A dictionary containing parameters to pass to the scraper's constructor.\r\n                Defaults to None.\r\n            scrapers_configs (dict[str, ScraperConfigBase] | None, optional): A dictionary containing configurations\r\n                for scrapers, mapping scraper IDs to their configurations. Defaults to None.\r\n            raise_error (bool, optional): Whether to raise an error if no scraper was found. Defaults to False.\r\n\r\n        Returns:\r\n            ScraperT | Scraper | None: An instance of a scraper that matches the given URL or ID,\r\n                None otherwise (if raise_error is False).\r\n\r\n        Raises:\r\n            ValueError: If no scraper was found and 'raise_error' is True.\r\n        \"\"\"\r\n        if not scrapers_configs:\r\n            scrapers_configs = {}  # Allow `.get()` usage without checking for None\r\n\r\n        if not any((scraper_class, scraper_id, url)):\r\n            raise ValueError(\"At least one of: 'scraper_class', 'scraper_id', or 'url' must be provided.\")\r\n\r\n        if scraper_class:\r\n            return cls._get_scraper_instance(\r\n                scraper_class=scraper_class,\r\n                kwargs=kwargs,\r\n                scraper_config=scrapers_configs.get(scraper_class.id),\r\n            )\r\n\r\n        if scraper_id:\r\n            logger.debug(f\"Searching for a scraper object with ID '{scraper_id}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.id == scraper_id:\r\n                    return cls._get_scraper_instance(\r\n                        scraper_class=scraper,\r\n                        kwargs=kwargs,\r\n                        scraper_config=scrapers_configs.get(scraper_id),\r\n                    )\r\n\r\n        elif url:\r\n            logger.debug(f\"Searching for a scraper object that matches URL '{url}'...\")\r\n            for scraper in cls.get_scraper_classes():\r\n                if scraper.match_url(url) is not None:\r\n                    return cls._get_scraper_instance(\r\n                        scraper_class=scraper,\r\n                        kwargs=kwargs,\r\n                        scraper_config=scrapers_configs.get(scraper.id),\r\n                    )\r\n\r\n        error_message = \"No matching scraper was found.\"\r\n\r\n        if raise_error:\r\n            raise ValueError(error_message)\r\n\r\n        logger.debug(error_message)\r\n        return None\r\n\r\n\r\nclass ScraperError(Exception):\r\n    pass\r\n\r\n\r\nclass DownloadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass PlaylistLoadError(ScraperError):\r\n    pass\r\n\r\n\r\nclass SubtitlesDownloadError(ScraperError):\r\n    def __init__(self, language_code: str, language_name: str | None = None, special_type: SubtitlesType | None = None,\r\n                 original_exc: Exception | None = None, *args: Any, **kwargs: dict[str, Any]):\r\n        \"\"\"\r\n        Initialize a SubtitlesDownloadError instance.\r\n\r\n        Args:\r\n            language_code (str): Language code of the subtitles that failed to download.\r\n            language_name (str | None, optional): Language name of the subtitles that failed to download.\r\n            special_type (SubtitlesType | None, optional): Type of the subtitles that failed to download.\r\n            original_exc (Exception | None, optional): The original exception that caused the error.\r\n        \"\"\"\r\n        super().__init__(*args, **kwargs)\r\n        self.language_code = language_code\r\n        self.language_name = language_name\r\n        self.special_type = special_type\r\n        self.original_exc = original_exc\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/isubrip/scrapers/scraper.py b/isubrip/scrapers/scraper.py
--- a/isubrip/scrapers/scraper.py	(revision ea78d7310b108e0a7e0392d467d7b2aef5731fea)
+++ b/isubrip/scrapers/scraper.py	(date 1725475197558)
@@ -30,7 +30,7 @@
     merge_dict_values,
     normalize_config_name,
     return_first_valid,
-    single_to_list,
+    single_string_to_list,
 )
 
 if TYPE_CHECKING:
@@ -423,7 +423,7 @@
         _headers = headers or self._session.headers
         result: m3u8.M3U8 | None = None
 
-        for url_item in single_to_list(url):
+        for url_item in single_string_to_list(item=url):
             try:
                 response = await self._async_session.get(url=url_item, headers=_headers, timeout=5)
 
Index: pyproject.toml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>[tool.poetry]\r\nname = \"isubrip\"\r\nversion = \"2.5.6\"\r\ndescription = \"A Python package for scraping and downloading subtitles from AppleTV / iTunes movie pages.\"\r\nlicense = \"MIT\"\r\nauthors = [\"Michael Yochpaz\"]\r\nreadme = \"README.md\"\r\nhomepage = \"https://github.com/MichaelYochpaz/iSubRip\"\r\nrepository = \"https://github.com/MichaelYochpaz/iSubRip\"\r\nkeywords = [\r\n    \"iTunes\",\r\n    \"AppleTV\",\r\n    \"movies\",\r\n    \"subtitles\",\r\n    \"scrape\",\r\n    \"scraper\",\r\n    \"download\",\r\n    \"m3u8\"\r\n]\r\nclassifiers = [\r\n    \"Development Status :: 5 - Production/Stable\",\r\n    \"Intended Audience :: End Users/Desktop\",\r\n    \"Intended Audience :: Developers\",\r\n    \"Operating System :: Microsoft :: Windows\",\r\n    \"Operating System :: MacOS\",\r\n    \"Operating System :: POSIX :: Linux\",\r\n    \"Topic :: Utilities\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Programming Language :: Python :: 3.8\",\r\n    \"Programming Language :: Python :: 3.9\",\r\n    \"Programming Language :: Python :: 3.10\",\r\n    \"Programming Language :: Python :: 3.11\",\r\n    \"Programming Language :: Python :: 3.12\",\r\n]\r\npackages = [\r\n    { include = \"isubrip\" },\r\n]\r\ninclude = [\r\n    \"isubrip/resources\", \"README.md\", \"LICENSE\"\r\n]\r\n\r\n[tool.mypy]\r\ncheck_untyped_defs = true\r\ndisallow_untyped_defs = true\r\nexplicit_package_bases = true\r\nignore_missing_imports = true\r\npython_version = \"3.8\"\r\nwarn_return_any = true\r\nplugins = [\r\n    \"pydantic.mypy\"\r\n]\r\n\r\n[tool.poetry.scripts]\r\nisubrip = \"isubrip.__main__:main\"\r\n\r\n[tool.poetry.urls]\r\n\"Bug Reports\" = \"https://github.com/MichaelYochpaz/iSubRip/issues\"\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\nhttpx = {extras = [\"http2\"], version = \"^0.27.0\"}\r\nm3u8 = \"^4.1.0\"\r\npydantic = \"^2.7.0\"\r\ntomli = \"^2.0.1\"\r\n\r\n\r\n[tool.poetry.group.dev.dependencies]\r\nmypy = \"^1.10.0\"\r\nruff = \"^0.4.2\"\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry_bumpversion.file.\"isubrip/constants.py\"]\r\nsearch = 'PACKAGE_VERSION = \"{current_version}\"'\r\nreplace = 'PACKAGE_VERSION = \"{new_version}\"'\r\n\r\n\r\n[tool.ruff]\r\nline-length = 120\r\ntarget-version = \"py38\"\r\n\r\n[tool.ruff.lint]\r\nselect = [\r\n    \"ARG\",\r\n    \"ASYNC\",\r\n    \"B\",\r\n    \"C4\",\r\n    \"COM\",\r\n    \"E\",\r\n    \"F\",\r\n    \"FA\",\r\n    \"I\",\r\n    \"INP\",\r\n    \"ISC\",\r\n    \"N\",\r\n    \"PIE\",\r\n    \"PGH\",\r\n    \"PT\",\r\n    \"PTH\",\r\n    \"Q\",\r\n    \"RSE\",\r\n    \"RET\",\r\n    \"RUF\",\r\n    \"S\",\r\n    \"SIM\",\r\n    \"SLF\",\r\n    \"T20\",\r\n    \"TCH\",\r\n    \"TID\",\r\n    \"TRY\",\r\n    \"UP\",\r\n]\r\nignore = [\r\n    \"C416\",\r\n    \"Q000\",\r\n    \"RUF010\",\r\n    \"RUF012\",\r\n    \"SIM108\",\r\n    \"TD002\",\r\n    \"TD003\",\r\n    \"TRY003\",\r\n]\r\nunfixable = [\"ARG\"]\r\n\r\n[tool.ruff.lint.flake8-tidy-imports]\r\nban-relative-imports = \"all\"\r\n\r\n[tool.ruff.lint.flake8-quotes]\r\ndocstring-quotes = \"double\"\r\n\r\n[tool.ruff.lint.isort]\r\nforce-sort-within-sections = true\r\n\r\n[tool.ruff.lint.pyupgrade]\r\nkeep-runtime-typing = true
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pyproject.toml b/pyproject.toml
--- a/pyproject.toml	(revision ea78d7310b108e0a7e0392d467d7b2aef5731fea)
+++ b/pyproject.toml	(date 1725436869658)
@@ -57,7 +57,7 @@
 "Bug Reports" = "https://github.com/MichaelYochpaz/iSubRip/issues"
 
 [tool.poetry.dependencies]
-python = "^3.8"
+python = "^3.8"  # TODO: Update to 3.9
 httpx = {extras = ["http2"], version = "^0.27.0"}
 m3u8 = "^4.1.0"
 pydantic = "^2.7.0"
