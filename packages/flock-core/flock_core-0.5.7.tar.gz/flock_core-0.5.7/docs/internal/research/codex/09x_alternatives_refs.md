# Comparator Frameworks & Benchmarks (Quick References)

Use these references when building head‑to‑head evaluations.

- LangGraph docs. https://langchain-ai.github.io/langgraph/
- CrewAI docs. https://docs.crewai.com/
- AutoGen docs. https://microsoft.github.io/autogen/
- WebArena. https://webarena.dev/
- BrowserGym paper. https://arxiv.org/abs/2406.05294
- WorkArena paper. https://arxiv.org/abs/2406.15864
- SWE‑bench. https://arxiv.org/abs/2310.06770
- SWE‑bench Verified. https://arxiv.org/abs/2405.19308
- OpenAI Structured Outputs. https://openai.com/index/introducing-structured-outputs-in-the-api/
- MCP overview (Anthropic). https://www.anthropic.com/news/model-context-protocol
- OpenAI Agents SDK MCP docs. https://openai.github.io/openai-agents-python/mcp/
- Blackboard architectures (Nii). https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/550
- Kafka semantics. https://kafka.apache.org/documentation/#semantics
- OpenTelemetry. https://opentelemetry.io/
