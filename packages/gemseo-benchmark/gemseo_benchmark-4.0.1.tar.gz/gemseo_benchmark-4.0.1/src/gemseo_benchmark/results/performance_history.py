# Copyright 2021 IRT Saint ExupÃ©ry, https://www.irt-saintexupery.com
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License version 3 as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
# Contributors:
#    INITIAL AUTHORS - initial API and implementation and/or initial
#                           documentation
#        :author: Benoit Pauwels
#    OTHER AUTHORS   - MACROSCOPIC CHANGES
"""Class that generates performance measures out of data generated by an algorithm."""

from __future__ import annotations

import collections.abc
import json
from copy import copy
from datetime import timedelta
from pathlib import Path
from typing import TYPE_CHECKING
from typing import Final

from numpy import atleast_1d
from numpy import inf

from gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration
from gemseo_benchmark.results.history_item import HistoryItem

if TYPE_CHECKING:
    import datetime
    from collections.abc import Iterable
    from collections.abc import Sequence

    from gemseo.algos.optimization_problem import OptimizationProblem


class PerformanceHistory(collections.abc.Sequence):
    r"""A history of performance measures generated by an algorithm.

    A :class:`.PerformanceHistory` is a sequence of :class:`.HistoryItem`\ s.

    Iterative algorithms that solve, for example, optimization problems or equations,
    produce histories of data such as the value of the objective to minimize,
    or the size of the equation residual, at each iteration.
    The best value obtained up until each iteration can be generated out of this data.
    Here we call "performance history" the history of the
    best values obtained up until each iteration.

    Infeasible data can be discarded based upon histories of infeasibility measures or
    boolean feasibility statuses.

    Performance histories can be used to generate target values for a problem, or to
    generate the data profile of an algorithm.

    Attributes:
        problem_name (str): The name of the problem.
        total_time (float): The run time of the algorithm.
    """

    __ALGORITHM_CONFIGURATION: Final[str] = "algorithm_configuration"
    __CONSTRAINTS_NAMES: Final[str] = "constraints_names"
    __DOE_SIZE: Final[str] = "DOE_size"
    __EXECUTION_TIME: Final[str] = "execution_time"
    __HISTORY_ITEMS: Final[str] = "history_items"
    __NUMBER_OF_VARIABLES: Final[str] = "number_of_variables"
    __OBJECTIVE_NAME: Final[str] = "objective_name"
    __PROBLEM: Final[str] = "problem"

    def __init__(
        self,
        performance_measures: Sequence[float] = (),
        infeasibility_measures: Sequence[float] = (),
        feasibility_statuses: Sequence[bool] = (),
        n_unsatisfied_constraints: Sequence[int] = (),
        problem_name: str = "",
        objective_name: str = "",
        constraints_names: Sequence[str] = (),
        doe_size: int | None = None,
        total_time: float | None = None,
        algorithm_configuration: AlgorithmConfiguration | None = None,
        number_of_variables: int | None = None,
        elapsed_times: Sequence[timedelta] = (),
        number_of_discipline_executions: Sequence[int] = (),
    ) -> None:
        """
        Args:
            performance_measures: The history of performance measures.
            infeasibility_measures: The history of infeasibility measures.
                An infeasibility measure is a non-negative real number representing
                the gap between the design and the feasible space,
                a zero value meaning feasibility.
                If empty and `feasibility_statuses` is not empty
                then the infeasibility measures are set to zero in case of feasibility,
                and set to infinity otherwise.
                If empty and `feasibility_statuses` is empty
                then every infeasibility measure is set to zero.
            feasibility_statuses: The history of the (boolean) feasibility statuses.
                If `infeasibility_measures` is not empty then `feasibility_statuses` is
                disregarded.
                If empty and 'infeasibility_measures' is empty
                then every infeasibility measure is set to zero.
            n_unsatisfied_constraints: The history of the number of unsatisfied
                constraints.
                If empty, the entries will be set to 0 for feasible entries
                and ``None`` for infeasible entries.
            problem_name: The name of the problem.
            objective_name: The name of the objective function.
            constraints_names: The names the scalar constraints.
                Each name must correspond to a scalar value.
                If empty, they will not be set.
            doe_size: The size of the initial design of experiments.
                If ``None``, it will not be set.
            total_time: The total time of the optimization, in seconds.
                If ``None``, it will not be set.
            algorithm_configuration: The name of the algorithm which generated the
                history.
                If ``None``, it will not be set.
            number_of_variables: The number of optimization variables.
                If ``None``, it will not be set.
            elapsed_times: The history of elapsed times.
                If empty, the elapsed times are set to zero.
            number_of_discipline_executions: The history of the number
                of discipline executions.
                If `empty`, the number of discipline executions are set to zero.

        Raises:
            ValueError: If the lengths of the histories do not match.
        """  # noqa: D205, D212, D415
        self._constraints_names = constraints_names
        self._objective_name = objective_name
        self.algorithm_configuration = algorithm_configuration
        self.doe_size = doe_size
        self.items = self.__get_history_items(
            performance_measures,
            infeasibility_measures,
            feasibility_statuses,
            n_unsatisfied_constraints,
            elapsed_times,
            number_of_discipline_executions,
        )
        self.problem_name = problem_name
        self._number_of_variables = number_of_variables
        self.total_time = total_time

    @property
    def performance_measures(self) -> list[float]:
        """The performance measures."""
        return [item.performance_measure for item in self.items]

    @property
    def infeasibility_measures(self) -> list[float]:
        """The infeasibility measures."""
        return [item.infeasibility_measure for item in self.items]

    @property
    def n_unsatisfied_constraints(self) -> list[int]:
        """The numbers of unsatisfied constraints."""
        return [item.n_unsatisfied_constraints for item in self.items]

    @property
    def items(self) -> list[HistoryItem]:
        """The history items."""
        return self.__items

    @items.setter
    def items(
        self,
        history_items: Iterable[HistoryItem],
    ) -> None:
        self.__items = list(history_items)

    @staticmethod
    def __get_history_items(
        performance_measures: Sequence[float],
        infeasibility_measures: Sequence[float],
        feasibility_statuses: Sequence[bool],
        n_unsatisfied_constraints: Sequence[int],
        elapsed_times: Sequence[timedelta],
        number_of_discipline_executions: Sequence[int] = (),
    ) -> list[HistoryItem]:
        """Return history items based on values histories.

        Args:
            performance_measures: The history of performance measures.
            infeasibility_measures: The history of infeasibility measures.
                An infeasibility measure is a non-negative real number representing
                the gap between the design and the feasible space,
                a zero value meaning feasibility.
                If empty and `feasibility_statuses` is not empty
                then the infeasibility measures are set to zero in case of feasibility,
                and set to infinity otherwise.
                If empty and `feasibility_statuses` is empty
                then every infeasibility measure is set to zero.
            feasibility_statuses: The history of the (boolean) feasibility statuses.
                If `infeasibility_measures` is not empty then `feasibility_statuses` is
                disregarded.
                If empty and 'infeasibility_measures' is empty
                then every infeasibility measure is set to zero.
            n_unsatisfied_constraints: The history of the number of unsatisfied
                constraints.
                If empty, the entries will be set to 0 for feasible entries
                and ``None`` for infeasible entries.
            elapsed_times: The history of elapsed times.
                If empty, the elapsed times are set to zero.
            number_of_discipline_executions: The history of the number
                of discipline executions.
                If `empty`, the number of discipline executions are set to zero.

        Returns:
            The history items.
        """
        if infeasibility_measures:
            if len(infeasibility_measures) != len(performance_measures):
                msg = (
                    "The performance history and the infeasibility history "
                    "must have same length"
                    f": {len(performance_measures)} != {len(infeasibility_measures)}."
                )
                raise ValueError(msg)
        elif feasibility_statuses:
            if len(feasibility_statuses) != len(performance_measures):
                msg = (
                    "The performance history and the feasibility history "
                    "must have same length"
                    f": {len(performance_measures)} != {len(feasibility_statuses)}."
                )
                raise ValueError(msg)
            infeasibility_measures = [
                0.0 if is_feas else inf for is_feas in feasibility_statuses
            ]
        else:
            infeasibility_measures = [0.0] * len(performance_measures)

        if not n_unsatisfied_constraints:
            n_unsatisfied_constraints = [
                0 if entry == 0.0 else None for entry in infeasibility_measures
            ]
        elif len(n_unsatisfied_constraints) != len(infeasibility_measures):
            msg = (
                "The unsatisfied constraints history and the infeasibility history"
                " must have same length"
                f": {len(n_unsatisfied_constraints)} != {len(infeasibility_measures)}."
            )
            raise ValueError(msg)

        if not elapsed_times:
            elapsed_times = [timedelta() for _ in performance_measures]
        elif len(elapsed_times) != len(performance_measures):
            msg = (
                "The performance history and the elasped time history "
                "must have same length"
                f": {len(performance_measures)} != {len(elapsed_times)}."
            )
            raise ValueError(msg)

        if not number_of_discipline_executions:
            number_of_discipline_executions = [0] * len(performance_measures)
        elif len(number_of_discipline_executions) != len(performance_measures):
            msg = (
                "The performance history "
                "and the number of discipline executions history "
                "must have same length"
                f": {len(performance_measures)} != "
                f"{len(number_of_discipline_executions)}."
            )
            raise ValueError(msg)

        return list(
            map(
                HistoryItem,
                performance_measures,
                infeasibility_measures,
                n_unsatisfied_constraints,
                elapsed_times,
                number_of_discipline_executions,
            )
        )

    def __len__(self) -> int:
        return len(self.__items)

    def __getitem__(
        self,
        i: int,
    ) -> HistoryItem:
        return self.__items[i]

    def __repr__(self) -> str:
        return str(list(self))

    def compute_cumulated_minimum(self) -> PerformanceHistory:
        """Return the history of the cumulated minimum.

        Returns:
            The history of the cumulated minimum.
        """
        minimum_history = copy(self)
        minimum_history.items = minimum_history.items[:1]
        for item in self.items[1:]:
            last_item = minimum_history.items[-1]
            if item >= last_item:
                new_item = last_item.copy()
                # N.B. The copy ensures the new items are independent objects.
                new_item.elapsed_time = item.elapsed_time
            else:
                new_item = item

            minimum_history.items.append(new_item)

        return minimum_history

    def remove_leading_infeasible(self) -> PerformanceHistory:
        """Return the history starting from the first feasible item.

        Returns:
            The truncated performance history.
        """
        first_feasible = len(self)
        for index, item in enumerate(self):
            if item.is_feasible:
                first_feasible = index
                break

        truncated_history = copy(self)
        truncated_history.items = self.items[first_feasible:]
        return truncated_history

    def to_file(
        self,
        path: str | Path,
    ) -> None:
        """Save the performance history in a file.

        Args:
            path: The path where to write the file.
        """
        data = {}
        if self.problem_name:
            data[self.__PROBLEM] = self.problem_name

        if self._number_of_variables is not None:
            data[self.__NUMBER_OF_VARIABLES] = self._number_of_variables

        if self._objective_name:
            data[self.__OBJECTIVE_NAME] = self._objective_name

        if self._constraints_names:
            data[self.__CONSTRAINTS_NAMES] = self._constraints_names

        if self.algorithm_configuration is not None:
            data[self.__ALGORITHM_CONFIGURATION] = self.algorithm_configuration.to_dict(
                True
            )

        if self.doe_size is not None:
            data[self.__DOE_SIZE] = self.doe_size

        if self.total_time is not None:
            data[self.__EXECUTION_TIME] = self.total_time

        data[self.__HISTORY_ITEMS] = [item.to_dict() for item in self.items]
        with Path(path).open("w") as file:
            json.dump(data, file, indent=2, separators=(",", ": "))

    @classmethod
    def from_file(cls, path: str | Path) -> PerformanceHistory:
        """Create a new performance history from a file.

        Args:
            path: The path to the file.

        Returns:
            The performance history.
        """
        with Path(path).open("r") as file:
            data = json.load(file)

        history = cls()
        history.problem_name = data.get(cls.__PROBLEM)
        history._number_of_variables = data.get(cls.__NUMBER_OF_VARIABLES)
        history._objective_name = data.get(cls.__OBJECTIVE_NAME)
        history._constraints_names = data.get(cls.__CONSTRAINTS_NAMES, [])
        if cls.__ALGORITHM_CONFIGURATION in data:
            history.algorithm_configuration = AlgorithmConfiguration.from_dict(
                data[cls.__ALGORITHM_CONFIGURATION]
            )

        history.doe_size = data.get(cls.__DOE_SIZE)
        history.total_time = data.get(cls.__EXECUTION_TIME)
        history.items = [
            HistoryItem.from_dict(item_data) for item_data in data[cls.__HISTORY_ITEMS]
        ]
        return history

    @classmethod
    def from_problem(
        cls,
        problem: OptimizationProblem,
        problem_name: str = "",
        elapsed_times: Sequence[timedelta] = (),
        number_of_discipline_executions: Sequence[int] = (),
    ) -> PerformanceHistory:
        """Create a performance history from a solved optimization problem.

        Args:
            problem: The optimization problem.
            problem_name: The name of the problem.
            elapsed_times: The history of elapsed times.
                If empty, the elapsed times are set to zero.
            number_of_discipline_executions: The history of the number
                of discipline executions.
                If `empty`, the number of discipline executions are set to zero.

        Returns:
            The performance history.
        """
        obj_name = problem.objective.name
        obj_values = []
        infeas_measures = []
        feas_statuses = []
        n_unsatisfied_constraints = []
        retained_elapsed_times = []
        retained_number_of_discipline_executions = []
        functions_names = {obj_name, *problem.constraints.get_names()}
        for index, (design_values, output_values) in enumerate(
            problem.database.items()
        ):
            # Only consider points with all functions values
            if not functions_names <= set(output_values.keys()):
                continue

            x_vect = design_values.unwrap()
            obj_values.append(atleast_1d(output_values[obj_name]).real[0])
            feasibility, measure = problem.history.check_design_point_is_feasible(
                x_vect
            )
            number_of_unsatisfied_constraints = (
                problem.constraints.get_number_of_unsatisfied_constraints(output_values)
            )
            infeas_measures.append(measure)
            feas_statuses.append(feasibility)
            n_unsatisfied_constraints.append(number_of_unsatisfied_constraints)
            if elapsed_times:
                retained_elapsed_times.append(elapsed_times[index])

            if number_of_discipline_executions:
                retained_number_of_discipline_executions.append(
                    number_of_discipline_executions[index]
                )

        return cls(
            obj_values,
            infeas_measures,
            feas_statuses,
            n_unsatisfied_constraints,
            problem_name,
            problem.objective.name,
            problem.scalar_constraint_names,
            number_of_variables=problem.design_space.dimension,
            elapsed_times=retained_elapsed_times,
            number_of_discipline_executions=retained_number_of_discipline_executions,
        )

    def get_plot_data(
        self, feasible: bool = False, minimum_history: bool = False
    ) -> tuple[list[int], list[HistoryItem]]:
        """Return the data to plot the performance history.

        Args:
            feasible: Whether to get only feasible values.
            minimum_history: Whether to get the history of the cumulated minimum
                instead of the history of the performance measure.

        Returns:
            The abscissas and the ordinates of the plot.
        """
        history = self.compute_cumulated_minimum() if minimum_history else self

        # Find the index of the first feasible history item
        if feasible:
            first_feasible_index = len(history)
            for index, item in enumerate(history):
                if item.is_feasible:
                    first_feasible_index = index
                    break
        else:
            first_feasible_index = 0

        return (
            list(range(first_feasible_index + 1, len(history) + 1)),
            history[first_feasible_index:],
        )

    def extend(self, size: int) -> PerformanceHistory:
        """Extend the performance history by repeating its last item.

        If the history is longer than the expected size then it will not be altered.

        Args:
            size: The expected size of the extended performance history.

        Returns:
            The extended performance history.

        Raises:
            ValueError: If the expected size is smaller than the history size.
        """
        if size < len(self):
            msg = (
                f"The expected size ({size}) is smaller than "
                f"the history size ({len(self)})."
            )
            raise ValueError(msg)

        history = copy(self)
        history.items = list(self)
        for _ in range(size - len(self)):
            history.items.append(self[-1].copy())
            # N.B. The copy ensures the new items are independent objects.

        return history

    def shorten(self, size: int) -> PerformanceHistory:
        """Shorten the performance history to a given size.

        If the history is shorter than the expected size then it will not be altered.

        Args:
            size: The expected size of the shortened performance history.

        Returns:
            The shortened performance history.
        """
        history = copy(self)
        history.items = self.items[:size]
        return history

    def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -> None:
        """Apply a tolerance on the infeasibility measures of the history items.

        Mark the history items with an infeasibility measure below the tolerance
        as feasible.

        Args:
            infeasibility_tolerance: the tolerance on the infeasibility measure.
        """
        for item in self.items:
            item.apply_infeasibility_tolerance(infeasibility_tolerance)

    def switch_performance_measure_sign(self) -> None:
        """Switch the sign of the performance measure."""
        for item in self:
            item.switch_performance_measure_sign()

    def spread_over_timeline(
        self, timeline: Iterable[timedelta], number_of_scalar_constraints: int
    ) -> PerformanceHistory:
        """Spread the history over a timeline of elapsed times.

        !!! note

            The timeline is assumed to be sorted in increasing order.

        Args:
            timeline: An increasing sequence of elapsed times.
            number_of_scalar_constraints: The number of scalar constraints.

        Returns:
            The performance history with has as many items as elements in the timeline.
        """
        return self.__spread_over_scale(
            timeline, number_of_scalar_constraints, HistoryItem.elapsed_time
        )

    def spread_over_numbers_of_discipline_executions(
        self,
        numbers_of_discipline_executions: Iterable[int],
        number_of_scalar_constraints: int,
    ) -> PerformanceHistory:
        """Spread the history over a scale of number discipline executions.

        !!! note

            The scale is assumed to be sorted in increasing order.

        Args:
            numbers_of_discipline_executions: An increasing sequence
                of numbers of discipline executions.
            number_of_scalar_constraints: The number of scalar constraints.

        Returns:
            The performance history with has as many items as elements in the scale.
        """
        return self.__spread_over_scale(
            numbers_of_discipline_executions,
            number_of_scalar_constraints,
            HistoryItem.number_of_discipline_executions,
        )

    def __spread_over_scale(
        self,
        scale: Iterable[datetime.timedelta] | Iterable[int],
        number_of_scalar_constraints: int,
        scale_property: property,
    ) -> PerformanceHistory:
        """Spread the history over a scale.

        !!! note

            The scale is assumed to be sorted in increasing order.

        Args:
            numbers_of_discipline_executions: An increasing sequence
                of numbers of discipline executions.
            number_of_scalar_constraints: The number of scalar constraints.
            scale_property: The history item property to access the scale data.

        Returns:
            The performance history with has as many items as elements in the scale.
        """
        history_items = []
        item_index = 0
        for step in scale:
            if item_index >= len(self):
                history_item = self[-1].copy()
            elif step < scale_property.fget(self[item_index]):
                if item_index == 0:
                    history_item = HistoryItem(
                        float("nan"), float("nan"), number_of_scalar_constraints
                    )
                else:
                    history_item = self[item_index - 1].copy()
            else:
                history_item = self[item_index].copy()
                item_index += 1

            scale_property.fset(history_item, step)
            history_items.append(history_item)

        history = copy(self)
        history.items = history_items
        return history
