"""
Meta-tools module for IsoFinancial-MCP optimization.

This module provides consolidated meta-tools that retrieve all financial data
in 1-2 calls instead of 10+ individual calls, optimizing for LLM agent efficiency.
"""

import asyncio
import json
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union
import logging

# Import datasources
from .datasources import yfinance_source
from .datasources import news_source
from .datasources import sec_source
from .datasources import earnings_source
from .datasources import finra_source
from .datasources import trends_source

# Configure logging
logger = logging.getLogger(__name__)


def truncate_string(text: str, max_length: int = 500) -> str:
    """
    Truncate long text strings intelligently to save tokens.
    
    Args:
        text: The text to truncate
        max_length: Maximum length before truncation (default: 500)
    
    Returns:
        Truncated string with "[truncated]" indicator if needed
    
    Example:
        >>> truncate_string("A" * 1000, max_length=100)
        'AAAA... [truncated]'
    """
    if not text or not isinstance(text, str):
        return str(text) if text is not None else ""
    
    if len(text) <= max_length:
        return text
    
    # Truncate and add indicator
    truncated = text[:max_length].rstrip()
    return f"{truncated}... [truncated]"


def format_compact_data(data: Any, max_items: int = 5) -> str:
    """
    Format data structures in a compact way to save tokens.
    
    Handles lists, dictionaries, and other data types by limiting the number
    of items displayed and providing a summary for the rest.
    
    Args:
        data: The data to format (list, dict, or other)
        max_items: Maximum number of items to display (default: 5)
    
    Returns:
        Compact string representation of the data
    
    Example:
        >>> format_compact_data(list(range(20)), max_items=5)
        '[0, 1, 2, 3, 4] +15 more items'
    """
    if data is None:
        return "None"
    
    # Handle lists
    if isinstance(data, list):
        if len(data) <= max_items:
            return str(data)
        
        displayed = data[:max_items]
        remaining = len(data) - max_items
        return f"{displayed} +{remaining} more items"
    
    # Handle dictionaries
    if isinstance(data, dict):
        if len(data) <= max_items:
            return str(data)
        
        items = list(data.items())[:max_items]
        remaining = len(data) - max_items
        displayed = {k: v for k, v in items}
        return f"{displayed} +{remaining} more items"
    
    # Handle other types
    return str(data)


async def get_financial_snapshot(
    ticker: str,
    include_options: bool = False,
    lookback_days: int = 30
) -> Dict[str, Any]:
    """
    Retrieve ALL financial data for a single ticker in one parallel call.
    
    This meta-tool consolidates data from multiple sources (yfinance, news, SEC,
    earnings, FINRA, Google Trends) and retrieves them in parallel using asyncio.gather
    for maximum efficiency.
    
    Args:
        ticker: Stock ticker symbol (e.g., 'AAPL')
        include_options: Whether to include options data (increases token usage)
        lookback_days: Number of days for historical data (default: 30)
    
    Returns:
        Dictionary with structure:
        {
            "ticker": "AAPL",
            "timestamp": "2025-01-15T10:30:00",
            "data": {
                "info": {...},
                "prices": {...},
                "news": [...],
                "sec_filings": [...],
                "earnings": {...},
                "short_volume": {...},
                "google_trends": {...},
                "options": {...}  # optional
            },
            "errors": []  # List of partial errors if any source fails
        }
    
    Example:
        >>> snapshot = await get_financial_snapshot("AAPL", lookback_days=7)
        >>> print(f"Retrieved data for {snapshot['ticker']}")
        >>> print(f"Errors: {len(snapshot['errors'])}")
    """
    # Initialize snapshot structure
    snapshot = {
        "ticker": ticker.upper(),
        "timestamp": datetime.now().isoformat(),
        "data": {},
        "errors": []
    }
    
    try:
        # Create tasks for parallel execution
        tasks = []
        task_names = []
        
        # Task 1: Company info
        tasks.append(yfinance_source.get_info(ticker))
        task_names.append("info")
        
        # Task 2: Historical prices (map lookback_days to valid yfinance periods)
        # Valid periods: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max
        if lookback_days <= 5:
            period = "5d"
        elif lookback_days <= 30:
            period = "1mo"
        elif lookback_days <= 90:
            period = "3mo"
        elif lookback_days <= 180:
            period = "6mo"
        elif lookback_days <= 365:
            period = "1y"
        else:
            period = "2y"
        tasks.append(yfinance_source.get_historical_prices(ticker, period=period))
        task_names.append("prices")
        
        # Task 3: News headlines
        tasks.append(news_source.get_news_headlines(ticker, limit=5, lookback_days=lookback_days))
        task_names.append("news")
        
        # Task 4: SEC filings
        tasks.append(sec_source.get_sec_filings(ticker, ["8-K", "10-Q", "10-K"], lookback_days=lookback_days))
        task_names.append("sec_filings")
        
        # Task 5: Earnings calendar
        tasks.append(earnings_source.get_earnings_calendar(ticker))
        task_names.append("earnings")
        
        # Task 6: FINRA short volume (uses start_date/end_date, not lookback_days)
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")
        tasks.append(finra_source.get_finra_short_volume(ticker, start_date=start_date, end_date=end_date))
        task_names.append("short_volume")
        
        # Task 7: Google Trends
        tasks.append(trends_source.get_google_trends(ticker, window_days=lookback_days))
        task_names.append("google_trends")
        
        # Task 8: Options data (optional)
        if include_options:
            tasks.append(yfinance_source.get_options_expirations(ticker))
            task_names.append("options")
        
        # Execute all tasks in parallel with error handling
        logger.info(f"Fetching {len(tasks)} data sources for {ticker} in parallel...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results with graceful degradation
        for task_name, result in zip(task_names, results):
            if isinstance(result, Exception):
                # Log error and continue with other sources
                error_msg = f"{task_name}: {type(result).__name__} - {str(result)}"
                snapshot["errors"].append(error_msg)
                logger.warning(f"Error fetching {task_name} for {ticker}: {result}")
            elif result is None or (isinstance(result, (list, dict)) and not result):
                # Empty result
                snapshot["errors"].append(f"{task_name}: No data available")
                logger.info(f"No data available for {task_name} for {ticker}")
            else:
                # Success - format and store data compactly
                snapshot["data"][task_name] = _format_data_compact(task_name, result, lookback_days)
                logger.debug(f"Successfully fetched {task_name} for {ticker}")
        
        logger.info(f"Snapshot complete for {ticker}: {len(snapshot['data'])} sources, {len(snapshot['errors'])} errors")
        
    except Exception as e:
        # Catch-all for unexpected errors
        error_msg = f"Unexpected error in get_financial_snapshot: {type(e).__name__} - {str(e)}"
        snapshot["errors"].append(error_msg)
        logger.error(error_msg)
    
    return snapshot


async def get_multi_ticker_snapshot(
    tickers: List[str],
    include_options: bool = False,
    lookback_days: int = 30,
    max_tickers: int = 10
) -> Dict[str, Any]:
    """
    Retrieve financial snapshots for MULTIPLE tickers in parallel.
    
    This meta-tool analyzes multiple tickers simultaneously using asyncio.gather
    for maximum efficiency. It limits the number of tickers to prevent timeouts
    and handles errors gracefully on a per-ticker basis.
    
    Args:
        tickers: List of stock ticker symbols (e.g., ['AAPL', 'MSFT', 'GOOGL'])
        include_options: Whether to include options data (increases token usage)
        lookback_days: Number of days for historical data (default: 30)
        max_tickers: Maximum number of tickers to analyze (default: 10)
    
    Returns:
        Dictionary with structure:
        {
            "timestamp": "2025-01-15T10:30:00",
            "tickers_count": 3,
            "snapshots": {
                "AAPL": {...},  # Full snapshot for each ticker
                "MSFT": {...},
                "GOOGL": {...}
            },
            "global_errors": []  # Errors that affected multiple tickers
        }
    
    Example:
        >>> multi_snapshot = await get_multi_ticker_snapshot(
        ...     ["AAPL", "MSFT", "GOOGL"],
        ...     lookback_days=7
        ... )
        >>> print(f"Analyzed {multi_snapshot['tickers_count']} tickers")
        >>> print(f"Successful: {len(multi_snapshot['snapshots'])}")
    """
    # Initialize multi-snapshot structure
    multi_snapshot = {
        "timestamp": datetime.now().isoformat(),
        "tickers_count": 0,
        "snapshots": {},
        "global_errors": []
    }
    
    try:
        # Validate and clean ticker list
        if not tickers or not isinstance(tickers, list):
            multi_snapshot["global_errors"].append("Invalid tickers list provided")
            return multi_snapshot
        
        # Clean and uppercase tickers
        clean_tickers = [ticker.strip().upper() for ticker in tickers if ticker and ticker.strip()]
        
        if not clean_tickers:
            multi_snapshot["global_errors"].append("No valid tickers provided after cleaning")
            return multi_snapshot
        
        # Limit to max_tickers to avoid timeouts
        if len(clean_tickers) > max_tickers:
            logger.warning(f"Limiting analysis from {len(clean_tickers)} to {max_tickers} tickers")
            multi_snapshot["global_errors"].append(
                f"Ticker list limited from {len(clean_tickers)} to {max_tickers} to prevent timeout"
            )
            clean_tickers = clean_tickers[:max_tickers]
        
        multi_snapshot["tickers_count"] = len(clean_tickers)
        
        # Create tasks for parallel execution
        logger.info(f"Creating parallel tasks for {len(clean_tickers)} tickers...")
        tasks = [
            get_financial_snapshot(ticker, include_options, lookback_days)
            for ticker in clean_tickers
        ]
        
        # Execute all ticker snapshots in parallel with error handling
        logger.info(f"Executing {len(tasks)} ticker snapshots in parallel...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results with per-ticker error handling
        for ticker, result in zip(clean_tickers, results):
            if isinstance(result, Exception):
                # Ticker-level error - log and continue with others
                error_msg = f"{ticker}: {type(result).__name__} - {str(result)}"
                multi_snapshot["global_errors"].append(error_msg)
                logger.error(f"Failed to fetch snapshot for {ticker}: {result}")
            elif result is None:
                # Unexpected None result
                multi_snapshot["global_errors"].append(f"{ticker}: Returned None (unexpected)")
                logger.warning(f"Snapshot for {ticker} returned None")
            else:
                # Success - store the snapshot
                multi_snapshot["snapshots"][ticker] = result
                logger.debug(f"Successfully fetched snapshot for {ticker}")
        
        # Log summary
        successful = len(multi_snapshot["snapshots"])
        failed = len(multi_snapshot["global_errors"])
        logger.info(
            f"Multi-ticker snapshot complete: {successful} successful, "
            f"{failed} errors out of {len(clean_tickers)} tickers"
        )
        
    except Exception as e:
        # Catch-all for unexpected errors
        error_msg = f"Unexpected error in get_multi_ticker_snapshot: {type(e).__name__} - {str(e)}"
        multi_snapshot["global_errors"].append(error_msg)
        logger.error(error_msg)
    
    return multi_snapshot


def format_snapshot_for_llm(snapshot: Dict[str, Any]) -> str:
    """
    Format a financial snapshot into compact text optimized for LLM consumption.
    
    This function converts the structured snapshot data into a human-readable
    text format with clear sections, truncated content, and error indicators.
    Optimized to save 50-70% tokens compared to raw JSON format.
    
    Args:
        snapshot: Financial snapshot dictionary from get_financial_snapshot()
    
    Returns:
        Formatted text string with all financial data in compact sections
    
    Example:
        >>> snapshot = await get_financial_snapshot("AAPL")
        >>> formatted = format_snapshot_for_llm(snapshot)
        >>> print(formatted)
        === FINANCIAL SNAPSHOT: AAPL ===
        Timestamp: 2025-01-15T10:30:00
        ...
    """
    lines = []
    ticker = snapshot.get("ticker", "UNKNOWN")
    timestamp = snapshot.get("timestamp", "")
    data = snapshot.get("data", {})
    errors = snapshot.get("errors", [])
    
    # Header
    lines.append(f"=== FINANCIAL SNAPSHOT: {ticker} ===")
    lines.append(f"Timestamp: {timestamp}")
    lines.append("")
    
    # Section 1: Company Info
    info = data.get("info", {})
    if info:
        lines.append("--- COMPANY INFORMATION ---")
        lines.append(f"Company: {info.get('longName', 'N/A')}")
        lines.append(f"Sector: {info.get('sector', 'N/A')} | Industry: {info.get('industry', 'N/A')}")
        
        # Market Cap formatting
        market_cap = info.get('marketCap')
        if market_cap:
            if market_cap >= 1e12:
                market_cap_str = f"${market_cap/1e12:.2f}T"
            elif market_cap >= 1e9:
                market_cap_str = f"${market_cap/1e9:.2f}B"
            elif market_cap >= 1e6:
                market_cap_str = f"${market_cap/1e6:.2f}M"
            else:
                market_cap_str = f"${market_cap:,.0f}"
        else:
            market_cap_str = "N/A"
        
        current_price = info.get('currentPrice', 'N/A')
        lines.append(f"Market Cap: {market_cap_str} | Price: ${current_price}")
        
        # Additional metrics
        pe_ratio = info.get('trailingPE')
        forward_pe = info.get('forwardPE')
        beta = info.get('beta')
        div_yield = info.get('dividendYield')
        
        metrics = []
        if pe_ratio:
            metrics.append(f"P/E: {pe_ratio:.2f}")
        if forward_pe:
            metrics.append(f"Fwd P/E: {forward_pe:.2f}")
        if beta:
            metrics.append(f"Beta: {beta:.2f}")
        if div_yield:
            metrics.append(f"Div Yield: {div_yield*100:.2f}%")
        
        if metrics:
            lines.append(" | ".join(metrics))
        
        # Summary
        summary = info.get('summary', '')
        if summary:
            lines.append(f"Summary: {summary}")
        
        lines.append("")
    
    # Section 2: Price Data
    prices = data.get("prices", {})
    if prices:
        lines.append("--- PRICE DATA ---")
        latest_close = prices.get('latest_close')
        latest_date = prices.get('latest_date')
        change_30d = prices.get('change_30d_pct')
        
        if latest_close:
            lines.append(f"Latest Close: ${latest_close:.2f} (as of {latest_date})")
        
        if change_30d is not None:
            direction = "↑" if change_30d > 0 else "↓" if change_30d < 0 else "→"
            lines.append(f"30-day Change: {direction} {change_30d:+.2f}%")
        
        lines.append("")
    
    # Section 3: News (max 3 articles)
    news = data.get("news", [])
    if news:
        # Handle both list and dict formats
        articles = news.get("articles", news) if isinstance(news, dict) else news
        total_available = news.get("total_available", len(articles)) if isinstance(news, dict) else len(articles)
        
        lines.append(f"--- RECENT NEWS ({total_available} articles) ---")
        
        # Display max 3 articles
        for i, article in enumerate(articles[:3], 1):
            title = article.get('title', 'No title')
            source = article.get('source', 'Unknown')
            published = article.get('published_at', '')
            lines.append(f"{i}. {title} ({source})")
            if published:
                lines.append(f"   Published: {published}")
        
        if total_available > 3:
            lines.append(f"   ... and {total_available - 3} more articles")
        
        lines.append("")
    
    # Section 4: SEC Filings (max 2)
    sec_filings = data.get("sec_filings", [])
    if sec_filings:
        # Handle both list and dict formats
        filings = sec_filings.get("filings", sec_filings) if isinstance(sec_filings, dict) else sec_filings
        total_available = sec_filings.get("total_available", len(filings)) if isinstance(sec_filings, dict) else len(filings)
        
        lines.append(f"--- RECENT SEC FILINGS ({total_available} total) ---")
        
        # Display max 2 filings
        for i, filing in enumerate(filings[:2], 1):
            form = filing.get('form', 'N/A')
            date = filing.get('date', 'N/A')
            title = filing.get('title', 'No title')
            lines.append(f"{i}. {form} - {date}")
            lines.append(f"   {title}")
        
        if total_available > 2:
            lines.append(f"   ... and {total_available - 2} more filings")
        
        lines.append("")
    
    # Section 5: Earnings
    earnings = data.get("earnings", [])
    if earnings:
        # Handle both list and dict formats
        earnings_data = earnings.get("recent_earnings", earnings) if isinstance(earnings, dict) else earnings
        
        lines.append("--- EARNINGS ---")
        
        # Try to identify next earnings vs historical
        if earnings_data:
            # Assume first is most recent/upcoming
            next_earnings = earnings_data[0]
            lines.append(f"Next Earnings: {next_earnings.get('date', 'N/A')}")
            
            # Show recent performance summary (up to 3 historical)
            if len(earnings_data) > 1:
                lines.append("Recent Performance:")
                for earning in earnings_data[1:4]:  # Skip first (next), show up to 3 historical
                    date = earning.get('date', 'N/A')
                    eps_actual = earning.get('epsActual', earning.get('eps_actual'))
                    eps_estimate = earning.get('epsEstimate', earning.get('eps_estimate'))
                    
                    if eps_actual and eps_estimate:
                        beat = "Beat" if eps_actual > eps_estimate else "Miss" if eps_actual < eps_estimate else "Met"
                        lines.append(f"  {date}: ${eps_actual:.2f} vs ${eps_estimate:.2f} est. ({beat})")
                    elif eps_actual:
                        lines.append(f"  {date}: ${eps_actual:.2f}")
        
        lines.append("")
    
    # Section 6: Short Volume
    short_volume = data.get("short_volume", {})
    if short_volume and isinstance(short_volume, dict):
        lines.append("--- SHORT VOLUME ---")
        
        avg_ratio = short_volume.get('average_short_ratio')
        recent_ratio = short_volume.get('recent_short_ratio')
        trend = short_volume.get('trend', 'unknown')
        days = short_volume.get('days_analyzed', 0)
        
        if avg_ratio:
            lines.append(f"Average Short Ratio: {avg_ratio:.2%} ({days} days)")
        if recent_ratio:
            lines.append(f"Recent Short Ratio: {recent_ratio:.2%}")
        if trend:
            trend_symbol = "↑" if trend == "increasing" else "↓" if trend == "decreasing" else "→"
            lines.append(f"Trend: {trend_symbol} {trend.capitalize()}")
        
        lines.append("")
    
    # Section 7: Google Trends
    google_trends = data.get("google_trends", {})
    if google_trends and isinstance(google_trends, dict):
        lines.append("--- SEARCH INTEREST (Google Trends) ---")
        
        latest = google_trends.get('latest', 0)
        average = google_trends.get('average', 0)
        trend = google_trends.get('trend', 'unknown')
        timeframe = google_trends.get('timeframe', '')
        
        lines.append(f"Latest Interest: {latest}/100")
        lines.append(f"Average Interest: {average:.1f}/100")
        
        if trend:
            trend_symbol = "↑" if trend == "increasing" else "↓" if trend == "decreasing" else "→"
            lines.append(f"Trend: {trend_symbol} {trend.capitalize()}")
        
        if timeframe:
            lines.append(f"Timeframe: {timeframe}")
        
        lines.append("")
    
    # Error section (if any)
    if errors:
        lines.append(f"⚠️ Partial data - {len(errors)} error(s) occurred:")
        for error in errors[:5]:  # Show max 5 errors
            lines.append(f"  • {error}")
        if len(errors) > 5:
            lines.append(f"  ... and {len(errors) - 5} more errors")
        lines.append("")
    
    return "\n".join(lines)


def format_multi_snapshot_for_llm(multi_snapshot: Dict[str, Any]) -> str:
    """
    Format a multi-ticker snapshot into compact text optimized for LLM consumption.
    
    This function converts multiple ticker snapshots into a consolidated text format
    with clear sections for each ticker, avoiding repetition and optimizing for
    token efficiency.
    
    Args:
        multi_snapshot: Multi-ticker snapshot dictionary from get_multi_ticker_snapshot()
    
    Returns:
        Formatted text string with all tickers' financial data in compact sections
    
    Example:
        >>> multi_snapshot = await get_multi_ticker_snapshot(["AAPL", "MSFT", "GOOGL"])
        >>> formatted = format_multi_snapshot_for_llm(multi_snapshot)
        >>> print(formatted)
        === MULTI-TICKER ANALYSIS ===
        Timestamp: 2025-01-15T10:30:00
        Tickers Analyzed: 3
        ...
    """
    lines = []
    timestamp = multi_snapshot.get("timestamp", "")
    tickers_count = multi_snapshot.get("tickers_count", 0)
    snapshots = multi_snapshot.get("snapshots", {})
    global_errors = multi_snapshot.get("global_errors", [])
    
    # Header
    lines.append("=== MULTI-TICKER ANALYSIS ===")
    lines.append(f"Timestamp: {timestamp}")
    lines.append(f"Tickers Analyzed: {tickers_count}")
    lines.append(f"Successful: {len(snapshots)}")
    
    if global_errors:
        lines.append(f"Errors: {len(global_errors)}")
    
    lines.append("")
    lines.append("=" * 60)
    lines.append("")
    
    # Format each ticker snapshot
    if snapshots:
        for i, (ticker, snapshot) in enumerate(snapshots.items(), 1):
            # Add separator between tickers (except for first)
            if i > 1:
                lines.append("")
                lines.append("-" * 60)
                lines.append("")
            
            # Format individual snapshot
            formatted_snapshot = format_snapshot_for_llm(snapshot)
            lines.append(formatted_snapshot)
    else:
        lines.append("⚠️ No ticker data available")
        lines.append("")
    
    # Global errors section (if any)
    if global_errors:
        lines.append("")
        lines.append("=" * 60)
        lines.append("")
        lines.append(f"⚠️ GLOBAL ERRORS ({len(global_errors)}):")
        lines.append("")
        
        # Show all global errors (they're already per-ticker, so should be limited)
        for i, error in enumerate(global_errors, 1):
            lines.append(f"{i}. {error}")
        
        lines.append("")
    
    # Footer with summary
    lines.append("=" * 60)
    lines.append(f"END OF MULTI-TICKER ANALYSIS ({len(snapshots)}/{tickers_count} successful)")
    
    return "\n".join(lines)


def _format_data_compact(data_type: str, data: Any, lookback_days: int) -> Any:
    """
    Format data in a compact way to save tokens.
    
    Args:
        data_type: Type of data (info, prices, news, etc.)
        data: Raw data from datasource
        lookback_days: Lookback period for context
    
    Returns:
        Compactly formatted data
    """
    try:
        if data_type == "info":
            # Truncate company summary and keep essential fields
            if isinstance(data, dict):
                compact_info = {
                    "longName": data.get("longName", ""),
                    "sector": data.get("sector", ""),
                    "industry": data.get("industry", ""),
                    "marketCap": data.get("marketCap"),
                    "currentPrice": data.get("currentPrice"),
                    "fiftyTwoWeekHigh": data.get("fiftyTwoWeekHigh"),
                    "fiftyTwoWeekLow": data.get("fiftyTwoWeekLow"),
                    "averageVolume": data.get("averageVolume"),
                    "beta": data.get("beta"),
                    "trailingPE": data.get("trailingPE"),
                    "forwardPE": data.get("forwardPE"),
                    "dividendYield": data.get("dividendYield"),
                    "summary": truncate_string(data.get("longBusinessSummary", ""), max_length=300)
                }
                return compact_info
            return data
        
        elif data_type == "prices":
            # Keep only last 5 days + calculate 30-day change
            if hasattr(data, 'tail'):  # pandas DataFrame
                last_5 = data.tail(5)
                
                # Calculate 30-day change if we have enough data
                change_30d_pct = None
                if len(data) >= 2:
                    first_close = data['Close'].iloc[0]
                    last_close = data['Close'].iloc[-1]
                    if first_close and first_close != 0:
                        change_30d_pct = round(((last_close - first_close) / first_close) * 100, 2)
                
                # Convert to compact dict format
                compact_prices = {
                    "latest_close": float(last_5['Close'].iloc[-1]) if len(last_5) > 0 else None,
                    "latest_date": str(last_5.index[-1].date()) if len(last_5) > 0 else None,
                    "change_30d_pct": change_30d_pct,
                    "last_5_days": [
                        {
                            "date": str(row.name.date()),
                            "close": round(float(row['Close']), 2),
                            "volume": int(row['Volume'])
                        }
                        for _, row in last_5.iterrows()
                    ]
                }
                return compact_prices
            return data
        
        elif data_type == "news":
            # Limit to 5 articles with truncated summaries
            if isinstance(data, list):
                compact_news = []
                for article in data[:5]:
                    compact_article = {
                        "published_at": article.get("published_at", ""),
                        "source": article.get("source", ""),
                        "title": truncate_string(article.get("title", ""), max_length=150),
                        "url": article.get("url", ""),
                        "summary": truncate_string(article.get("summary", ""), max_length=150)
                    }
                    compact_news.append(compact_article)
                
                if len(data) > 5:
                    return {"articles": compact_news, "total_available": len(data)}
                return compact_news
            return data
        
        elif data_type == "sec_filings":
            # Limit to 3 most recent filings
            if isinstance(data, list):
                compact_filings = []
                for filing in data[:3]:
                    compact_filing = {
                        "date": filing.get("date", ""),
                        "form": filing.get("form", ""),
                        "title": truncate_string(filing.get("title", ""), max_length=100),
                        "url": filing.get("url", "")
                    }
                    compact_filings.append(compact_filing)
                
                if len(data) > 3:
                    return {"filings": compact_filings, "total_available": len(data)}
                return compact_filings
            return data
        
        elif data_type == "earnings":
            # Keep next earnings + 3 recent quarters
            if isinstance(data, list):
                # Sort by date to get most recent
                sorted_earnings = sorted(data, key=lambda x: x.get("date", ""), reverse=True)
                compact_earnings = sorted_earnings[:4]  # Next + 3 recent
                
                if len(data) > 4:
                    return {"recent_earnings": compact_earnings, "total_available": len(data)}
                return compact_earnings
            return data
        
        elif data_type == "short_volume":
            # Return only aggregated metrics, not daily data
            if isinstance(data, list) and data:
                # Calculate aggregate metrics
                total_short = sum(d.get("short_volume", 0) for d in data)
                total_volume = sum(d.get("total_volume", 0) for d in data)
                avg_ratio = sum(d.get("short_ratio", 0) for d in data) / len(data)
                
                # Recent vs historical
                recent = data[:5] if len(data) >= 5 else data
                recent_avg_ratio = sum(d.get("short_ratio", 0) for d in recent) / len(recent) if recent else 0
                
                return {
                    "average_short_ratio": round(avg_ratio, 4),
                    "recent_short_ratio": round(recent_avg_ratio, 4),
                    "trend": "increasing" if recent_avg_ratio > avg_ratio else "decreasing",
                    "days_analyzed": len(data),
                    "latest_date": data[0].get("date", "") if data else ""
                }
            return data
        
        elif data_type == "google_trends":
            # Return only summary metrics, not full series
            if isinstance(data, dict):
                compact_trends = {
                    "latest": data.get("latest", 0),
                    "average": data.get("average", 0),
                    "trend": data.get("trend", "unknown"),
                    "peak_value": data.get("peak_value", 0),
                    "peak_date": data.get("peak_date", ""),
                    "timeframe": data.get("timeframe", "")
                }
                return compact_trends
            return data
        
        elif data_type == "options":
            # Return only expiration dates, not full chain
            if isinstance(data, (tuple, list)):
                return {
                    "available_expirations": list(data)[:10],  # Limit to 10
                    "total_expirations": len(data)
                }
            return data
        
        else:
            # Unknown data type - return as is
            return data
            
    except Exception as e:
        logger.warning(f"Error formatting {data_type}: {e}")
        return data
