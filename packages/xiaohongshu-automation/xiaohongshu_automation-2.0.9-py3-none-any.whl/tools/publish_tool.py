"""
å°çº¢ä¹¦å‘å¸ƒå·¥å…·
å®ç°å†…å®¹å‘å¸ƒåŠŸèƒ½ï¼ŒåŒ…å«å›¾ç‰‡å’Œæ–‡å­—
"""

import re
from typing import Any, Dict, List
from urllib.parse import urlparse
from tools.base_tool import BaseTool, ToolResult

class PublishTool(BaseTool):
    """å°çº¢ä¹¦å†…å®¹å‘å¸ƒå·¥å…·"""
    
    def __init__(self, adapter):
        super().__init__(
            name="xiaohongshu_publish",
            description="å‘å¸ƒå†…å®¹åˆ°å°çº¢ä¹¦ï¼Œæ”¯æŒå¤šå¼ å›¾ç‰‡å’Œå¯Œæ–‡æœ¬å†…å®¹"
        )
        self.adapter = adapter
    
    def get_schema(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å‚æ•° Schema"""
        return {
            "type": "object",
            "properties": {
                "pic_urls": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "å›¾ç‰‡URLåˆ—è¡¨ï¼Œæ”¯æŒ1-9å¼ å›¾ç‰‡",
                    "minItems": 1,
                    "maxItems": 9
                },
                "title": {
                    "type": "string",
                    "description": "å‘å¸ƒæ ‡é¢˜ï¼Œå»ºè®®20å­—ä»¥å†…",
                    "minLength": 1,
                    "maxLength": 50
                },
                "content": {
                    "type": "string",
                    "description": "å‘å¸ƒå†…å®¹ï¼Œæ”¯æŒè¯é¢˜æ ‡ç­¾å’Œè¡¨æƒ…ç¬¦å·",
                    "minLength": 1,
                    "maxLength": 1000
                },
                "labels": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "è‡ªå®šä¹‰æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤æ ‡ç­¾ ['#å°çº¢ä¹¦']",
                    "required": False
                },
                "phone_number": {
                    "type": "string",
                    "description": "æŒ‡å®šå‘å¸ƒè´¦å·çš„æ‰‹æœºå·ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰é»˜è®¤è´¦å·",
                    "required": False
                }
            },
            "required": ["pic_urls", "title", "content"]
        }
    
    def _validate_urls(self, urls: List[str]) -> List[str]:
        """
        éªŒè¯å›¾ç‰‡URLæ ¼å¼
        
        Args:
            urls: URLåˆ—è¡¨
            
        Returns:
            éªŒè¯åçš„URLåˆ—è¡¨
            
        Raises:
            ValueError: URLæ ¼å¼æ— æ•ˆæˆ–å…¶ä»–éªŒè¯é”™è¯¯
        """
        validated_urls = []
        
        for i, url in enumerate(urls):
            # æ£€æŸ¥URLæ˜¯å¦ä¸ºå­—ç¬¦ä¸²
            if not isinstance(url, str):
                raise ValueError(f"ç¬¬{i+1}ä¸ªå›¾ç‰‡URLä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(url).__name__}")
            
            # æ£€æŸ¥URLæ˜¯å¦ä¸ºç©º
            if not url.strip():
                raise ValueError(f"ç¬¬{i+1}ä¸ªå›¾ç‰‡URLä¸ºç©º")
            
            url = url.strip()
            
            # åŸºæœ¬URLæ ¼å¼éªŒè¯
            try:
                parsed = urlparse(url)
                if not all([parsed.scheme, parsed.netloc]):
                    raise ValueError(f"ç¬¬{i+1}ä¸ªå›¾ç‰‡URLæ ¼å¼æ— æ•ˆ: {url}")
                    
                # æ£€æŸ¥åè®®æ˜¯å¦ä¸ºhttpæˆ–https
                if parsed.scheme not in ['http', 'https']:
                    raise ValueError(f"ç¬¬{i+1}ä¸ªå›¾ç‰‡URLå¿…é¡»ä½¿ç”¨httpæˆ–httpsåè®®: {url}")
                    
            except Exception as e:
                if isinstance(e, ValueError):
                    raise e
                else:
                    raise ValueError(f"ç¬¬{i+1}ä¸ªå›¾ç‰‡URLè§£æå¤±è´¥: {url} - {str(e)}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡URLï¼ˆç®€å•æ£€æŸ¥ï¼‰
            image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
            url_lower = url.lower()
            
            # å¦‚æœURLåŒ…å«å›¾ç‰‡æ‰©å±•åæˆ–è€…æ˜¯å¸¸è§å›¾ç‰‡æœåŠ¡åŸŸå
            is_image = (
                any(ext in url_lower for ext in image_extensions) or
                any(domain in url_lower for domain in ['imgur.com', 'cloudinary.com', 'unsplash.com', 'pixabay.com', 'pexels.com', 'images.unsplash.com'])
            )
            
            if not is_image:
                self.logger.warning(f"ç¬¬{i+1}ä¸ªURLå¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url}")
                # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯è­¦å‘Šï¼Œä¸æŠ›å‡ºé”™è¯¯ï¼Œå› ä¸ºæœ‰äº›å›¾ç‰‡URLå¯èƒ½æ²¡æœ‰æ˜æ˜¾çš„æ‰©å±•å
            
            validated_urls.append(url)
            self.logger.info(f"ç¬¬{i+1}ä¸ªå›¾ç‰‡URLéªŒè¯é€šè¿‡: {url}")
        
        self.logger.info(f"æ‰€æœ‰{len(validated_urls)}ä¸ªå›¾ç‰‡URLéªŒè¯å®Œæˆ")
        return validated_urls
    
    def _validate_content(self, title: str, content: str) -> tuple[str, str]:
        """
        éªŒè¯å’Œä¼˜åŒ–å†…å®¹
        
        Args:
            title: æ ‡é¢˜
            content: å†…å®¹
            
        Returns:
            ä¼˜åŒ–åçš„æ ‡é¢˜å’Œå†…å®¹
        """
        # æ ‡é¢˜ä¼˜åŒ–
        title = title.strip()
        if len(title) > 20:
            self.logger.warning("æ ‡é¢˜è¶…è¿‡20å­—ï¼Œå¯èƒ½å½±å“å±•ç¤ºæ•ˆæœ")
        
        # å†…å®¹ä¼˜åŒ–
        content = content.strip()
        
        # æ£€æŸ¥è¯é¢˜æ ‡ç­¾æ ¼å¼
        hashtag_pattern = r'#[^#\s]+#?'
        hashtags = re.findall(hashtag_pattern, content)
        if hashtags:
            self.logger.info(f"æ£€æµ‹åˆ°è¯é¢˜æ ‡ç­¾: {hashtags}")
        
        # æ£€æŸ¥@ç”¨æˆ·æ ¼å¼
        mention_pattern = r'@[^\s@]+'
        mentions = re.findall(mention_pattern, content)
        if mentions:
            self.logger.info(f"æ£€æµ‹åˆ°@ç”¨æˆ·: {mentions}")
        
        return title, content
    
    def _analyze_content_quality(self, title: str, content: str) -> Dict[str, Any]:
        """
        åˆ†æå†…å®¹è´¨é‡
        
        Args:
            title: æ ‡é¢˜
            content: å†…å®¹
            
        Returns:
            å†…å®¹è´¨é‡åˆ†æç»“æœ
        """
        analysis = {
            "title_length": len(title),
            "content_length": len(content),
            "has_hashtags": bool(re.search(r'#[^#\s]+#?', content)),
            "has_mentions": bool(re.search(r'@[^\s@]+', content)),
            "has_emojis": bool(re.search(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', content)),
            "paragraph_count": len([p for p in content.split('\n') if p.strip()]),
            "quality_score": 0
        }
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        score = 0
        
        # æ ‡é¢˜é•¿åº¦é€‚ä¸­ (5-20å­—)
        if 5 <= analysis["title_length"] <= 20:
            score += 20
        elif analysis["title_length"] > 0:
            score += 10
        
        # å†…å®¹é•¿åº¦é€‚ä¸­ (50-500å­—)
        if 50 <= analysis["content_length"] <= 500:
            score += 30
        elif analysis["content_length"] >= 20:
            score += 15
        
        # åŒ…å«è¯é¢˜æ ‡ç­¾
        if analysis["has_hashtags"]:
            score += 20
        
        # åŒ…å«è¡¨æƒ…ç¬¦å·
        if analysis["has_emojis"]:
            score += 15
        
        # å†…å®¹åˆ†æ®µ
        if analysis["paragraph_count"] > 1:
            score += 15
        
        analysis["quality_score"] = score
        
        return analysis
    
    async def execute(self, arguments: Dict[str, Any]) -> ToolResult:
        """
        æ‰§è¡Œå‘å¸ƒæ“ä½œ
        
        Args:
            arguments: å·¥å…·å‚æ•°
            
        Returns:
            å‘å¸ƒç»“æœ
        """
        try:
            # æå–å‚æ•°
            pic_urls = arguments["pic_urls"]
            title = arguments["title"]
            content = arguments["content"]
            labels = arguments.get("labels", None)  # è·å–å¯é€‰çš„labelså‚æ•°
            phone_number = arguments.get("phone_number", None)  # è·å–å¯é€‰çš„phone_numberå‚æ•°

            # éªŒè¯ pic_urls å¿…é¡»æ˜¯æ•°ç»„
            if not isinstance(pic_urls, list):
                raise ValueError("å›¾ç‰‡URL 'pic_urls' å¿…é¡»æ˜¯ä¸€ä¸ªæ•°ç»„ã€‚")
            
            if len(pic_urls) == 0:
                raise ValueError("å›¾ç‰‡URLæ•°ç»„ 'pic_urls' ä¸èƒ½ä¸ºç©ºï¼Œè‡³å°‘éœ€è¦æä¾›ä¸€å¼ å›¾ç‰‡ã€‚")

            # éªŒè¯ content ä¸å¾—åŒ…å« # ç¬¦å·
            if "#" in content:
                raise ValueError("å‘å¸ƒå†…å®¹ 'content' ä¸­ä¸å¾—åŒ…å« '#' ç¬¦å·ï¼Œè¯·å°†æ ‡ç­¾å†…å®¹ç§»è‡³ 'labels' å‚æ•°ä¸­ã€‚")

            # éªŒè¯ labels å‚æ•°
            if labels is not None:
                if not isinstance(labels, list):
                    raise ValueError("è‡ªå®šä¹‰æ ‡ç­¾ 'labels' å¿…é¡»æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚")
                for i, label in enumerate(labels):
                    if not isinstance(label, str):
                        raise ValueError(f"è‡ªå®šä¹‰æ ‡ç­¾åˆ—è¡¨ä¸­çš„ç¬¬ {i+1} ä¸ªå…ƒç´  '{label}' ä¸æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚")
                    if not label.startswith("#"):
                        raise ValueError(f"è‡ªå®šä¹‰æ ‡ç­¾ '{label}' å¿…é¡»ä»¥ '#' å¼€å¤´ã€‚")
            
            # éªŒè¯å›¾ç‰‡URL
            try:
                validated_urls = self._validate_urls(pic_urls)
            except Exception as e:
                raise ValueError(f"å›¾ç‰‡URLéªŒè¯å¤±è´¥: {str(e)}")
            
            # éªŒè¯å’Œä¼˜åŒ–å†…å®¹
            optimized_title, optimized_content = self._validate_content(title, content)
            
            # åˆ†æå†…å®¹è´¨é‡
            quality_analysis = self._analyze_content_quality(optimized_title, optimized_content)
            
            # è°ƒç”¨é€‚é…å™¨å‘å¸ƒå†…å®¹
            result = await self.adapter.publish_content(
                pic_urls=validated_urls,
                title=optimized_title,
                content=optimized_content,
                labels=labels,
                phone_number=phone_number
            )
            
            # æ„å»ºæˆåŠŸç»“æœ
            success_data = {
                "publish_result": result,
                "content_analysis": quality_analysis,
                "optimizations": {
                    "title_optimized": title != optimized_title,
                    "content_optimized": content != optimized_content,
                    "url_count": len(validated_urls),
                    "labels_used": labels or ["#å°çº¢ä¹¦"]  # æ˜¾ç¤ºä½¿ç”¨çš„æ ‡ç­¾
                }
            }
            
            message = f"æˆåŠŸå‘å¸ƒå†…å®¹åˆ°å°çº¢ä¹¦ï¼"
            if quality_analysis["quality_score"] >= 80:
                message += " å†…å®¹è´¨é‡ä¼˜ç§€ â­â­â­"
            elif quality_analysis["quality_score"] >= 60:
                message += " å†…å®¹è´¨é‡è‰¯å¥½ â­â­"
            else:
                message += " å»ºè®®ä¼˜åŒ–å†…å®¹è´¨é‡ â­"
            
            return ToolResult(
                success=True,
                data=success_data,
                message=message
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                error=str(e),
                message="å‘å¸ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œå‚æ•°è®¾ç½®"
            )
    
    def _format_success_result(self, result: ToolResult) -> str:
        """è‡ªå®šä¹‰æˆåŠŸç»“æœæ ¼å¼åŒ–"""
        output = f"âœ… å°çº¢ä¹¦å†…å®¹å‘å¸ƒæˆåŠŸï¼\n\n"
        
        if result.message:
            output += f"ğŸ“ {result.message}\n\n"
        
        if result.data:
            data = result.data
            publish_result = data.get("publish_result", {})
            analysis = data.get("content_analysis", {})
            optimizations = data.get("optimizations", {})
            
            # å‘å¸ƒç»“æœ
            if publish_result.get("status") == "success":
                output += "ğŸ‰ å‘å¸ƒçŠ¶æ€: æˆåŠŸ\n"
                if publish_result.get("urls"):
                    output += f"ğŸ”— å‘å¸ƒé“¾æ¥: {', '.join(publish_result['urls'])}\n"
            
            output += "\nğŸ“Š å†…å®¹åˆ†æ:\n"
            output += f"   â€¢ æ ‡é¢˜é•¿åº¦: {analysis.get('title_length', 0)} å­—\n"
            output += f"   â€¢ å†…å®¹é•¿åº¦: {analysis.get('content_length', 0)} å­—\n"
            output += f"   â€¢ è´¨é‡è¯„åˆ†: {analysis.get('quality_score', 0)}/100\n"
            output += f"   â€¢ è¯é¢˜æ ‡ç­¾: {'âœ…' if analysis.get('has_hashtags') else 'âŒ'}\n"
            output += f"   â€¢ è¡¨æƒ…ç¬¦å·: {'âœ…' if analysis.get('has_emojis') else 'âŒ'}\n"
            output += f"   â€¢ å†…å®¹åˆ†æ®µ: {'âœ…' if analysis.get('paragraph_count', 0) > 1 else 'âŒ'}\n"
            
            if optimizations.get("url_count"):
                output += f"\nğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {optimizations['url_count']} å¼ \n"
            
            if optimizations.get("labels_used"):
                output += f"ğŸ·ï¸ ä½¿ç”¨æ ‡ç­¾: {', '.join(optimizations['labels_used'])}\n"
        
        if result.execution_time:
            output += f"\nâ±ï¸ å‘å¸ƒè€—æ—¶: {result.execution_time:.2f}ç§’"
        
        output += f"\nğŸ• å®Œæˆæ—¶é—´: {result.timestamp}"
        
        return output 