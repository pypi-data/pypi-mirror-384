// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
#define FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 23 &&
              FLATBUFFERS_VERSION_MINOR == 3 &&
              FLATBUFFERS_VERSION_REVISION == 3,
             "Non-compatible flatbuffers version included");

namespace nvfuser {
namespace serde {

struct State;

struct Scalar;
struct ScalarBuilder;

struct ScalarCpu;
struct ScalarCpuBuilder;

struct TensorArg;
struct TensorArgBuilder;

struct PolymorphicValue;
struct PolymorphicValueBuilder;

struct KernelArgumentHolder;
struct KernelArgumentHolderBuilder;

struct TensorShape;
struct TensorShapeBuilder;

struct LaunchParams;
struct LaunchParamsBuilder;

struct GlobalBufferInfo;
struct GlobalBufferInfoBuilder;

struct KernelExecutorEntry;
struct KernelExecutorEntryBuilder;

struct At;
struct AtBuilder;

struct BatchNorm;
struct BatchNormBuilder;

struct Broadcast;
struct BroadcastBuilder;

struct BroadcastInDim;
struct BroadcastInDimBuilder;

struct Cat;
struct CatBuilder;

struct Dtype;
struct DtypeBuilder;

struct Dimension;
struct DimensionBuilder;

struct Norm;
struct NormBuilder;

struct Output;
struct OutputBuilder;

struct Dims;
struct DimsBuilder;

struct Reduction;
struct ReductionBuilder;

struct Size;
struct SizeBuilder;

struct Slice;
struct SliceBuilder;

struct Squeeze;
struct SqueezeBuilder;

struct Tensor;
struct TensorBuilder;

struct TensorCreationSymbolic;
struct TensorCreationSymbolicBuilder;

struct Vector;
struct VectorBuilder;

struct Welford;
struct WelfordBuilder;

struct Sort;
struct SortBuilder;

struct TopK;
struct TopKBuilder;

struct ScaledOp;
struct ScaledOpBuilder;

struct ScanOp;
struct ScanOpBuilder;

struct CudaKernel;
struct CudaKernelBuilder;

struct KernelExecutor;
struct KernelExecutorBuilder;

struct SegmentedEdge;
struct SegmentedEdgeBuilder;

struct SegmentedGroup;
struct SegmentedGroupBuilder;

struct SegmentedFusion;
struct SegmentedFusionBuilder;

struct FusionKernelRuntime;
struct FusionKernelRuntimeBuilder;

struct EncodingEntry;

struct InputsIdLookup;
struct InputsIdLookupBuilder;

struct KernelRuntimeState;
struct KernelRuntimeStateBuilder;

struct FusionExecutorCache;
struct FusionExecutorCacheBuilder;

struct RecordFunctor;
struct RecordFunctorBuilder;

struct TrieNode;
struct TrieNodeBuilder;

struct FusionCache;
struct FusionCacheBuilder;

enum class StateType : int32_t {
  Tensor = 0,
  Scalar = 1,
  Vector = 2,
  None = 3,
  MIN = Tensor,
  MAX = None
};

inline const StateType (&EnumValuesStateType())[4] {
  static const StateType values[] = {
    StateType::Tensor,
    StateType::Scalar,
    StateType::Vector,
    StateType::None
  };
  return values;
}

inline const char * const *EnumNamesStateType() {
  static const char * const names[5] = {
    "Tensor",
    "Scalar",
    "Vector",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameStateType(StateType e) {
  if (::flatbuffers::IsOutRange(e, StateType::Tensor, StateType::None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesStateType()[index];
}

enum class Contiguity : int32_t {
  Strided = 0,
  Contiguous = 1,
  None = 2,
  MIN = Strided,
  MAX = None
};

inline const Contiguity (&EnumValuesContiguity())[3] {
  static const Contiguity values[] = {
    Contiguity::Strided,
    Contiguity::Contiguous,
    Contiguity::None
  };
  return values;
}

inline const char * const *EnumNamesContiguity() {
  static const char * const names[4] = {
    "Strided",
    "Contiguous",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameContiguity(Contiguity e) {
  if (::flatbuffers::IsOutRange(e, Contiguity::Strided, Contiguity::None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesContiguity()[index];
}

enum class RecordType : int32_t {
  Base = 0,
  AtOp = 1,
  BatchNormOp = 2,
  BroadcastOp = 3,
  BroadcastInDim = 4,
  CastTv = 5,
  CastVal = 6,
  CatOp = 7,
  EmbeddingFwdOp = 8,
  End = 9,
  ExpandOp = 10,
  FullOp = 11,
  IotaOp = 12,
  IndexSelectOp = 13,
  IndexPutAccumulateOp = 14,
  SelectOp = 15,
  ScatterOp = 16,
  GatherOp = 17,
  TakeAlongAxisOp = 18,
  Unary_TV = 19,
  Unary_VAL = 20,
  Binary_TV = 21,
  Binary_VAL = 22,
  Binary_TV_VAL = 23,
  Binary_VAL_TV = 24,
  Ternary_TV = 25,
  Ternary_VAL = 26,
  Ternary_TV_TV_VAL = 27,
  Ternary_TV_VAL_TV = 28,
  Ternary_VAL_TV_TV = 29,
  Ternary_VAL_VAL_TV = 30,
  Ternary_TV_VAL_VAL = 31,
  Ternary_VAL_TV_VAL = 32,
  Ternary_Alpha_TV = 33,
  Ternary_Alpha_VAL = 34,
  Ternary_Alpha_TV_TV_VAL = 35,
  Ternary_Alpha_TV_VAL_TV = 36,
  Ternary_Alpha_VAL_TV_TV = 37,
  Ternary_Alpha_VAL_VAL_TV = 38,
  Ternary_Alpha_TV_VAL_VAL = 39,
  Ternary_Alpha_VAL_TV_VAL = 40,
  NormalDistOp = 41,
  OutputTv = 42,
  OutputVal = 43,
  PadOp = 44,
  PermuteOp = 45,
  StrideOrderOp = 46,
  ReductionMax = 47,
  ReductionMin = 48,
  ReductionProd = 49,
  ReductionSum = 50,
  ReshapeOp = 51,
  Scalar = 52,
  ScaledGroupedMmaOp = 53,
  ScaledMmaOp = 54,
  CutlassNvfp4GroupedMmaOp = 55,
  SdpaFwdOp = 56,
  SdpaBwdOp = 57,
  ShapeOp = 58,
  SizeOp = 59,
  SliceOp = 60,
  SqueezeOp = 61,
  Start = 62,
  Tensor = 63,
  TensorSizes = 64,
  UniformDistOp = 65,
  VarianceOp = 66,
  VarianceMeanOp = 67,
  Vector = 68,
  WelfordOp = 69,
  ArgsortOp = 70,
  TopKOp = 71,
  ScanOpCumsum = 72,
  MIN = Base,
  MAX = ScanOpCumsum
};

inline const RecordType (&EnumValuesRecordType())[73] {
  static const RecordType values[] = {
    RecordType::Base,
    RecordType::AtOp,
    RecordType::BatchNormOp,
    RecordType::BroadcastOp,
    RecordType::BroadcastInDim,
    RecordType::CastTv,
    RecordType::CastVal,
    RecordType::CatOp,
    RecordType::EmbeddingFwdOp,
    RecordType::End,
    RecordType::ExpandOp,
    RecordType::FullOp,
    RecordType::IotaOp,
    RecordType::IndexSelectOp,
    RecordType::IndexPutAccumulateOp,
    RecordType::SelectOp,
    RecordType::ScatterOp,
    RecordType::GatherOp,
    RecordType::TakeAlongAxisOp,
    RecordType::Unary_TV,
    RecordType::Unary_VAL,
    RecordType::Binary_TV,
    RecordType::Binary_VAL,
    RecordType::Binary_TV_VAL,
    RecordType::Binary_VAL_TV,
    RecordType::Ternary_TV,
    RecordType::Ternary_VAL,
    RecordType::Ternary_TV_TV_VAL,
    RecordType::Ternary_TV_VAL_TV,
    RecordType::Ternary_VAL_TV_TV,
    RecordType::Ternary_VAL_VAL_TV,
    RecordType::Ternary_TV_VAL_VAL,
    RecordType::Ternary_VAL_TV_VAL,
    RecordType::Ternary_Alpha_TV,
    RecordType::Ternary_Alpha_VAL,
    RecordType::Ternary_Alpha_TV_TV_VAL,
    RecordType::Ternary_Alpha_TV_VAL_TV,
    RecordType::Ternary_Alpha_VAL_TV_TV,
    RecordType::Ternary_Alpha_VAL_VAL_TV,
    RecordType::Ternary_Alpha_TV_VAL_VAL,
    RecordType::Ternary_Alpha_VAL_TV_VAL,
    RecordType::NormalDistOp,
    RecordType::OutputTv,
    RecordType::OutputVal,
    RecordType::PadOp,
    RecordType::PermuteOp,
    RecordType::StrideOrderOp,
    RecordType::ReductionMax,
    RecordType::ReductionMin,
    RecordType::ReductionProd,
    RecordType::ReductionSum,
    RecordType::ReshapeOp,
    RecordType::Scalar,
    RecordType::ScaledGroupedMmaOp,
    RecordType::ScaledMmaOp,
    RecordType::CutlassNvfp4GroupedMmaOp,
    RecordType::SdpaFwdOp,
    RecordType::SdpaBwdOp,
    RecordType::ShapeOp,
    RecordType::SizeOp,
    RecordType::SliceOp,
    RecordType::SqueezeOp,
    RecordType::Start,
    RecordType::Tensor,
    RecordType::TensorSizes,
    RecordType::UniformDistOp,
    RecordType::VarianceOp,
    RecordType::VarianceMeanOp,
    RecordType::Vector,
    RecordType::WelfordOp,
    RecordType::ArgsortOp,
    RecordType::TopKOp,
    RecordType::ScanOpCumsum
  };
  return values;
}

inline const char * const *EnumNamesRecordType() {
  static const char * const names[74] = {
    "Base",
    "AtOp",
    "BatchNormOp",
    "BroadcastOp",
    "BroadcastInDim",
    "CastTv",
    "CastVal",
    "CatOp",
    "EmbeddingFwdOp",
    "End",
    "ExpandOp",
    "FullOp",
    "IotaOp",
    "IndexSelectOp",
    "IndexPutAccumulateOp",
    "SelectOp",
    "ScatterOp",
    "GatherOp",
    "TakeAlongAxisOp",
    "Unary_TV",
    "Unary_VAL",
    "Binary_TV",
    "Binary_VAL",
    "Binary_TV_VAL",
    "Binary_VAL_TV",
    "Ternary_TV",
    "Ternary_VAL",
    "Ternary_TV_TV_VAL",
    "Ternary_TV_VAL_TV",
    "Ternary_VAL_TV_TV",
    "Ternary_VAL_VAL_TV",
    "Ternary_TV_VAL_VAL",
    "Ternary_VAL_TV_VAL",
    "Ternary_Alpha_TV",
    "Ternary_Alpha_VAL",
    "Ternary_Alpha_TV_TV_VAL",
    "Ternary_Alpha_TV_VAL_TV",
    "Ternary_Alpha_VAL_TV_TV",
    "Ternary_Alpha_VAL_VAL_TV",
    "Ternary_Alpha_TV_VAL_VAL",
    "Ternary_Alpha_VAL_TV_VAL",
    "NormalDistOp",
    "OutputTv",
    "OutputVal",
    "PadOp",
    "PermuteOp",
    "StrideOrderOp",
    "ReductionMax",
    "ReductionMin",
    "ReductionProd",
    "ReductionSum",
    "ReshapeOp",
    "Scalar",
    "ScaledGroupedMmaOp",
    "ScaledMmaOp",
    "CutlassNvfp4GroupedMmaOp",
    "SdpaFwdOp",
    "SdpaBwdOp",
    "ShapeOp",
    "SizeOp",
    "SliceOp",
    "SqueezeOp",
    "Start",
    "Tensor",
    "TensorSizes",
    "UniformDistOp",
    "VarianceOp",
    "VarianceMeanOp",
    "Vector",
    "WelfordOp",
    "ArgsortOp",
    "TopKOp",
    "ScanOpCumsum",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordType(RecordType e) {
  if (::flatbuffers::IsOutRange(e, RecordType::Base, RecordType::ScanOpCumsum)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordType()[index];
}

enum class RecordData : uint8_t {
  NONE = 0,
  At = 1,
  BatchNorm = 2,
  Broadcast = 3,
  BroadcastInDim = 4,
  Cat = 5,
  Dimension = 6,
  Dtype = 7,
  Norm = 8,
  Output = 9,
  Dims = 10,
  Slice = 11,
  Squeeze = 12,
  Reduction = 13,
  Scalar = 14,
  Size = 15,
  Tensor = 16,
  TensorCreationSymbolic = 17,
  Vector = 18,
  Welford = 19,
  Sort = 20,
  TopK = 21,
  ScaledOp = 22,
  ScanOp = 23,
  MIN = NONE,
  MAX = ScanOp
};

inline const RecordData (&EnumValuesRecordData())[24] {
  static const RecordData values[] = {
    RecordData::NONE,
    RecordData::At,
    RecordData::BatchNorm,
    RecordData::Broadcast,
    RecordData::BroadcastInDim,
    RecordData::Cat,
    RecordData::Dimension,
    RecordData::Dtype,
    RecordData::Norm,
    RecordData::Output,
    RecordData::Dims,
    RecordData::Slice,
    RecordData::Squeeze,
    RecordData::Reduction,
    RecordData::Scalar,
    RecordData::Size,
    RecordData::Tensor,
    RecordData::TensorCreationSymbolic,
    RecordData::Vector,
    RecordData::Welford,
    RecordData::Sort,
    RecordData::TopK,
    RecordData::ScaledOp,
    RecordData::ScanOp
  };
  return values;
}

inline const char * const *EnumNamesRecordData() {
  static const char * const names[25] = {
    "NONE",
    "At",
    "BatchNorm",
    "Broadcast",
    "BroadcastInDim",
    "Cat",
    "Dimension",
    "Dtype",
    "Norm",
    "Output",
    "Dims",
    "Slice",
    "Squeeze",
    "Reduction",
    "Scalar",
    "Size",
    "Tensor",
    "TensorCreationSymbolic",
    "Vector",
    "Welford",
    "Sort",
    "TopK",
    "ScaledOp",
    "ScanOp",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordData(RecordData e) {
  if (::flatbuffers::IsOutRange(e, RecordData::NONE, RecordData::ScanOp)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordData()[index];
}

template<typename T> struct RecordDataTraits {
  static const RecordData enum_value = RecordData::NONE;
};

template<> struct RecordDataTraits<nvfuser::serde::At> {
  static const RecordData enum_value = RecordData::At;
};

template<> struct RecordDataTraits<nvfuser::serde::BatchNorm> {
  static const RecordData enum_value = RecordData::BatchNorm;
};

template<> struct RecordDataTraits<nvfuser::serde::Broadcast> {
  static const RecordData enum_value = RecordData::Broadcast;
};

template<> struct RecordDataTraits<nvfuser::serde::BroadcastInDim> {
  static const RecordData enum_value = RecordData::BroadcastInDim;
};

template<> struct RecordDataTraits<nvfuser::serde::Cat> {
  static const RecordData enum_value = RecordData::Cat;
};

template<> struct RecordDataTraits<nvfuser::serde::Dimension> {
  static const RecordData enum_value = RecordData::Dimension;
};

template<> struct RecordDataTraits<nvfuser::serde::Dtype> {
  static const RecordData enum_value = RecordData::Dtype;
};

template<> struct RecordDataTraits<nvfuser::serde::Norm> {
  static const RecordData enum_value = RecordData::Norm;
};

template<> struct RecordDataTraits<nvfuser::serde::Output> {
  static const RecordData enum_value = RecordData::Output;
};

template<> struct RecordDataTraits<nvfuser::serde::Dims> {
  static const RecordData enum_value = RecordData::Dims;
};

template<> struct RecordDataTraits<nvfuser::serde::Slice> {
  static const RecordData enum_value = RecordData::Slice;
};

template<> struct RecordDataTraits<nvfuser::serde::Squeeze> {
  static const RecordData enum_value = RecordData::Squeeze;
};

template<> struct RecordDataTraits<nvfuser::serde::Reduction> {
  static const RecordData enum_value = RecordData::Reduction;
};

template<> struct RecordDataTraits<nvfuser::serde::Scalar> {
  static const RecordData enum_value = RecordData::Scalar;
};

template<> struct RecordDataTraits<nvfuser::serde::Size> {
  static const RecordData enum_value = RecordData::Size;
};

template<> struct RecordDataTraits<nvfuser::serde::Tensor> {
  static const RecordData enum_value = RecordData::Tensor;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreationSymbolic> {
  static const RecordData enum_value = RecordData::TensorCreationSymbolic;
};

template<> struct RecordDataTraits<nvfuser::serde::Vector> {
  static const RecordData enum_value = RecordData::Vector;
};

template<> struct RecordDataTraits<nvfuser::serde::Welford> {
  static const RecordData enum_value = RecordData::Welford;
};

template<> struct RecordDataTraits<nvfuser::serde::Sort> {
  static const RecordData enum_value = RecordData::Sort;
};

template<> struct RecordDataTraits<nvfuser::serde::TopK> {
  static const RecordData enum_value = RecordData::TopK;
};

template<> struct RecordDataTraits<nvfuser::serde::ScaledOp> {
  static const RecordData enum_value = RecordData::ScaledOp;
};

template<> struct RecordDataTraits<nvfuser::serde::ScanOp> {
  static const RecordData enum_value = RecordData::ScanOp;
};

bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type);
bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<RecordData> *types);

enum class PolymorphicValueData : uint8_t {
  NONE = 0,
  Scalar = 1,
  ScalarCpu = 2,
  TensorArg = 3,
  MIN = NONE,
  MAX = TensorArg
};

inline const PolymorphicValueData (&EnumValuesPolymorphicValueData())[4] {
  static const PolymorphicValueData values[] = {
    PolymorphicValueData::NONE,
    PolymorphicValueData::Scalar,
    PolymorphicValueData::ScalarCpu,
    PolymorphicValueData::TensorArg
  };
  return values;
}

inline const char * const *EnumNamesPolymorphicValueData() {
  static const char * const names[5] = {
    "NONE",
    "Scalar",
    "ScalarCpu",
    "TensorArg",
    nullptr
  };
  return names;
}

inline const char *EnumNamePolymorphicValueData(PolymorphicValueData e) {
  if (::flatbuffers::IsOutRange(e, PolymorphicValueData::NONE, PolymorphicValueData::TensorArg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesPolymorphicValueData()[index];
}

template<typename T> struct PolymorphicValueDataTraits {
  static const PolymorphicValueData enum_value = PolymorphicValueData::NONE;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::Scalar> {
  static const PolymorphicValueData enum_value = PolymorphicValueData::Scalar;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::ScalarCpu> {
  static const PolymorphicValueData enum_value = PolymorphicValueData::ScalarCpu;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::TensorArg> {
  static const PolymorphicValueData enum_value = PolymorphicValueData::TensorArg;
};

bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type);
bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<PolymorphicValueData> *types);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) State FLATBUFFERS_FINAL_CLASS {
 private:
  int32_t index_;
  int32_t type_;

 public:
  State()
      : index_(0),
        type_(0) {
  }
  State(int32_t _index, nvfuser::serde::StateType _type)
      : index_(::flatbuffers::EndianScalar(_index)),
        type_(::flatbuffers::EndianScalar(static_cast<int32_t>(_type))) {
  }
  int32_t index() const {
    return ::flatbuffers::EndianScalar(index_);
  }
  nvfuser::serde::StateType type() const {
    return static_cast<nvfuser::serde::StateType>(::flatbuffers::EndianScalar(type_));
  }
};
FLATBUFFERS_STRUCT_END(State, 8);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(8) EncodingEntry FLATBUFFERS_FINAL_CLASS {
 private:
  uint64_t id_;
  uint64_t lru_iter_;

 public:
  EncodingEntry()
      : id_(0),
        lru_iter_(0) {
  }
  EncodingEntry(uint64_t _id, uint64_t _lru_iter)
      : id_(::flatbuffers::EndianScalar(_id)),
        lru_iter_(::flatbuffers::EndianScalar(_lru_iter)) {
  }
  uint64_t id() const {
    return ::flatbuffers::EndianScalar(id_);
  }
  uint64_t lru_iter() const {
    return ::flatbuffers::EndianScalar(lru_iter_);
  }
};
FLATBUFFERS_STRUCT_END(EncodingEntry, 16);

struct Scalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_HAS_VALUE = 6,
    VT_VALUE_TYPE = 8,
    VT_BOOL_VALUE = 10,
    VT_LONG_VALUE = 12,
    VT_DOUBLE_VALUE = 14,
    VT_REAL_VALUE = 16,
    VT_IMAG_VALUE = 18
  };
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool has_value() const {
    return GetField<uint8_t>(VT_HAS_VALUE, 0) != 0;
  }
  int64_t value_type() const {
    return GetField<int64_t>(VT_VALUE_TYPE, 0);
  }
  bool bool_value() const {
    return GetField<uint8_t>(VT_BOOL_VALUE, 0) != 0;
  }
  int64_t long_value() const {
    return GetField<int64_t>(VT_LONG_VALUE, 0);
  }
  double double_value() const {
    return GetField<double>(VT_DOUBLE_VALUE, 0.0);
  }
  double real_value() const {
    return GetField<double>(VT_REAL_VALUE, 0.0);
  }
  double imag_value() const {
    return GetField<double>(VT_IMAG_VALUE, 0.0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           VerifyField<uint8_t>(verifier, VT_HAS_VALUE, 1) &&
           VerifyField<int64_t>(verifier, VT_VALUE_TYPE, 8) &&
           VerifyField<uint8_t>(verifier, VT_BOOL_VALUE, 1) &&
           VerifyField<int64_t>(verifier, VT_LONG_VALUE, 8) &&
           VerifyField<double>(verifier, VT_DOUBLE_VALUE, 8) &&
           VerifyField<double>(verifier, VT_REAL_VALUE, 8) &&
           VerifyField<double>(verifier, VT_IMAG_VALUE, 8) &&
           verifier.EndTable();
  }
};

struct ScalarBuilder {
  typedef Scalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(Scalar::VT_DTYPE, dtype, 0);
  }
  void add_has_value(bool has_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_HAS_VALUE, static_cast<uint8_t>(has_value), 0);
  }
  void add_value_type(int64_t value_type) {
    fbb_.AddElement<int64_t>(Scalar::VT_VALUE_TYPE, value_type, 0);
  }
  void add_bool_value(bool bool_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_BOOL_VALUE, static_cast<uint8_t>(bool_value), 0);
  }
  void add_long_value(int64_t long_value) {
    fbb_.AddElement<int64_t>(Scalar::VT_LONG_VALUE, long_value, 0);
  }
  void add_double_value(double double_value) {
    fbb_.AddElement<double>(Scalar::VT_DOUBLE_VALUE, double_value, 0.0);
  }
  void add_real_value(double real_value) {
    fbb_.AddElement<double>(Scalar::VT_REAL_VALUE, real_value, 0.0);
  }
  void add_imag_value(double imag_value) {
    fbb_.AddElement<double>(Scalar::VT_IMAG_VALUE, imag_value, 0.0);
  }
  explicit ScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Scalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Scalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Scalar> CreateScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dtype = 0,
    bool has_value = false,
    int64_t value_type = 0,
    bool bool_value = false,
    int64_t long_value = 0,
    double double_value = 0.0,
    double real_value = 0.0,
    double imag_value = 0.0) {
  ScalarBuilder builder_(_fbb);
  builder_.add_imag_value(imag_value);
  builder_.add_real_value(real_value);
  builder_.add_double_value(double_value);
  builder_.add_long_value(long_value);
  builder_.add_value_type(value_type);
  builder_.add_dtype(dtype);
  builder_.add_bool_value(bool_value);
  builder_.add_has_value(has_value);
  return builder_.Finish();
}

struct ScalarCpu FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarCpuBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SCALAR_VALUE = 4
  };
  const nvfuser::serde::Scalar *scalar_value() const {
    return GetPointer<const nvfuser::serde::Scalar *>(VT_SCALAR_VALUE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SCALAR_VALUE) &&
           verifier.VerifyTable(scalar_value()) &&
           verifier.EndTable();
  }
};

struct ScalarCpuBuilder {
  typedef ScalarCpu Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_scalar_value(::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value) {
    fbb_.AddOffset(ScalarCpu::VT_SCALAR_VALUE, scalar_value);
  }
  explicit ScalarCpuBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScalarCpu> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScalarCpu>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value = 0) {
  ScalarCpuBuilder builder_(_fbb);
  builder_.add_scalar_value(scalar_value);
  return builder_.Finish();
}

struct TensorArg FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorArgBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PTR = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10
  };
  uint64_t ptr() const {
    return GetField<uint64_t>(VT_PTR, 0);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_PTR, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           verifier.EndTable();
  }
};

struct TensorArgBuilder {
  typedef TensorArg Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_ptr(uint64_t ptr) {
    fbb_.AddElement<uint64_t>(TensorArg::VT_PTR, ptr, 0);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(TensorArg::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(TensorArg::VT_STRIDES, strides);
  }
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(TensorArg::VT_DTYPE, dtype, 0);
  }
  explicit TensorArgBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorArg> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorArg>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    int64_t dtype = 0) {
  TensorArgBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_ptr(ptr);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArgDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    int64_t dtype = 0) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      ptr,
      sizes__,
      strides__,
      dtype);
}

struct PolymorphicValue FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PolymorphicValueBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::PolymorphicValueData data_type() const {
    return static_cast<nvfuser::serde::PolymorphicValueData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::PolymorphicValueData::Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::ScalarCpu *data_as_ScalarCpu() const {
    return data_type() == nvfuser::serde::PolymorphicValueData::ScalarCpu ? static_cast<const nvfuser::serde::ScalarCpu *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorArg *data_as_TensorArg() const {
    return data_type() == nvfuser::serde::PolymorphicValueData::TensorArg ? static_cast<const nvfuser::serde::TensorArg *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyPolymorphicValueData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::Scalar *PolymorphicValue::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::ScalarCpu *PolymorphicValue::data_as<nvfuser::serde::ScalarCpu>() const {
  return data_as_ScalarCpu();
}

template<> inline const nvfuser::serde::TensorArg *PolymorphicValue::data_as<nvfuser::serde::TensorArg>() const {
  return data_as_TensorArg();
}

struct PolymorphicValueBuilder {
  typedef PolymorphicValue Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::PolymorphicValueData data_type) {
    fbb_.AddElement<uint8_t>(PolymorphicValue::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(PolymorphicValue::VT_DATA, data);
  }
  explicit PolymorphicValueBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<PolymorphicValue> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<PolymorphicValue>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::PolymorphicValueData data_type = nvfuser::serde::PolymorphicValueData::NONE,
    ::flatbuffers::Offset<void> data = 0) {
  PolymorphicValueBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

struct KernelArgumentHolder FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelArgumentHolderBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGUMENTS = 4,
    VT_DEVICE_INDEX = 6,
    VT_CACHE_ID = 8
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *>(VT_ARGUMENTS);
  }
  int8_t device_index() const {
    return GetField<int8_t>(VT_DEVICE_INDEX, 0);
  }
  uint64_t cache_id() const {
    return GetField<uint64_t>(VT_CACHE_ID, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGUMENTS) &&
           verifier.VerifyVector(arguments()) &&
           verifier.VerifyVectorOfTables(arguments()) &&
           VerifyField<int8_t>(verifier, VT_DEVICE_INDEX, 1) &&
           VerifyField<uint64_t>(verifier, VT_CACHE_ID, 8) &&
           verifier.EndTable();
  }
};

struct KernelArgumentHolderBuilder {
  typedef KernelArgumentHolder Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_arguments(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments) {
    fbb_.AddOffset(KernelArgumentHolder::VT_ARGUMENTS, arguments);
  }
  void add_device_index(int8_t device_index) {
    fbb_.AddElement<int8_t>(KernelArgumentHolder::VT_DEVICE_INDEX, device_index, 0);
  }
  void add_cache_id(uint64_t cache_id) {
    fbb_.AddElement<uint64_t>(KernelArgumentHolder::VT_CACHE_ID, cache_id, 0);
  }
  explicit KernelArgumentHolderBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelArgumentHolder> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelArgumentHolder>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments = 0,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  KernelArgumentHolderBuilder builder_(_fbb);
  builder_.add_cache_id(cache_id);
  builder_.add_arguments(arguments);
  builder_.add_device_index(device_index);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolderDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments = nullptr,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  auto arguments__ = arguments ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>(*arguments) : 0;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      arguments__,
      device_index,
      cache_id);
}

struct TensorShape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorShapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           verifier.EndTable();
  }
};

struct TensorShapeBuilder {
  typedef TensorShape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorShape::VT_SHAPE, shape);
  }
  explicit TensorShapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorShape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorShape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0) {
  TensorShapeBuilder builder_(_fbb);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      shape__);
}

struct LaunchParams FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef LaunchParamsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_GDIMX = 4,
    VT_GDIMY = 6,
    VT_GDIMZ = 8,
    VT_BDIMX = 10,
    VT_BDIMY = 12,
    VT_BDIMZ = 14,
    VT_SMEM = 16,
    VT_OUTPUT_SIZES = 18
  };
  int64_t gdimx() const {
    return GetField<int64_t>(VT_GDIMX, 0);
  }
  int64_t gdimy() const {
    return GetField<int64_t>(VT_GDIMY, 0);
  }
  int64_t gdimz() const {
    return GetField<int64_t>(VT_GDIMZ, 0);
  }
  int64_t bdimx() const {
    return GetField<int64_t>(VT_BDIMX, 0);
  }
  int64_t bdimy() const {
    return GetField<int64_t>(VT_BDIMY, 0);
  }
  int64_t bdimz() const {
    return GetField<int64_t>(VT_BDIMZ, 0);
  }
  int64_t smem() const {
    return GetField<int64_t>(VT_SMEM, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *>(VT_OUTPUT_SIZES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_GDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_SMEM, 8) &&
           VerifyOffset(verifier, VT_OUTPUT_SIZES) &&
           verifier.VerifyVector(output_sizes()) &&
           verifier.VerifyVectorOfTables(output_sizes()) &&
           verifier.EndTable();
  }
};

struct LaunchParamsBuilder {
  typedef LaunchParams Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_gdimx(int64_t gdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMX, gdimx, 0);
  }
  void add_gdimy(int64_t gdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMY, gdimy, 0);
  }
  void add_gdimz(int64_t gdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMZ, gdimz, 0);
  }
  void add_bdimx(int64_t bdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMX, bdimx, 0);
  }
  void add_bdimy(int64_t bdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMY, bdimy, 0);
  }
  void add_bdimz(int64_t bdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMZ, bdimz, 0);
  }
  void add_smem(int64_t smem) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_SMEM, smem, 0);
  }
  void add_output_sizes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes) {
    fbb_.AddOffset(LaunchParams::VT_OUTPUT_SIZES, output_sizes);
  }
  explicit LaunchParamsBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<LaunchParams> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<LaunchParams>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes = 0) {
  LaunchParamsBuilder builder_(_fbb);
  builder_.add_smem(smem);
  builder_.add_bdimz(bdimz);
  builder_.add_bdimy(bdimy);
  builder_.add_bdimx(bdimx);
  builder_.add_gdimz(gdimz);
  builder_.add_gdimy(gdimy);
  builder_.add_gdimx(gdimx);
  builder_.add_output_sizes(output_sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParamsDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes = nullptr) {
  auto output_sizes__ = output_sizes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>(*output_sizes) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      gdimx,
      gdimy,
      gdimz,
      bdimx,
      bdimy,
      bdimz,
      smem,
      output_sizes__);
}

struct GlobalBufferInfo FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GlobalBufferInfoBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV_POS = 4,
    VT_LOGICAL_SIZES = 6,
    VT_LOGICAL_STRIDES = 8,
    VT_UNSHARDED_LOGICAL_SIZES = 10,
    VT_ALLOC_SIZES = 12,
    VT_ALLOC_STRIDES = 14,
    VT_DTYPE = 16,
    VT_ZERO_INIT = 18,
    VT_RESETS_TO_ZERO = 20,
    VT_IS_PROFILE_BUFFER = 22,
    VT_IS_FUSION_OUTPUT = 24,
    VT_IS_FUSION_INPUT = 26
  };
  int64_t tv_pos() const {
    return GetField<int64_t>(VT_TV_POS, -1LL);
  }
  const ::flatbuffers::Vector<int64_t> *logical_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_LOGICAL_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *logical_strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_LOGICAL_STRIDES);
  }
  const ::flatbuffers::Vector<int64_t> *unsharded_logical_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_UNSHARDED_LOGICAL_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *alloc_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ALLOC_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *alloc_strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ALLOC_STRIDES);
  }
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool resets_to_zero() const {
    return GetField<uint8_t>(VT_RESETS_TO_ZERO, 0) != 0;
  }
  bool is_profile_buffer() const {
    return GetField<uint8_t>(VT_IS_PROFILE_BUFFER, 0) != 0;
  }
  bool is_fusion_output() const {
    return GetField<uint8_t>(VT_IS_FUSION_OUTPUT, 0) != 0;
  }
  bool is_fusion_input() const {
    return GetField<uint8_t>(VT_IS_FUSION_INPUT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_TV_POS, 8) &&
           VerifyOffset(verifier, VT_LOGICAL_SIZES) &&
           verifier.VerifyVector(logical_sizes()) &&
           VerifyOffset(verifier, VT_LOGICAL_STRIDES) &&
           verifier.VerifyVector(logical_strides()) &&
           VerifyOffset(verifier, VT_UNSHARDED_LOGICAL_SIZES) &&
           verifier.VerifyVector(unsharded_logical_sizes()) &&
           VerifyOffset(verifier, VT_ALLOC_SIZES) &&
           verifier.VerifyVector(alloc_sizes()) &&
           VerifyOffset(verifier, VT_ALLOC_STRIDES) &&
           verifier.VerifyVector(alloc_strides()) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_RESETS_TO_ZERO, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_PROFILE_BUFFER, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_FUSION_OUTPUT, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_FUSION_INPUT, 1) &&
           verifier.EndTable();
  }
};

struct GlobalBufferInfoBuilder {
  typedef GlobalBufferInfo Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv_pos(int64_t tv_pos) {
    fbb_.AddElement<int64_t>(GlobalBufferInfo::VT_TV_POS, tv_pos, -1LL);
  }
  void add_logical_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> logical_sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_LOGICAL_SIZES, logical_sizes);
  }
  void add_logical_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> logical_strides) {
    fbb_.AddOffset(GlobalBufferInfo::VT_LOGICAL_STRIDES, logical_strides);
  }
  void add_unsharded_logical_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> unsharded_logical_sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_UNSHARDED_LOGICAL_SIZES, unsharded_logical_sizes);
  }
  void add_alloc_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> alloc_sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_ALLOC_SIZES, alloc_sizes);
  }
  void add_alloc_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> alloc_strides) {
    fbb_.AddOffset(GlobalBufferInfo::VT_ALLOC_STRIDES, alloc_strides);
  }
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(GlobalBufferInfo::VT_DTYPE, dtype, 0);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  void add_resets_to_zero(bool resets_to_zero) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_RESETS_TO_ZERO, static_cast<uint8_t>(resets_to_zero), 0);
  }
  void add_is_profile_buffer(bool is_profile_buffer) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_PROFILE_BUFFER, static_cast<uint8_t>(is_profile_buffer), 0);
  }
  void add_is_fusion_output(bool is_fusion_output) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_FUSION_OUTPUT, static_cast<uint8_t>(is_fusion_output), 0);
  }
  void add_is_fusion_input(bool is_fusion_input) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_FUSION_INPUT, static_cast<uint8_t>(is_fusion_input), 0);
  }
  explicit GlobalBufferInfoBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GlobalBufferInfo> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GlobalBufferInfo>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv_pos = -1LL,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> logical_sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> logical_strides = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> unsharded_logical_sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> alloc_sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> alloc_strides = 0,
    int64_t dtype = 0,
    bool zero_init = false,
    bool resets_to_zero = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false,
    bool is_fusion_input = false) {
  GlobalBufferInfoBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_tv_pos(tv_pos);
  builder_.add_alloc_strides(alloc_strides);
  builder_.add_alloc_sizes(alloc_sizes);
  builder_.add_unsharded_logical_sizes(unsharded_logical_sizes);
  builder_.add_logical_strides(logical_strides);
  builder_.add_logical_sizes(logical_sizes);
  builder_.add_is_fusion_input(is_fusion_input);
  builder_.add_is_fusion_output(is_fusion_output);
  builder_.add_is_profile_buffer(is_profile_buffer);
  builder_.add_resets_to_zero(resets_to_zero);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfoDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv_pos = -1LL,
    const std::vector<int64_t> *logical_sizes = nullptr,
    const std::vector<int64_t> *logical_strides = nullptr,
    const std::vector<int64_t> *unsharded_logical_sizes = nullptr,
    const std::vector<int64_t> *alloc_sizes = nullptr,
    const std::vector<int64_t> *alloc_strides = nullptr,
    int64_t dtype = 0,
    bool zero_init = false,
    bool resets_to_zero = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false,
    bool is_fusion_input = false) {
  auto logical_sizes__ = logical_sizes ? _fbb.CreateVector<int64_t>(*logical_sizes) : 0;
  auto logical_strides__ = logical_strides ? _fbb.CreateVector<int64_t>(*logical_strides) : 0;
  auto unsharded_logical_sizes__ = unsharded_logical_sizes ? _fbb.CreateVector<int64_t>(*unsharded_logical_sizes) : 0;
  auto alloc_sizes__ = alloc_sizes ? _fbb.CreateVector<int64_t>(*alloc_sizes) : 0;
  auto alloc_strides__ = alloc_strides ? _fbb.CreateVector<int64_t>(*alloc_strides) : 0;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      tv_pos,
      logical_sizes__,
      logical_strides__,
      unsharded_logical_sizes__,
      alloc_sizes__,
      alloc_strides__,
      dtype,
      zero_init,
      resets_to_zero,
      is_profile_buffer,
      is_fusion_output,
      is_fusion_input);
}

struct KernelExecutorEntry FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelExecutorEntryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INIT = 4,
    VT_LAUNCH_PARAMS = 6,
    VT_OUTPUTS = 8,
    VT_INTERMEDIATES = 10,
    VT_INPUTS = 12,
    VT_OUTPUT_ALIASED_TO_INPUT = 14
  };
  bool init() const {
    return GetField<uint8_t>(VT_INIT, 0) != 0;
  }
  const nvfuser::serde::LaunchParams *launch_params() const {
    return GetPointer<const nvfuser::serde::LaunchParams *>(VT_LAUNCH_PARAMS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_INTERMEDIATES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *inputs() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_INPUTS);
  }
  const ::flatbuffers::Vector<int32_t> *output_aliased_to_input() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_OUTPUT_ALIASED_TO_INPUT);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_INIT, 1) &&
           VerifyOffset(verifier, VT_LAUNCH_PARAMS) &&
           verifier.VerifyTable(launch_params()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           verifier.VerifyVectorOfTables(outputs()) &&
           VerifyOffset(verifier, VT_INTERMEDIATES) &&
           verifier.VerifyVector(intermediates()) &&
           verifier.VerifyVectorOfTables(intermediates()) &&
           VerifyOffset(verifier, VT_INPUTS) &&
           verifier.VerifyVector(inputs()) &&
           verifier.VerifyVectorOfTables(inputs()) &&
           VerifyOffset(verifier, VT_OUTPUT_ALIASED_TO_INPUT) &&
           verifier.VerifyVector(output_aliased_to_input()) &&
           verifier.EndTable();
  }
};

struct KernelExecutorEntryBuilder {
  typedef KernelExecutorEntry Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_init(bool init) {
    fbb_.AddElement<uint8_t>(KernelExecutorEntry::VT_INIT, static_cast<uint8_t>(init), 0);
  }
  void add_launch_params(::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params) {
    fbb_.AddOffset(KernelExecutorEntry::VT_LAUNCH_PARAMS, launch_params);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs) {
    fbb_.AddOffset(KernelExecutorEntry::VT_OUTPUTS, outputs);
  }
  void add_intermediates(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates) {
    fbb_.AddOffset(KernelExecutorEntry::VT_INTERMEDIATES, intermediates);
  }
  void add_inputs(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> inputs) {
    fbb_.AddOffset(KernelExecutorEntry::VT_INPUTS, inputs);
  }
  void add_output_aliased_to_input(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliased_to_input) {
    fbb_.AddOffset(KernelExecutorEntry::VT_OUTPUT_ALIASED_TO_INPUT, output_aliased_to_input);
  }
  explicit KernelExecutorEntryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelExecutorEntry> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelExecutorEntry>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelExecutorEntry> CreateKernelExecutorEntry(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> inputs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliased_to_input = 0) {
  KernelExecutorEntryBuilder builder_(_fbb);
  builder_.add_output_aliased_to_input(output_aliased_to_input);
  builder_.add_inputs(inputs);
  builder_.add_intermediates(intermediates);
  builder_.add_outputs(outputs);
  builder_.add_launch_params(launch_params);
  builder_.add_init(init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelExecutorEntry> CreateKernelExecutorEntryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *inputs = nullptr,
    const std::vector<int32_t> *output_aliased_to_input = nullptr) {
  auto outputs__ = outputs ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*outputs) : 0;
  auto intermediates__ = intermediates ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*intermediates) : 0;
  auto inputs__ = inputs ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*inputs) : 0;
  auto output_aliased_to_input__ = output_aliased_to_input ? _fbb.CreateVector<int32_t>(*output_aliased_to_input) : 0;
  return nvfuser::serde::CreateKernelExecutorEntry(
      _fbb,
      init,
      launch_params,
      outputs__,
      intermediates__,
      inputs__,
      output_aliased_to_input__);
}

struct At FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AtBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INDEX = 4
  };
  int64_t index() const {
    return GetField<int64_t>(VT_INDEX, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INDEX, 8) &&
           verifier.EndTable();
  }
};

struct AtBuilder {
  typedef At Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_index(int64_t index) {
    fbb_.AddElement<int64_t>(At::VT_INDEX, index, 0);
  }
  explicit AtBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<At> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<At>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<At> CreateAt(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t index = 0) {
  AtBuilder builder_(_fbb);
  builder_.add_index(index);
  return builder_.Finish();
}

struct BatchNorm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BatchNormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TRAINING = 4,
    VT_CHANNELS_LAST = 6
  };
  bool training() const {
    return GetField<uint8_t>(VT_TRAINING, 0) != 0;
  }
  bool channels_last() const {
    return GetField<uint8_t>(VT_CHANNELS_LAST, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_TRAINING, 1) &&
           VerifyField<uint8_t>(verifier, VT_CHANNELS_LAST, 1) &&
           verifier.EndTable();
  }
};

struct BatchNormBuilder {
  typedef BatchNorm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_training(bool training) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_TRAINING, static_cast<uint8_t>(training), 0);
  }
  void add_channels_last(bool channels_last) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_CHANNELS_LAST, static_cast<uint8_t>(channels_last), 0);
  }
  explicit BatchNormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BatchNorm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BatchNorm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool training = false,
    bool channels_last = false) {
  BatchNormBuilder builder_(_fbb);
  builder_.add_channels_last(channels_last);
  builder_.add_training(training);
  return builder_.Finish();
}

struct Broadcast FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BROADCAST_DIMS = 4
  };
  const ::flatbuffers::Vector<uint8_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastBuilder {
  typedef Broadcast Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims) {
    fbb_.AddOffset(Broadcast::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Broadcast> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Broadcast>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims = 0) {
  BroadcastBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcastDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint8_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<uint8_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      broadcast_dims__);
}

struct BroadcastInDim FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastInDimBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_SIZE = 4,
    VT_BROADCAST_DIMS = 6
  };
  uint64_t output_size() const {
    return GetField<uint64_t>(VT_OUTPUT_SIZE, 0);
  }
  const ::flatbuffers::Vector<int64_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_OUTPUT_SIZE, 8) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastInDimBuilder {
  typedef BroadcastInDim Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_output_size(uint64_t output_size) {
    fbb_.AddElement<uint64_t>(BroadcastInDim::VT_OUTPUT_SIZE, output_size, 0);
  }
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims) {
    fbb_.AddOffset(BroadcastInDim::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastInDimBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BroadcastInDim> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BroadcastInDim>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims = 0) {
  BroadcastInDimBuilder builder_(_fbb);
  builder_.add_output_size(output_size);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDimDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    const std::vector<int64_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<int64_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      output_size,
      broadcast_dims__);
}

struct Cat FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef CatBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4,
    VT_MANUAL_PADDING = 6
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool manual_padding() const {
    return GetField<uint8_t>(VT_MANUAL_PADDING, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           VerifyField<uint8_t>(verifier, VT_MANUAL_PADDING, 1) &&
           verifier.EndTable();
  }
};

struct CatBuilder {
  typedef Cat Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Cat::VT_DIM, dim, 0);
  }
  void add_manual_padding(bool manual_padding) {
    fbb_.AddElement<uint8_t>(Cat::VT_MANUAL_PADDING, static_cast<uint8_t>(manual_padding), 0);
  }
  explicit CatBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Cat> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Cat>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Cat> CreateCat(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0,
    bool manual_padding = false) {
  CatBuilder builder_(_fbb);
  builder_.add_dim(dim);
  builder_.add_manual_padding(manual_padding);
  return builder_.Finish();
}

struct Dtype FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DtypeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           verifier.EndTable();
  }
};

struct DtypeBuilder {
  typedef Dtype Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(Dtype::VT_DTYPE, dtype, 0);
  }
  explicit DtypeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dtype> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dtype>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dtype> CreateDtype(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dtype = 0) {
  DtypeBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct Dimension FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DimensionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct DimensionBuilder {
  typedef Dimension Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Dimension::VT_DIM, dim, 0);
  }
  explicit DimensionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dimension> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dimension>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dimension> CreateDimension(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  DimensionBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct Norm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_CORRECTION = 6,
    VT_KEEP_DIM = 8
  };
  const ::flatbuffers::Vector<int64_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_AXES);
  }
  int64_t correction() const {
    return GetField<int64_t>(VT_CORRECTION, 0);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<int64_t>(verifier, VT_CORRECTION, 8) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           verifier.EndTable();
  }
};

struct NormBuilder {
  typedef Norm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> axes) {
    fbb_.AddOffset(Norm::VT_AXES, axes);
  }
  void add_correction(int64_t correction) {
    fbb_.AddElement<int64_t>(Norm::VT_CORRECTION, correction, 0);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Norm::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  explicit NormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Norm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Norm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Norm> CreateNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> axes = 0,
    int64_t correction = 0,
    bool keep_dim = false) {
  NormBuilder builder_(_fbb);
  builder_.add_correction(correction);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Norm> CreateNormDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *axes = nullptr,
    int64_t correction = 0,
    bool keep_dim = false) {
  auto axes__ = axes ? _fbb.CreateVector<int64_t>(*axes) : 0;
  return nvfuser::serde::CreateNorm(
      _fbb,
      axes__,
      correction,
      keep_dim);
}

struct Output FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef OutputBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_STRIDE_ORDER = 4
  };
  const ::flatbuffers::Vector<int64_t> *stride_order() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDE_ORDER);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_STRIDE_ORDER) &&
           verifier.VerifyVector(stride_order()) &&
           verifier.EndTable();
  }
};

struct OutputBuilder {
  typedef Output Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_stride_order(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order) {
    fbb_.AddOffset(Output::VT_STRIDE_ORDER, stride_order);
  }
  explicit OutputBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Output> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Output>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Output> CreateOutput(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order = 0) {
  OutputBuilder builder_(_fbb);
  builder_.add_stride_order(stride_order);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Output> CreateOutputDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *stride_order = nullptr) {
  auto stride_order__ = stride_order ? _fbb.CreateVector<int64_t>(*stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      stride_order__);
}

struct Dims FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DimsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<int64_t> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.EndTable();
  }
};

struct DimsBuilder {
  typedef Dims Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims) {
    fbb_.AddOffset(Dims::VT_DIMS, dims);
  }
  explicit DimsBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dims> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dims>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dims> CreateDims(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims = 0) {
  DimsBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Dims> CreateDimsDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<int64_t>(*dims) : 0;
  return nvfuser::serde::CreateDims(
      _fbb,
      dims__);
}

struct Reduction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReductionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_KEEP_DIM = 6,
    VT_DTYPE = 8
  };
  const ::flatbuffers::Vector<int64_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_AXES);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           verifier.EndTable();
  }
};

struct ReductionBuilder {
  typedef Reduction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> axes) {
    fbb_.AddOffset(Reduction::VT_AXES, axes);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Reduction::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(Reduction::VT_DTYPE, dtype, 0);
  }
  explicit ReductionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reduction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reduction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reduction> CreateReduction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> axes = 0,
    bool keep_dim = false,
    int64_t dtype = 0) {
  ReductionBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reduction> CreateReductionDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *axes = nullptr,
    bool keep_dim = false,
    int64_t dtype = 0) {
  auto axes__ = axes ? _fbb.CreateVector<int64_t>(*axes) : 0;
  return nvfuser::serde::CreateReduction(
      _fbb,
      axes__,
      keep_dim,
      dtype);
}

struct Size FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct SizeBuilder {
  typedef Size Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Size::VT_DIM, dim, 0);
  }
  explicit SizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Size> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Size>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Size> CreateSize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  SizeBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct Slice FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SliceBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MANUAL_NORMALIZATION = 4
  };
  bool manual_normalization() const {
    return GetField<uint8_t>(VT_MANUAL_NORMALIZATION, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_MANUAL_NORMALIZATION, 1) &&
           verifier.EndTable();
  }
};

struct SliceBuilder {
  typedef Slice Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_manual_normalization(bool manual_normalization) {
    fbb_.AddElement<uint8_t>(Slice::VT_MANUAL_NORMALIZATION, static_cast<uint8_t>(manual_normalization), 0);
  }
  explicit SliceBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Slice> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Slice>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Slice> CreateSlice(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool manual_normalization = false) {
  SliceBuilder builder_(_fbb);
  builder_.add_manual_normalization(manual_normalization);
  return builder_.Finish();
}

struct Squeeze FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SqueezeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SQUEEZE_DIMS = 4,
    VT_SQUEEZE_EXPANDED = 6
  };
  const ::flatbuffers::Vector<int64_t> *squeeze_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SQUEEZE_DIMS);
  }
  bool squeeze_expanded() const {
    return GetField<uint8_t>(VT_SQUEEZE_EXPANDED, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SQUEEZE_DIMS) &&
           verifier.VerifyVector(squeeze_dims()) &&
           VerifyField<uint8_t>(verifier, VT_SQUEEZE_EXPANDED, 1) &&
           verifier.EndTable();
  }
};

struct SqueezeBuilder {
  typedef Squeeze Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_squeeze_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims) {
    fbb_.AddOffset(Squeeze::VT_SQUEEZE_DIMS, squeeze_dims);
  }
  void add_squeeze_expanded(bool squeeze_expanded) {
    fbb_.AddElement<uint8_t>(Squeeze::VT_SQUEEZE_EXPANDED, static_cast<uint8_t>(squeeze_expanded), 0);
  }
  explicit SqueezeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Squeeze> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Squeeze>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims = 0,
    bool squeeze_expanded = false) {
  SqueezeBuilder builder_(_fbb);
  builder_.add_squeeze_dims(squeeze_dims);
  builder_.add_squeeze_expanded(squeeze_expanded);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueezeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *squeeze_dims = nullptr,
    bool squeeze_expanded = false) {
  auto squeeze_dims__ = squeeze_dims ? _fbb.CreateVector<int64_t>(*squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      squeeze_dims__,
      squeeze_expanded);
}

struct Tensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SIZES = 4,
    VT_CONTIGUITY = 6,
    VT_STRIDE_ORDER = 8,
    VT_DTYPE = 10,
    VT_IS_CPU = 12
  };
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<nvfuser::serde::Contiguity> *contiguity() const {
    return GetPointer<const ::flatbuffers::Vector<nvfuser::serde::Contiguity> *>(VT_CONTIGUITY);
  }
  const ::flatbuffers::Vector<int64_t> *stride_order() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDE_ORDER);
  }
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool is_cpu() const {
    return GetField<uint8_t>(VT_IS_CPU, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_CONTIGUITY) &&
           verifier.VerifyVector(contiguity()) &&
           VerifyOffset(verifier, VT_STRIDE_ORDER) &&
           verifier.VerifyVector(stride_order()) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           VerifyField<uint8_t>(verifier, VT_IS_CPU, 1) &&
           verifier.EndTable();
  }
};

struct TensorBuilder {
  typedef Tensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(Tensor::VT_SIZES, sizes);
  }
  void add_contiguity(::flatbuffers::Offset<::flatbuffers::Vector<nvfuser::serde::Contiguity>> contiguity) {
    fbb_.AddOffset(Tensor::VT_CONTIGUITY, contiguity);
  }
  void add_stride_order(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order) {
    fbb_.AddOffset(Tensor::VT_STRIDE_ORDER, stride_order);
  }
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(Tensor::VT_DTYPE, dtype, 0);
  }
  void add_is_cpu(bool is_cpu) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_CPU, static_cast<uint8_t>(is_cpu), 0);
  }
  explicit TensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Tensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Tensor> CreateTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<nvfuser::serde::Contiguity>> contiguity = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order = 0,
    int64_t dtype = 0,
    bool is_cpu = false) {
  TensorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_stride_order(stride_order);
  builder_.add_contiguity(contiguity);
  builder_.add_sizes(sizes);
  builder_.add_is_cpu(is_cpu);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Tensor> CreateTensorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<nvfuser::serde::Contiguity> *contiguity = nullptr,
    const std::vector<int64_t> *stride_order = nullptr,
    int64_t dtype = 0,
    bool is_cpu = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto contiguity__ = contiguity ? _fbb.CreateVector<nvfuser::serde::Contiguity>(*contiguity) : 0;
  auto stride_order__ = stride_order ? _fbb.CreateVector<int64_t>(*stride_order) : 0;
  return nvfuser::serde::CreateTensor(
      _fbb,
      sizes__,
      contiguity__,
      stride_order__,
      dtype,
      is_cpu);
}

struct TensorCreationSymbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationSymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           verifier.EndTable();
  }
};

struct TensorCreationSymbolicBuilder {
  typedef TensorCreationSymbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(TensorCreationSymbolic::VT_DTYPE, dtype, 0);
  }
  explicit TensorCreationSymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreationSymbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreationSymbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dtype = 0) {
  TensorCreationSymbolicBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct Vector FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef VectorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           verifier.EndTable();
  }
};

struct VectorBuilder {
  typedef Vector Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(Vector::VT_DTYPE, dtype, 0);
  }
  explicit VectorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Vector> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Vector>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Vector> CreateVector(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dtype = 0) {
  VectorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct Welford FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef WelfordBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4
  };
  const ::flatbuffers::Vector<int64_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_AXES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           verifier.EndTable();
  }
};

struct WelfordBuilder {
  typedef Welford Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> axes) {
    fbb_.AddOffset(Welford::VT_AXES, axes);
  }
  explicit WelfordBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Welford> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Welford>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Welford> CreateWelford(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> axes = 0) {
  WelfordBuilder builder_(_fbb);
  builder_.add_axes(axes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Welford> CreateWelfordDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *axes = nullptr) {
  auto axes__ = axes ? _fbb.CreateVector<int64_t>(*axes) : 0;
  return nvfuser::serde::CreateWelford(
      _fbb,
      axes__);
}

struct Sort FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SortBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4,
    VT_DESCENDING = 6,
    VT_STABLE = 8
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool descending() const {
    return GetField<uint8_t>(VT_DESCENDING, 0) != 0;
  }
  bool stable() const {
    return GetField<uint8_t>(VT_STABLE, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           VerifyField<uint8_t>(verifier, VT_DESCENDING, 1) &&
           VerifyField<uint8_t>(verifier, VT_STABLE, 1) &&
           verifier.EndTable();
  }
};

struct SortBuilder {
  typedef Sort Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Sort::VT_DIM, dim, 0);
  }
  void add_descending(bool descending) {
    fbb_.AddElement<uint8_t>(Sort::VT_DESCENDING, static_cast<uint8_t>(descending), 0);
  }
  void add_stable(bool stable) {
    fbb_.AddElement<uint8_t>(Sort::VT_STABLE, static_cast<uint8_t>(stable), 0);
  }
  explicit SortBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Sort> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Sort>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Sort> CreateSort(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0,
    bool descending = false,
    bool stable = false) {
  SortBuilder builder_(_fbb);
  builder_.add_dim(dim);
  builder_.add_stable(stable);
  builder_.add_descending(descending);
  return builder_.Finish();
}

struct TopK FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TopKBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4,
    VT_LARGEST = 6,
    VT_SORTED = 8
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool largest() const {
    return GetField<uint8_t>(VT_LARGEST, 0) != 0;
  }
  bool sorted() const {
    return GetField<uint8_t>(VT_SORTED, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           VerifyField<uint8_t>(verifier, VT_LARGEST, 1) &&
           VerifyField<uint8_t>(verifier, VT_SORTED, 1) &&
           verifier.EndTable();
  }
};

struct TopKBuilder {
  typedef TopK Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(TopK::VT_DIM, dim, 0);
  }
  void add_largest(bool largest) {
    fbb_.AddElement<uint8_t>(TopK::VT_LARGEST, static_cast<uint8_t>(largest), 0);
  }
  void add_sorted(bool sorted) {
    fbb_.AddElement<uint8_t>(TopK::VT_SORTED, static_cast<uint8_t>(sorted), 0);
  }
  explicit TopKBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TopK> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TopK>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TopK> CreateTopK(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0,
    bool largest = false,
    bool sorted = false) {
  TopKBuilder builder_(_fbb);
  builder_.add_dim(dim);
  builder_.add_sorted(sorted);
  builder_.add_largest(largest);
  return builder_.Finish();
}

struct ScaledOp FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScaledOpBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_BLOCK_SCALE_SIZE = 6,
    VT_BLOCK_SCALE_DTYPE = 8,
    VT_GAMMA = 10
  };
  int64_t dtype() const {
    return GetField<int64_t>(VT_DTYPE, 0);
  }
  int64_t block_scale_size() const {
    return GetField<int64_t>(VT_BLOCK_SCALE_SIZE, 0);
  }
  int64_t block_scale_dtype() const {
    return GetField<int64_t>(VT_BLOCK_SCALE_DTYPE, 0);
  }
  bool gamma() const {
    return GetField<uint8_t>(VT_GAMMA, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DTYPE, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SCALE_SIZE, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SCALE_DTYPE, 8) &&
           VerifyField<uint8_t>(verifier, VT_GAMMA, 1) &&
           verifier.EndTable();
  }
};

struct ScaledOpBuilder {
  typedef ScaledOp Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(int64_t dtype) {
    fbb_.AddElement<int64_t>(ScaledOp::VT_DTYPE, dtype, 0);
  }
  void add_block_scale_size(int64_t block_scale_size) {
    fbb_.AddElement<int64_t>(ScaledOp::VT_BLOCK_SCALE_SIZE, block_scale_size, 0);
  }
  void add_block_scale_dtype(int64_t block_scale_dtype) {
    fbb_.AddElement<int64_t>(ScaledOp::VT_BLOCK_SCALE_DTYPE, block_scale_dtype, 0);
  }
  void add_gamma(bool gamma) {
    fbb_.AddElement<uint8_t>(ScaledOp::VT_GAMMA, static_cast<uint8_t>(gamma), 0);
  }
  explicit ScaledOpBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScaledOp> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScaledOp>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScaledOp> CreateScaledOp(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dtype = 0,
    int64_t block_scale_size = 0,
    int64_t block_scale_dtype = 0,
    bool gamma = false) {
  ScaledOpBuilder builder_(_fbb);
  builder_.add_block_scale_dtype(block_scale_dtype);
  builder_.add_block_scale_size(block_scale_size);
  builder_.add_dtype(dtype);
  builder_.add_gamma(gamma);
  return builder_.Finish();
}

struct ScanOp FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScanOpBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct ScanOpBuilder {
  typedef ScanOp Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(ScanOp::VT_DIM, dim, 0);
  }
  explicit ScanOpBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScanOp> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScanOp>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScanOp> CreateScanOp(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  ScanOpBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct CudaKernel FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef CudaKernelBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KERNEL_NAME = 4,
    VT_COMPILE_ARGS = 6,
    VT_CUBIN = 8,
    VT_CUBIN_FILENAME = 10,
    VT_PTX = 12,
    VT_PTX_FILENAME = 14,
    VT_BLOCK_SIZE = 16
  };
  const ::flatbuffers::String *kernel_name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_NAME);
  }
  const ::flatbuffers::String *compile_args() const {
    return GetPointer<const ::flatbuffers::String *>(VT_COMPILE_ARGS);
  }
  const ::flatbuffers::Vector<uint8_t> *cubin() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_CUBIN);
  }
  const ::flatbuffers::String *cubin_filename() const {
    return GetPointer<const ::flatbuffers::String *>(VT_CUBIN_FILENAME);
  }
  const ::flatbuffers::Vector<uint8_t> *ptx() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_PTX);
  }
  const ::flatbuffers::String *ptx_filename() const {
    return GetPointer<const ::flatbuffers::String *>(VT_PTX_FILENAME);
  }
  int64_t block_size() const {
    return GetField<int64_t>(VT_BLOCK_SIZE, -1LL);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_KERNEL_NAME) &&
           verifier.VerifyString(kernel_name()) &&
           VerifyOffset(verifier, VT_COMPILE_ARGS) &&
           verifier.VerifyString(compile_args()) &&
           VerifyOffset(verifier, VT_CUBIN) &&
           verifier.VerifyVector(cubin()) &&
           VerifyOffset(verifier, VT_CUBIN_FILENAME) &&
           verifier.VerifyString(cubin_filename()) &&
           VerifyOffset(verifier, VT_PTX) &&
           verifier.VerifyVector(ptx()) &&
           VerifyOffset(verifier, VT_PTX_FILENAME) &&
           verifier.VerifyString(ptx_filename()) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SIZE, 8) &&
           verifier.EndTable();
  }
};

struct CudaKernelBuilder {
  typedef CudaKernel Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_kernel_name(::flatbuffers::Offset<::flatbuffers::String> kernel_name) {
    fbb_.AddOffset(CudaKernel::VT_KERNEL_NAME, kernel_name);
  }
  void add_compile_args(::flatbuffers::Offset<::flatbuffers::String> compile_args) {
    fbb_.AddOffset(CudaKernel::VT_COMPILE_ARGS, compile_args);
  }
  void add_cubin(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> cubin) {
    fbb_.AddOffset(CudaKernel::VT_CUBIN, cubin);
  }
  void add_cubin_filename(::flatbuffers::Offset<::flatbuffers::String> cubin_filename) {
    fbb_.AddOffset(CudaKernel::VT_CUBIN_FILENAME, cubin_filename);
  }
  void add_ptx(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> ptx) {
    fbb_.AddOffset(CudaKernel::VT_PTX, ptx);
  }
  void add_ptx_filename(::flatbuffers::Offset<::flatbuffers::String> ptx_filename) {
    fbb_.AddOffset(CudaKernel::VT_PTX_FILENAME, ptx_filename);
  }
  void add_block_size(int64_t block_size) {
    fbb_.AddElement<int64_t>(CudaKernel::VT_BLOCK_SIZE, block_size, -1LL);
  }
  explicit CudaKernelBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<CudaKernel> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<CudaKernel>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<CudaKernel> CreateCudaKernel(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_name = 0,
    ::flatbuffers::Offset<::flatbuffers::String> compile_args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> cubin = 0,
    ::flatbuffers::Offset<::flatbuffers::String> cubin_filename = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> ptx = 0,
    ::flatbuffers::Offset<::flatbuffers::String> ptx_filename = 0,
    int64_t block_size = -1LL) {
  CudaKernelBuilder builder_(_fbb);
  builder_.add_block_size(block_size);
  builder_.add_ptx_filename(ptx_filename);
  builder_.add_ptx(ptx);
  builder_.add_cubin_filename(cubin_filename);
  builder_.add_cubin(cubin);
  builder_.add_compile_args(compile_args);
  builder_.add_kernel_name(kernel_name);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<CudaKernel> CreateCudaKernelDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const char *kernel_name = nullptr,
    const char *compile_args = nullptr,
    const std::vector<uint8_t> *cubin = nullptr,
    const char *cubin_filename = nullptr,
    const std::vector<uint8_t> *ptx = nullptr,
    const char *ptx_filename = nullptr,
    int64_t block_size = -1LL) {
  auto kernel_name__ = kernel_name ? _fbb.CreateString(kernel_name) : 0;
  auto compile_args__ = compile_args ? _fbb.CreateString(compile_args) : 0;
  auto cubin__ = cubin ? _fbb.CreateVector<uint8_t>(*cubin) : 0;
  auto cubin_filename__ = cubin_filename ? _fbb.CreateString(cubin_filename) : 0;
  auto ptx__ = ptx ? _fbb.CreateVector<uint8_t>(*ptx) : 0;
  auto ptx_filename__ = ptx_filename ? _fbb.CreateString(ptx_filename) : 0;
  return nvfuser::serde::CreateCudaKernel(
      _fbb,
      kernel_name__,
      compile_args__,
      cubin__,
      cubin_filename__,
      ptx__,
      ptx_filename__,
      block_size);
}

struct KernelExecutor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelExecutorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_SMEM_LIMIT = 4,
    VT_BLOCK_SIZE_HIGH_WATERMARK = 6,
    VT_MAXRREGCOUNT_HIGH_WATERMARK = 8,
    VT_WARP_SIZE = 10,
    VT_HEURISTIC = 12,
    VT_FUSION_ID = 14,
    VT_CONCRETE_ID = 16,
    VT_RUNTIME_ID = 18,
    VT_GROUP_ID = 20,
    VT_KERNEL_CODE = 22,
    VT_EXECUTOR_ENTRY_LOOKUP_KEYS = 24,
    VT_EXECUTOR_ENTRY_LOOKUP_VALUES = 26,
    VT_INDEX_TYPE = 28,
    VT_COMPILED_KERNEL = 30,
    VT_HAS_RNG = 32,
    VT_HAS_TMA = 34,
    VT_HAS_DYNAMIC_ALIAS = 36
  };
  int64_t device_smem_limit() const {
    return GetField<int64_t>(VT_DEVICE_SMEM_LIMIT, 0);
  }
  int64_t block_size_high_watermark() const {
    return GetField<int64_t>(VT_BLOCK_SIZE_HIGH_WATERMARK, 0);
  }
  int64_t maxrregcount_high_watermark() const {
    return GetField<int64_t>(VT_MAXRREGCOUNT_HIGH_WATERMARK, 0);
  }
  int64_t warp_size() const {
    return GetField<int64_t>(VT_WARP_SIZE, 0);
  }
  int64_t heuristic() const {
    return GetField<int64_t>(VT_HEURISTIC, 0);
  }
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  int64_t concrete_id() const {
    return GetField<int64_t>(VT_CONCRETE_ID, 0);
  }
  int64_t runtime_id() const {
    return GetField<int64_t>(VT_RUNTIME_ID, 0);
  }
  int64_t group_id() const {
    return GetField<int64_t>(VT_GROUP_ID, 0);
  }
  const ::flatbuffers::String *kernel_code() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_CODE);
  }
  const ::flatbuffers::Vector<uint64_t> *executor_entry_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_EXECUTOR_ENTRY_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutorEntry>> *executor_entry_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutorEntry>> *>(VT_EXECUTOR_ENTRY_LOOKUP_VALUES);
  }
  int64_t index_type() const {
    return GetField<int64_t>(VT_INDEX_TYPE, 0);
  }
  const nvfuser::serde::CudaKernel *compiled_kernel() const {
    return GetPointer<const nvfuser::serde::CudaKernel *>(VT_COMPILED_KERNEL);
  }
  bool has_rng() const {
    return GetField<uint8_t>(VT_HAS_RNG, 0) != 0;
  }
  bool has_tma() const {
    return GetField<uint8_t>(VT_HAS_TMA, 0) != 0;
  }
  bool has_dynamic_alias() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_ALIAS, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_SMEM_LIMIT, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SIZE_HIGH_WATERMARK, 8) &&
           VerifyField<int64_t>(verifier, VT_MAXRREGCOUNT_HIGH_WATERMARK, 8) &&
           VerifyField<int64_t>(verifier, VT_WARP_SIZE, 8) &&
           VerifyField<int64_t>(verifier, VT_HEURISTIC, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_CONCRETE_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_RUNTIME_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_GROUP_ID, 8) &&
           VerifyOffset(verifier, VT_KERNEL_CODE) &&
           verifier.VerifyString(kernel_code()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_KEYS) &&
           verifier.VerifyVector(executor_entry_lookup_keys()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_VALUES) &&
           verifier.VerifyVector(executor_entry_lookup_values()) &&
           verifier.VerifyVectorOfTables(executor_entry_lookup_values()) &&
           VerifyField<int64_t>(verifier, VT_INDEX_TYPE, 8) &&
           VerifyOffset(verifier, VT_COMPILED_KERNEL) &&
           verifier.VerifyTable(compiled_kernel()) &&
           VerifyField<uint8_t>(verifier, VT_HAS_RNG, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_TMA, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_ALIAS, 1) &&
           verifier.EndTable();
  }
};

struct KernelExecutorBuilder {
  typedef KernelExecutor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_smem_limit(int64_t device_smem_limit) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_DEVICE_SMEM_LIMIT, device_smem_limit, 0);
  }
  void add_block_size_high_watermark(int64_t block_size_high_watermark) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_BLOCK_SIZE_HIGH_WATERMARK, block_size_high_watermark, 0);
  }
  void add_maxrregcount_high_watermark(int64_t maxrregcount_high_watermark) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_MAXRREGCOUNT_HIGH_WATERMARK, maxrregcount_high_watermark, 0);
  }
  void add_warp_size(int64_t warp_size) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_WARP_SIZE, warp_size, 0);
  }
  void add_heuristic(int64_t heuristic) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_HEURISTIC, heuristic, 0);
  }
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_FUSION_ID, fusion_id, 0);
  }
  void add_concrete_id(int64_t concrete_id) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_CONCRETE_ID, concrete_id, 0);
  }
  void add_runtime_id(int64_t runtime_id) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_RUNTIME_ID, runtime_id, 0);
  }
  void add_group_id(int64_t group_id) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_GROUP_ID, group_id, 0);
  }
  void add_kernel_code(::flatbuffers::Offset<::flatbuffers::String> kernel_code) {
    fbb_.AddOffset(KernelExecutor::VT_KERNEL_CODE, kernel_code);
  }
  void add_executor_entry_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys) {
    fbb_.AddOffset(KernelExecutor::VT_EXECUTOR_ENTRY_LOOKUP_KEYS, executor_entry_lookup_keys);
  }
  void add_executor_entry_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutorEntry>>> executor_entry_lookup_values) {
    fbb_.AddOffset(KernelExecutor::VT_EXECUTOR_ENTRY_LOOKUP_VALUES, executor_entry_lookup_values);
  }
  void add_index_type(int64_t index_type) {
    fbb_.AddElement<int64_t>(KernelExecutor::VT_INDEX_TYPE, index_type, 0);
  }
  void add_compiled_kernel(::flatbuffers::Offset<nvfuser::serde::CudaKernel> compiled_kernel) {
    fbb_.AddOffset(KernelExecutor::VT_COMPILED_KERNEL, compiled_kernel);
  }
  void add_has_rng(bool has_rng) {
    fbb_.AddElement<uint8_t>(KernelExecutor::VT_HAS_RNG, static_cast<uint8_t>(has_rng), 0);
  }
  void add_has_tma(bool has_tma) {
    fbb_.AddElement<uint8_t>(KernelExecutor::VT_HAS_TMA, static_cast<uint8_t>(has_tma), 0);
  }
  void add_has_dynamic_alias(bool has_dynamic_alias) {
    fbb_.AddElement<uint8_t>(KernelExecutor::VT_HAS_DYNAMIC_ALIAS, static_cast<uint8_t>(has_dynamic_alias), 0);
  }
  explicit KernelExecutorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelExecutor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelExecutor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelExecutor> CreateKernelExecutor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_watermark = 0,
    int64_t maxrregcount_high_watermark = 0,
    int64_t warp_size = 0,
    int64_t heuristic = 0,
    int64_t fusion_id = 0,
    int64_t concrete_id = 0,
    int64_t runtime_id = 0,
    int64_t group_id = 0,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_code = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutorEntry>>> executor_entry_lookup_values = 0,
    int64_t index_type = 0,
    ::flatbuffers::Offset<nvfuser::serde::CudaKernel> compiled_kernel = 0,
    bool has_rng = false,
    bool has_tma = false,
    bool has_dynamic_alias = false) {
  KernelExecutorBuilder builder_(_fbb);
  builder_.add_index_type(index_type);
  builder_.add_group_id(group_id);
  builder_.add_runtime_id(runtime_id);
  builder_.add_concrete_id(concrete_id);
  builder_.add_fusion_id(fusion_id);
  builder_.add_heuristic(heuristic);
  builder_.add_warp_size(warp_size);
  builder_.add_maxrregcount_high_watermark(maxrregcount_high_watermark);
  builder_.add_block_size_high_watermark(block_size_high_watermark);
  builder_.add_device_smem_limit(device_smem_limit);
  builder_.add_compiled_kernel(compiled_kernel);
  builder_.add_executor_entry_lookup_values(executor_entry_lookup_values);
  builder_.add_executor_entry_lookup_keys(executor_entry_lookup_keys);
  builder_.add_kernel_code(kernel_code);
  builder_.add_has_dynamic_alias(has_dynamic_alias);
  builder_.add_has_tma(has_tma);
  builder_.add_has_rng(has_rng);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelExecutor> CreateKernelExecutorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_watermark = 0,
    int64_t maxrregcount_high_watermark = 0,
    int64_t warp_size = 0,
    int64_t heuristic = 0,
    int64_t fusion_id = 0,
    int64_t concrete_id = 0,
    int64_t runtime_id = 0,
    int64_t group_id = 0,
    const char *kernel_code = nullptr,
    const std::vector<uint64_t> *executor_entry_lookup_keys = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutorEntry>> *executor_entry_lookup_values = nullptr,
    int64_t index_type = 0,
    ::flatbuffers::Offset<nvfuser::serde::CudaKernel> compiled_kernel = 0,
    bool has_rng = false,
    bool has_tma = false,
    bool has_dynamic_alias = false) {
  auto kernel_code__ = kernel_code ? _fbb.CreateString(kernel_code) : 0;
  auto executor_entry_lookup_keys__ = executor_entry_lookup_keys ? _fbb.CreateVector<uint64_t>(*executor_entry_lookup_keys) : 0;
  auto executor_entry_lookup_values__ = executor_entry_lookup_values ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelExecutorEntry>>(*executor_entry_lookup_values) : 0;
  return nvfuser::serde::CreateKernelExecutor(
      _fbb,
      device_smem_limit,
      block_size_high_watermark,
      maxrregcount_high_watermark,
      warp_size,
      heuristic,
      fusion_id,
      concrete_id,
      runtime_id,
      group_id,
      kernel_code__,
      executor_entry_lookup_keys__,
      executor_entry_lookup_values__,
      index_type,
      compiled_kernel,
      has_rng,
      has_tma,
      has_dynamic_alias);
}

struct SegmentedEdge FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SegmentedEdgeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FROM_SEGMENTED_GROUP = 4,
    VT_TO_SEGMENTED_GROUP = 6,
    VT_VAL = 8
  };
  int64_t from_segmented_group() const {
    return GetField<int64_t>(VT_FROM_SEGMENTED_GROUP, 0);
  }
  int64_t to_segmented_group() const {
    return GetField<int64_t>(VT_TO_SEGMENTED_GROUP, 0);
  }
  int64_t val() const {
    return GetField<int64_t>(VT_VAL, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_FROM_SEGMENTED_GROUP, 8) &&
           VerifyField<int64_t>(verifier, VT_TO_SEGMENTED_GROUP, 8) &&
           VerifyField<int64_t>(verifier, VT_VAL, 8) &&
           verifier.EndTable();
  }
};

struct SegmentedEdgeBuilder {
  typedef SegmentedEdge Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_from_segmented_group(int64_t from_segmented_group) {
    fbb_.AddElement<int64_t>(SegmentedEdge::VT_FROM_SEGMENTED_GROUP, from_segmented_group, 0);
  }
  void add_to_segmented_group(int64_t to_segmented_group) {
    fbb_.AddElement<int64_t>(SegmentedEdge::VT_TO_SEGMENTED_GROUP, to_segmented_group, 0);
  }
  void add_val(int64_t val) {
    fbb_.AddElement<int64_t>(SegmentedEdge::VT_VAL, val, 0);
  }
  explicit SegmentedEdgeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<SegmentedEdge> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<SegmentedEdge>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<SegmentedEdge> CreateSegmentedEdge(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t from_segmented_group = 0,
    int64_t to_segmented_group = 0,
    int64_t val = 0) {
  SegmentedEdgeBuilder builder_(_fbb);
  builder_.add_val(val);
  builder_.add_to_segmented_group(to_segmented_group);
  builder_.add_from_segmented_group(from_segmented_group);
  return builder_.Finish();
}

struct SegmentedGroup FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SegmentedGroupBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PRODUCER_EDGES = 4,
    VT_CONSUMER_EDGES = 6,
    VT_INPUT_VALS = 8,
    VT_OUTPUT_VALS = 10,
    VT_GROUP_ID = 12,
    VT_HEURISTIC = 14,
    VT_EXPRS = 16,
    VT_LEVEL = 18,
    VT_MERGE_WITH_SEGMENTED_GROUP = 20,
    VT_MERGE_THROUGH_SEGMENTED_EDGE = 22,
    VT_MERGED = 24
  };
  const ::flatbuffers::Vector<int64_t> *producer_edges() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_PRODUCER_EDGES);
  }
  const ::flatbuffers::Vector<int64_t> *consumer_edges() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_CONSUMER_EDGES);
  }
  const ::flatbuffers::Vector<int64_t> *input_vals() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_INPUT_VALS);
  }
  const ::flatbuffers::Vector<int64_t> *output_vals() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_OUTPUT_VALS);
  }
  int32_t group_id() const {
    return GetField<int32_t>(VT_GROUP_ID, 0);
  }
  int64_t heuristic() const {
    return GetField<int64_t>(VT_HEURISTIC, 0);
  }
  const ::flatbuffers::Vector<int64_t> *exprs() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_EXPRS);
  }
  int32_t level() const {
    return GetField<int32_t>(VT_LEVEL, 0);
  }
  int64_t merge_with_segmented_group() const {
    return GetField<int64_t>(VT_MERGE_WITH_SEGMENTED_GROUP, 0);
  }
  int64_t merge_through_segmented_edge() const {
    return GetField<int64_t>(VT_MERGE_THROUGH_SEGMENTED_EDGE, 0);
  }
  bool merged() const {
    return GetField<uint8_t>(VT_MERGED, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_PRODUCER_EDGES) &&
           verifier.VerifyVector(producer_edges()) &&
           VerifyOffset(verifier, VT_CONSUMER_EDGES) &&
           verifier.VerifyVector(consumer_edges()) &&
           VerifyOffset(verifier, VT_INPUT_VALS) &&
           verifier.VerifyVector(input_vals()) &&
           VerifyOffset(verifier, VT_OUTPUT_VALS) &&
           verifier.VerifyVector(output_vals()) &&
           VerifyField<int32_t>(verifier, VT_GROUP_ID, 4) &&
           VerifyField<int64_t>(verifier, VT_HEURISTIC, 8) &&
           VerifyOffset(verifier, VT_EXPRS) &&
           verifier.VerifyVector(exprs()) &&
           VerifyField<int32_t>(verifier, VT_LEVEL, 4) &&
           VerifyField<int64_t>(verifier, VT_MERGE_WITH_SEGMENTED_GROUP, 8) &&
           VerifyField<int64_t>(verifier, VT_MERGE_THROUGH_SEGMENTED_EDGE, 8) &&
           VerifyField<uint8_t>(verifier, VT_MERGED, 1) &&
           verifier.EndTable();
  }
};

struct SegmentedGroupBuilder {
  typedef SegmentedGroup Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_producer_edges(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> producer_edges) {
    fbb_.AddOffset(SegmentedGroup::VT_PRODUCER_EDGES, producer_edges);
  }
  void add_consumer_edges(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> consumer_edges) {
    fbb_.AddOffset(SegmentedGroup::VT_CONSUMER_EDGES, consumer_edges);
  }
  void add_input_vals(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> input_vals) {
    fbb_.AddOffset(SegmentedGroup::VT_INPUT_VALS, input_vals);
  }
  void add_output_vals(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> output_vals) {
    fbb_.AddOffset(SegmentedGroup::VT_OUTPUT_VALS, output_vals);
  }
  void add_group_id(int32_t group_id) {
    fbb_.AddElement<int32_t>(SegmentedGroup::VT_GROUP_ID, group_id, 0);
  }
  void add_heuristic(int64_t heuristic) {
    fbb_.AddElement<int64_t>(SegmentedGroup::VT_HEURISTIC, heuristic, 0);
  }
  void add_exprs(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> exprs) {
    fbb_.AddOffset(SegmentedGroup::VT_EXPRS, exprs);
  }
  void add_level(int32_t level) {
    fbb_.AddElement<int32_t>(SegmentedGroup::VT_LEVEL, level, 0);
  }
  void add_merge_with_segmented_group(int64_t merge_with_segmented_group) {
    fbb_.AddElement<int64_t>(SegmentedGroup::VT_MERGE_WITH_SEGMENTED_GROUP, merge_with_segmented_group, 0);
  }
  void add_merge_through_segmented_edge(int64_t merge_through_segmented_edge) {
    fbb_.AddElement<int64_t>(SegmentedGroup::VT_MERGE_THROUGH_SEGMENTED_EDGE, merge_through_segmented_edge, 0);
  }
  void add_merged(bool merged) {
    fbb_.AddElement<uint8_t>(SegmentedGroup::VT_MERGED, static_cast<uint8_t>(merged), 0);
  }
  explicit SegmentedGroupBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<SegmentedGroup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<SegmentedGroup>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<SegmentedGroup> CreateSegmentedGroup(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> producer_edges = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> consumer_edges = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> input_vals = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> output_vals = 0,
    int32_t group_id = 0,
    int64_t heuristic = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> exprs = 0,
    int32_t level = 0,
    int64_t merge_with_segmented_group = 0,
    int64_t merge_through_segmented_edge = 0,
    bool merged = false) {
  SegmentedGroupBuilder builder_(_fbb);
  builder_.add_merge_through_segmented_edge(merge_through_segmented_edge);
  builder_.add_merge_with_segmented_group(merge_with_segmented_group);
  builder_.add_heuristic(heuristic);
  builder_.add_level(level);
  builder_.add_exprs(exprs);
  builder_.add_group_id(group_id);
  builder_.add_output_vals(output_vals);
  builder_.add_input_vals(input_vals);
  builder_.add_consumer_edges(consumer_edges);
  builder_.add_producer_edges(producer_edges);
  builder_.add_merged(merged);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<SegmentedGroup> CreateSegmentedGroupDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *producer_edges = nullptr,
    const std::vector<int64_t> *consumer_edges = nullptr,
    const std::vector<int64_t> *input_vals = nullptr,
    const std::vector<int64_t> *output_vals = nullptr,
    int32_t group_id = 0,
    int64_t heuristic = 0,
    const std::vector<int64_t> *exprs = nullptr,
    int32_t level = 0,
    int64_t merge_with_segmented_group = 0,
    int64_t merge_through_segmented_edge = 0,
    bool merged = false) {
  auto producer_edges__ = producer_edges ? _fbb.CreateVector<int64_t>(*producer_edges) : 0;
  auto consumer_edges__ = consumer_edges ? _fbb.CreateVector<int64_t>(*consumer_edges) : 0;
  auto input_vals__ = input_vals ? _fbb.CreateVector<int64_t>(*input_vals) : 0;
  auto output_vals__ = output_vals ? _fbb.CreateVector<int64_t>(*output_vals) : 0;
  auto exprs__ = exprs ? _fbb.CreateVector<int64_t>(*exprs) : 0;
  return nvfuser::serde::CreateSegmentedGroup(
      _fbb,
      producer_edges__,
      consumer_edges__,
      input_vals__,
      output_vals__,
      group_id,
      heuristic,
      exprs__,
      level,
      merge_with_segmented_group,
      merge_through_segmented_edge,
      merged);
}

struct SegmentedFusion FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SegmentedFusionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VALID = 4,
    VT_SEGMENTED_FUSION_NAME = 6,
    VT_NUM_VALS = 8,
    VT_NUM_EXPRS = 10,
    VT_EDGES = 12,
    VT_GROUPS = 14,
    VT_FORCE_FP16_TV_SET = 16,
    VT_FORCE_HALF_PRECISION_TYPE = 18
  };
  bool valid() const {
    return GetField<uint8_t>(VT_VALID, 0) != 0;
  }
  uint64_t segmented_fusion_name() const {
    return GetField<uint64_t>(VT_SEGMENTED_FUSION_NAME, 0);
  }
  uint64_t num_vals() const {
    return GetField<uint64_t>(VT_NUM_VALS, 0);
  }
  uint64_t num_exprs() const {
    return GetField<uint64_t>(VT_NUM_EXPRS, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedEdge>> *edges() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedEdge>> *>(VT_EDGES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedGroup>> *groups() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedGroup>> *>(VT_GROUPS);
  }
  const ::flatbuffers::Vector<int64_t> *force_fp16_tv_set() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_FORCE_FP16_TV_SET);
  }
  int64_t force_half_precision_type() const {
    return GetField<int64_t>(VT_FORCE_HALF_PRECISION_TYPE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_VALID, 1) &&
           VerifyField<uint64_t>(verifier, VT_SEGMENTED_FUSION_NAME, 8) &&
           VerifyField<uint64_t>(verifier, VT_NUM_VALS, 8) &&
           VerifyField<uint64_t>(verifier, VT_NUM_EXPRS, 8) &&
           VerifyOffset(verifier, VT_EDGES) &&
           verifier.VerifyVector(edges()) &&
           verifier.VerifyVectorOfTables(edges()) &&
           VerifyOffset(verifier, VT_GROUPS) &&
           verifier.VerifyVector(groups()) &&
           verifier.VerifyVectorOfTables(groups()) &&
           VerifyOffset(verifier, VT_FORCE_FP16_TV_SET) &&
           verifier.VerifyVector(force_fp16_tv_set()) &&
           VerifyField<int64_t>(verifier, VT_FORCE_HALF_PRECISION_TYPE, 8) &&
           verifier.EndTable();
  }
};

struct SegmentedFusionBuilder {
  typedef SegmentedFusion Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_valid(bool valid) {
    fbb_.AddElement<uint8_t>(SegmentedFusion::VT_VALID, static_cast<uint8_t>(valid), 0);
  }
  void add_segmented_fusion_name(uint64_t segmented_fusion_name) {
    fbb_.AddElement<uint64_t>(SegmentedFusion::VT_SEGMENTED_FUSION_NAME, segmented_fusion_name, 0);
  }
  void add_num_vals(uint64_t num_vals) {
    fbb_.AddElement<uint64_t>(SegmentedFusion::VT_NUM_VALS, num_vals, 0);
  }
  void add_num_exprs(uint64_t num_exprs) {
    fbb_.AddElement<uint64_t>(SegmentedFusion::VT_NUM_EXPRS, num_exprs, 0);
  }
  void add_edges(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedEdge>>> edges) {
    fbb_.AddOffset(SegmentedFusion::VT_EDGES, edges);
  }
  void add_groups(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedGroup>>> groups) {
    fbb_.AddOffset(SegmentedFusion::VT_GROUPS, groups);
  }
  void add_force_fp16_tv_set(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> force_fp16_tv_set) {
    fbb_.AddOffset(SegmentedFusion::VT_FORCE_FP16_TV_SET, force_fp16_tv_set);
  }
  void add_force_half_precision_type(int64_t force_half_precision_type) {
    fbb_.AddElement<int64_t>(SegmentedFusion::VT_FORCE_HALF_PRECISION_TYPE, force_half_precision_type, 0);
  }
  explicit SegmentedFusionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<SegmentedFusion> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<SegmentedFusion>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<SegmentedFusion> CreateSegmentedFusion(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool valid = false,
    uint64_t segmented_fusion_name = 0,
    uint64_t num_vals = 0,
    uint64_t num_exprs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedEdge>>> edges = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::SegmentedGroup>>> groups = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> force_fp16_tv_set = 0,
    int64_t force_half_precision_type = 0) {
  SegmentedFusionBuilder builder_(_fbb);
  builder_.add_force_half_precision_type(force_half_precision_type);
  builder_.add_num_exprs(num_exprs);
  builder_.add_num_vals(num_vals);
  builder_.add_segmented_fusion_name(segmented_fusion_name);
  builder_.add_force_fp16_tv_set(force_fp16_tv_set);
  builder_.add_groups(groups);
  builder_.add_edges(edges);
  builder_.add_valid(valid);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<SegmentedFusion> CreateSegmentedFusionDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool valid = false,
    uint64_t segmented_fusion_name = 0,
    uint64_t num_vals = 0,
    uint64_t num_exprs = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::SegmentedEdge>> *edges = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::SegmentedGroup>> *groups = nullptr,
    const std::vector<int64_t> *force_fp16_tv_set = nullptr,
    int64_t force_half_precision_type = 0) {
  auto edges__ = edges ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::SegmentedEdge>>(*edges) : 0;
  auto groups__ = groups ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::SegmentedGroup>>(*groups) : 0;
  auto force_fp16_tv_set__ = force_fp16_tv_set ? _fbb.CreateVector<int64_t>(*force_fp16_tv_set) : 0;
  return nvfuser::serde::CreateSegmentedFusion(
      _fbb,
      valid,
      segmented_fusion_name,
      num_vals,
      num_exprs,
      edges__,
      groups__,
      force_fp16_tv_set__,
      force_half_precision_type);
}

struct FusionKernelRuntime FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionKernelRuntimeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSION_ID = 4,
    VT_CONCRETE_ID = 6,
    VT_RUNTIME_ID = 8,
    VT_ARGS = 10,
    VT_EXECUTORS = 12,
    VT_SEGMENTED_FUSION = 14
  };
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  int64_t concrete_id() const {
    return GetField<int64_t>(VT_CONCRETE_ID, 0);
  }
  int64_t runtime_id() const {
    return GetField<int64_t>(VT_RUNTIME_ID, 0);
  }
  const nvfuser::serde::KernelArgumentHolder *args() const {
    return GetPointer<const nvfuser::serde::KernelArgumentHolder *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutor>> *executors() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutor>> *>(VT_EXECUTORS);
  }
  const nvfuser::serde::SegmentedFusion *segmented_fusion() const {
    return GetPointer<const nvfuser::serde::SegmentedFusion *>(VT_SEGMENTED_FUSION);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_CONCRETE_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_RUNTIME_ID, 8) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyTable(args()) &&
           VerifyOffset(verifier, VT_EXECUTORS) &&
           verifier.VerifyVector(executors()) &&
           verifier.VerifyVectorOfTables(executors()) &&
           VerifyOffset(verifier, VT_SEGMENTED_FUSION) &&
           verifier.VerifyTable(segmented_fusion()) &&
           verifier.EndTable();
  }
};

struct FusionKernelRuntimeBuilder {
  typedef FusionKernelRuntime Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(FusionKernelRuntime::VT_FUSION_ID, fusion_id, 0);
  }
  void add_concrete_id(int64_t concrete_id) {
    fbb_.AddElement<int64_t>(FusionKernelRuntime::VT_CONCRETE_ID, concrete_id, 0);
  }
  void add_runtime_id(int64_t runtime_id) {
    fbb_.AddElement<int64_t>(FusionKernelRuntime::VT_RUNTIME_ID, runtime_id, 0);
  }
  void add_args(::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args) {
    fbb_.AddOffset(FusionKernelRuntime::VT_ARGS, args);
  }
  void add_executors(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutor>>> executors) {
    fbb_.AddOffset(FusionKernelRuntime::VT_EXECUTORS, executors);
  }
  void add_segmented_fusion(::flatbuffers::Offset<nvfuser::serde::SegmentedFusion> segmented_fusion) {
    fbb_.AddOffset(FusionKernelRuntime::VT_SEGMENTED_FUSION, segmented_fusion);
  }
  explicit FusionKernelRuntimeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionKernelRuntime> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionKernelRuntime>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t fusion_id = 0,
    int64_t concrete_id = 0,
    int64_t runtime_id = 0,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutor>>> executors = 0,
    ::flatbuffers::Offset<nvfuser::serde::SegmentedFusion> segmented_fusion = 0) {
  FusionKernelRuntimeBuilder builder_(_fbb);
  builder_.add_runtime_id(runtime_id);
  builder_.add_concrete_id(concrete_id);
  builder_.add_fusion_id(fusion_id);
  builder_.add_segmented_fusion(segmented_fusion);
  builder_.add_executors(executors);
  builder_.add_args(args);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntimeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t fusion_id = 0,
    int64_t concrete_id = 0,
    int64_t runtime_id = 0,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelExecutor>> *executors = nullptr,
    ::flatbuffers::Offset<nvfuser::serde::SegmentedFusion> segmented_fusion = 0) {
  auto executors__ = executors ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelExecutor>>(*executors) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      fusion_id,
      concrete_id,
      runtime_id,
      args,
      executors__,
      segmented_fusion);
}

struct InputsIdLookup FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InputsIdLookupBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_CACHE_SIZE = 4,
    VT_CURRENT_ID = 6,
    VT_LRU_CACHE = 8,
    VT_ENCODING_LOOKUP_KEYS = 10,
    VT_ENCODING_LOOKUP_VALUES = 12
  };
  uint64_t max_cache_size() const {
    return GetField<uint64_t>(VT_MAX_CACHE_SIZE, 0);
  }
  uint64_t current_id() const {
    return GetField<uint64_t>(VT_CURRENT_ID, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_LRU_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_ENCODING_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *encoding_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *>(VT_ENCODING_LOOKUP_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_CACHE_SIZE, 8) &&
           VerifyField<uint64_t>(verifier, VT_CURRENT_ID, 8) &&
           VerifyOffset(verifier, VT_LRU_CACHE) &&
           verifier.VerifyVector(lru_cache()) &&
           verifier.VerifyVectorOfStrings(lru_cache()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_KEYS) &&
           verifier.VerifyVector(encoding_lookup_keys()) &&
           verifier.VerifyVectorOfStrings(encoding_lookup_keys()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_VALUES) &&
           verifier.VerifyVector(encoding_lookup_values()) &&
           verifier.EndTable();
  }
};

struct InputsIdLookupBuilder {
  typedef InputsIdLookup Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_cache_size(uint64_t max_cache_size) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_MAX_CACHE_SIZE, max_cache_size, 0);
  }
  void add_current_id(uint64_t current_id) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_CURRENT_ID, current_id, 0);
  }
  void add_lru_cache(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache) {
    fbb_.AddOffset(InputsIdLookup::VT_LRU_CACHE, lru_cache);
  }
  void add_encoding_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_KEYS, encoding_lookup_keys);
  }
  void add_encoding_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_VALUES, encoding_lookup_values);
  }
  explicit InputsIdLookupBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<InputsIdLookup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<InputsIdLookup>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values = 0) {
  InputsIdLookupBuilder builder_(_fbb);
  builder_.add_current_id(current_id);
  builder_.add_max_cache_size(max_cache_size);
  builder_.add_encoding_lookup_values(encoding_lookup_values);
  builder_.add_encoding_lookup_keys(encoding_lookup_keys);
  builder_.add_lru_cache(lru_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookupDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache = nullptr,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys = nullptr,
    const std::vector<nvfuser::serde::EncodingEntry> *encoding_lookup_values = nullptr) {
  auto lru_cache__ = lru_cache ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*lru_cache) : 0;
  auto encoding_lookup_keys__ = encoding_lookup_keys ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*encoding_lookup_keys) : 0;
  auto encoding_lookup_values__ = encoding_lookup_values ? _fbb.CreateVectorOfStructs<nvfuser::serde::EncodingEntry>(*encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      max_cache_size,
      current_id,
      lru_cache__,
      encoding_lookup_keys__,
      encoding_lookup_values__);
}

struct KernelRuntimeState FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelRuntimeStateBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_ID = 4,
    VT_CONCRETE_ID = 6,
    VT_HAS_DYNAMIC_TRANSFORM_INFO = 8,
    VT_RUNTIMES = 10
  };
  int64_t device_id() const {
    return GetField<int64_t>(VT_DEVICE_ID, 0);
  }
  int64_t concrete_id() const {
    return GetField<int64_t>(VT_CONCRETE_ID, 0);
  }
  bool has_dynamic_transform_info() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_TRANSFORM_INFO, 0) != 0;
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *>(VT_RUNTIMES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_CONCRETE_ID, 8) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_TRANSFORM_INFO, 1) &&
           VerifyOffset(verifier, VT_RUNTIMES) &&
           verifier.VerifyVector(runtimes()) &&
           verifier.VerifyVectorOfTables(runtimes()) &&
           verifier.EndTable();
  }
};

struct KernelRuntimeStateBuilder {
  typedef KernelRuntimeState Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_id(int64_t device_id) {
    fbb_.AddElement<int64_t>(KernelRuntimeState::VT_DEVICE_ID, device_id, 0);
  }
  void add_concrete_id(int64_t concrete_id) {
    fbb_.AddElement<int64_t>(KernelRuntimeState::VT_CONCRETE_ID, concrete_id, 0);
  }
  void add_has_dynamic_transform_info(bool has_dynamic_transform_info) {
    fbb_.AddElement<uint8_t>(KernelRuntimeState::VT_HAS_DYNAMIC_TRANSFORM_INFO, static_cast<uint8_t>(has_dynamic_transform_info), 0);
  }
  void add_runtimes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes) {
    fbb_.AddOffset(KernelRuntimeState::VT_RUNTIMES, runtimes);
  }
  explicit KernelRuntimeStateBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelRuntimeState> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelRuntimeState>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_id = 0,
    int64_t concrete_id = 0,
    bool has_dynamic_transform_info = false,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes = 0) {
  KernelRuntimeStateBuilder builder_(_fbb);
  builder_.add_concrete_id(concrete_id);
  builder_.add_device_id(device_id);
  builder_.add_runtimes(runtimes);
  builder_.add_has_dynamic_transform_info(has_dynamic_transform_info);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeStateDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_id = 0,
    int64_t concrete_id = 0,
    bool has_dynamic_transform_info = false,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes = nullptr) {
  auto runtimes__ = runtimes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>(*runtimes) : 0;
  return nvfuser::serde::CreateKernelRuntimeState(
      _fbb,
      device_id,
      concrete_id,
      has_dynamic_transform_info,
      runtimes__);
}

struct FusionExecutorCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_FUSION_ID = 4,
    VT_INPUTS_CACHE = 6,
    VT_KERNEL_RUNTIMES_MAP = 8,
    VT_KERNEL_CACHE_KEYS = 10,
    VT_KERNEL_CACHE_VALUES = 12
  };
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  const nvfuser::serde::InputsIdLookup *inputs_cache() const {
    return GetPointer<const nvfuser::serde::InputsIdLookup *>(VT_INPUTS_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *>(VT_KERNEL_RUNTIMES_MAP);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_KEYS);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_values() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyOffset(verifier, VT_INPUTS_CACHE) &&
           verifier.VerifyTable(inputs_cache()) &&
           VerifyOffset(verifier, VT_KERNEL_RUNTIMES_MAP) &&
           verifier.VerifyVector(kernel_runtimes_map()) &&
           verifier.VerifyVectorOfTables(kernel_runtimes_map()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_KEYS) &&
           verifier.VerifyVector(kernel_cache_keys()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_VALUES) &&
           verifier.VerifyVector(kernel_cache_values()) &&
           verifier.EndTable();
  }
};

struct FusionExecutorCacheBuilder {
  typedef FusionExecutorCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(FusionExecutorCache::VT_FUSION_ID, fusion_id, 0);
  }
  void add_inputs_cache(::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache) {
    fbb_.AddOffset(FusionExecutorCache::VT_INPUTS_CACHE, inputs_cache);
  }
  void add_kernel_runtimes_map(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_RUNTIMES_MAP, kernel_runtimes_map);
  }
  void add_kernel_cache_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_KEYS, kernel_cache_keys);
  }
  void add_kernel_cache_values(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_VALUES, kernel_cache_values);
  }
  explicit FusionExecutorCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutorCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutorCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t fusion_id = 0,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values = 0) {
  FusionExecutorCacheBuilder builder_(_fbb);
  builder_.add_fusion_id(fusion_id);
  builder_.add_kernel_cache_values(kernel_cache_values);
  builder_.add_kernel_cache_keys(kernel_cache_keys);
  builder_.add_kernel_runtimes_map(kernel_runtimes_map);
  builder_.add_inputs_cache(inputs_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t fusion_id = 0,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map = nullptr,
    const std::vector<uint64_t> *kernel_cache_keys = nullptr,
    const std::vector<uint64_t> *kernel_cache_values = nullptr) {
  auto kernel_runtimes_map__ = kernel_runtimes_map ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>(*kernel_runtimes_map) : 0;
  auto kernel_cache_keys__ = kernel_cache_keys ? _fbb.CreateVector<uint64_t>(*kernel_cache_keys) : 0;
  auto kernel_cache_values__ = kernel_cache_values ? _fbb.CreateVector<uint64_t>(*kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      fusion_id,
      inputs_cache,
      kernel_runtimes_map__,
      kernel_cache_keys__,
      kernel_cache_values__);
}

struct RecordFunctor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RecordFunctorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_OUTPUTS = 6,
    VT_NAME = 8,
    VT_TYPE = 10,
    VT_DATA_TYPE = 12,
    VT_DATA = 14
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *args() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  nvfuser::serde::RecordType type() const {
    return static_cast<nvfuser::serde::RecordType>(GetField<int32_t>(VT_TYPE, 0));
  }
  nvfuser::serde::RecordData data_type() const {
    return static_cast<nvfuser::serde::RecordData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::At *data_as_At() const {
    return data_type() == nvfuser::serde::RecordData::At ? static_cast<const nvfuser::serde::At *>(data()) : nullptr;
  }
  const nvfuser::serde::BatchNorm *data_as_BatchNorm() const {
    return data_type() == nvfuser::serde::RecordData::BatchNorm ? static_cast<const nvfuser::serde::BatchNorm *>(data()) : nullptr;
  }
  const nvfuser::serde::Broadcast *data_as_Broadcast() const {
    return data_type() == nvfuser::serde::RecordData::Broadcast ? static_cast<const nvfuser::serde::Broadcast *>(data()) : nullptr;
  }
  const nvfuser::serde::BroadcastInDim *data_as_BroadcastInDim() const {
    return data_type() == nvfuser::serde::RecordData::BroadcastInDim ? static_cast<const nvfuser::serde::BroadcastInDim *>(data()) : nullptr;
  }
  const nvfuser::serde::Cat *data_as_Cat() const {
    return data_type() == nvfuser::serde::RecordData::Cat ? static_cast<const nvfuser::serde::Cat *>(data()) : nullptr;
  }
  const nvfuser::serde::Dimension *data_as_Dimension() const {
    return data_type() == nvfuser::serde::RecordData::Dimension ? static_cast<const nvfuser::serde::Dimension *>(data()) : nullptr;
  }
  const nvfuser::serde::Dtype *data_as_Dtype() const {
    return data_type() == nvfuser::serde::RecordData::Dtype ? static_cast<const nvfuser::serde::Dtype *>(data()) : nullptr;
  }
  const nvfuser::serde::Norm *data_as_Norm() const {
    return data_type() == nvfuser::serde::RecordData::Norm ? static_cast<const nvfuser::serde::Norm *>(data()) : nullptr;
  }
  const nvfuser::serde::Output *data_as_Output() const {
    return data_type() == nvfuser::serde::RecordData::Output ? static_cast<const nvfuser::serde::Output *>(data()) : nullptr;
  }
  const nvfuser::serde::Dims *data_as_Dims() const {
    return data_type() == nvfuser::serde::RecordData::Dims ? static_cast<const nvfuser::serde::Dims *>(data()) : nullptr;
  }
  const nvfuser::serde::Slice *data_as_Slice() const {
    return data_type() == nvfuser::serde::RecordData::Slice ? static_cast<const nvfuser::serde::Slice *>(data()) : nullptr;
  }
  const nvfuser::serde::Squeeze *data_as_Squeeze() const {
    return data_type() == nvfuser::serde::RecordData::Squeeze ? static_cast<const nvfuser::serde::Squeeze *>(data()) : nullptr;
  }
  const nvfuser::serde::Reduction *data_as_Reduction() const {
    return data_type() == nvfuser::serde::RecordData::Reduction ? static_cast<const nvfuser::serde::Reduction *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::RecordData::Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Size *data_as_Size() const {
    return data_type() == nvfuser::serde::RecordData::Size ? static_cast<const nvfuser::serde::Size *>(data()) : nullptr;
  }
  const nvfuser::serde::Tensor *data_as_Tensor() const {
    return data_type() == nvfuser::serde::RecordData::Tensor ? static_cast<const nvfuser::serde::Tensor *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolic *data_as_TensorCreationSymbolic() const {
    return data_type() == nvfuser::serde::RecordData::TensorCreationSymbolic ? static_cast<const nvfuser::serde::TensorCreationSymbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Vector *data_as_Vector() const {
    return data_type() == nvfuser::serde::RecordData::Vector ? static_cast<const nvfuser::serde::Vector *>(data()) : nullptr;
  }
  const nvfuser::serde::Welford *data_as_Welford() const {
    return data_type() == nvfuser::serde::RecordData::Welford ? static_cast<const nvfuser::serde::Welford *>(data()) : nullptr;
  }
  const nvfuser::serde::Sort *data_as_Sort() const {
    return data_type() == nvfuser::serde::RecordData::Sort ? static_cast<const nvfuser::serde::Sort *>(data()) : nullptr;
  }
  const nvfuser::serde::TopK *data_as_TopK() const {
    return data_type() == nvfuser::serde::RecordData::TopK ? static_cast<const nvfuser::serde::TopK *>(data()) : nullptr;
  }
  const nvfuser::serde::ScaledOp *data_as_ScaledOp() const {
    return data_type() == nvfuser::serde::RecordData::ScaledOp ? static_cast<const nvfuser::serde::ScaledOp *>(data()) : nullptr;
  }
  const nvfuser::serde::ScanOp *data_as_ScanOp() const {
    return data_type() == nvfuser::serde::RecordData::ScanOp ? static_cast<const nvfuser::serde::ScanOp *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyVector(args()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyRecordData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::At *RecordFunctor::data_as<nvfuser::serde::At>() const {
  return data_as_At();
}

template<> inline const nvfuser::serde::BatchNorm *RecordFunctor::data_as<nvfuser::serde::BatchNorm>() const {
  return data_as_BatchNorm();
}

template<> inline const nvfuser::serde::Broadcast *RecordFunctor::data_as<nvfuser::serde::Broadcast>() const {
  return data_as_Broadcast();
}

template<> inline const nvfuser::serde::BroadcastInDim *RecordFunctor::data_as<nvfuser::serde::BroadcastInDim>() const {
  return data_as_BroadcastInDim();
}

template<> inline const nvfuser::serde::Cat *RecordFunctor::data_as<nvfuser::serde::Cat>() const {
  return data_as_Cat();
}

template<> inline const nvfuser::serde::Dimension *RecordFunctor::data_as<nvfuser::serde::Dimension>() const {
  return data_as_Dimension();
}

template<> inline const nvfuser::serde::Dtype *RecordFunctor::data_as<nvfuser::serde::Dtype>() const {
  return data_as_Dtype();
}

template<> inline const nvfuser::serde::Norm *RecordFunctor::data_as<nvfuser::serde::Norm>() const {
  return data_as_Norm();
}

template<> inline const nvfuser::serde::Output *RecordFunctor::data_as<nvfuser::serde::Output>() const {
  return data_as_Output();
}

template<> inline const nvfuser::serde::Dims *RecordFunctor::data_as<nvfuser::serde::Dims>() const {
  return data_as_Dims();
}

template<> inline const nvfuser::serde::Slice *RecordFunctor::data_as<nvfuser::serde::Slice>() const {
  return data_as_Slice();
}

template<> inline const nvfuser::serde::Squeeze *RecordFunctor::data_as<nvfuser::serde::Squeeze>() const {
  return data_as_Squeeze();
}

template<> inline const nvfuser::serde::Reduction *RecordFunctor::data_as<nvfuser::serde::Reduction>() const {
  return data_as_Reduction();
}

template<> inline const nvfuser::serde::Scalar *RecordFunctor::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Size *RecordFunctor::data_as<nvfuser::serde::Size>() const {
  return data_as_Size();
}

template<> inline const nvfuser::serde::Tensor *RecordFunctor::data_as<nvfuser::serde::Tensor>() const {
  return data_as_Tensor();
}

template<> inline const nvfuser::serde::TensorCreationSymbolic *RecordFunctor::data_as<nvfuser::serde::TensorCreationSymbolic>() const {
  return data_as_TensorCreationSymbolic();
}

template<> inline const nvfuser::serde::Vector *RecordFunctor::data_as<nvfuser::serde::Vector>() const {
  return data_as_Vector();
}

template<> inline const nvfuser::serde::Welford *RecordFunctor::data_as<nvfuser::serde::Welford>() const {
  return data_as_Welford();
}

template<> inline const nvfuser::serde::Sort *RecordFunctor::data_as<nvfuser::serde::Sort>() const {
  return data_as_Sort();
}

template<> inline const nvfuser::serde::TopK *RecordFunctor::data_as<nvfuser::serde::TopK>() const {
  return data_as_TopK();
}

template<> inline const nvfuser::serde::ScaledOp *RecordFunctor::data_as<nvfuser::serde::ScaledOp>() const {
  return data_as_ScaledOp();
}

template<> inline const nvfuser::serde::ScanOp *RecordFunctor::data_as<nvfuser::serde::ScanOp>() const {
  return data_as_ScanOp();
}

struct RecordFunctorBuilder {
  typedef RecordFunctor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args) {
    fbb_.AddOffset(RecordFunctor::VT_ARGS, args);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs) {
    fbb_.AddOffset(RecordFunctor::VT_OUTPUTS, outputs);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(RecordFunctor::VT_NAME, name);
  }
  void add_type(nvfuser::serde::RecordType type) {
    fbb_.AddElement<int32_t>(RecordFunctor::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_data_type(nvfuser::serde::RecordData data_type) {
    fbb_.AddElement<uint8_t>(RecordFunctor::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(RecordFunctor::VT_DATA, data);
  }
  explicit RecordFunctorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<RecordFunctor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<RecordFunctor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType::Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData::NONE,
    ::flatbuffers::Offset<void> data = 0) {
  RecordFunctorBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_type(type);
  builder_.add_name(name);
  builder_.add_outputs(outputs);
  builder_.add_args(args);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *args = nullptr,
    const std::vector<nvfuser::serde::State> *outputs = nullptr,
    const char *name = nullptr,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType::Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData::NONE,
    ::flatbuffers::Offset<void> data = 0) {
  auto args__ = args ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*args) : 0;
  auto outputs__ = outputs ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*outputs) : 0;
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      args__,
      outputs__,
      name__,
      type,
      data_type,
      data);
}

struct TrieNode FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TrieNodeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RECORD = 4,
    VT_CHILDREN = 6,
    VT_FUSION_ID = 8,
    VT_VISITS = 10,
    VT_IS_TERMINAL = 12
  };
  const nvfuser::serde::RecordFunctor *record() const {
    return GetPointer<const nvfuser::serde::RecordFunctor *>(VT_RECORD);
  }
  const ::flatbuffers::Vector<uint64_t> *children() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_CHILDREN);
  }
  uint64_t fusion_id() const {
    return GetField<uint64_t>(VT_FUSION_ID, 0);
  }
  uint64_t visits() const {
    return GetField<uint64_t>(VT_VISITS, 0);
  }
  bool is_terminal() const {
    return GetField<uint8_t>(VT_IS_TERMINAL, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_RECORD) &&
           verifier.VerifyTable(record()) &&
           VerifyOffset(verifier, VT_CHILDREN) &&
           verifier.VerifyVector(children()) &&
           VerifyField<uint64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<uint64_t>(verifier, VT_VISITS, 8) &&
           VerifyField<uint8_t>(verifier, VT_IS_TERMINAL, 1) &&
           verifier.EndTable();
  }
};

struct TrieNodeBuilder {
  typedef TrieNode Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_record(::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record) {
    fbb_.AddOffset(TrieNode::VT_RECORD, record);
  }
  void add_children(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children) {
    fbb_.AddOffset(TrieNode::VT_CHILDREN, children);
  }
  void add_fusion_id(uint64_t fusion_id) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_FUSION_ID, fusion_id, 0);
  }
  void add_visits(uint64_t visits) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_VISITS, visits, 0);
  }
  void add_is_terminal(bool is_terminal) {
    fbb_.AddElement<uint8_t>(TrieNode::VT_IS_TERMINAL, static_cast<uint8_t>(is_terminal), 0);
  }
  explicit TrieNodeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TrieNode> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TrieNode>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children = 0,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  TrieNodeBuilder builder_(_fbb);
  builder_.add_visits(visits);
  builder_.add_fusion_id(fusion_id);
  builder_.add_children(children);
  builder_.add_record(record);
  builder_.add_is_terminal(is_terminal);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNodeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    const std::vector<uint64_t> *children = nullptr,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  auto children__ = children ? _fbb.CreateVector<uint64_t>(*children) : 0;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      record,
      children__,
      fusion_id,
      visits,
      is_terminal);
}

struct FusionCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_FUSIONS = 4,
    VT_STRUCTURE = 6,
    VT_TERMINAL_NODES = 8,
    VT_AUTO_GEN_SCHEDULES = 10,
    VT_GLOBAL_FUSION_COUNT = 12,
    VT_DEVICE_MAJOR = 14,
    VT_DEVICE_MINOR = 16,
    VT_CUDA_MAJOR = 18,
    VT_CUDA_MINOR = 20
  };
  uint64_t max_fusions() const {
    return GetField<uint64_t>(VT_MAX_FUSIONS, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *>(VT_STRUCTURE);
  }
  const ::flatbuffers::Vector<uint64_t> *terminal_nodes() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_TERMINAL_NODES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *>(VT_AUTO_GEN_SCHEDULES);
  }
  int64_t global_fusion_count() const {
    return GetField<int64_t>(VT_GLOBAL_FUSION_COUNT, 0);
  }
  int64_t device_major() const {
    return GetField<int64_t>(VT_DEVICE_MAJOR, 0);
  }
  int64_t device_minor() const {
    return GetField<int64_t>(VT_DEVICE_MINOR, 0);
  }
  int64_t cuda_major() const {
    return GetField<int64_t>(VT_CUDA_MAJOR, 0);
  }
  int64_t cuda_minor() const {
    return GetField<int64_t>(VT_CUDA_MINOR, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_FUSIONS, 8) &&
           VerifyOffset(verifier, VT_STRUCTURE) &&
           verifier.VerifyVector(structure()) &&
           verifier.VerifyVectorOfTables(structure()) &&
           VerifyOffset(verifier, VT_TERMINAL_NODES) &&
           verifier.VerifyVector(terminal_nodes()) &&
           VerifyOffset(verifier, VT_AUTO_GEN_SCHEDULES) &&
           verifier.VerifyVector(auto_gen_schedules()) &&
           verifier.VerifyVectorOfTables(auto_gen_schedules()) &&
           VerifyField<int64_t>(verifier, VT_GLOBAL_FUSION_COUNT, 8) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_MAJOR, 8) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_MINOR, 8) &&
           VerifyField<int64_t>(verifier, VT_CUDA_MAJOR, 8) &&
           VerifyField<int64_t>(verifier, VT_CUDA_MINOR, 8) &&
           verifier.EndTable();
  }
};

struct FusionCacheBuilder {
  typedef FusionCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_fusions(uint64_t max_fusions) {
    fbb_.AddElement<uint64_t>(FusionCache::VT_MAX_FUSIONS, max_fusions, 0);
  }
  void add_structure(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure) {
    fbb_.AddOffset(FusionCache::VT_STRUCTURE, structure);
  }
  void add_terminal_nodes(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes) {
    fbb_.AddOffset(FusionCache::VT_TERMINAL_NODES, terminal_nodes);
  }
  void add_auto_gen_schedules(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules) {
    fbb_.AddOffset(FusionCache::VT_AUTO_GEN_SCHEDULES, auto_gen_schedules);
  }
  void add_global_fusion_count(int64_t global_fusion_count) {
    fbb_.AddElement<int64_t>(FusionCache::VT_GLOBAL_FUSION_COUNT, global_fusion_count, 0);
  }
  void add_device_major(int64_t device_major) {
    fbb_.AddElement<int64_t>(FusionCache::VT_DEVICE_MAJOR, device_major, 0);
  }
  void add_device_minor(int64_t device_minor) {
    fbb_.AddElement<int64_t>(FusionCache::VT_DEVICE_MINOR, device_minor, 0);
  }
  void add_cuda_major(int64_t cuda_major) {
    fbb_.AddElement<int64_t>(FusionCache::VT_CUDA_MAJOR, cuda_major, 0);
  }
  void add_cuda_minor(int64_t cuda_minor) {
    fbb_.AddElement<int64_t>(FusionCache::VT_CUDA_MINOR, cuda_minor, 0);
  }
  explicit FusionCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules = 0,
    int64_t global_fusion_count = 0,
    int64_t device_major = 0,
    int64_t device_minor = 0,
    int64_t cuda_major = 0,
    int64_t cuda_minor = 0) {
  FusionCacheBuilder builder_(_fbb);
  builder_.add_cuda_minor(cuda_minor);
  builder_.add_cuda_major(cuda_major);
  builder_.add_device_minor(device_minor);
  builder_.add_device_major(device_major);
  builder_.add_global_fusion_count(global_fusion_count);
  builder_.add_max_fusions(max_fusions);
  builder_.add_auto_gen_schedules(auto_gen_schedules);
  builder_.add_terminal_nodes(terminal_nodes);
  builder_.add_structure(structure);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure = nullptr,
    const std::vector<uint64_t> *terminal_nodes = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules = nullptr,
    int64_t global_fusion_count = 0,
    int64_t device_major = 0,
    int64_t device_minor = 0,
    int64_t cuda_major = 0,
    int64_t cuda_minor = 0) {
  auto structure__ = structure ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>(*structure) : 0;
  auto terminal_nodes__ = terminal_nodes ? _fbb.CreateVector<uint64_t>(*terminal_nodes) : 0;
  auto auto_gen_schedules__ = auto_gen_schedules ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>(*auto_gen_schedules) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      max_fusions,
      structure__,
      terminal_nodes__,
      auto_gen_schedules__,
      global_fusion_count,
      device_major,
      device_minor,
      cuda_major,
      cuda_minor);
}

inline bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type) {
  switch (type) {
    case RecordData::NONE: {
      return true;
    }
    case RecordData::At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Cat: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Cat *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Dims: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dims *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Welford: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Welford *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::Sort: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Sort *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::TopK: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TopK *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::ScaledOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScaledOp *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData::ScanOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScanOp *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<RecordData> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyRecordData(
        verifier,  values->Get(i), types->GetEnum<RecordData>(i))) {
      return false;
    }
  }
  return true;
}

inline bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type) {
  switch (type) {
    case PolymorphicValueData::NONE: {
      return true;
    }
    case PolymorphicValueData::Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData::ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData::TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<PolymorphicValueData> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyPolymorphicValueData(
        verifier,  values->Get(i), types->GetEnum<PolymorphicValueData>(i))) {
      return false;
    }
  }
  return true;
}

inline const nvfuser::serde::FusionCache *GetFusionCache(const void *buf) {
  return ::flatbuffers::GetRoot<nvfuser::serde::FusionCache>(buf);
}

inline const nvfuser::serde::FusionCache *GetSizePrefixedFusionCache(const void *buf) {
  return ::flatbuffers::GetSizePrefixedRoot<nvfuser::serde::FusionCache>(buf);
}

inline const char *FusionCacheIdentifier() {
  return "NV01";
}

inline bool FusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier());
}

inline bool SizePrefixedFusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier(), true);
}

inline bool VerifyFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline bool VerifySizePrefixedFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline void FinishFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.Finish(root, FusionCacheIdentifier());
}

inline void FinishSizePrefixedFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.FinishSizePrefixed(root, FusionCacheIdentifier());
}

}  // namespace serde
}  // namespace nvfuser

#endif  // FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
