Metadata-Version: 2.1
Name: langchain-baseten
Version: 0.1.7
Summary: An integration package connecting Baseten and LangChain
License: MIT
Project-URL: Source Code, https://github.com/basetenlabs/langchain-baseten/tree/master/libs/baseten
Project-URL: Release Notes, https://github.com/langchain-ai/langchain/releases?q=tag%3A%22langchain-baseten%3D%3D0%22&expanded=true
Project-URL: repository, https://github.com/langchain-ai/langchain
Requires-Python: <4.0.0,>=3.9.0
Requires-Dist: langchain-core<1.0.0,>=0.3.55
Requires-Dist: openai<2.0.0,>=1.0.0
Requires-Dist: baseten-performance-client>=0.0.11
Description-Content-Type: text/markdown

# langchain-baseten

This package contains the LangChain integration with Baseten.

## Installation

```bash
pip install langchain-baseten
```

The embeddings functionality uses Baseten's Performance Client for optimized performance, which is automatically included as a dependency.

## Chat Models

`ChatBaseten` class exposes chat models from Baseten.

```python
from langchain_baseten import ChatBaseten

# Option 1: Use Model APIs with model slug
chat = ChatBaseten(
    model="deepseek-ai/DeepSeek-V3-0324",  # Choose from available model slugs
    api_key="your-api-key",  # Or set BASETEN_API_KEY env var
)

# Option 2: Use dedicated model URL for deployed models
chat = ChatBaseten(
    model_url="https://model-<id>.api.baseten.co/environments/production/predict",
    api_key="your-api-key",

 
)

# Use the chat model
response = chat.invoke("Hello, how are you?")
print(response.content)
```

## Embeddings

`BasetenEmbeddings` class exposes embedding models from Baseten.

```python
from langchain_baseten import BasetenEmbeddings

# Initialize the embeddings model
embeddings = BasetenEmbeddings(
    model_url="https://model-<id>.api.baseten.co/environments/production/sync",  # Your model URL
    api_key="your-api-key",  # Or set BASETEN_API_KEY env var
)

# Embed documents
vectors = embeddings.embed_documents(["Hello world", "How are you?"])
print(f"Generated {len(vectors)} embeddings of dimension {len(vectors[0])}")

# Embed a single query
query_vector = embeddings.embed_query("What is the meaning of life?")
print(f"Query embedding dimension: {len(query_vector)}")
```

## Configuration

You can configure the Baseten integration using environment variables:

- `BASETEN_API_KEY`: Your Baseten API key

## Deployment Options

**Chat Models:**
- **Model APIs**: Use model slugs with shared infrastructure
- **Dedicated URLs**: Use specific model deployments with dedicated resources

**Embeddings:**
- **Dedicated URLs only**: Requires specific model deployment URL for Performance Client optimization

## Supported Models

Baseten supports various models through their OpenAI-compatible API. You can use any model slug available in your Baseten account, or deploy custom models with dedicated URLs.

For more information about available models, visit the [Baseten documentation](https://docs.baseten.co/).
